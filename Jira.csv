Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project lead id,Project description,Project url,Priority,Resolution,Assignee,Assignee Id,Reporter,Reporter Id,Creator,Creator Id,Created,Updated,Last Viewed,Resolved,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Affects versions,Fix versions,Fix versions,Fix versions,Fix versions,Fix versions,Fix versions,Fix versions,Fix versions,Components,Components,Components,Components,Due date,Votes,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Watchers Id,Original estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Associated Controls),Inward issue link (Associated Controls),Inward issue link (Associated Controls),Inward issue link (Associated Controls),Inward issue link (Associated Controls),Inward issue link (Associated Controls),Inward issue link (Associated Controls),Inward issue link (Associated Controls),Outward issue link (Associated Controls),Outward issue link (Associated Controls),Inward issue link (Blocks),Outward issue link (Blocks),Inward issue link (Cloners),Inward issue link (Cloners),Inward issue link (Cloners),Inward issue link (Cloners),Inward issue link (Cloners),Inward issue link (Cloners),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Dependency),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Fixed),Outward issue link (Known Issue),Outward issue link (Problem Statement/Feature Links),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Related),Outward issue link (Related),Outward issue link (Related),Inward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Inward issue link (Test),Inward issue link (Work item split),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field ( Zurich),Custom field (# of SVCs requested),Custom field (% of PM time),Custom field (AC Review Start),Custom field (AI Assistant Internal Customer SLO breach (mins)),Custom field (AI Assistant SLA breach (mins)),Custom field (APM ICSLO breach (mins)),Custom field (APM Internal Customer SLO breach (mins)),Custom field (APM SLA breach (mins)),Custom field (APPCERT Ticket Number),Custom field (ARR),Custom field (ARR (numeric)),Custom field (ARR (txt formart)),Custom field (AWS Account Number),Custom field (Able to fix Issue),Custom field (Above the line?),Custom field (Acceptance Criteria),Custom field (Acceptance Criteria Met (Y/N)),Custom field (Accessibility Category),Custom field (Accessibility User Impact),Custom field (Account),Custom field (Account Approver),Custom field (Account Details),Custom field (Account Name),Custom field (Account Number),Custom field (Action),Custom field (Action Count),Custom field (Action Item),Custom field (Action Item Theme),Custom field (Action Requested),Custom field (Action Type),Custom field (Action item - Short),Custom field (Activity/Timeline),Custom field (Actual Behavior),Custom field (Actual Cost(bps)),Custom field (Actual Spend),Custom field (Actual Time (days)),Custom field (Actual Time Spent),Custom field (Actual user base (BMC Internal)),Custom field (Additional Correlation Searches),Custom field (Additional Correlation Searches_Archived),Custom field (Additional Data Models),Custom field (Additional Data Models_Archived),Custom field (Additional EPAT Engineer),Custom field (Additional Volume (GB)),Custom field (Affected ES version),Custom field (Affected Systems),Custom field (Affected hardware),Custom field (Affected services),Custom field (Affects Customer-facing Web Application),Custom field (Affects Platform),Custom field (Affects docs),Custom field (Aha End Date),Custom field (Aha Epic Rank),Custom field (Aha Start Date),Custom field (Aha! URL),Custom field (Alert Level),Custom field (Alert URL),Custom field (Algosec Ticket ID),Custom field (Alternate Ship-To contact),Custom field (Any other information we need to know when reviewing ?),Custom field (App Count),Custom field (App ID),Custom field (App Version),Custom field (App Version (Build)),Custom field (App encrypted or utilizes encryption technology?),Custom field (AppInspect API request_id),Custom field (Applicable?),Custom field (Application),Custom field (Approach Summary),Custom field (Approvals),Custom field (Approved CCAB),Custom field (Approver),Custom field (Approximate number of Splunkers impacted by this change?),Custom field (Architecture Review Needed),Custom field (Architecture Review Status),Custom field (Are there any PRDs or planning documents that outline the project's scope and contacts?),Custom field (Area),Custom field (Artifact Version),Custom field (Artifact(s) used for investigation),Custom field (Artifact(s) used for investigation),Custom field (Artifact(s) used for investigation),Custom field (Artifactory Repository KEY),Custom field (Artifactory Repository Type),Custom field (Artifacts),Custom field (Assessment Type),Custom field (Asset),Custom field (Associated Risk),Custom field (Associated URL),Custom field (Association Count),Custom field (Associations),Custom field (Assumptions),Custom field (At Risk (Y/N)),"Custom field (At a high level, what is the customer experience before, during, and after this change?)",Custom field (Atlassian project),Custom field (Atlassian project status),Custom field (Attached To),Custom field (Attachment count),Custom field (Attorney-Client Privilege (Y/N)),Custom field (Audience),Custom field (Audience),Custom field (Audit Location),Custom field (Audit Type),Custom field (Auditor),Custom field (Authentication Method),Custom field (Author),Custom field (Automate-able),Custom field (Automated Monitoring),Custom field (Automated Node Pool Count),Custom field (Automated?),Custom field (Automation Status),Custom field (Automation in use from),Custom field (Availability Zones),Custom field (Average Daily Cost ($K/day)),Custom field (Average Daily Savings ($K/day)),Custom field (Average number of weekly alerts),Custom field (Avg Actual Cost($K/month)),Custom field (Avg Actual Savings($K/month)),Custom field (BRT Review),Custom field (BSA),Custom field (Backout plan),Custom field (Badging Supply List),Custom field (Begin Date),Custom field (Benefits),Custom field (Beta 1 Launch),Custom field (Beta 2 Launch),Custom field (Big Rock),Custom field (Bitbucket Project KEY),Custom field (Blast Radius),Custom field (Blocked),Custom field (Blocked Reason),Custom field (Boulder Room Names),Custom field (Branch Cut Date),Custom field (Branch Ready),Custom field (Breach Type),Custom field (Brussels ),Custom field (Budge Value - Boundary System),Custom field (Budget Allocated),Custom field (Budget Category- Finsys Budget),Custom field (Budget Category- Finsys Resource),Custom field (Budget Category-Boundary System),Custom field (Budget ID),Custom field (Budget Reviewed (Y/N)?),Custom field (Budget Value- Finsys Budget),Custom field (Budget Value- Finsys Resource),Custom field (Bug Category),Custom field (Bug Frequency),Custom field (Bug Severity),Custom field (Bugs of Epic count),Custom field (Bugs of Epic in Done status category count),Custom field (Bugs of Epic in In Progress status category count),Custom field (Bugs of Epic in To Do status category count),Custom field (Build),Custom field (Building Support is needed),Custom field (Bulk PII),Custom field (Business Approval),Custom field (Business Area),Custom field (Business Case),Custom field (Business Category),Custom field (Business Context),Custom field (Business Entity),Custom field (Business Justification),Custom field (Business Justification (migrated)),Custom field (Business Justification for the Risk Acceptance),Custom field (Business Justification-),Custom field (Business Owner),Custom field (Business Owner),Custom field (Business Priority Number),Custom field (Business Problem/Requirement),Custom field (Business Problem/Requirement_archived),Custom field (Business Requestor),Custom field (Business System Analyst),Custom field (Business Value),Custom field (Business Value (migrated)),Custom field (Business Value/Impact),Custom field (Business Value/Impact_archived),Custom field (Business requested?),Custom field (Business requirements),Custom field (CAB Status),Custom field (CAB Vote),Custom field (CAB vote required),Custom field (CAP OrgID),Custom field (CCAB with Go/NoGo),Custom field (CCB Vote),Custom field (CCB vote required),Custom field (CCF Control),Custom field (CCF Control Change Type),Custom field (CCVM Approver),Custom field (CDOC ticket),Custom field (CI Type),Custom field (CINC Ticket Link),Custom field (CINC number),Custom field (CMON link),Custom field (COEM-Status),Custom field (CORM),Custom field (CSDL ID),Custom field (CSO-StackRank),Custom field (CSOne Ticket Count),Custom field (CSOne Ticket Ids),Custom field (CSP),Custom field (CSP Region),Custom field (CSX Expected Use Frequency (Prioritization field)),Custom field (CSX Expected user base (Prioritization field)),Custom field (CSX Priority  (Prioritization field)),Custom field (CSX Request Type),Custom field (CSX Requirements),Custom field (CSX Teams),Custom field (CVSS Score),Custom field (CWE),Custom field (Call Required),Custom field (Can by worked by),Custom field (CapEx Original Estimate),Custom field (CapEx Spent),Custom field (Carpool Buddies),Custom field (Case Record Type),Custom field (Categories),Custom field (Category),Custom field (Category),Custom field (Category),Custom field (Category),Custom field (Category),Custom field (Category),Custom field (Category),Custom field (Category),Custom field (Category of Incident),Custom field (Cause Code),Custom field (Certification),Custom field (Challenges),Custom field (Change Class),Custom field (Change Description),Custom field (Change Start Time (Retired)),Custom field (Change completion date),Custom field (Change reason),Custom field (Change risk),Custom field (Change start date),Custom field (Change type),Custom field (Change type),Custom field (Changes Made Prior To Issues),Custom field (Changes Summary),Custom field (Changes to RBAC),Custom field (Channel Link),Custom field (Check Name),Custom field (Checklist Completed),Custom field (Checklist Content YAML),Custom field (Checklist Text),Custom field (Children of Epic count),Custom field (Children of Epic in Done status category count),Custom field (Children of Epic in In Progress status category count),Custom field (Children of Epic in To Do status category count),Custom field (ChildrenCount),Custom field (Cisco Focus Area),Custom field (Classification),Custom field (Classification ),Custom field (Classification II),Custom field (Client),Custom field (Close Maintenance Window),Custom field (ClosedDate),Custom field (Cloud Additional Install Instructions),Custom field (Cloud Approval Message),Custom field (Cloud Architecture),Custom field (Cloud Customer),Custom field (Cloud Due Date),Custom field (Cloud Environment),Custom field (Cloud Install File),Custom field (Cloud Risk),Custom field (Cloud Risk Score),Custom field (Cloud Service Provider),Custom field (Cloud Severity),Custom field (Cloud Stack Type),Custom field (Code Added),Custom field (Code Removed),Custom field (Code included in Splunk products or projects?),Custom field (Collaborators),Custom field (Collaborators),Custom field (Collaborators)Id,Custom field (Collaborators)Id,Custom field (Color),Custom field (Color_pilot),Custom field (Comment count),Custom field (Comments),Custom field (Commit Date),Custom field (Committed),Custom field (Committed),Custom field (Committed Date),Custom field (Committed?),Custom field (Company size),Custom field (Compensation),Custom field (Competition),Custom field (Competitive Impact),Custom field (Competitive Product(s)),Custom field (Complaint?),Custom field (Completion Date),Custom field (Completion Date),Custom field (Completion Date),Custom field (Complexity (BMC Internal)),Custom field (Compliance Certification Type),Custom field (Compliance Framework),Custom field (Compliance Requirements),Custom field (Compliance Roadmap),Custom field (Compliance Type),Custom field (Compliant?),Custom field (Concept),Custom field (Confidence),Custom field (Confidence),Custom field (Confidence),Custom field (Confidence),Custom field (Confidence),Custom field (Confidence),Custom field (Confidence),Custom field (Confidence (old)),Custom field (Confidence Level),Custom field (Confidence Score),Custom field (Config File),Custom field (Configuration File),Custom field (Confirm that your app/add-on is NOT a part of Splunk's current Product Roadmap),Custom field (Confluence),Custom field (Confluence pages),Custom field (Consequence),Custom field (Consumers),Custom field (Contact),Custom field (Contact Email),Custom field (Contained Date),Custom field (Contains Custom Code),Custom field (Contains existing CVE),Custom field (Content Packs Installed),Custom field (Contingency Workers Estimate),Custom field (Contract Offering/Obligation),Custom field (Contractor Company),Custom field (Contractor Company Contact Email),Custom field (Contractor Company Name),Custom field (Contractor End Date),Custom field (Control Sub-Family),Custom field (Control Type),Custom field (Corporate Initiative),Custom field (Corporate Priority),Custom field (Cost Impact Due to Delay),Custom field (Cost saving),Custom field (Cost saving impact),Custom field (Country(ies) where services are being provided),Custom field (Creates New Endpoints / Interfaces),Custom field (Critical Initiative),Custom field (Criticality),Custom field (Cross Region),Custom field (Current Cloud Stack Name),Custom field (Current Escalations),Custom field (Current O11y Cap (if one exists)),Custom field (Current Region),Custom field (Current SVC or Ingest Entitlement),Custom field (Current SVCs provisioned),Custom field (Current Ship to Contact),Custom field (Current Target Completion Date),Custom field (Custom1),Custom field (Custom2),Custom field (Custom3),Custom field (Customer),Custom field (Customer Account Name),Custom field (Customer Account SFDC Link),Custom field (Customer Deployment),Custom field (Customer Docs Needed?),Custom field (Customer Escalation),Custom field (Customer Facing),Custom field (Customer Impact),Custom field (Customer Implementation Timeframe),Custom field (Customer Instance Type),Custom field (Customer Issue Status),Custom field (Customer Maintenance Window),Custom field (Customer Name),Custom field (Customer Outreach Pipeline Link),Custom field (Customer Priority),Custom field (Customer Profile),Custom field (Customer Request Type),Custom field (Customer Request Type),Custom field (Customer Scenario),Custom field (Customer Time Zone),Custom field (Customer Type),Custom field (Customer Use Case),Custom field (Customer Value),Custom field (Customer Value-Cisco),Custom field (Customer type: SECTOR),Custom field (Customer type: SIZE),Custom field (DBO Warning Message-1),Custom field (DPM or Host request),Custom field (Dashboard),Custom field (Dashboard),Custom field (Data Confidentiality Level),Custom field (Data Protection Approval),Custom field (Data Sets),Custom field (Date Of Event),Custom field (Date Reported),Custom field (Date notified by HR),Custom field (Date of First Response),Custom field (Days since last comment),Custom field (Days to complete),Custom field (Days with Customer),Custom field (Days with Dev),Custom field (Days with SWAT),Custom field (Deadline),Custom field (Deal Type),Custom field (Deferment Date),Custom field (Definition of Done),Custom field (Deliverable External Description),Custom field (Deliverable External Name),Custom field (Deliverable URL),Custom field (Deliverable URL),Custom field (Delivery Date),Custom field (Delivery Date Justification),Custom field (Delivery phase),Custom field (Delivery progress),Custom field (Delivery status),Custom field (DeliveryVehicle),Custom field (DeliveryVehicle),Custom field (Demo-Workshop),Custom field (Department),Custom field (Dependencies),Custom field (Dependency),Custom field (Dependency impact),Custom field (Deployed Product Version),Custom field (Deployment Environment),Custom field (Deployment Type),Custom field (Deployment Window),"Custom field (Describe at a high level, who is impacted by this project and what changes will they need to adopt.)",Custom field (Design Discovery Research),Custom field (Design Needed),Custom field (Design North Star),Custom field (Design Requirements),Custom field (Design Validation Research),Custom field (Designs ready),Custom field (Designs ready),Custom field (Designs ready),Custom field (Designs ready),Custom field (Designs ready),Custom field (Designs ready),Custom field (Designs ready),Custom field (Designs ready),Custom field (Designs ready),Custom field (Desired Outcomes),Custom field (Destination),Custom field (Destination IP),Custom field (Detailed Change Description),Custom field (Details),Custom field (Detected Date),Custom field (Detection Name),Custom field (Detection Source),Custom field (Detection Use Case Request),Custom field (Dev Canary/Canary Comms),Custom field (Dev Effort),Custom field (Developer),Custom field (Development),Custom field (Development Shop),Custom field (Did you send the customer the relevant documentation but additional assistance is needed beyond the docs?),Custom field (Direct Manager),Custom field (Director),Custom field (Director Level or Higher Sponsor),Custom field (Do we have an existing relationship/agreement with provider (Y/N)),Custom field (Do you have Sales Leadership Approval?),Custom field (Document as),Custom field (Documentation Complete),Custom field (Documentation Created),Custom field (Documentation Link),Custom field (Documents),Custom field (Documents),Custom field (Documents),Custom field (Documents),Custom field (Documents),Custom field (Documents),Custom field (Documents),"Custom field (Does the app contain any new functionality, new user interface, underlying algorithm, method, process or improvement to an existing Splunk product on which we should consider filing a patent application?)",Custom field (Does the customer expect a refund for the remaining time?),Custom field (Does this change need SAG to review?),Custom field (Domain Names),Custom field (Done),Custom field (Driving Change),Custom field (Due Date),Custom field (Due Date (migrated)),Custom field (Due date (SE)),Custom field (Due date (security)),Custom field (Duration),Custom field (EA Approval),Custom field (ECAP Review),Custom field (EDM Approval),Custom field (ELT Owner),Custom field (ELT update required),Custom field (EOS Date),Custom field (EPAT PAM),Custom field (EPAT Project Link),Custom field (ERD),Custom field (ERD Link),Custom field (ESR),Custom field (ETA),Custom field (ETA Date),Custom field (Effort),Custom field (Effort),Custom field (Effort),Custom field (Effort),Custom field (Effort),Custom field (Effort),Custom field (Effort),Custom field (Effort),Custom field (Effort),Custom field (Effort),Custom field (Effort),Custom field (Effort),Custom field (Effort Score),Custom field (Effort old),Custom field (Email),Custom field (Email Participants),Custom field (Email Sender Address),Custom field (Email Sender Name),Custom field (Employee Role),Custom field (Employee Termination Date),Custom field (Employment Business Title),Custom field (Employment Job Level),Custom field (Employment Type),Custom field (Enablement(Y/N)?),Custom field (End Date),Custom field (End Date (migrated)),Custom field (End of Quarter Deal),Custom field (Endpoint Type),Custom field (Eng Approved),Custom field (Engagement Type),Custom field (Engg Planning Confidence),Custom field (Engg Planning Confidence),Custom field (Engine ),Custom field (Engineering Lead),Custom field (Engineering Manager),Custom field (Engineering Manager)Id,Custom field (Engineering Metrics),Custom field (Engineering/Support Template),Custom field (Environment),Custom field (Environment Details),Custom field (Environment Details),Custom field (Environment Details),Custom field (Epic Colour),Custom field (Epic Name),Custom field (Epic Name),Custom field (Epic Status),Custom field (Epic T Shirt Story Points),Custom field (Epic T-Shirt Size),Custom field (Error log messages),Custom field (Escalation Alert),Custom field (Escalation Level),Custom field (Escalation Type),Custom field (Estimate date),Custom field (Estimated Cost Savings($K/month)),Custom field (Estimated Cost Spent($K/month)),Custom field (Estimated Customer Downtime (minutes)),Custom field (Estimated Time (days)),Custom field (Estimation),Custom field (Eval OR Prod Contract End Date),Custom field (Event Name),"Custom field (Example (Link, url))",Custom field (Exception),Custom field (Exception Approver),Custom field (Exception Justification),Custom field (Exception Requested),Custom field (Exception Until Date),Custom field (Execution Type),Custom field (Executive Sponsor),Custom field (Executive Sponsor-Cisco),Custom field (Executive Summary),Custom field (Existing Feature),Custom field (Existing Product),Custom field (Existing requirement that needs to be changed),Custom field (Expected ATS (Active Time Series)),Custom field (Expected Behavior),Custom field (Expected Budget Impact),Custom field (Expected Churn),Custom field (Expected DPM),Custom field (Expected Impact),Custom field (Expected Outcome),Custom field (Expected Scenarios),Custom field (Expected Volume),Custom field (Expected impact during deployment),Custom field (Expertise Level),Custom field (Expires),Custom field (Extension Granted),Custom field (Extension Granted Till),Custom field (Extension Justification),Custom field (External Third-Party Audit Achievement),Custom field (External Tracker URL),Custom field (External Tracker URL),Custom field (External Tracker URL),Custom field (External issue ID),Custom field (External issue ID),Custom field (External issue ID),Custom field (FAST Create Message),Custom field (FE Customer),Custom field (FLT Sponsor),Custom field (FOSSA Scan URL),Custom field (False Positive),Custom field (Fault),Custom field (Feature Name),Custom field (Federal Customer (Y/N)?),Custom field (Feedback Type),Custom field (Figma Link),Custom field (Final Priority),Custom field (Finance PR Number),Custom field (Findings),Custom field (Fiori ChaRM Id(s)),Custom field (First Touch),Custom field (Fix Available (Y/N)),Custom field (Fix/Solution),Custom field (Fix/Solution_archived),Custom field (Flag for Discussion ),Custom field (Flexera Report),Custom field (Focus Areas),Custom field (For Group Access Only: Please provide AD Group ID),Custom field (Forensic Ticket),Custom field (Formatted Description),Custom field (Found in Release),Custom field (Freeze Applied),Custom field (Freeze End),Custom field (Freeze Start),Custom field (Frequency),"Custom field (Functional Effort (BRD, FS, Config))",Custom field (Functional Lead),Custom field (GA Date),Custom field (GA Ready),Custom field (GAD Kick Off),Custom field (GRC Effort),Custom field (GTM Go/NoGo),Custom field (GTM Process),Custom field (GTM Release Milestone),Custom field (GTMO BizApps Status),Custom field (GTMO Ticket #),Custom field (General description of services/deliverables),Custom field (GitHub Issue),Custom field (GitHub Sharing),Custom field (Github Acct),Custom field (Github Issue),Custom field (Global Component),Custom field (Goal),Custom field (Goal impact),Custom field (Goal impact),Custom field (Goal impact),Custom field (Goals),Custom field (GovCloud/Fedramp),Custom field (Grant Role SLA),Custom field (Gross Margin),Custom field (Guest Account Type),Custom field (Guest Organization),Custom field (HH Priority),Custom field (Hand-Off Status),Custom field (Hardware Type),Custom field (Has Customer provided consent to access their data?),Custom field (Hide),Custom field (High-Level Classification),"Custom field (Hiring Backfill for LastName, FirstName)",Custom field (Hiring Business ID),Custom field (Hiring Cost Center ),Custom field (Hiring Dept. Code),Custom field (Hiring Initiative),Custom field (Hiring Justification),Custom field (Hiring Manager),Custom field (Hiring Profit Center),Custom field (Hiring Quarter),Custom field (Hiring Region),Custom field (Hiring department),Custom field (Hiring manager (migrated)),Custom field (Historical Data Migration),Custom field (Hong Kong),Custom field (Honor Change Freezes),Custom field (How Found),Custom field (How Long(Duration)),Custom field (How Many),Custom field (How Often),Custom field (How many stacks need this Splunk-initiated change applied? How are eligible stacks identified?),Custom field (Hyderabad ),Custom field (Hyderabad Zoom Rooms),Custom field (IAM Changes),Custom field (IDX Number),Custom field (IM ICSLO breach (mins)),Custom field (IM Internal Customer SLO breach (mins)),Custom field (IM SLA breach (mins)),Custom field (IMOC),Custom field (INH #),Custom field (INH Link),Custom field (IP Addresses),Custom field (IP filing required),Custom field (IR Description),Custom field (IT Architecture Approval),Custom field (IT Done Date),Custom field (ITS Capability),Custom field (ITS Demand Type),Custom field (Idea archived),Custom field (Idea archived on),Custom field (Idea short description),Custom field (Idea short description),Custom field (Idea short description),Custom field (Idea short description),Custom field (Idea short description),Custom field (Idea short description),Custom field (Idea short description),Custom field (Ideas Link),Custom field (Ideas Overview URL),Custom field (Identified Customer Count),Custom field (Identified On),"Custom field (If Classic, are there any Apps/TAs needed on the SH?)","Custom field (If the customer expects a refund, has Order Management been looped in?)",Custom field (Impact),Custom field (Impact),Custom field (Impact),Custom field (Impact),Custom field (Impact),Custom field (Impact),Custom field (Impact),Custom field (Impact),Custom field (Impact),Custom field (Impact),Custom field (Impact),Custom field (Impact),Custom field (Impact Analysis),Custom field (Impact If Not Implemented),Custom field (Impact Justification),Custom field (Impact Score),Custom field (Impact score),Custom field (Impact score),Custom field (Impact score),Custom field (Impact score),Custom field (Impact score),Custom field (Impact score),Custom field (Impact score),Custom field (Impact score),Custom field (Impact score),Custom field (Impact score),Custom field (Impact score),Custom field (Impact score old),Custom field (Impact to customers),Custom field (Impact vs. effort),Custom field (Impact vs. effort),Custom field (Impact vs. effort),Custom field (Impact vs. effort),Custom field (Impacted Customer Count),Custom field (Impacted Population),Custom field (Implementation plan),Custom field (In Design Phase),Custom field (Incbot Resolution),Custom field (Incident Category),Custom field (Incident Coordinator),Custom field (Incident Date),Custom field (Incident Folder ID),Custom field (Incident Intelligence SLA breach (mins)),Custom field (Incident Origination),Custom field (Incident Related),Custom field (Incident Root Cause),Custom field (Incident Start Date),Custom field (Incident Summary),Custom field (Includes confidential information?),Custom field (Increase of),Custom field (Incremental SW iACV),Custom field (Indexer Type),Custom field (Indexer(s)),Custom field (Infra Approval),Custom field (Infra Effort),Custom field (Infrastructure Solutions Approval),Custom field (Initial),Custom field (Initial Priority),Custom field (Initial Sprint Commitment),Custom field (Initiative Start Date),Custom field (Initiative Status),Custom field (Initiative Status),Custom field (Insights),Custom field (Insights impact),Custom field (Insights impact),Custom field (Insights impact),Custom field (Insights impact),Custom field (Instance Type),Custom field (Instructions),Custom field (Instructions),Custom field (Integration User),Custom field (Internal),Custom field (Internal Order),Custom field (Intro In Release),Custom field (Investigation reason),Custom field (Invitation Link),Custom field (Involves Slack Bots),Custom field (Involves Splunk Add-ons),Custom field (Is Problem),Custom field (Is Splunk App / Add-on),Custom field (Is the customer account or the Support case in an escalated status?),Custom field (Is the customer aware they will immediately lose access to the stack?),Custom field (Is there an Active OnDemand Entitlement? (Not a deal breaker)),Custom field (Is there an approved CCAB ticket?),Custom field (Is this NPI/NFI expected to be implemented in our Splunk Cloud Space?),Custom field (Is this a Maintenance Window Extension?),"Custom field (Is this a paid (e.g., Splunk Enterprise Security) or free-offering (App/TA))",Custom field (Is this a production system?),Custom field (Is this request a significant compliance change?),Custom field (Is your work completely stopped?),Custom field (Issue),Custom field (Issue color),Custom field (Issue link count),Custom field (Jira-QA),Custom field (Job Level),Custom field (Job Level of Existing Employee),Custom field (Job role),Custom field (Key Asks),Custom field (Key Customer Benefit),Custom field (Keywords),Custom field (Krakow Room names),Custom field (L0-Process Name),Custom field (Last Reviewed Date),Custom field (Last Update),Custom field (Last Update),Custom field (Last comment),Custom field (Last comment date),Custom field (Last commented by),Custom field (Last updater or commenter),Custom field (Launch Phase),Custom field (Launch Readiness PM Help Requested),Custom field (Launch Tier),Custom field (Leadership Priority),Custom field (Legal Exposure),Custom field (Legal Mitigation),Custom field (Legal Reference),Custom field (Level of Effort),Custom field (Likelihood),Custom field (Likelihood Justification),Custom field (Link),Custom field (Link to the solution (BMC Internal)),Custom field (Linked items),Custom field (Location),Custom field (Locked forms),Custom field (LogO SLA breach (mins)),Custom field (London Room Names),Custom field (MAO Exclusion Reason),Custom field (MTTR),Custom field (MTTR Display),Custom field (MW Duration),Custom field (MW Freeze),Custom field (MW required),Custom field (Maintenance Window),Custom field (Maintenance Window Duration),Custom field (Maintenance Window Required),Custom field (Maintenance Window Timeframe),Custom field (Maintenance Window Type),Custom field (Major incident),Custom field (Manager Name),Custom field (Market),Custom field (Market Access Type),Custom field (Market Impact),Custom field (Marketing Short Description),Custom field (Matrix),Custom field (Maximum Value),Custom field (Measurements & Targets),Custom field (Meeting Notes),Custom field (Melbourne Room Names),Custom field (Merge Request),Custom field (Message),Custom field (Message Field),Custom field (Message header),Custom field (Migration Guidelines),Custom field (Minimum Value),Custom field (Mission Team),Custom field (Mission Team),Custom field (Mitigation),Custom field (Mitigation Category),Custom field (Mitigation Options),Custom field (MoSCoW),Custom field (MoSCoW),Custom field (Monitoring Trigger),Custom field (Month),Custom field (Munich),Custom field (NCC Pre-CCAB),Custom field (NOC Remediation Type Label),Custom field (NOC pstack Label),Custom field (Name of App/Add-on),Custom field (Name of Stack),Custom field (Name of Stack - TEST),Custom field (Narrative Location),Custom field (Needed By),Custom field (Network Approval),Custom field (Network/VPN Access Type),Custom field (New Change),Custom field (New Cloud Stack Name),Custom field (New Date of Cancellation),Custom field (New Extension End Date),Custom field (New Market Opportunities),Custom field (New Region),Custom field (New SKU Required/Modified),Custom field (New SLA for CRE),Custom field (New Slack Channel Name),Custom field (New since upgrade of Core),Custom field (New since upgrade of ES),Custom field (Next Steps),Custom field (Next Steps),Custom field (Next Update),Custom field (Next Update),Custom field (Next steps (migrated)),Custom field (No of Days until Secret Expiry),Custom field (Non-Standard Infrastructure Re-Shape Request),Custom field (Notes),Custom field (Notes),Custom field (Notification Source),Custom field (Number of attachment),Custom field (Number of comments),Custom field (Number of expected alerts),Custom field (Numeric Count),Custom field (O11y Category),Custom field (O11y Engineering Pillar),Custom field (O11y Products),Custom field (O11y Program Team),Custom field (O11y Satisfaction Score (1–5)),Custom field (O11y Value Stream),Custom field (O11y-Acceptance Criteria),Custom field (O11y-CSP Region),Custom field (O11y-CVSS Score),Custom field (O11y-Color),Custom field (O11y-Compliance Certification),Custom field (O11y-Confidence Level),Custom field (O11y-Customer Delivery Model),Custom field (O11y-Customer Name),Custom field (O11y-Customer Type),Custom field (O11y-EPAT Project Link),Custom field (O11y-Engineering Manager),Custom field (O11y-GTM: Field Enablement),Custom field (O11y-GTM: Marketing Comms),Custom field (O11y-GTM: Tech/CS Enablement),Custom field (O11y-Ideas Link),Custom field (O11y-Impact),Custom field (O11y-Initiative Status),Custom field (O11y-Launch Phase),Custom field (O11y-Launch Tier),Custom field (O11y-Launch Tracker Approved),Custom field (O11y-Location),Custom field (O11y-Offering PM),Custom field (O11y-PMM Owner),Custom field (O11y-PRFAQ Link),Custom field (O11y-Pillars),Custom field (O11y-Product Area),Custom field (O11y-Product Area VP),Custom field (O11y-Product Manager),Custom field (O11y-Product Name and Feature Name),Custom field (O11y-Product Owner),Custom field (O11y-Program Team),Custom field (O11y-Release Date),Custom field (O11y-Reviewer),Custom field (O11y-Self-Managed Due Date),Custom field (O11y-Severity),Custom field (O11y-Severity (migrated)),Custom field (O11y-Status Explanation),Custom field (O11y-Status Update),Custom field (O11y-Steps to Reproduce),Custom field (O11y-Success Metrics),Custom field (O11y-Value Stream),Custom field (O11y-Verified as of),Custom field (OKR Category),Custom field (OS Version),Custom field (OSS/COTS Solution),Custom field (Occurred Date),Custom field (Off Cycle),Custom field (Offering PM),Custom field (OnDemand Case Number),Custom field (OnPrem Due Date),Custom field (OnPrem Risk),Custom field (Onboarded to CTS),Custom field (OpEx Original Estimate),Custom field (OpEx Spent),Custom field (Open forms),Custom field (Opportunities Impacted),Custom field (Opportunity),Custom field (Opportunity Probability),Custom field (Opportunity link),Custom field (Org ID),Custom field (Org and Group),Custom field (Org existence),Custom field (Organization Function),Custom field (Origin),Custom field (Origin),Custom field (Original Estimated Cost Savings($K/month)),Custom field (Original Estimated Cost Spent($K/month)),Custom field (Original GA Date),Custom field (Original Request Date),Custom field (Original Target Completion Date),Custom field (Other Architectural Type Request),Custom field (Other Artifacts & Challenges),Custom field (Other Details or Comments),Custom field (Other Information),Custom field (Outage Duration),Custom field (Overall Go/NoGo),Custom field (Owner),Custom field (P&T Approval (Y/N)?),Custom field (P&T OKRs),Custom field (PIR Link),Custom field (PM Review),Custom field (PM Review Start),Custom field (PM priority rank),Custom field (PMM Owner),Custom field (POC Request Link (SFDC)),Custom field (POC Request Link (SFDC)_archive),Custom field (POC Type),Custom field (PR FAQ/PRD Link),Custom field (PR Link),Custom field (PR Number),Custom field (PR Requestor),Custom field (PRD),Custom field (PRD/Wiki link),Custom field (PSR Request Type),Custom field (PTA Project),Custom field (Pair),Custom field (Parameter(s)),Custom field (Parent Link),Custom field (Parent idea),Custom field (Paris),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants),Custom field (Participants and/or Contacts),Custom field (Partner Value),Custom field (Party Name),Custom field (Path to Green),Custom field (Pen Testing Required),Custom field (Pending Approval Date),Custom field (Pending reason),Custom field (Pending reason),Custom field (Percent Market Share Acquired (%)),Custom field (Phase Description),Custom field (Phase Name),Custom field (Phone number),Custom field (Pillars),Custom field (Pipeline(s)),Custom field (Plan ready),Custom field (Planned Due Date),Custom field (Planned Fiscal Quarter),Custom field (Planned Fiscal Year),Custom field (Planned end),Custom field (Planned rollout approach),Custom field (Planned start),Custom field (Planning),Custom field (Planning / BU Code),Custom field (Plano Room Names),Custom field (Planview WorkID URL),Custom field (Platform Approval),Custom field (Platform Area),Custom field (Platform/OS),Custom field (Please confirm AppInspect has been run on your app and is approved),Custom field (Please list any error messages being presented.),Custom field (Please provide the names of stakeholders so we can follow up with them to clarify questions and additional discoveries.),Custom field (Plugin ID),Custom field (Poland R&D Project Q1),Custom field (Poland R&D Project Q2),Custom field (Poland R&D Project Q3),Custom field (Poland R&D Project Q4),Custom field (Poland R&D Project Q5),Custom field (Poland R&D Project Q6),Custom field (Poland R&D Project Q7),Custom field (Policy / Standard),Custom field (Port),Custom field (Portfolio/Area),Custom field (Post-CCAB NCC),Custom field (Post-Check(s)),Custom field (Potential Threat Count),Custom field (Prague Room Names),Custom field (Pre Release Date),Custom field (Pre-Check(s)),Custom field (Pre-Release Portal Link),Custom field (Pre-release Program Plan Link),Custom field (Premium App Requested),Custom field (Preview Link),Custom field (Preview Status Explanation),Custom field (Previous Backfill Location),Custom field (Primary EPAT Engineer),Custom field (Primary System),Custom field (Prioritization Score),Custom field (Priority Reason),Custom field (Priority Score),Custom field (Priority Score),Custom field (Priority Sequence Number),Custom field (Private Ideas Portal),Custom field (Probability),Custom field (Problem),Custom field (Problem Cause Code),Custom field (Product),Custom field (Product - SDLSEC),Custom field (Product Area),Custom field (Product Area VP),Custom field (Product Area and Offering),Custom field (Product Area and Offering (migrated)),Custom field (Product Compliance: Overview),Custom field (Product Metrics),Custom field (Product Name and Feature Name),Custom field (Product Owner),Custom field (Product Owner)Id,Custom field (Product Priority),Custom field (Product Requirements-1),Custom field (Product Type),Custom field (Product URL),Custom field (Product Version),Custom field (Product Versions Affected),Custom field (Product area),Custom field (Product area),Custom field (Product area),Custom field (Product categorization),Custom field (Product fix date),Custom field (Product fix ticket),Custom field (Product to Upgrade),Custom field (Product/Process Improvement Category),Custom field (Product/Process Improvement Category),Custom field (ProductBacklogArea),Custom field (Productivity Impact),Custom field (Products),Custom field (Program Google Drive Link),Custom field (Program Owner (EM/TPM)),Custom field (Program Summary),Custom field (Program Summary Link),Custom field (Progress),Custom field (Project Assessment and Assignment),Custom field (Project Manager (PMO/TPM)),Custom field (Project Name),Custom field (Project Poster),Custom field (Project Type),Custom field (Project overview key),Custom field (Project overview status),Custom field (Project start),Custom field (Project start),Custom field (Project start),Custom field (Project start),Custom field (Project start),Custom field (Project start),Custom field (Project start),Custom field (Project start),Custom field (Project start),Custom field (Project start),Custom field (Project start),Custom field (Project target),Custom field (Project target),Custom field (Project target),Custom field (Project target),Custom field (Project target),Custom field (Project target),Custom field (Project target),Custom field (Project target),Custom field (Project target),Custom field (Project target),Custom field (Project target end),Custom field (Projected Due Date),Custom field (Projected End Date),Custom field (Proposed Implementation Date),Custom field (Proposed delivery method?),Custom field (Proposed solution),Custom field (Public),Custom field (Public description),Custom field (Public description),Custom field (Public description),Custom field (Python EOL Support),Custom field (QA),Custom field (QA)Id,Custom field (QA Quality),Custom field (QA User),Custom field (QE Owner),Custom field (QE Request Type),Custom field (Qualified Risk Score),Custom field (Quarter),Custom field (Quarterly Actual),Custom field (Quarterly Target),Custom field (Questionnaire Link),Custom field (Questions),Custom field (RC Candidate),Custom field (RCM Requested),Custom field (RICE Score),Custom field (RICE Score),Custom field (RUM ICSLO breach (mins)),Custom field (RUM Internal Customer SLO breach (mins)),Custom field (RUM SLA breach (mins)),Custom field (Rank),Custom field (Re-shape change requested),Custom field (Reach),Custom field (Reach Score),Custom field (Reactions),Custom field (Ready for Verification Notes),Custom field (Realm),Custom field (Reason),Custom field (Reason),Custom field (Reason App Built),Custom field (Reason App Built_Archived),Custom field (Reason for Cancellation),Custom field (Reason for Change Freeze),Custom field (Reason for Needs Info),Custom field (Reason for Precheck Failed),Custom field (Reason?),Custom field (Recipient Email),Custom field (Recruiter),Custom field (Recruiting Location (others)),Custom field (Recruiting Location (primary)),Custom field (Redacted_Description),Custom field (Redacted_Summary),Custom field (Reference),Custom field (Region),Custom field (Regional Availability Roadmap),Custom field (Regression),Custom field (Regression Assessment),Custom field (Regulated Environment),Custom field (Related Paid Stack Name),Custom field (Related Product),Custom field (Related Runbook KBA),Custom field (Release),Custom field (Release CI Kickoff),Custom field (Release Date),Custom field (Release Due Date),Custom field (Release Freeze),Custom field (Release Vehicle),Custom field (Release notes),Custom field (Release ticket),Custom field (Remediation Blockers),Custom field (Remediation Plan),Custom field (Remediation Status),Custom field (Remediation Timeline),Custom field (Repeat Issue),Custom field (Repo URI),Custom field (Report Template Link),Custom field (Report Template Status),Custom field (Reporting Team Name),Custom field (Reporting vuln on behalf of),Custom field (Req ready),Custom field (Request Team),Custom field (Request Type),Custom field (Request Type ),Custom field (Request Types),Custom field (Request Urgency),Custom field (Request language),Custom field (Request participants),Custom field (Requested By),Custom field (Requested Cap),Custom field (Requested FixVersion),Custom field (Requested For),Custom field (Requested Health Check Completion Date),Custom field (Required Service Days),Custom field (Required Service Hours),Custom field (Requires Advanced Customer Communication ?),Custom field (Requires Security Token / Authentication),Custom field (Resolution Date),Custom field (Resolution Details),Custom field (Resource ID of Secret),Custom field (Responders),Custom field (Responsible Org (BMC Internal)),Custom field (Review Classification),Custom field (Review Script),Custom field (Reviewed By),Custom field (Reviewer),Custom field (Reviewer)Id,Custom field (Reviewer(s)),Custom field (Reviewer(s)),Custom field (Reviewer(s)),Custom field (Reviewer(s)),Custom field (Reviewer(s))Id,Custom field (Reviewer(s))Id,Custom field (Reviewer(s))Id,Custom field (Reviewer(s))Id,Custom field (Revised Due Date),Custom field (Revised End Date),Custom field (Revision),Custom field (Risk),Custom field (Risk Accepted For Release),Custom field (Risk Assessment),Custom field (Risk Criticality),Custom field (Risk Description (BMC Internal)),Custom field (Risk Family),Custom field (Risk Impact),Custom field (Risk Level),Custom field (Risk Level (BMC Internal)),Custom field (Risk Memo),Custom field (Risk Owner),Custom field (Risk Owner),Custom field (Risk Rating (migrated)),Custom field (Risk Reviewed),Custom field (Risk Score),Custom field (Risk Score Type),Custom field (Risk Status),Custom field (Risk Treatment Plan(s) Considered),Custom field (Risk rating),Custom field (Roadmap),Custom field (Roadmap),Custom field (Roadmap),Custom field (Roadmap),Custom field (Roadmap),Custom field (Roadmap),Custom field (Roadmap),Custom field (Roadmap),Custom field (Roadmap),Custom field (Roadmap),Custom field (Roadmap),Custom field (Roadmap Initiative Type),Custom field (Roadmap identifier),Custom field (Rollback Plan),Custom field (Room Name),Custom field (Root Cause),Custom field (Root Cause Description),Custom field (Root Cause System),Custom field (Root Cause System Program),Custom field (Root cause explanation),Custom field (Root-Cause Analysis),Custom field (Runbook),Custom field (Runbook Used),Custom field (Runbook Used Multi),Custom field (S.W.A.T. Ticket - APM Component),Custom field (S/4 ChaRM Id(s)),Custom field (SAA Job Link),Custom field (SAA Provision Type),Custom field (SAA Reported By (Tenant)),Custom field (SAA Reported By (Username)),Custom field (SAP Cost Center),Custom field (SAP Department Name),Custom field (SAP Location),Custom field (SBOM Link),Custom field (SE Contact),Custom field (SE Manager),Custom field (SEC-Consumes new data type?),Custom field (SEC-Contains new user types?),Custom field (SEC-Exposes new endpoints?),Custom field (SEC-Introduces new roles?),Custom field (SEC-New encryption strategy?),Custom field (SELECT YES TO SUBMIT FOR COMPLIANCE REVIEW),Custom field (SF250 Room Name),Custom field (SFDC Account ID),Custom field (SFDC Case Number),Custom field (SFDC Opportunity Link),Custom field (SFDC Owner Email),Custom field (SFDC Requestor Name),Custom field (SFDC-Comment-Automation),Custom field (SFDC-Status-Automation),Custom field (SFDCAccount),Custom field (SFDCCaseAssociation),Custom field (SFDCCaseRecordType),Custom field (SFDCCustomerPriority),Custom field (SFDCEscalationAlert),Custom field (SFDCRecordID),Custom field (SFDCStackID),Custom field (SFDCStatus),Custom field (SFDCSubStatus),Custom field (SFDCSupportOffering),Custom field (SFDCTier),Custom field (SIT Effort),Custom field (SLA (Days)),Custom field (SLO days remaining),Custom field (SNOW Release Ticket),Custom field (SNOW Release Ticket_archived),Custom field (SNOW Ticket Number),Custom field (SOW),Custom field (SP - Corp Initiative),Custom field (SP - Corporate Initiative Owner),Custom field (SP - Customer Outcome),Custom field (SP - Initiative Status),Custom field (SP - Planning Year),Custom field (SP - Product Area),Custom field (SP - Product Strategy Wave),Custom field (SPLC Stage),Custom field (SR500 Room Names),Custom field (SRE Component),Custom field (SRE-Priority),Custom field (STRAT Sponsor),Custom field (SVC Approved),Custom field (Sales Contact),Custom field (Sales Engineer),Custom field (Sales Program),Custom field (Sales Region),Custom field (Sales Rep),Custom field (SalesForce: Coverage Recommendation Formula),Custom field (SalesForce: Region),Custom field (SalesForce: SFX Industry),Custom field (Salesforce Case #),Custom field (Salesforce POC link),Custom field (Salesforce Ref #),Custom field (SalesforceID),Custom field (Satisfaction),Satisfaction rating,Custom field (Satisfaction date),Custom field (Scheduled > Rescheduled),Custom field (Scheduled Implementation Close),Custom field (Scheduled Implementation Start),Custom field (Scheduled Implementation Timeframe),Custom field (Score),Custom field (Scrum),Custom field (Search Head(s)),Custom field (Seattle office),Custom field (Secondary Assignee),Custom field (Secret Age),Custom field (Secret Creation Date),Custom field (Secret Expiry Date),Custom field (Security Approval),Custom field (Security Category),Custom field (Security Credits),Custom field (Security Feature),Custom field (Security Issue),Custom field (Security Risk Score),Custom field (Security Score Test Field),Custom field (Security Severity),Custom field (Security Themes),Custom field (Security Threat Assessment Completed?),Custom field (Security certification hold by the service provider),Custom field (Segment),Custom field (Self Service),Custom field (Self-Attested Release),Custom field (Self-Managed Project),Custom field (Sender Name),Custom field (Sender email Address),Custom field (Sentiment),Custom field (Service),Custom field (Service Paid (Y/N)),Custom field (Service Port),Custom field (ServiceNow ID),Custom field (ServiceNow Item),Custom field (ServiceNow Link),Custom field (ServiceNow Number),Custom field (ServiceNow Updated By),Custom field (ServiceNow User Comments),Custom field (Severity),Custom field (Severity),Custom field (Severity Justification),Custom field (Severity Level),Custom field (Shared Product/Service Ownership),Custom field (Short description),Custom field (Short description),Custom field (Short description),Custom field (Side effect to existing features/API ?),Custom field (Side-effect to existing feature/API ?),Custom field (Singapore Zoom Rooms),Custom field (Size),Custom field (Slack Channel),Custom field (Slack Connect),Custom field (Slack ID),Custom field (Slack License),Custom field (Software License),Custom field (Solutions Approval),Custom field (Sort),Custom field (Source),Custom field (Source Details),Custom field (Source IP),Custom field (Source Repository),Custom field (Spec ready),Custom field (Spec ready),Custom field (Spec ready),Custom field (Spec ready),Custom field (Spec ready),Custom field (Spec ready),Custom field (Spec ready),Custom field (Spec ready),Custom field (Spec ready),Custom field (Splcore Pipeline Component),Custom field (Splunk Advisory Ref),Custom field (Splunk Certified Interviewer),Custom field (Splunk Data),Custom field (Splunk Entity),Custom field (Splunk Environment),Custom field (Splunk Idea),Custom field (Splunk Instance FQDN),Custom field (Splunk Instance FQDN (deprecated)),Custom field (Splunk Integration),Custom field (Splunk Office Locations),Custom field (Splunk RTO),Custom field (Splunk System Integration),Custom field (Splunk Version),Custom field (Splunkbase URL),Custom field (Splunkbase URL),Custom field (Sponsor),Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Custom field (Sr-Director-Dir),Custom field (Stack End Date),Custom field (Stack Experience),Custom field (Stack Name),Custom field (Stack Rank),Custom field (Stack Rank (Cross Functional)),Custom field (Stack Rank (Track)),Custom field (Stack Region),Custom field (Stack Size),Custom field (Stack Start Date),Custom field (Stack Type),Custom field (Stackid),Custom field (Standard Designation Notes),Custom field (Standard vs Non-Standard),Custom field (Stanza),Custom field (Stanza),Custom field (Start Date),Custom field (Start Maintenance Window),Custom field (Start date),Custom field (Start recognizing savings),Custom field (Start recognizing savings date),Custom field (Status Category),Custom field (Status Color Code),Custom field (Status Explanation),Custom field (Status Message),Custom field (Status Update),Custom field (Status Update),Custom field (Status color),Custom field (Statuspage Incident),Custom field (Steps to reproduce),Custom field (Steps to reproduce),Custom field (Stockholm),Custom field (Stories of Epic count),Custom field (Stories of Epic in Done status category count),Custom field (Stories of Epic in In Progress status category count),Custom field (Stories of Epic in To Do status category count),Custom field (Story ID),Custom field (Story Link),Custom field (Story Point Actual),Custom field (Story Points),Custom field (Story Points (QA)),Custom field (Story point estimate),Custom field (Sub Status),Custom field (Sub Status Description),Custom field (Sub-Initiative),Custom field (Sub-component),Custom field (Sub-status),Custom field (Sub-theme),Custom field (Subject Alternate Name(s)),Custom field (Submitted forms),Custom field (Subtask count),Custom field (Success Metrics),Custom field (Suffix),Custom field (Superstar Nomination),Custom field (Support Analysis),Custom field (Support Case Number),Custom field (Support Initiative),Custom field (Support Offering),Custom field (Supported by Splunk?),Custom field (Sustaining Lead),Custom field (Sydney),Custom field (Synthetics ICSLO Breach (mins)),Custom field (Synthetics Internal Customer SLO breach (mins)),Custom field (Synthetics SLA breach (mins)),Custom field (System Information),Custom field (T-shirt size),Custom field (T-shirt size DO),Custom field (T-shirt size EP),Custom field (T-shirt size IP),Custom field (T-shirt size LSP),Custom field (T-shirt size PXUI),Custom field (T-shirt size UI),Custom field (TC Complete),Custom field (TEST  (SLA)),Custom field (TEST 2),"Custom field (TM:  Have you started using encryption, decryption, hashing, signing or validation of authentication/authorization not used before(Y/N)? )",Custom field (TM: Any new Endpoints (Y/N)?),Custom field (TM: Any new type of data not used before  (Y/N)?),Custom field (TM: Are there any security-related bug fixes? (Y/N)),Custom field (TM: Changes in the use of security tokens or authentication (Y/N)?),Custom field (TM: Changes to roles/capabilities (Y/N)?),Custom field (TM: Continuous Integration (CI) Type),Custom field (TM: Estimated Release Version #),"Custom field (TM: Git Repos (folders, code paths, or MRs))",Custom field (TM: Has a penetration test been performed on this product before? (Y/N)),Custom field (TM: Has a threat model been performed on this product before? (Y/N)),"Custom field (TM: Instruction for new products, features, or technical add-ons)",Custom field (TM: Instructions for Bug Fixing Releases),"Custom field (TM: Instructions for New Features, Bug Fixing Releases, or Configuration Changes)",Custom field (TM: Integrates or utilizes new 3rd party services (Y/N)?),Custom field (TM: Link to DAST scans),Custom field (TM: Link to FOSSA scans),Custom field (TM: Link to SAST scans),Custom field (TM: Link to Vuln ticket),Custom field (TM: Link to penetration test ticket),Custom field (TM: Link to threat model test ticket),Custom field (TM: Overview),Custom field (TM: Project / Repo onboarded to Centralized Ticketing Service (CTS) (Y/N)?),Custom field (TM: Release Type),Custom field (TM: SELECT YES TO SUBMIT A PRODUCT SECURITY REVIEW),Custom field (TM: Tab Instructions),Custom field (TMM Revision),Custom field (TPM Owner),Custom field (TYPE OF DATA PROCESSED),Custom field (Target Adoption Rate (%)),Custom field (Target Completion Date),Custom field (Target Entitlements),Custom field (Target GA Date),Custom field (Target Go-Live),Custom field (Target Number of Customers),Custom field (Target Start Date),Custom field (Target Timeframe),Custom field (Target end),Custom field (Target start),Custom field (Task),Custom field (Task End Date (BMC Internal)),Custom field (Task Start Date BMC Internal)),Custom field (Team),Custom field (Team),Custom field (Team Email(s)),Custom field (Team Name),Custom field (Tech Contact),Custom field (Tech Writer Reviewer),Custom field (Technical Dependencies),Custom field (Technical Diagram Link),Custom field (Technical Lead (TO)),Custom field (Technical Owner),Custom field (Technical Program Manager),Custom field (Technical Success Engineer),Custom field (Temporary or Permanent?),Custom field (Tenant Name),Custom field (Test),Custom field (Test Criteria),Custom field (Test Data Availability),Custom field (Test Plan),Custom field (Test Results),Custom field (Test plan),Custom field (Tester),Custom field (Testing Plan),Custom field (Theater),Custom field (Theme),Custom field (Third-party software included?_Archived),Custom field (Threat Scenario),Custom field (Ticket Number),Custom field (Ticket Origin),Custom field (Ticket Type),Custom field (Tier),Custom field (Tier),Custom field (Time Invested (BMC Internal)),Custom field (Time Lost in Days),Custom field (Time To Delivery),Custom field (Time Waiting for External),Custom field (Time Waiting for Internal),Custom field (Time in CCM - Needs Scheduling),Custom field (Time in CCM Review),Custom field (Time in Implementation),Custom field (Time in Needs Info),Custom field (Time in Needs Pre-Check),Custom field (Time in Open),Custom field (Time in Post Validation),Custom field (Time in Pre-Check),Custom field (Time in Progress),Custom field (Time in Scheduled),Custom field (Time in Status),Custom field (Time in Waiting for Customer),Custom field (Time to Complete (days)),Custom field (Time to approve normal change),Custom field (Time to close after resolution),Custom field (Time to first response),Custom field (Time to resolution),Custom field (Time to review normal change),Custom field (Timeline Context),Custom field (Tokyo Room Names),Custom field (Tool),Custom field (Topic Area),Custom field (Topic Areas),Custom field (Total Booking (USD M)),Custom field (Total Budget Value),Custom field (Total forms),Custom field (Total incident duration (mins)),Custom field (Total length of POC if extended),Custom field (Tracker URL),"Custom field (Trademarks: For any logos, icons, screenshots, graphics, artwork or any other content used or shown in the product or listing page (collectively, the ""Marks""), have you obtained written permission from the owner?)",Custom field (Training Opportunity),Custom field (Transfer Justification),Custom field (Transfer to Next Shift (Y/N)),Custom field (Transformation Lead),Custom field (Transition Status),Custom field (Transportation Protocols),Custom field (Triage-Jira-Reviewer checklist),Custom field (Triage-Jira-Reviewer checklist),Custom field (Triage-Jira-Reviewer checklist),Custom field (Triage-Jira-Reviewer checklist),Custom field (Type),Custom field (Type of Architecture Change Requested),Custom field (Type of Deliverable Other),Custom field (Type of Employee),Custom field (Type of Incident),Custom field (Type of Request),Custom field (Type of Service),Custom field (Type of Traiing),Custom field (Tyson Corner),Custom field (UAT Owner),Custom field (UAT Start Date),Custom field (UAT Support Effort),Custom field (URL),Custom field (UX Mocks),Custom field (UX-Design link),Custom field (Unit Testing),Custom field (Unplanned Work? (Y/N)),Custom field (Unplanned Work? (Y/N)),Custom field (Unsuccessful),Custom field (Update Section),Custom field (Update Section),Custom field (Updated by users count),Custom field (Upgrade Issue),Custom field (Urgency),Custom field (Urgency),Custom field (Usage Scope),Custom field (Use Case),Custom field (User Comments),Custom field (User Story),Custom field (VE Ratio),Custom field (VEX),Custom field (VEX Justification),Custom field (VEX Status),Custom field (VIA Stage1),Custom field (VPat Due Date),Custom field (Validation Complete),Custom field (Value),Custom field (Value),Custom field (Value),Custom field (Value),Custom field (Value),Custom field (Value),Custom field (Value),Custom field (Value),Custom field (Vancouver Room Names),Custom field (Vehicles),Custom field (Velocity),Custom field (Vendor API),Custom field (Vendor Case ID),Custom field (Vendor Infrastructure with Admin Access),Custom field (Vendor Issue ID),Custom field (Vendor Name),Custom field (Vendor POC),Custom field (Version Type),Custom field (Vertical),Custom field (VoC Guild),Custom field (Vote),Custom field (Vuln Comments),Custom field (Vuln Description),Custom field (Vulnerability),Custom field (Vulnerability Overview),Custom field (Vulnerability Type),Custom field (Vulnerable Domain),Custom field (Vulnerable Parameter),Custom field (Vulnerable URL),Custom field (Waiting For Customer),Custom field (Waiting for Team),Custom field (Webex Space Link),Custom field (Website),Custom field (Weight),Custom field (What Do We Need to Improve?),Custom field (What We Did Well?),Custom field (What happened?),"Custom field (What is being cancelled? (Full stack, product))",Custom field (What is the OnDemand Entitlement Number?),Custom field (What is the change to be delivered to our customers?),Custom field (What is the customer benefit or business purpose of this change?),Custom field (What other prioritized release projects or initiatives are affected by or related to this delivery?),Custom field (What related projects/programs are interdependent with this project? Please include any dependencies outside of IT that ),Custom field (What troubleshooting has been performed?),Custom field (What type of GTM initiative is the request?),"Custom field (What would have, or did help you?)",Custom field (What's your request?),Custom field (Which Jira project needs assistance?),Custom field (Which project or repository do you need to access?),Custom field (Who Found),Custom field (Why is it being cancelled?),Custom field (Will provider contact any government officials (“lobbyists”) on our behalf (Y/N)),Custom field (Will the provider or their personnel access or use Splunk customer or employee data (Y/N)),Custom field (Will the provider or their personnel access or use any Splunk systems (Y/N)),Custom field (Work Done Date),Custom field (Work Done by? Full Name),Custom field (Work Effort),Custom field (Work Phase),Custom field (Work Type),Custom field (Work category),Custom field (Workaround),Custom field (Workflow Engine Instance),Custom field (Workfront Task ID),Custom field (XRDR),Custom field (Xray Begin Date),Custom field (Xray End Date),Custom field (Xray Revision),"Custom field (Your first, last name)",Custom field (Zendesk Ticket),Custom field (Zoom ID),Custom field ([CHART] Date of First Response),Custom field ([CHART] Date of First Response (migrated)),Custom field ([Deprecated] SLA breach duration),Custom field (account_ref_id),Custom field (cc email),Custom field (check_id),Custom field (coa_automation_input),Custom field (developer),Custom field (developer)Id,Custom field (resolution),Custom field (smartsheet-requester-email),Custom field (sourcetype),Custom field (time spent on project),Custom field (uuid),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Parent,Parent key,Parent summary,Status Category,Status Category Changed
[CLONE] [sustain/splunk-10.0] MachineTypeFilter doesn't work when DS works in clustered mode - applications are getting wrongfully updated into clients when performing OS filtering in serverclass,SPL-280187,4012537,Bug,Resolved,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,srv- jira-backportbot,6421d6766b29c052ab2eddb1,srv- jira-backportbot,6421d6766b29c052ab2eddb1,30/Jun/25 9:54 AM,08/Sep/25 1:30 AM,24/Sep/25 8:09 AM,01/Jul/25 1:36 AM,10.0.x,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0.x,,,,,,,,Deployment Server,,,,,0,Backport_Approved,bis_release_notes_10.0.2503.2,dwest_reviewed,emea_support:approved,SP:Not-DP,support_reviewed,,This ticket is a backport of [https://splunk.atlassian.net/browse/SPL-270345|https://splunk.atlassian.net/browse/SPL-270345|smart-link],,Automation for Jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:f58131cb-b67d-43c7-b30d-6b58d40bd077,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-270345,,,,,,,,,,,,,,,,,,,,30/Jun/25 9:54 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250205-150919.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249159,30/Jun/25 9:54 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250205-150927.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249160,30/Jun/25 9:54 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250212-170017.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249161,30/Jun/25 9:54 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250326-170510.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@32616a29,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,7344000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 

What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,Yes,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tue Jul 01 08:35:49 UTC 2025,true,addon_com.servicerocket.jira.salesforce(addon_com.servicerocket.jira.salesforce),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Agent Management,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,4.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,"Pre-conditions (If any):

Steps to reproduce:
1. 
2. 

Expected:

Actual:

Browser:
OS:",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,33262a15-2583-4549-9d35-9080747b998e(33262a15-2583-4549-9d35-9080747b998e),addon_com.codebarrel.addons.automation(addon_com.codebarrel.addons.automation),d6fe2abf-ede8-4562-8f6d-7e2af3554d00(d6fe2abf-ede8-4562-8f6d-7e2af3554d00),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Completing this form allows the Product Compliance team to review GTM or internal initiatives in regulated markets such as Healthcare, Financial Services, or Government. See go/compliance for more information. After the review, you will receive documented milestones and requirements, including any steps for external audit preparation (if applicable).

*Who Should Complete*: PMs releasing new products, features, or initiatives that require compliance validation for regulated markets. 

*When to Complete*: Complete after submitting the product security intake review and link product security ticket (PRODSECOPS)  below. This review must be completed within the last 30 days and cover the entire scope of the product or feature to be deployed.

*Instructions*

⚠️ Please note: Updating any fields in this tab after submission will not automatically update the linked FPMDA ticket, contact  #product-compliance for changes post submission. 

⚠️ Please note: When you select 'Yes' to submit for the Product Compliance Review, OneJira will automatically link your Product Compliance Intake ticket to the RDMP after successful generation. If the submission is rejected, the field will revert to 'No,' with a remediation comment added to the RDMP.",,,,,,,,,,,,,,,,,,,,GDI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,,,,,,,,,,,,No,,,,,,0|ifu2lv:,,,,,,,,,,,None,,None,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Rated,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vodafone Portugal,3811317,012400000005WzCAAU,P2,No,500KW00001tk5vuYAA,,Open,Customer Update,Standard,Tier4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev1-Blocker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2025-07-01 08:35:49.149,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_56534065,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2025-07-01 08:35:49.149,2025-07-01 08:35:49.149,,,,,,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,,,,,,"01/Jul/25 1:35 AM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:712020:a5230362-acfe-487f-9235-36c30ff00ae3] since you have selected to document this in the release notes, please ensure that the Jira's title is clearly written and hide any internal terms or code names in the title by surrounding them with square brackets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3850786,DMXAM-1300,DS Maintenance ,Done,01/Jul/25 1:36 AM
[CLONE] [sustain/europium] MachineTypeFilter doesn't work when DS works in clustered mode - applications are getting wrongfully updated into clients when performing OS filtering in serverclass,SPL-280186,4012535,Bug,Resolved,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,srv- jira-backportbot,6421d6766b29c052ab2eddb1,srv- jira-backportbot,6421d6766b29c052ab2eddb1,30/Jun/25 9:54 AM,01/Jul/25 1:44 AM,,01/Jul/25 1:37 AM,10.0.x,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.4.4,,,,,,,,Deployment Server,,,,,0,Backport_Approved,dwest_reviewed,emea_support:approved,SP:Not-DP,support_reviewed,,,This ticket is a backport of [https://splunk.atlassian.net/browse/SPL-270345|https://splunk.atlassian.net/browse/SPL-270345|smart-link],,Automation for Jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:f58131cb-b67d-43c7-b30d-6b58d40bd077,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-270345,,,,,,,,,,,,,,,,,,,,30/Jun/25 9:54 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250205-150919.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249153,30/Jun/25 9:54 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250205-150927.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249154,30/Jun/25 9:54 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250212-170017.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249155,30/Jun/25 9:54 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250326-170510.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249158,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@b2d49d4,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,7344000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 

What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,Yes,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tue Jul 01 08:37:02 UTC 2025,true,33262a15-2583-4549-9d35-9080747b998e(33262a15-2583-4549-9d35-9080747b998e),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Agent Management,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,4.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,"Pre-conditions (If any):

Steps to reproduce:
1. 
2. 

Expected:

Actual:

Browser:
OS:",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,33262a15-2583-4549-9d35-9080747b998e(33262a15-2583-4549-9d35-9080747b998e),addon_com.codebarrel.addons.automation(addon_com.codebarrel.addons.automation),d6fe2abf-ede8-4562-8f6d-7e2af3554d00(d6fe2abf-ede8-4562-8f6d-7e2af3554d00),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Completing this form allows the Product Compliance team to review GTM or internal initiatives in regulated markets such as Healthcare, Financial Services, or Government. See go/compliance for more information. After the review, you will receive documented milestones and requirements, including any steps for external audit preparation (if applicable).

*Who Should Complete*: PMs releasing new products, features, or initiatives that require compliance validation for regulated markets. 

*When to Complete*: Complete after submitting the product security intake review and link product security ticket (PRODSECOPS)  below. This review must be completed within the last 30 days and cover the entire scope of the product or feature to be deployed.

*Instructions*

⚠️ Please note: Updating any fields in this tab after submission will not automatically update the linked FPMDA ticket, contact  #product-compliance for changes post submission. 

⚠️ Please note: When you select 'Yes' to submit for the Product Compliance Review, OneJira will automatically link your Product Compliance Intake ticket to the RDMP after successful generation. If the submission is rejected, the field will revert to 'No,' with a remediation comment added to the RDMP.",,,,,,,,,,,,,,,,,,,,GDI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,,,,,,,,,,,,No,,,,,,0|ifu2ln:,,,,,,,,,,,None,,None,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Rated,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,RAITEC GmbH,3641593,012400000005WzMAAU,P2,Yes,5005a0000316BHFAA2,,Closed,Resolved - Work Around,Standard,Tier4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev1-Blocker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2025-07-01 08:37:02.082,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_56594726,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2025-07-01 08:37:02.082,2025-07-01 08:37:02.082,,,,,,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,,,,,,"01/Jul/25 1:37 AM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:712020:a5230362-acfe-487f-9235-36c30ff00ae3] since you have selected to document this in the release notes, please ensure that the Jira's title is clearly written and hide any internal terms or code names in the title by surrounding them with square brackets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3850786,DMXAM-1300,DS Maintenance ,Done,01/Jul/25 1:37 AM
[CLONE] [sustain/duranium] MachineTypeFilter doesn't work when DS works in clustered mode - applications are getting wrongfully updated into clients when performing OS filtering in serverclass,SPL-280185,4012534,Bug,Resolved,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,srv- jira-backportbot,6421d6766b29c052ab2eddb1,srv- jira-backportbot,6421d6766b29c052ab2eddb1,30/Jun/25 9:53 AM,01/Jul/25 1:44 AM,,01/Jul/25 1:38 AM,10.0.x,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.3.6,,,,,,,,Deployment Server,,,,,0,Backport_Approved,dwest_reviewed,emea_support:approved,SP:Not-DP,support_reviewed,,,This ticket is a backport of [https://splunk.atlassian.net/browse/SPL-270345|https://splunk.atlassian.net/browse/SPL-270345|smart-link],,Automation for Jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:f58131cb-b67d-43c7-b30d-6b58d40bd077,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-270345,,,,,,,,,,,,,,,,,,,,30/Jun/25 9:53 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250205-150919.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249149,30/Jun/25 9:54 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250205-150927.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249150,30/Jun/25 9:54 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250212-170017.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249151,30/Jun/25 9:54 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250326-170510.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249152,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@44bc4dcc,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,7344000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 

What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,Yes,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tue Jul 01 08:37:56 UTC 2025,true,33262a15-2583-4549-9d35-9080747b998e(33262a15-2583-4549-9d35-9080747b998e),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Agent Management,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,4.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,"Pre-conditions (If any):

Steps to reproduce:
1. 
2. 

Expected:

Actual:

Browser:
OS:",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,33262a15-2583-4549-9d35-9080747b998e(33262a15-2583-4549-9d35-9080747b998e),addon_com.codebarrel.addons.automation(addon_com.codebarrel.addons.automation),d6fe2abf-ede8-4562-8f6d-7e2af3554d00(d6fe2abf-ede8-4562-8f6d-7e2af3554d00),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Completing this form allows the Product Compliance team to review GTM or internal initiatives in regulated markets such as Healthcare, Financial Services, or Government. See go/compliance for more information. After the review, you will receive documented milestones and requirements, including any steps for external audit preparation (if applicable).

*Who Should Complete*: PMs releasing new products, features, or initiatives that require compliance validation for regulated markets. 

*When to Complete*: Complete after submitting the product security intake review and link product security ticket (PRODSECOPS)  below. This review must be completed within the last 30 days and cover the entire scope of the product or feature to be deployed.

*Instructions*

⚠️ Please note: Updating any fields in this tab after submission will not automatically update the linked FPMDA ticket, contact  #product-compliance for changes post submission. 

⚠️ Please note: When you select 'Yes' to submit for the Product Compliance Review, OneJira will automatically link your Product Compliance Intake ticket to the RDMP after successful generation. If the submission is rejected, the field will revert to 'No,' with a remediation comment added to the RDMP.",,,,,,,,,,,,,,,,,,,,GDI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,,,,,,,,,,,,No,,,,,,0|ifu2lf:,,,,,,,,,,,None,,None,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Rated,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,RAITEC GmbH,3641593,012400000005WzMAAU,P2,Yes,5005a0000316BHFAA2,,Closed,Resolved - Work Around,Standard,Tier4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev1-Blocker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2025-07-01 08:37:56.595,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_56654422,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2025-07-01 08:37:56.595,2025-07-01 08:37:56.595,,,,,,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,,,,,,"01/Jul/25 1:37 AM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:712020:a5230362-acfe-487f-9235-36c30ff00ae3] since you have selected to document this in the release notes, please ensure that the Jira's title is clearly written and hide any internal terms or code names in the title by surrounding them with square brackets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3850786,DMXAM-1300,DS Maintenance ,Done,01/Jul/25 1:38 AM
[CLONE] [sustain/cobalt] MachineTypeFilter doesn't work when DS works in clustered mode - applications are getting wrongfully updated into clients when performing OS filtering in serverclass,SPL-280184,4012533,Bug,Resolved,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,srv- jira-backportbot,6421d6766b29c052ab2eddb1,srv- jira-backportbot,6421d6766b29c052ab2eddb1,30/Jun/25 9:53 AM,01/Jul/25 5:26 AM,,01/Jul/25 5:26 AM,10.0.x,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.2.8,,,,,,,,Deployment Server,,,,,0,Backport_Approved,dwest_reviewed,emea_support:approved,SP:Not-DP,support_reviewed,,,This ticket is a backport of [https://splunk.atlassian.net/browse/SPL-270345|https://splunk.atlassian.net/browse/SPL-270345|smart-link],,Automation for Jira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:f58131cb-b67d-43c7-b30d-6b58d40bd077,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-270345,,,,,,,,,,,,,,,,,,,,30/Jun/25 9:53 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250205-150919.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249145,30/Jun/25 9:53 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250205-150927.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249146,30/Jun/25 9:53 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250212-170017.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249147,30/Jun/25 9:53 AM;d6fe2abf-ede8-4562-8f6d-7e2af3554d00;image-20250326-170510.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8249148,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@540e2276,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,7344000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 

What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,Yes,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tue Jul 01 12:26:32 UTC 2025,true,addon_com.innovalog.jmwe.jira-misc-workflow-extensions(addon_com.innovalog.jmwe.jira-misc-workflow-extensions),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Agent Management,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,4.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,"Pre-conditions (If any):

Steps to reproduce:
1. 
2. 

Expected:

Actual:

Browser:
OS:",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,33262a15-2583-4549-9d35-9080747b998e(33262a15-2583-4549-9d35-9080747b998e),addon_com.codebarrel.addons.automation(addon_com.codebarrel.addons.automation),d6fe2abf-ede8-4562-8f6d-7e2af3554d00(d6fe2abf-ede8-4562-8f6d-7e2af3554d00),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Completing this form allows the Product Compliance team to review GTM or internal initiatives in regulated markets such as Healthcare, Financial Services, or Government. See go/compliance for more information. After the review, you will receive documented milestones and requirements, including any steps for external audit preparation (if applicable).

*Who Should Complete*: PMs releasing new products, features, or initiatives that require compliance validation for regulated markets. 

*When to Complete*: Complete after submitting the product security intake review and link product security ticket (PRODSECOPS)  below. This review must be completed within the last 30 days and cover the entire scope of the product or feature to be deployed.

*Instructions*

⚠️ Please note: Updating any fields in this tab after submission will not automatically update the linked FPMDA ticket, contact  #product-compliance for changes post submission. 

⚠️ Please note: When you select 'Yes' to submit for the Product Compliance Review, OneJira will automatically link your Product Compliance Intake ticket to the RDMP after successful generation. If the submission is rejected, the field will revert to 'No,' with a remediation comment added to the RDMP.",,,,,,,,,,,,,,,,,,,,GDI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,,,,,,,,,,,,No,,,,,,0|ifu2l7:,,,,,,,,,,,None,,None,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Rated,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,RAITEC GmbH,3641593,012400000005WzMAAU,P2,Yes,5005a0000316BHFAA2,,Closed,Resolved - Work Around,Standard,Tier4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev1-Blocker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2025-07-01 12:26:32.452,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_70371996,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2025-07-01 12:26:32.452,2025-07-01 12:26:32.452,,,,,,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,,,,,,"01/Jul/25 5:26 AM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:712020:a5230362-acfe-487f-9235-36c30ff00ae3] since you have selected to document this in the release notes, please ensure that the Jira's title is clearly written and hide any internal terms or code names in the title by surrounding them with square brackets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3850786,DMXAM-1300,DS Maintenance ,Done,01/Jul/25 5:26 AM
"After upgrade to 9.3.0+, UFs are reporting 'Failed to download app', while DS reports 'app does not exist on the server’. This issue does not appear if you configure the environment in such a way that each app is assigned to a maximum of one server class",SPL-279750,4000870,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Fixed,Beniamin Barret,712020:c45d9c5f-3c5e-46fd-8e0a-fef5e24f4dad,Piotr Lenarczyk,712020:38f52430-cea6-4dec-8d56-45a83045e998,Piotr Lenarczyk,712020:38f52430-cea6-4dec-8d56-45a83045e998,23/Jun/25 6:14 AM,04/Sep/25 12:29 PM,,31/Jul/25 5:05 AM,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.4.0(Europium),9.4.1,9.4.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deployment Server,,,,,0,,,,,,,,"After Splunk upgrade (both Deployment Servers and UFs) from 9.2.2 to 9.3.1, the deployment of apps using their Deployment Server is not working properly - Universal Forwarders are reporting ‘'Failed to download app'’, while Deployment Server is reporting that app ‘does not exist on the server’.

The problem happens with a couple of apps and a couple of forwarders - not all the apps and all the forwarders are affected, but different apps are affected with different forwarders. The issue is present with untouched apps from Splunk Base, modified apps from Splunk Base and also custom written apps just containing inputs.conf.



Customer has two different infrastructures with separate clients and splunk environment.
The other infrastructure is having the exact same behavior,



Deployment Server is showing following messages:

{noformat}03-28-2025 10:28:33.953 +0100 WARN  FileDownloadRestHandler [861717 TcpChannelThread] - File: default:spl_hf_01:rosen_hf_routing does not exist on the server. FQ name= /opt/splunk/var/run/tmp/_global_bundles/rosen_hf_routing-1743074739.bundle{noformat}



While forwarders are showing:

{noformat}03-28-2025 10:25:49.133 +0100 ERROR DeployedServerclass [926172 HttpClientPollingThread_41B7AE41-1551-4B65-BB65-874B2D06DB51] - name=spl_hf_01 Failed to download app=Splunk_TA_windows_901{noformat}



The Deployment Server log indicates that the app ""does not exist on the server"", and specify the .bundle files in '/opt/splunk/var/run/tmp/_global_bundles/' - while it was confirmed that the .bundle file is present and the permissions are set correctly.



Following changes were already tested:

* changed restartSplunkd to 1, help for some time, but issue reappeared after a couple of days
* upgraded to 9.3.3, which also helped for a couple of days, but then the issue reappeared again
* shutdown DS, clear '/opt/splunk/var/run/tmp/_global_bundles/', start DS (this fixes the issue for a short time to publish required apps on short notice.)



Additionally, the serverclass.conf on DS was checked for correct naming, folder permissions on DS were verified, new apps and server classed were tested.



Diags with debug logging enabled:

[Heavy Forwarder|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0010b00001si66aaas&job_idtok=210de0aa-3e39-446e-baf7-77fa973060de&case_numbertok=3674731&hosttok=linuxsplhfprd03.roseninspection.net&case_filetok=linuxsplhfprd03.roseninspection.net-hf_-20250423-205248-HHL1D27M&found_anontok=0&job_typetok=classic&form.mount.tok=raw&form.mount.filter=&form.network.tok=raw&form.network.filter=&form.interfaces.filter=&form.cpuInfo.tok=raw&form.cpuInfo.filter=&form.processes.tok=raw&form.processes.filter=&form.ulimit.tok=raw&form.ulimit.filter=&form.memory.tok=raw&form.memory.filter=&form.kvstore.filter=&form.searchPeerBundlesDir.filter=&form.sinkholeDir.filter=&form.authDir.filter=]

[Deployment Server|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0010b00001si66aaas&job_idtok=2812d714-e0bf-4312-861e-a55b7f22e518&case_numbertok=3674731&hosttok=linuxspldsprd01.roseninspection.net&case_filetok=linuxspldsprd01.roseninspection.net-sh_ds_-20250423-205319-KD6sQfBo&found_anontok=0&job_typetok=classic&form.mount.tok=raw&form.mount.filter=&form.network.tok=raw&form.network.filter=&form.interfaces.filter=&form.cpuInfo.tok=raw&form.cpuInfo.filter=&form.processes.tok=raw&form.processes.filter=&form.ulimit.tok=raw&form.ulimit.filter=&form.memory.tok=raw&form.memory.filter=&form.kvstore.filter=&form.searchPeerBundlesDir.filter=&form.sinkholeDir.filter=&form.authDir.filter=]



Test scenario present in the logs:

# Change stateOnClient = noop on serverclass level at 10:31 28/03/2025
# Reload serverclass at 10:31:45 28/03/2025
# Confirmed that the issue is not present then.
# Disabled stateOnClient = noop at 10:45:00 28/03/2025",,Adarsh KR (C),Jakub Sotwin,Monika Małgorzata Mrozek,Piotr Lenarczyk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,712020:fa2809c5-7f19-421d-8f3f-fedb3f09adc3,712020:7b3a83ef-896c-42c1-a06b-913b64551248,712020:55814e95-0f05-461b-8c63-3c719714fef2,712020:38f52430-cea6-4dec-8d56-45a83045e998,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,SPL-275509,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@358ca2ca,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,3628800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 

What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives

List of individuals to include in a review of this incident

h1. Dependent Service Incident ID

(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers

h1. What Happened?

h1. What Did We Do Well?

h1. What Do We Need to Improve?

h1. Dogfood Our Own Observability Products

||*Product*||*Use*||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.

h3. Please share feedback about our products during this incident.

h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items

JIRAs should be linked to this ticket",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wed Aug 13 07:11:45 UTC 2025,true,addon_com.servicerocket.jira.salesforce(addon_com.servicerocket.jira.salesforce),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Agent Management,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,d6850ade-7208-4068-abf3-ba401b4d73f6(d6850ade-7208-4068-abf3-ba401b4d73f6),dec57fe5-8893-45a6-8870-4208fb08e962(dec57fe5-8893-45a6-8870-4208fb08e962),cfd3c2d6-52bd-406d-b760-8452b7f7693b(cfd3c2d6-52bd-406d-b760-8452b7f7693b),61fc840d-d56a-4d98-babe-0f20172e8165(61fc840d-d56a-4d98-babe-0f20172e8165),b89bb2ff-4575-4335-ad90-f2b5b6c085a7(b89bb2ff-4575-4335-ad90-f2b5b6c085a7),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Completing this form allows the Product Compliance team to review GTM or internal initiatives in regulated markets such as Healthcare, Financial Services, or Government. See go/compliance for more information. After the review, you will receive documented milestones and requirements, including any steps for external audit preparation (if applicable).

*Who Should Complete*: PMs releasing new products, features, or initiatives that require compliance validation for regulated markets. 

*When to Complete*: Complete after submitting the product security intake review and link product security ticket (PRODSECOPS)  below. This review must be completed within the last 30 days and cover the entire scope of the product or feature to be deployed.

*Instructions*

⚠️ Please note: Updating any fields in this tab after submission will not automatically update the linked FPMDA ticket, contact  #product-compliance for changes post submission. 

⚠️ Please note: When you select 'Yes' to submit for the Product Compliance Review, OneJira will automatically link your Product Compliance Intake ticket to the RDMP after successful generation. If the submission is rejected, the field will revert to 'No,' with a remediation comment added to the RDMP.",,,,,,,,,,,,,,,,,,,,GDI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,No,,,,,,0|ifwapc:y,,,,,,,,,,,None,,None,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Rated,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,State of Missouri Office of Administration,3841869,012400000005WzCAAU,P3,No,500KW00001w1SFoYAM,,Open,Waiting on Customer,Standard,Tier4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3674731,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FY25 Q4 S18,FY25 Q4 S20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not able to reproduce.,,,,,,,,,,,,,,,,,,,,,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 [https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs|https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs]”?

*  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 [https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs|https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs]”?

*  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data [https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection|https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection]

*  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

*  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 

* What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.",,,Standard,,,,,,,,,,,,,,,,2025-06-24 09:27:31.569,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1_*:*_1_*:*_120626_*|*_3_*:*_1_*:*_0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Beniamin Barret,,,,,,,,,,,,,,,2025-06-24 09:27:31.569,2025-06-24 09:27:31.569,,,,,,Beniamin Barret,712020:c45d9c5f-3c5e-46fd-8e0a-fef5e24f4dad,,,,,,"24/Jun/25 2:27 AM;712020:55814e95-0f05-461b-8c63-3c719714fef2;Original ticket title: [After upgrade to 9.3.1, Universal Forwarders are reporting 'Failed to download app', while Deployment Server is reporting that app ‘does not exist on the server’] ","31/Jul/25 5:05 AM;712020:7b3a83ef-896c-42c1-a06b-913b64551248;*Closing this issue.  This issue is in fact Duplicate of* [https://splunk.atlassian.net/browse/SPL-275509|https://splunk.atlassian.net/browse/SPL-275509|smart-link] *and for this issue* 
The fix has been delivered in *SE 10.0*, which is already *GA*.
For the *Europium* release, the fix will be included in *version 9.4.5*. While the exact GA date for 9.4.5 is not yet confirmed, we expect it to be approximately *six weeks after the 9.4.4 GA*, so likely around *mid-September*.","12/Aug/25 3:10 AM;712020:fa2809c5-7f19-421d-8f3f-fedb3f09adc3;Hi [~accountid:712020:c45d9c5f-3c5e-46fd-8e0a-fef5e24f4dad] [~accountid:712020:7b3a83ef-896c-42c1-a06b-913b64551248] 

I have a case *3824473* where the customer, after upgrading the deployment server to *9.3.3*, is experiencing a similar issue. We informed the customer that it is a known bug and that it will be fixed in *10.0.0* and *9.4.5*.

However, upon reviewing the documentation, I found that the SPL is documented in *9.3.3* and up to *9.4.2*, but it is not documented in the known issues for *9.4.3* and *9.4.4*, nor is it included in the fix issues document.

Now, the customer wants confirmation on whether this issue is still present in *9.4.3* and *9.4.4* or if it has been fixed in these versions. Since its not documented. ",12/Aug/25 2:40 PM;712020:7b3a83ef-896c-42c1-a06b-913b64551248;[~accountid:712020:fa2809c5-7f19-421d-8f3f-fedb3f09adc3] It doc error. This issie is present as you can expect cont till fix release number. ,13/Aug/25 12:11 AM;712020:fa2809c5-7f19-421d-8f3f-fedb3f09adc3;[~accountid:712020:7b3a83ef-896c-42c1-a06b-913b64551248] Thank you for your response ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3850786,DMXAM-1300,DS Maintenance ,Done,31/Jul/25 5:05 AM
"Distributed search may lose information about bundles from cluster peers on new search generations, removes peers when best-effort mode is enabled.",SPL-279657,3998268,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P1-Immediate,Fixed,Iuri Chaer,6053a60f94d7b90069f835a9,Iuri Chaer,6053a60f94d7b90069f835a9,Iuri Chaer,6053a60f94d7b90069f835a9,19/Jun/25 4:06 PM,01/Aug/25 8:38 AM,,30/Jun/25 3:26 PM,10.0.x,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.4.0(Europium),9.4.1,9.4.2,9.4.3,Nutella,Oreo,Pocky,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0.x,9.2.8,9.3.6,9.4.4,develop,Nutella(9.2.2406.107),Oreo,Pocky,Distributed Search,,,,,0,Backport_Approved,,,,,,,"When we add cluster peers after a search generation change, we try to prime their information (including available bundles) before exposing the information to search. [https://splunk.atlassian.net/browse/SPL-216146|https://splunk.atlassian.net/browse/SPL-216146|smart-link] introduced a regression through which, by mistake, that information is being exposed prematurely, with no bundles available. This will cause a short pause in search requests when best-effort mode is disabled, as the search head blocks search while it refreshes its bundle information, but with best-effort enabled we’ll see incomplete results and warnings straight away.

In the codebase, what happens is that {{DistributedPeerManager::prepClusterPeersForPotentialAdd()}}relies on its local {{HeartbeatTransactionSynchronizer}} collecting peer information before it publishes it all, but [commit d6cf0fbdf2bd changed the auth transaction such that it will always update|https://cd.splunkdev.com/splcore/main/-/commit/d6cf0fbdf2bd#d700c0d60e4cdc6ba2d58fdd1287c87e00c09884_279_287] the {{DistributedPeerManager}} directly.

Found through code inspection while investigating [https://splunk.atlassian.net/browse/SPL-268481|https://splunk.atlassian.net/browse/SPL-268481|smart-link].",,Andrew Brown,Automation for Jira,Iuri Chaer,Michael Hodges,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,557058:f58131cb-b67d-43c7-b30d-6b58d40bd077,6053a60f94d7b90069f835a9,5af8dc3d3e69495598b91641,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,144000,0,,0%,144000,0,,,,,,,,,,,,,,,SPL-280226,SPL-280227,SPL-280228,SPL-280229,SPL-280230,SPL-280231,SPL-280232,,,,,,,,,SPL-281854,,,,,,,,,,,SPL-268481,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3b71c27e,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,4579200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Peter Brown,630c7d3dea661fd37d4e68fc,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,Fri Aug 01 15:38:23 UTC 2025,true,andrewb(andrewb),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,"Pre-conditions (If any):

Steps to reproduce:
1. 
2. 

Expected:

Actual:

Browser:
OS:",,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),addon_com.codebarrel.addons.automation(addon_com.codebarrel.addons.automation),ichaer(ichaer),mhodges(mhodges),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Completing this form allows the Product Compliance team to review GTM or internal initiatives in regulated markets such as Healthcare, Financial Services, or Government. See go/compliance for more information. After the review, you will receive documented milestones and requirements, including any steps for external audit preparation (if applicable).

*Who Should Complete*: PMs releasing new products, features, or initiatives that require compliance validation for regulated markets. 

*When to Complete*: Complete after submitting the product security intake review and link product security ticket (PRODSECOPS)  below. This review must be completed within the last 30 days and cover the entire scope of the product or feature to be deployed.

*Instructions*

⚠️ Please note: Updating any fields in this tab after submission will not automatically update the linked FPMDA ticket, contact  #product-compliance for changes post submission. 

⚠️ Please note: When you select 'Yes' to submit for the Product Compliance Review, OneJira will automatically link your Product Compliance Intake ticket to the RDMP after successful generation. If the submission is rejected, the field will revert to 'No,' with a remediation comment added to the RDMP.",,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Iuri Chaer,6053a60f94d7b90069f835a9,,,,,,,,,,,,No,,,,,,0|ifrzjn:,,,,,,,,,,,None,,None,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,Iuri Chaer,6053a60f94d7b90069f835a9,,,,,,,,,,,,,,,,,,,Not Rated,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev1-Blocker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Jun/25 12:00 AM,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,1.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2025-06-30 22:25:54.846,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_1_*:*_525032190_*|*_3_*:*_1_*:*_356755163_*|*_5_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_5177_*|*_10039_*:*_1_*:*_66187969,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2025-06-30 22:25:54.846,2025-06-30 22:25:54.846,,,,,,Iuri Chaer,6053a60f94d7b90069f835a9,,,,,,"20/Jun/25 10:31 AM;6053a60f94d7b90069f835a9;Just to provide a status update: I’ve got a code change to fix this, I’m just working on adding better test coverage before inviting reviewers.","27/Jun/25 1:14 PM;6053a60f94d7b90069f835a9;Another status update! I had all approvals just now, and then I got a conflict in a single file, {{src/framework/tests/CMakeLists.txt}}. You’d think that resolving that conflict would only reset the approval of the owner of that file, wouldn’t you? And you’d thing wrong 😃. It reset ALL THE APPROVALS! And some reviewers are now out, so I’m trying to find people to help get it in.","30/Jun/25 3:25 PM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:6053a60f94d7b90069f835a9] has submitted a backport request. Since no EM/PM Owner was defined, no additional notifications were sent. Please manually notify an approver.


","30/Jun/25 3:28 PM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:6053a60f94d7b90069f835a9] has submitted a backport request to [~accountid:630c7d3dea661fd37d4e68fc]


","01/Jul/25 2:34 AM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:630c7d3dea661fd37d4e68fc] has Approved the backport. approved my side. sorry for delay - i am on PTO. 

[~accountid:6053a60f94d7b90069f835a9], please proceed with the backport",17/Jul/25 9:37 AM;5af8dc3d3e69495598b91641;Hello [~accountid:6053a60f94d7b90069f835a9] ! What are the fix versions for this? Thanks!,"31/Jul/25 3:31 PM;6053a60f94d7b90069f835a9;[~accountid:5af8dc3d3e69495598b91641] , sorry, missed the notification for this comment. Each clone ticket will have the version for its corresponding maintenance release. This one, which I forgot to update, is the least interesting one, because it targets {{develop}}, the current development branch.","31/Jul/25 3:40 PM;6053a60f94d7b90069f835a9;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] , [~accountid:5d13efc9f327650c122e7289] : I noticed you’ve added more versions to the “affects versions” field: 9.2.8, 9.3.6, and 9.4.4. I’ve removed them because they are the versions targeted by SPL-280232, SPL-280231, and SPL-280230. Those versions are the first ones with the bugfix in their respective branches.","31/Jul/25 3:50 PM;6053a60f94d7b90069f835a9;Oh, I see now… the docs automation is updating this ticket because it’s marked with all the versions affected 😕. That’s… not ideal. I can see how it makes sense from the perspective of automation, but bugfixes for different versions are meant to be tracked by their own clone tickets… I guess you could argue that the “affects versions” should then be split across the clones, but that’s also a bit weird. I don’t know. I guess I’ll just copy the fix versions here. My philosophical question for today: Is it automation when it’s powered by human toil? 😬 ",01/Aug/25 8:38 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:6053a60f94d7b90069f835a9] thank you. We try to catch these situations when we run our bulk update job but sometimes we miss. Thanks for the corrective help,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,30/Jun/25 3:26 PM
KV store upgrade to server version 7.0 fails when SSL compression is not set to its default value.,SPL-278716,3979447,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Oleksandr Hladniev,5ace957301a2012a6c32308d,Peter Lin,6036b7abd416ea0070072354,Peter Lin,6036b7abd416ea0070072354,06/Jun/25 11:15 AM,18/Aug/25 2:19 PM,,24/Jun/25 9:18 AM,9.4.0(Europium),9.4.1,9.4.2,9.4.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.4.4,,,,,,,,KV Store,,,,,0,kvstore:cos:ssl_compression,kvstore:sx:mongo_7_upgrade_failed,kvstore:type:local,kvstore-oncall,reviewer-BrandonFernandez,support_reviewed,,"Notes: Customer is usually free 7am-3pm Central US time if Zoom session require, they are happy to jump on a call move around schedule if needed. 

Customer recently upgrade Splunk to 9.4.2 version. They have total of 3 members in this clustered environment. 

All three members have auto upgrade disabled

{{etc/system/local/server.conf kvstoreUpgradeOnStartupEnabled = false}}
Auto upgrade has been turn off which is good.

Checked migration.log
{{KVStore upgrade version file created: ""/opt/splunk/var/run/splunk/kvstore_upgrade/versionFile42"".}}
{{-> Currently used KVSTore version=4.2.17-linux-splunk-v4. Expected version=4.2 or version=7.0}}
{{-> Currently configured KVSTore database path=""/opt/splunk/var/lib/splunk/kvstore""}}
{{Active KVStore version upgrade precheck PASSED}}

Checked all certs: 
the active combination that is in use is:
{{sslRootCAPath = /etc/ssl/localhost.splunk._ca_chain.pem}}
{{sslPassword = DIAG_REDACTED_ENCRYPTED_PASSWORD}}
{{serverCert = /etc/ssl/localhost.splunk.crt+chain+key}}

Mongod.log showing 
2025-06-05T17:16:46.104Z I ELECTION [replexec-0] VoteRequester(term 76) failed to receive response from [splunkazure-shc-e5f79cb.prod-cus-01-az1.github.net|https://splunkazure-shc-e5f79cb.prod-cus-01-az1.github.net/]:8191: HostUnreachable: Error connecting to [splunkazure-shc-e5f79cb.prod-cus-01-az1.github.net|https://splunkazure-shc-e5f79cb.prod-cus-01-az1.github.net/]:8191 (10.132.0.233:8191) :: caused by :: compression disabled

Recommended customer to set 
{{[sslConfig] }}
{{useClientSSLCompression = true }}
{{SplunkdClientSSLCompression = true}}
 And run the manual upgrade again see if successful. 
Rolling restart done. 2 of the 3 nodes are fine, but one is saying
replicationStatus:Down
(All Screenshots attached on the JIRA)

All three latest diags after rolling restart and sslconfig set as true attach on the Jira. 
Cus-01-az1
[https://downloadsvc.splunk.com/download/support/06-06-2025/uploadsvc-16case3771316-06-06-2025-USER-0035a00003UUJwnAAH-diag-splunkazure-shc-e5f79cb.prod-cus-01-az1.github.net-2025-06-06_10-00-24.tar.gz|https://downloadsvc.splunk.com/download/support/06-06-2025/uploadsvc-16case3771316-06-06-2025-USER-0035a00003UUJwnAAH-diag-splunkazure-shc-e5f79cb.prod-cus-01-az1.github.net-2025-06-06_10-00-24.tar.gz] 
Cus-01-az2
[https://downloadsvc.splunk.com/download/support/06-06-2025/uploadsvc-67case3771316-06-06-2025-USER-0035a00003UUJwnAAH-diag-splunkazure-shc-068d198.prod-cus-01-az2.github.net-2025-06-06_10-00-26.tar.gz|https://downloadsvc.splunk.com/download/support/06-06-2025/uploadsvc-67case3771316-06-06-2025-USER-0035a00003UUJwnAAH-diag-splunkazure-shc-068d198.prod-cus-01-az2.github.net-2025-06-06_10-00-26.tar.gz]
Cus-01-az3
[https://downloadsvc.splunk.com/download/support/06-06-2025/uploadsvc-72case3771316-06-06-2025-USER-0035a00003UUJwnAAH-diag-splunkazure-shc-037f411.prod-cus-01-az3.github.net-2025-06-06_10-00-28.tar.gz|https://downloadsvc.splunk.com/download/support/06-06-2025/uploadsvc-72case3771316-06-06-2025-USER-0035a00003UUJwnAAH-diag-splunkazure-shc-037f411.prod-cus-01-az3.github.net-2025-06-06_10-00-28.tar.gz]

We would like the mongod team to help us identify why the upgrade is failing. ",,Brandon Fernandez,Brian Dreyer,Karina Uppal,Oleksandr Hladniev,Peter Lin,Steph Mills,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6036b887e2020c0070c2eba2,629f0e54954f50006fcb50ae,6053b41e66c8790068391c46,5ace957301a2012a6c32308d,6036b7abd416ea0070072354,557058:611ecd3d-c00a-44a3-b460-31db5af8ed89,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@48e5a7c1,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,3110400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mon Aug 18 21:19:58 UTC 2025,true,stephaniem(stephaniem),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Structured Storage - KVStore,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bfernandez(bfernandez),karinau(karinau),ohladniev(ohladniev),peterl(peterl),stephaniem(stephaniem),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Completing this form allows the Product Compliance team to review GTM or internal initiatives in regulated markets such as Healthcare, Financial Services, or Government. See go/compliance for more information. After the review, you will receive documented milestones and requirements, including any steps for external audit preparation (if applicable).

*Who Should Complete*: PMs releasing new products, features, or initiatives that require compliance validation for regulated markets. 

*When to Complete*: Complete after submitting the product security intake review and link product security ticket (PRODSECOPS)  below. This review must be completed within the last 30 days and cover the entire scope of the product or feature to be deployed.

*Instructions*

⚠️ Please note: Updating any fields in this tab after submission will not automatically update the linked FPMDA ticket, contact  #product-compliance for changes post submission. 

⚠️ Please note: When you select 'Yes' to submit for the Product Compliance Review, OneJira will automatically link your Product Compliance Intake ticket to the RDMP after successful generation. If the submission is rejected, the field will revert to 'No,' with a remediation comment added to the RDMP.",,,,,,,,,,,,,,,,,,Codefix,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,No,,,,,,0|iesonh:z5cv,,,,,,,,,,,None,,None,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Rated,,,,,,,,,,,,,,,,,,,,,,,,,,,,SSL compression fix was not in 9.4.x - changes have been merged,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Bank of Thailand,3759105,012400000005WzMAAU,P2,No,500KW00001oRzokYAC,,Closed,Resolved - Work Around,Standard,Tier3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KVStore-CiscoFY25Q4-S4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment
Run the Kvstore manual upgrade and return kvstore upgrade is not possible due to unhealthy kvstore replication state. 

• If available, will customer upgrade to fixed version?
Yes

• If support is able to reproduce, share the setup.
N/A",,,,,,,,,,0.5,,,,,,,,,,,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2025-06-09 16:09:49.475,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_1_*:*_0_*|*_1_*:*_1_*:*_251641608_*|*_3_*:*_1_*:*_33805656_*|*_16785_*:*_2_*:*_66580_*|*_12230_*:*_1_*:*_1873_*|*_10001_*:*_2_*:*_950650938,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Oleksandr Hladniev,,,,,"In the sslConfig stanza of the server.conf file, temporarily set allowSslCompression to true while you upgrade the KV store. You might also need to temporarily set SplunkdClientSSLCompression, allowSslCompression, and useSplunkdClientSSLCompression to true and useClientSSLCompression to false (their default settings) during this upgrade.",,,,,,,,,,2025-06-09 16:09:49.475,2025-06-09 16:09:49.475,,,,,,,,,,,,,09/Jun/25 9:09 AM;6036b887e2020c0070c2eba2;[~accountid:6036b7abd416ea0070072354] This has been reviewed and moved on,"12/Jun/25 8:03 AM;5ace957301a2012a6c32308d;Unfortunately  the SSL compression fix didn;t go to 9.4.2 and 9.4.3 so it will be in 9.4.4

I suggested to leave compression in defaults across the cluster - this should work

If Customer experiencing the out of sync node - this is not related to the upgrade and need to be handled by another kvstore shc out of sync runbook

so lets focus on that. ","13/Jun/25 11:19 AM;6036b7abd416ea0070072354;After customer change back SSLCompression config back to default settings.

SplunkdClientSSLCompression = true need to change to useSplunkdClientSSLCompression = true (default)
allowSslCompression = <boolean> * Default: true
useClientSSLCompression = <boolean> * Default: false
useSplunkdClientSSLCompression = <boolean> * Default: true

They are successful upgrade the Kvstore to version 7.0, they have another SHC will also require to performance the upgrade. We will keep the case and Jira open until next week after follow up with them. ",20/Jun/25 9:16 AM;6053b41e66c8790068391c46;[~accountid:5ace957301a2012a6c32308d] will merge the SSL compression fix into 9.4.4 and work with [~accountid:712020:7e34a4c0-1b40-4444-97da-310a77ee1b07] to update existing documentation to provide a workaround for previous releases.,18/Aug/25 2:19 PM;557058:611ecd3d-c00a-44a3-b460-31db5af8ed89;Changed to a bug type so it will show up in the release notes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3426109,SPL-254806,KVStore Team Active Customer/Security Issues,Done,24/Jun/25 9:18 AM
Release notes content unavailable in docs for some versions of universal forwarder,SPL-278604,3975069,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,Unknown,,,,Andrew Brown,557058:37b4518c-e757-440f-87b7-181f5f425e80,Andrew Brown,557058:37b4518c-e757-440f-87b7-181f5f425e80,04/Jun/25 4:15 PM,30/Jul/25 4:25 PM,,,10.0.x,9.1.10,9.1.9,9.2.6,9.2.7,9.2.8,9.3.4,9.3.5,9.3.6,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Universal Forwarder,,,,,0,,,,,,,,"With the migration from docs.splunk.com to help.splunk.com we need to manually recreate information about UF known and fixed issues for every maintenance release. This work is planned but will take some time. The identical information is available in the Splunk Enterprise known and fixed issues lists, so customers can access it there in the interim.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7b66c98c,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,9590400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,2025-06-04 23:15:31.364,true,adallas(adallas),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,"Pre-conditions (If any):

Steps to reproduce:
1. 
2. 

Expected:

Actual:

Browser:
OS:",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Completing this form allows the Product Compliance team to review GTM or internal initiatives in regulated markets such as Healthcare, Financial Services, or Government. See go/compliance for more information. After the review, you will receive documented milestones and requirements, including any steps for external audit preparation (if applicable).

*Who Should Complete*: PMs releasing new products, features, or initiatives that require compliance validation for regulated markets. 

*When to Complete*: Complete after submitting the product security intake review and link product security ticket (PRODSECOPS)  below. This review must be completed within the last 30 days and cover the entire scope of the product or feature to be deployed.

*Instructions*

⚠️ Please note: Updating any fields in this tab after submission will not automatically update the linked FPMDA ticket, contact  #product-compliance for changes post submission. 

⚠️ Please note: When you select 'Yes' to submit for the Product Compliance Review, OneJira will automatically link your Product Compliance Intake ticket to the RDMP after successful generation. If the submission is rejected, the field will revert to 'No,' with a remediation comment added to the RDMP.",,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,No,,,,,,0|ifom6z:,,,,,,,,,,,None,,None,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Rated,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Consult the ''Universal forwarder issues'' sections of the Splunk Enterprise known and fixed issues lists for the corresponding version. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,04/Jun/25 4:15 PM
The default sourcetype has 'NO_BINARY_CHECK' as 'true' on Web UI while it's not set in props.conf,SPL-277301,3949736,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Fixed,Szymon Borda,712020:3b430d36-1427-4570-8719-743fbfe49f3e,Hiroatsu Noboru,6036b7ddac6e4e0069e92a8b,Hiroatsu Noboru,6036b7ddac6e4e0069e92a8b,19/May/25 8:57 PM,10/Jul/25 8:11 AM,,04/Jul/25 4:25 PM,8.0.6,9.2.1,9.4.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.2.x,,,,,,,,UI - Misc,,,,,0,Reviewer-sylim,support-reviewed,,,,,,"h3. Summary :

My customer found that ‘NO_BINARY_CHECK’ is displayed as ‘true’ by default on Web UI for the default sourcetypes when they cloned the default sourcetype.

They found, however, there is no ‘NO_BINARY_CHECK’ setting in ‘props.conf’ for the default sourcetype whilst cloned one has the setting in ‘props.conf’.

h3. Customer Impact : 

Since the customer can set ‘NO_BINARY_CHECK' as 'true' for 'csv' if they’d like by either one of the below.

* Modify ‘props.conf’
* Open the sourcetype, ‘csv’, and click ‘Save’

h3. Description :

My customer cloned ‘csv' sourcetype as ‘csv_gear’ on Web UI by the following steps and confirmed both sourcetypes have ‘NO_BINARY_CHECK’ setting as ‘true’ on Web UI.

[Steps]

(1) 'Settings' -> 'Source types'
(2) Click 'Clone' for 'csv', and name it as 'csv_gear'



However, they found that the original sourcetype, ‘csv’ doesn’t have corresponding setting in ‘props.conf’.

||*Web UI*||*props.conf*||
|!csv_the_original_sourcetype.png|width=50%!|{noformat}#splunk btool props list --debug csv
/opt/splunk/etc/system/default/props.conf [csv]
/opt/splunk/etc/system/default/props.conf ADD_EXTRA_TIME_FIELDS = True
/opt/splunk/etc/system/default/props.conf ANNOTATE_PUNCT = True
/opt/splunk/etc/system/default/props.conf AUTO_KV_JSON = true
/opt/splunk/etc/system/default/props.conf BREAK_ONLY_BEFORE =
/opt/splunk/etc/system/default/props.conf BREAK_ONLY_BEFORE_DATE = True
/opt/splunk/etc/system/default/props.conf CHARSET = UTF-8
/opt/splunk/etc/system/default/props.conf DATETIME_CONFIG = /etc/datetime.xml
/opt/splunk/etc/system/default/props.conf DEPTH_LIMIT = 1000
/opt/splunk/etc/system/default/props.conf DETERMINE_TIMESTAMP_DATE_WITH_SYSTEM_TIME = false
/opt/splunk/etc/system/default/props.conf HEADER_MODE =
/opt/splunk/etc/system/default/props.conf INDEXED_EXTRACTIONS = csv
/opt/splunk/etc/system/default/props.conf KV_MODE = none
/opt/splunk/etc/system/default/props.conf LB_CHUNK_BREAKER_TRUNCATE = 2000000
/opt/splunk/etc/system/default/props.conf LEARN_MODEL = true
/opt/splunk/etc/system/default/props.conf LEARN_SOURCETYPE = true
/opt/splunk/etc/system/default/props.conf LINE_BREAKER_LOOKBEHIND = 100
/opt/splunk/etc/system/default/props.conf MATCH_LIMIT = 100000
/opt/splunk/etc/system/default/props.conf MAX_DAYS_AGO = 2000
/opt/splunk/etc/system/default/props.conf MAX_DAYS_HENCE = 2
/opt/splunk/etc/system/default/props.conf MAX_DIFF_SECS_AGO = 3600
/opt/splunk/etc/system/default/props.conf MAX_DIFF_SECS_HENCE = 604800
/opt/splunk/etc/system/default/props.conf MAX_EVENTS = 256
/opt/splunk/etc/system/default/props.conf MAX_EXPECTED_EVENT_LINES = 7
/opt/splunk/etc/system/default/props.conf MAX_TIMESTAMP_LOOKAHEAD = 128
/opt/splunk/etc/system/default/props.conf MUST_BREAK_AFTER =
/opt/splunk/etc/system/default/props.conf MUST_NOT_BREAK_AFTER =
/opt/splunk/etc/system/default/props.conf MUST_NOT_BREAK_BEFORE =
/opt/splunk/etc/system/default/props.conf SEGMENTATION = indexing
/opt/splunk/etc/system/default/props.conf SEGMENTATION-all = full
/opt/splunk/etc/system/default/props.conf SEGMENTATION-inner = inner
/opt/splunk/etc/system/default/props.conf SEGMENTATION-outer = outer
/opt/splunk/etc/system/default/props.conf SEGMENTATION-raw = none
/opt/splunk/etc/system/default/props.conf SEGMENTATION-standard = standard
/opt/splunk/etc/system/default/props.conf SHOULD_LINEMERGE = False
/opt/splunk/etc/system/default/props.conf TRANSFORMS =
/opt/splunk/etc/system/default/props.conf TRUNCATE = 10000
/opt/splunk/etc/system/default/props.conf category = Structured
/opt/splunk/etc/system/default/props.conf description = Comma-separated value format. Set header and other settings in ""Delimited Settings""
/opt/splunk/etc/system/default/props.conf detect_trailing_nulls = false
/opt/splunk/etc/system/default/props.conf maxDist = 100
/opt/splunk/etc/system/default/props.conf priority =
/opt/splunk/etc/system/default/props.conf pulldown_type = true
/opt/splunk/etc/system/default/props.conf sourcetype =
/opt/splunk/etc/system/default/props.conf termFrequencyWeightedDist = false
/opt/splunk/etc/system/default/props.conf unarchive_cmd_start_mode = shell{noformat}|
|!csv_gear_the_cloned_sourcetype-20250520-075326.png|width=587,height=580,alt=""csv_gear_the_cloned_sourcetype-20250520-075326.png""!|{noformat}#splunk btool props list --debug csv_gear

/opt/splunk/etc/system/local/props.conf   [csv_gear]
/opt/splunk/etc/system/default/props.conf ADD_EXTRA_TIME_FIELDS = True
/opt/splunk/etc/system/default/props.conf ANNOTATE_PUNCT = True
/opt/splunk/etc/system/default/props.conf AUTO_KV_JSON = true
/opt/splunk/etc/system/default/props.conf BREAK_ONLY_BEFORE =
/opt/splunk/etc/system/default/props.conf BREAK_ONLY_BEFORE_DATE = True
/opt/splunk/etc/system/default/props.conf CHARSET = UTF-8
/opt/splunk/etc/system/local/props.conf   DATETIME_CONFIG =
/opt/splunk/etc/system/default/props.conf DEPTH_LIMIT = 1000
/opt/splunk/etc/system/default/props.conf DETERMINE_TIMESTAMP_DATE_WITH_SYSTEM_TIME = false
/opt/splunk/etc/system/default/props.conf HEADER_MODE =
/opt/splunk/etc/system/local/props.conf   INDEXED_EXTRACTIONS = csv
/opt/splunk/etc/system/local/props.conf   KV_MODE = none
/opt/splunk/etc/system/default/props.conf LB_CHUNK_BREAKER_TRUNCATE = 2000000
/opt/splunk/etc/system/default/props.conf LEARN_MODEL = true
/opt/splunk/etc/system/default/props.conf LEARN_SOURCETYPE = true
/opt/splunk/etc/system/local/props.conf   LINE_BREAKER = ([\r\n]+)
/opt/splunk/etc/system/default/props.conf LINE_BREAKER_LOOKBEHIND = 100
/opt/splunk/etc/system/default/props.conf MATCH_LIMIT = 100000
/opt/splunk/etc/system/default/props.conf MAX_DAYS_AGO = 2000
/opt/splunk/etc/system/default/props.conf MAX_DAYS_HENCE = 2
/opt/splunk/etc/system/default/props.conf MAX_DIFF_SECS_AGO = 3600
/opt/splunk/etc/system/default/props.conf MAX_DIFF_SECS_HENCE = 604800
/opt/splunk/etc/system/default/props.conf MAX_EVENTS = 256
/opt/splunk/etc/system/default/props.conf MAX_EXPECTED_EVENT_LINES = 7
/opt/splunk/etc/system/default/props.conf MAX_TIMESTAMP_LOOKAHEAD = 128
/opt/splunk/etc/system/default/props.conf MUST_BREAK_AFTER =
/opt/splunk/etc/system/default/props.conf MUST_NOT_BREAK_AFTER =
/opt/splunk/etc/system/default/props.conf MUST_NOT_BREAK_BEFORE =
/opt/splunk/etc/system/local/props.conf   NO_BINARY_CHECK = true   <---
/opt/splunk/etc/system/default/props.conf SEGMENTATION = indexing
/opt/splunk/etc/system/default/props.conf SEGMENTATION-all = full
/opt/splunk/etc/system/default/props.conf SEGMENTATION-inner = inner
/opt/splunk/etc/system/default/props.conf SEGMENTATION-outer = outer
/opt/splunk/etc/system/default/props.conf SEGMENTATION-raw = none
/opt/splunk/etc/system/default/props.conf SEGMENTATION-standard = standard
/opt/splunk/etc/system/local/props.conf   SHOULD_LINEMERGE = false
/opt/splunk/etc/system/default/props.conf TRANSFORMS =
/opt/splunk/etc/system/default/props.conf TRUNCATE = 10000
/opt/splunk/etc/system/local/props.conf   category = Structured
/opt/splunk/etc/system/local/props.conf   description = Comma-separated value format. Set header and other settings in ""Delimited Settings""
/opt/splunk/etc/system/default/props.conf detect_trailing_nulls = false
/opt/splunk/etc/system/default/props.conf maxDist = 100
/opt/splunk/etc/system/default/props.conf priority =
/opt/splunk/etc/system/local/props.conf   pulldown_type = 1
/opt/splunk/etc/system/default/props.conf sourcetype =
/opt/splunk/etc/system/default/props.conf termFrequencyWeightedDist = false
/opt/splunk/etc/system/default/props.conf unarchive_cmd_start_mode = shell{noformat}|

h3. Quetions : 

Based on the behaviour, I think this is a cosmetic issue.
However, since this is confusing the customer, would you investigate and fix this if possible, please?



h3. Problem Analysis :

(1) Confirm this behaviour on the latest version, 9.4.2 and old version, 8.0.6 as well as the version the customer uses, 9.2.1.



(2) Confirmed that the default value for ‘NO_BINARY_CHECK’ is ‘false’ in the doc.

[https://docs.splunk.com/Documentation/Splunk/latest/Admin/Propsconf|https://docs.splunk.com/Documentation/Splunk/latest/Admin/Propsconf|smart-link] 

{noformat}NO_BINARY_CHECK = <boolean>
* When set to true, Splunk software processes binary files.
* Can only be used on the basis of [<sourcetype>], or [source::<source>],
  not [host::<host>].
* Default: false (binary files are ignored).
* This setting applies at input time, when data is first read by Splunk
  software, such as on a forwarder that has configured inputs acquiring the
  data.{noformat}



(3) Confirmed that the default setting, ‘false’, is applied to the original sourcetype, ‘csv’ as a binary file isn’t processed.

<inputs.conf>

{noformat}[monitor:///data/test*]
_rcvbuf = 1572864
disabled = false
index = main
sourcetype = csv{noformat}

<splunkd.log>

{noformat}05-20-2025 08:25:33.654 +0000 INFO  TailReader [18524 tailreader0] - Ignoring file '/data/testing' due to: binary{noformat}



(4) Checked ‘Network’ on ‘Developer tools’ when I clicked the sourcetype, ‘csv’, on ‘Source Types’, however I don’t see any.

!Developer_Tools.png|width=100%,alt=""Developer_Tools.png""!

(5) Also, I don’t see any related logs in ‘splunkd_access.log’ and 'web_access.log' when I clicked the sourcetype, ‘csv’ on ‘Source Types’.



(6) Found ‘NO_BINARY_CHECK’ in ‘common.js’ on Sources on ‘Source Types’ page.

    I don’t have any understanding about Java Script, but it looks like something is done around ‘transposeFromPropsToUI’ and 'transposeFromUIToProps'.

{noformat}                transposeFromPropsToUI: function(e) {
                    var n = {};
                    return !e || t.isEmpty(e) || (""CURRENT"" === e.DATETIME_CONFIG ? n[""ui.timestamp.mode""] = ""current"" : -1 !== String(e.DATETIME_CONFIG).indexOf("".xml"") && ""/etc/datetime.xml"" !== e.DATETIME_CONFIG ? (n[""ui.timestamp.mode""] = ""filename"",
                    n[""ui.timestamp.filename""] = e.DATETIME_CONFIG) : e.TIME_FORMAT || e.TIME_PREFIX || e.TZ || e.TIMESTAMP_FIELDS || e.MAX_TIMESTAMP_LOOKAHEAD && e.MAX_TIMESTAMP_LOOKAHEAD !== this.defaults[""ui.timestamp.lookahead""] ? (n[""ui.timestamp.mode""] = ""advanced"",
                    (e.TIME_FORMAT || """" === e.TIME_FORMAT) && (n[""ui.timestamp.format""] = e.TIME_FORMAT),
                    (e.TIMESTAMP_FIELDS || """" === e.TIMESTAMP_FIELDS) && (n[""ui.timestamp.fields""] = e.TIMESTAMP_FIELDS),
                    (e.TIME_PREFIX || """" === e.TIME_PREFIX) && (n[""ui.timestamp.prefix""] = e.TIME_PREFIX),
                    (e.TZ || """" === e.TZ) && (n[""ui.timestamp.timezone""] = e.TZ),
                    e.MAX_TIMESTAMP_LOOKAHEAD && e.MAX_TIMESTAMP_LOOKAHEAD !== this.defaults[""ui.timestamp.lookahead""] && (n[""ui.timestamp.lookahead""] = e.MAX_TIMESTAMP_LOOKAHEAD)) : n[""ui.timestamp.mode""] = ""auto"",
                    !1 === e.SHOULD_LINEMERGE || ""false"" === (e.SHOULD_LINEMERGE + """" || """").toLowerCase() ? e.LINE_BREAKER ? (n[""ui.eventbreak.mode""] = ""regex"",
                    n[""ui.eventbreak.regexmode""] = ""linebreaker"",
                    n[""ui.eventbreak.regex""] = e.LINE_BREAKER) : n[""ui.eventbreak.mode""] = ""everyline"" : e.BREAK_ONLY_BEFORE ? (n[""ui.eventbreak.mode""] = ""regex"",
                    n[""ui.eventbreak.regexmode""] = ""before"",
                    n[""ui.eventbreak.regex""] = e.BREAK_ONLY_BEFORE) : e.MUST_BREAK_AFTER ? (n[""ui.eventbreak.mode""] = ""regex"",
                    n[""ui.eventbreak.regexmode""] = ""after"",
                    n[""ui.eventbreak.regex""] = e.MUST_BREAK_AFTER) : n[""ui.eventbreak.mode""] = ""auto"",
                    ""PREAMBLE_REGEX""in e && (n[""ui.structured.preamble_pattern""] = e.PREAMBLE_REGEX),
                    ""FIELD_HEADER_REGEX""in e && (n[""ui.structured.header_line_prefix""] = e.FIELD_HEADER_REGEX,
                    t.isEmpty(e.FIELD_HEADER_REGEX) || (n[""ui.structured.header_mode""] = ""regex"")),
                    ""HEADER_FIELD_LINE_NUMBER""in e && (n[""ui.structured.header_line_number""] = e.HEADER_FIELD_LINE_NUMBER,
                    t.isEmpty(e.HEADER_FIELD_LINE_NUMBER) || (n[""ui.structured.header_mode""] = ""line"")),
                    ""HEADER_FIELD_DELIMITER""in e && (n[""ui.structured.header_field_delimiter""] = e.HEADER_FIELD_DELIMITER),
                    ""HEADER_FIELD_QUOTE""in e && (n[""ui.structured.header_field_quote""] = e.HEADER_FIELD_QUOTE),
                    ""FIELD_NAMES""in e && (n[""ui.structured.header_fields""] = e.FIELD_NAMES,
                    t.isEmpty(e.FIELD_NAMES) || (n[""ui.structured.header_mode""] = ""custom"")),
                    ""FIELD_DELIMITER""in e && (n[""ui.structured.event_field_delimiter""] = e.FIELD_DELIMITER),
                    ""FIELD_QUOTE""in e && (n[""ui.structured.event_field_quote""] = e.FIELD_QUOTE),
                    ""METRIC-SCHEMA-TRANSFORMS""in e && (n[""ui.metric_transforms.schema_name""] = e[""METRIC-SCHEMA-TRANSFORMS""]),
                    ""NO_BINARY_CHECK""in e && (n[""ui.misc.process_binary_files""] = e.NO_BINARY_CHECK),
                    e.INDEXED_EXTRACTIONS ? n[""ui.misc.previous_indexed_extractions""] = !0 : e[""TRANSFORMS-EXTRACT""] && (this.entry.content.set(""INDEXED_EXTRACTIONS"", e[""TRANSFORMS-EXTRACT""]),
                    n[""ui.misc.previous_indexed_extractions""] = !0)),
                    n
                },

                transposeFromUIToProps: function(e) {
                    var n = this
                      , i = {};
                    if (!e || t.isEmpty(e))
                        return i;
                    function r(e) {
                        var t = n.entry.content.get(e);
                        null != t && (i[e] = """")
                    }
                    function o() {
                        r(""TZ""),
                        r(""TIME_FORMAT""),
                        r(""TIME_PREFIX""),
                        r(""TIMESTAMP_FIELDS""),
                        n.defaults[""ui.timestamp.lookahead""] !== n.entry.content.get(""MAX_TIMESTAMP_LOOKAHEAD"") && r(""MAX_TIMESTAMP_LOOKAHEAD"")
                    }
                    if (""ui.timestamp.mode""in e && (""current"" === e[""ui.timestamp.mode""] ? (i.DATETIME_CONFIG = ""CURRENT"",
                    o()) : ""advanced"" === e[""ui.timestamp.mode""] ? (i.DATETIME_CONFIG = """",
                    i.TIME_FORMAT = e[""ui.timestamp.format""],
                    i.TIME_PREFIX = e[""ui.timestamp.prefix""],
                    i.TZ = e[""ui.timestamp.timezone""],
                    i.MAX_TIMESTAMP_LOOKAHEAD = e[""ui.timestamp.lookahead""],
                    i.TIMESTAMP_FIELDS = e[""ui.timestamp.fields""]) : ""auto"" === e[""ui.timestamp.mode""] ? (i.DATETIME_CONFIG = """",
                    o()) : ""filename"" === e[""ui.timestamp.mode""] && (i.DATETIME_CONFIG = e[""ui.timestamp.filename""])),
                    ""ui.eventbreak.mode""in e)
                        if (""everyline"" === e[""ui.eventbreak.mode""])
                            i.SHOULD_LINEMERGE = !1,
                            i.LINE_BREAKER = ""([\\r\\n]+)"";
                        else if (""regex"" === e[""ui.eventbreak.mode""])
                            i.SHOULD_LINEMERGE = ""linebreaker"" !== e[""ui.eventbreak.regexmode""],
                            i.LINE_BREAKER = e[""ui.eventbreak.regex""],
                            r(""BREAK_ONLY_BEFORE_DATE"");
                        else {
                            i.SHOULD_LINEMERGE = !0;
                            const e = this.entry.content.get(""BREAK_ONLY_BEFORE"")
                              , t = this.entry.content.get(""MUST_BREAK_AFTER"")
                              , n = this.entry.content.get(""LINE_BREAKER"");
                            i.LINE_BREAKER = e || t || n || ""([\\r\\n]+)"",
                            i.BREAK_ONLY_BEFORE_DATE = !0
                        }             
                    return ""ui.structured.preamble_pattern""in e && !t.isUndefined(e[""ui.structured.preamble_pattern""]) && (i.PREAMBLE_REGEX = e[""ui.structured.preamble_pattern""]),
                    ""ui.structured.header_line_prefix""in e && !t.isUndefined(e[""ui.structured.header_line_prefix""]) && (i.FIELD_HEADER_REGEX = e[""ui.structured.header_line_prefix""]),
                    ""ui.structured.header_line_number""in e && !t.isUndefined(e[""ui.structured.header_line_number""]) && (i.HEADER_FIELD_LINE_NUMBER = e[""ui.structured.header_line_number""]),
                    ""ui.structured.header_fields""in e && !t.isUndefined(e[""ui.structured.header_fields""]) && (i.FIELD_NAMES = e[""ui.structured.header_fields""]),
                    ""ui.structured.header_field_delimiter""in e && !t.isUndefined(e[""ui.structured.header_field_delimiter""]) && (i.HEADER_FIELD_DELIMITER = e[""ui.structured.header_field_delimiter""]),
                    ""ui.structured.header_field_quote""in e && !t.isUndefined(e[""ui.structured.header_field_quote""]) && (i.HEADER_FIELD_QUOTE = e[""ui.structured.header_field_quote""]),
                    ""ui.structured.event_field_delimiter""in e && !t.isUndefined(e[""ui.structured.event_field_delimiter""]) && (i.FIELD_DELIMITER = e[""ui.structured.event_field_delimiter""]),
                    ""ui.structured.event_field_quote""in e && !t.isUndefined(e[""ui.structured.event_field_quote""]) && (i.FIELD_QUOTE = e[""ui.structured.event_field_quote""]),
                    ""ui.metric_transforms.schema_name""in e && !t.isUndefined(e[""ui.metric_transforms.schema_name""]) && (i[""METRIC-SCHEMA-TRANSFORMS""] = e[""ui.metric_transforms.schema_name""]),
                    ""ui.misc.process_binary_files""in e && !t.isUndefined(e[""ui.misc.process_binary_files""]) && (i.NO_BINARY_CHECK = e[""ui.misc.process_binary_files""]),
                    i
                },{noformat}



(7) It seems like ‘transposeFromPropsToUI’ and ‘transposeFromUIToProps' are defined in 'sourcetype.js’.

[https://cd.splunkdev.com/eui/splunkcore-web-ui/-/blob/develop/web/search_mrsparkle/exposed/js/models/knowledgeobjects/Sourcetype.js|https://cd.splunkdev.com/eui/splunkcore-web-ui/-/blob/develop/web/search_mrsparkle/exposed/js/models/knowledgeobjects/Sourcetype.js]

[https://cd.splunkdev.com/eui/splunkcore-web-ui/-/blob/develop/web_v2/search_mrsparkle/exposed/js/models/knowledgeobjects/Sourcetype.js|https://cd.splunkdev.com/eui/splunkcore-web-ui/-/blob/develop/web_v2/search_mrsparkle/exposed/js/models/knowledgeobjects/Sourcetype.js]

{noformat}            /*
             * Transposition helper methods to convert from/to ui namespace
             ****************************************************************
             */
            transposeFromPropsToUI: function (props) {
                var attr = {};

                if (!props || _.isEmpty(props)) {
                    return attr;
                }
...
                // 6) misc
                if ('NO_BINARY_CHECK' in props) {
                    attr['ui.misc.process_binary_files'] = props.NO_BINARY_CHECK;
                }
...
                return attr;
            },

            transposeFromUIToProps: function (uiAttrs) {
                var self = this;
                var props = {};

                if (!uiAttrs || _.isEmpty(uiAttrs)) {
                    return props;
                }

...
                // 5) misc
                if (
                    'ui.misc.process_binary_files' in uiAttrs &&
                    !_.isUndefined(uiAttrs['ui.misc.process_binary_files'])
                ) {
                    props.NO_BINARY_CHECK = uiAttrs['ui.misc.process_binary_files'];
                }
                return props;
            },{noformat}

(8) In “sourcetype.js', I see the following, which might be the reason why ‘NO_BINARY_CHECK’ is displayed as ‘true’.

{noformat}            defaults: {
                // witness if INDEXED_EXTRACTIONS has been set previously
                // must be an attribute so it will be present in any model clones
                'ui.misc.previous_indexed_extractions': false,
                'ui.misc.process_binary_files': true, // NO_BINARY_CHECK = true <---
                'ui.timestamp.mode': 'auto', // possible values: auto/current/advanced
                'ui.timestamp.format': undefined,
                'ui.timestamp.prefix': undefined,
                'ui.timestamp.timezone': undefined,
                'ui.timestamp.lookahead': 128,
                'ui.eventbreak.mode': 'auto', // possible values: auto/everyline/regex
                'ui.eventbreak.regexmode': 'linebreaker', // possible values: before/after/linebreaker
                'ui.eventbreak.regex': undefined,
                'ui.structured.preamble_pattern': undefined,
                'ui.structured.header_mode': 'auto',
                'ui.metric_transforms.schema_name': undefined,
                'ui.structured.header_line_prefix': undefined,
                'ui.structured.header_line_number': undefined,
                'ui.structured.header_field_delimiter': undefined,
                'ui.structured.header_field_quote': undefined,
                'ui.structured.header_fields': undefined,
                'ui.structured.event_field_delimiter': undefined,
                'ui.structured.event_field_quote': undefined,
                'ui.structured.timestamp_fields': undefined,
                name: '',
            },
{noformat}



(9) Went through our database, Jira, SFDC, Slack, KB, strangely, nobody raised this before.

h3. Workaround :

The customer can set ‘NO_BINARY_CHECK' as 'true' for 'csv' if they’d like by either one of the below.

* Modify ‘props.conf’
* Open the sourcetype, ‘csv’, and click ‘Save’

h3. Information :

diag-so1-2025-05-21_07-16-10.tar.gz … diag file from our in-house environment.","[Deployment]

* Search Head Cluster with 3 members
* Indexer Cluster with 1 cluster manager and 4 peer nodes
* SHC Deployer/License Master/etc：1 instance
* Heavy Forwarder: 1 instance
* The Splunk versions of the all instances above are 9.2.1.
* OS: Red Hat Enterprise Linux 8.10 (All instances)

This can be easily reproduced on a single instance.",Hiroatsu Noboru,Nic Bestauros,srv- ssc-gitlab,Sung Lim,Szymon Borda,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6036b7ddac6e4e0069e92a8b,712020:e4d13320-a98f-4b22-b346-ab5bba37f293,613254346fa73c006a9e37be,6036b832f8c057007083c0e3,712020:3b430d36-1427-4570-8719-743fbfe49f3e,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20/May/25 11:39 PM;hnoboru;Developer_Tools.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8167470,20/May/25 1:10 AM;hnoboru;csv_gear_the_cloned_sourcetype-20250520-075326.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8164983,20/May/25 1:10 AM;hnoboru;csv_the_original_sourcetype.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8164984,21/May/25 12:21 AM;hnoboru;diag-so1-2025-05-21_07-16-10.tar.gz;https://splunk.atlassian.net/rest/api/3/attachment/content/8167560,21/May/25 10:56 PM;frankl;image-20250522-055501.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8169896,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3f27a59,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"o Have they made any changes to their environment just before the issue started?

They cloned a default sourcetype, 'csv' as 'csv_gear'.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,7171200,,,,,,,,,,,,,,,,,,,,,,,,,Upgraded Version,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"o What is the customer environment for the issue? 
o If available, provide Splunk topology diagram, host name with IP mapping.

[Deployment]
- Search Head Cluster with 3 members
- Indexer Cluster with 1 cluster manager and 4 peer nodes
- SHC Deployer/License Master/etc：1 instance
- Heavy Forwarder: 1 instance
* The Splunk versions of the all instances above are 9.2.1.
* OS: Red Hat Enterprise Linux 8.10 (All instances)

This can be easily reproduced on a single instance.",,,,,,,,,,"o What errors are being reported?

No error is reported, the setting on Web UI doesn't match with the settings in 'props.conf'.",,,,,,,,,,,,,,,,,,,,,,,,,,"o What is the expectation when this problem is not there?

'NO_BINARY_CHECK' should be displayed as 'false' on Web UI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• How long has the customer been tracking this issue and how frequent this issue is being hit?

Since 5/13, always.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thu Jul 03 01:29:26 UTC 2025,true,addon_com.servicerocket.jira.salesforce(addon_com.servicerocket.jira.salesforce),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Core Admin UI,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,5.0,12.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,frankl(JIRAUSER52506),hnoboru(hnoboru),8fa4b7c2-aed6-4191-9b73-1284d36b5e7a(8fa4b7c2-aed6-4191-9b73-1284d36b5e7a),srv-ssc-gitlab(srv-ssc-gitlab),sylim(sylim),6a1a52f0-9f26-48f1-86d0-1ee5b4c26ee5(6a1a52f0-9f26-48f1-86d0-1ee5b4c26ee5),af35831e-0b29-4f21-8368-f6ba816b065b(af35831e-0b29-4f21-8368-f6ba816b065b),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Completing this form allows the Product Compliance team to review GTM or internal initiatives in regulated markets such as Healthcare, Financial Services, or Government. See go/compliance for more information. After the review, you will receive documented milestones and requirements, including any steps for external audit preparation (if applicable).

*Who Should Complete*: PMs releasing new products, features, or initiatives that require compliance validation for regulated markets. 

*When to Complete*: Complete after submitting the product security intake review and link product security ticket (PRODSECOPS)  below. This review must be completed within the last 30 days and cover the entire scope of the product or feature to be deployed.

*Instructions*

⚠️ Please note: Updating any fields in this tab after submission will not automatically update the linked FPMDA ticket, contact  #product-compliance for changes post submission. 

⚠️ Please note: When you select 'Yes' to submit for the Product Compliance Review, OneJira will automatically link your Product Compliance Intake ticket to the RDMP after successful generation. If the submission is rejected, the field will revert to 'No,' with a remediation comment added to the RDMP.",,,,,,,,,,,,,,,,,,,,Admin Experience,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Szymon Borda,712020:3b430d36-1427-4570-8719-743fbfe49f3e,,,,,,,,,,,,No,,,,,,0|ifkrrv:,,,,,,,,,,,None,,None,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Rated,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NTT Data Intellilink Corporation,3752629,012400000005WzMAAU,P3,No,500KW00001mnJWgYAM,,Closed,Resolved - Work Around,Standard,Tier3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Core Admin UI: Cisco-FY25Q4-S5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

Web UI : [http://10.202.2.9:8000/|http://10.202.2.9:8000/] : admin/creampan2

SSH : [10.202.2.9|http://10.202.2.9:8000/] : splunker / splk

• If available, will customer upgrade to fixed version?

I haven’t asked the customer yet, but I don’t think they will do it as there is workaround.

• If support is able to reproduce, share the setup.

[Steps]

(1) 'Settings' -> 'Source types'
(2) Click 'Clone' for 'csv', and name it as 'csv_gear'

(3) Compare ‘NO_BINARY_CHECK’ setting on Web UI and props.conf ($SPLUNK_HOME/bin/splunk btool props list --debug csv).",,,,,,,,,,,,,,,,,,,,,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

Yes.

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

Yes.

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

Collected a diag file from our in-house environment, just to be safe.

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

I'm assuming that this is happening due to the definition in 'sourcetype.js'.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 

Haven't done anything yet as I don't think we can solve this by changing configurations.

 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.

If possible, we'd like you to fix the issue.",,,Standard,,,,,,,,,,,,,,,,2025-05-22 01:40:23.311,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1_*:*_1_*:*_164587844_*|*_3_*:*_1_*:*_0_*|*_16785_*:*_1_*:*_2288_*|*_12230_*:*_1_*:*_4931_*|*_10001_*:*_1_*:*_2977535770,,,,,,,,,,,,,,,,,,,,,,,,,,"Sustaining/Support template Accuracy  (data collected at same time, Support analysis filled in etc.)",Sustaining/Support template complete,Valid JIRA mandatory fields,Valid JIRA problem statement in JIRA Summary,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Szymon Borda,,,,,"Either one of the below.

1. Modify ‘props.conf’

2. Open the default sourcetype on Web UI, and click ‘Save’ (This action will add the setting to ‘props.conf’.)",,,,,,,,,,2025-05-22 01:40:23.311,2025-05-22 01:40:23.311,,,,,,Szymon Borda,712020:3b430d36-1427-4570-8719-743fbfe49f3e,,,,,,"21/May/25 6:40 PM;6036b832f8c057007083c0e3;# Review the Jira Summary {color:#36B37E}*[ PASS ]*{color} 
# Review Accuracy and Completeness {color:#36B37E}*[ PASS ]*{color}  
# Review the Description Field {color:#36B37E}*[ PASS ]*{color} 

+Comments to engineering+

+Comments to TSE+

*_Triaged_*

","21/May/25 10:56 PM;6154ccf378e5e400701deb04;Hi [~accountid:712020:e4d13320-a98f-4b22-b346-ab5bba37f293] ,

thank you for having a look at this JIRA. I wonder if there is any reason why we need to set this to the backend team? 

We understand as per [https://docs.google.com/spreadsheets/d/1Trz_1fBBeeatPUrikx7AUZQi4aKJoXeKtJOr22Ytc_Q/edit?gid=0#gid=0|https://docs.google.com/spreadsheets/d/1Trz_1fBBeeatPUrikx7AUZQi4aKJoXeKtJOr22Ytc_Q/edit?gid=0#gid=0|smart-link]  , the backend team is indexing framework but since the issue is reproducible and ONLY happened over cloning from UI, do we think we need to investigate from UI point of view too? Not to mention the issue is very easily reproducible. 

!image-20250522-055501.png|width=480,height=436,alt=""image-20250522-055501.png""!

FYI [~accountid:70121:fe9cb428-87a9-43f5-8af7-1da83e873421] ","21/May/25 11:01 PM;6154ccf378e5e400701deb04;[~accountid:712020:e4d13320-a98f-4b22-b346-ab5bba37f293] just try to be more specific – our engineers went down to the frontend code base and checked 

{noformat}In “sourcetype.js', I see the following, which might be the reason why ‘NO_BINARY_CHECK’ is displayed as ‘true’.

            defaults: {
                // witness if INDEXED_EXTRACTIONS has been set previously
                // must be an attribute so it will be present in any model clones
                'ui.misc.previous_indexed_extractions': false,
                'ui.misc.process_binary_files': true, // NO_BINARY_CHECK = true <---
                'ui.timestamp.mode': 'auto', // possible values: auto/current/advanced
                'ui.timestamp.format': undefined,
                'ui.timestamp.prefix': undefined,
                'ui.timestamp.timezone': undefined,
                'ui.timestamp.lookahead': 128,
                'ui.eventbreak.mode': 'auto', // possible values: auto/everyline/regex
                'ui.eventbreak.regexmode': 'linebreaker', // possible values: before/after/linebreaker
                'ui.eventbreak.regex': undefined,
                'ui.structured.preamble_pattern': undefined,
                'ui.structured.header_mode': 'auto',
                'ui.metric_transforms.schema_name': undefined,
                'ui.structured.header_line_prefix': undefined,
                'ui.structured.header_line_number': undefined,
                'ui.structured.header_field_delimiter': undefined,
                'ui.structured.header_field_quote': undefined,
                'ui.structured.header_fields': undefined,
                'ui.structured.event_field_delimiter': undefined,
                'ui.structured.event_field_quote': undefined,
                'ui.structured.timestamp_fields': undefined,
                name: '',
            },{noformat}

For more info, please read the above analysis. ","22/May/25 10:11 AM;712020:e4d13320-a98f-4b22-b346-ab5bba37f293;[~accountid:6154ccf378e5e400701deb04] Thanks for the analysis! 

Pinging the oncall Core Admin UI engineer: [~accountid:712020:c9f1381b-01c6-402b-9cc0-c9e01b011994] ","23/May/25 12:13 AM;712020:c9f1381b-01c6-402b-9cc0-c9e01b011994;[~accountid:712020:e4d13320-a98f-4b22-b346-ab5bba37f293] We are putting this into our backlog, as this is P3 and it’s unclear if customer will update to the patched version","17/Jun/25 11:20 PM;6036b7ddac6e4e0069e92a8b;Hello Team, 

When you have have a chance, would you tell me when this will be checked by the team, please?

At the moment, the customer is not rushing, but I’d like to know the current status of the queues in the team.

Thank you and best regards,

Noboru","25/Jun/25 6:21 AM;712020:3b430d36-1427-4570-8719-743fbfe49f3e;I confirm that the problem is the default {{NO_BINARY_CHECK}} value in the UI, which means that no entry in the props file is interpreted as {{true}} by the UI. I will remove that default value. It applies to all of the listed versions.",25/Jun/25 6:37 AM;613254346fa73c006a9e37be;[Szymon Borda|https://cd.splunkdev.com/sborda] mentioned this issue in [a merge request|https://cd.splunkdev.com/eui/splunkcore-web-ui/-/merge_requests/5778] of [enterprise-core-ui / splunkcore-web-ui|https://cd.splunkdev.com/eui/splunkcore-web-ui] on branch [SPL-277301-the-default-sourcetype-has-no-binary-check-as-true-on-web-ui-while-its-not-set-in-props-conf|https://cd.splunkdev.com/eui/splunkcore-web-ui/-/tree/SPL-277301-the-default-sourcetype-has-no-binary-check-as-true-on-web-ui-while-its-not-set-in-props-conf]:{quote}Draft: Remove default value of NO_BINARY_CHECK in source types{quote},"26/Jun/25 2:25 AM;6036b7ddac6e4e0069e92a8b;Dear [~accountid:712020:3b430d36-1427-4570-8719-743fbfe49f3e] -san,

Thank you very much for looking into the issue!

When you have a chance, would you tell me to which version you’re going to put the fix, please?

Thank you for your email.

 Noboru","01/Jul/25 1:31 AM;712020:3b430d36-1427-4570-8719-743fbfe49f3e;[~accountid:6036b7ddac6e4e0069e92a8b] As I mentioned on Slack, It looks like to will be released in 10.2.x On Prem version on 15 Sep.","01/Jul/25 1:52 AM;6036b7ddac6e4e0069e92a8b;[~accountid:712020:3b430d36-1427-4570-8719-743fbfe49f3e] -san, thank you very much for the info as well as the conversation over SLACK! 🙂 ","02/Jul/25 6:29 PM;6036b7ddac6e4e0069e92a8b;Change the workaround as below as this is not only for ‘csv’ sourcetype.

h3. Workaround <Before>:

The customer can set ‘NO_BINARY_CHECK' as 'true' for 'csv' if they’d like by either one of the below.

* Modify ‘props.conf’
* Open the sourcetype, ‘csv’, and click ‘Save’



h3. Workaround <After>:

Either one of the below.  

1. Modify ‘props.conf’  

2. Open the default sourcetype on Web UI, and click ‘Save’ (This action will add the setting to ‘props.conf’.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,04/Jul/25 4:25 PM
"[PUBLIC] ""Show Source"" UI returns a javascript error when it should return a 404 error",SPL-273098,3857987,Bug,Resolved,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Hem Aravind Eswaran,6053be0686b0dd0071891318,Cuong Dong,6053a57786b0dd007187fe97,Cuong Dong,6053a57786b0dd007187fe97,20/Mar/25 1:35 PM,30/Jul/25 4:24 PM,,09/Jun/25 10:02 AM,10.0.2503.x,10.0.x,9.0.0(Aurum),9.0.2209(Hersheys),9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.2408.100,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Ice Breaker,Nutella(9.2.2406.107),Pocky,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.1.2507.x,,,,,,,,Search,Search Federation,,,,0,enterprise-ui-platform-search,EUI-Unprioritized,EUI-Untriaged,,,,,"Background info:

The “Show Source” *Field Action* runs a search that show the indexed events that precede and follow an event in your search results. The “Show Source” *Field Action* is documented [here|https://docs.splunk.com/Documentation/Splunk/9.0.4/Knowledge/Controlworkflowactionappearanceinfieldandeventmenus]. *Field Actions* - which were added to the Splunk platform around 2010 - are  actually a demonstration of a feature called [workflow actions|https://docs.splunk.com/Documentation/Splunk/9.0.4/Knowledge/CreateworkflowactionsinSplunkWeb]. [This Answers post from 2013|https://community.splunk.com/t5/Splunk-Search/no-of-events-in-show-source-view/td-p/94355] provides a bit more context about what the “Show Source” *Field Action* actually does & what endpoint it uses.

Problem:

Show Source UI makes a REST API call to the /jobs/<SID>/events endpoint to get the events. However, it doesn’t handle errors from the API call properly. For example, a common error is 404 error and it fails with a Javascript error. Because of the Javascript error, the Show Source UI is stuck with “Loading…” message. 

In the screenshot below, the UI shows “Loading…” message while the Console shows the API response message with the Javascript error stack trace.

!image-20250320-203937.png|width=1392,height=1527,alt=""image-20250320-203937.png""!

Steps to reproduce:

* Do a search. Click on “Show Source” for an event. This is the normal flow and Show Source UI should work as expected.
* The Show Source UI URL is something like this [http://myhost:1234/en-US/app/search/show_source?sid=1742502726.30&offset=1&latest_time=|http://mycoder:21000/en-US/app/search/show_source?sid=1742502726.30&offset=1&latest_time=] . Change the SID is to a new SID (e.g., “123”). This should result in 404 for an unknown SID.
* Expected: UI should handle 404 error and display an error message
* Actual: it fails with a Javascript error. Because of the Javascript error, the Show Source UI is stuck with “Loading…” message (see the screenshot above). 



Note: in the steps to reproduce above, we manually edit SID to quickly reproduce the issue. However, this is not the only way this could happen. This 404 error could also happen when the SID has expired after 10 minutes and the user then tries to refresh the Show Source UI.

Other possible errors from the API beside the 404 error above are 400 errors for validation errors and 500 errors for server-side exceptions.",,Cuong Dong,Nithin Krishna Reghunathan,srv- ssc-gitlab,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a57786b0dd007187fe97,712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0,613254346fa73c006a9e37be,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-238738,,,,,,,,,,,,,,,,,,,,09/Apr/25 4:52 PM;cdong;Screenshot 2025-04-09 at 4.50.20 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8079066,20/Mar/25 1:44 PM;cdong;image-20250320-203937.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8037816,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4ae14abf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,System,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,9504000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Fri Jun 06 06:11:32 UTC 2025,true,adallas(adallas),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise UI Platform,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,2.0,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,c3ae77fb-7a5e-43f8-860c-fa9cd07126b1(c3ae77fb-7a5e-43f8-860c-fa9cd07126b1),cdong(cdong),haeswaran(haeswaran),dc729936-08dc-4a23-8bff-df98279dc85b(dc729936-08dc-4a23-8bff-df98279dc85b),sgolchha(JIRAUSER44841),srv-ssc-gitlab(srv-ssc-gitlab),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,End User Experience,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Hem Aravind Eswaran,6053be0686b0dd0071891318,,,,,,,,,,,,,,,,,,0|ifj9gi:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Hem Aravind Eswaran,6053be0686b0dd0071891318,Boris Chen,Christina Noren,Matt Green,Rachel Perkins,61324249ac61dc00697cf056,613be989f6070d006b917dca,5af8dc294b719848347ec02c,5af5d6c28a1abd1427953558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Apple Inc.,,012400000005WzMAAU,P4,No,5005a00002kZpusAAC,,Closed,Resolved,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,EUI (Search) - FY25SQ6S2,EUI (Search) - FY25SQ6S3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2025-03-20 23:25:47.182,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_1_*:*_0_*|*_3_*:*_1_*:*_21233009_*|*_10001_*:*_1_*:*_4665_*|*_10039_*:*_1_*:*_6744888044,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Updated the severity level from Sev-3 to Sev-2. Please refer to the P&T Customer Issues SLO: [https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k/edit?usp=sharing|https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k/edit?usp=sharing].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2025-03-20 23:25:47.182,2025-03-20 23:25:47.182,,,,,,Hem Aravind Eswaran,6053be0686b0dd0071891318,,,,,,"20/Mar/25 2:03 PM;6053a57786b0dd007187fe97;Stack trace from Console

{noformat}Uncaught (in promise) TypeError: e.results is undefined
    value http://mycoder:21000/en-US/static/@A55AC748A91CE86F786EE5DD13E52C55DF8AC1DCEDDC4B835A02AF7C59732881.5/build/pages/light/show_source.js:1

{""messages"":[{""type"":""FATAL"",""text"":""Unknown sid.""}]}

Uncaught (in promise) TypeError: e.results is undefined
    value http://mycoder:21000/en-US/static/@A55AC748A91CE86F786EE5DD13E52C55DF8AC1DCEDDC4B835A02AF7C59732881.5/build/pages/light/show_source.js:1
    promise callback*value http://mycoder:21000/en-US/static/@A55AC748A91CE86F786EE5DD13E52C55DF8AC1DCEDDC4B835A02AF7C59732881.5/build/pages/light/show_source.js:1
    default http://mycoder:21000/en-US/static/@A55AC748A91CE86F786EE5DD13E52C55DF8AC1DCEDDC4B835A02AF7C59732881.5/build/pages/light/show_source.js:1
    value http://mycoder:21000/en-US/static/@A55AC748A91CE86F786EE5DD13E52C55DF8AC1DCEDDC4B835A02AF7C59732881.5/build/pages/light/show_source.js:1
    os http://mycoder:21000/en-US/static/@cdong-local-build.5/build/pages/light/common.js:223
    hl http://mycoder:21000/en-US/static/@cdong-local-build.5/build/pages/light/common.js:223
    unstable_runWithPriority http://mycoder:21000/en-US/static/@cdong-local-build.5/build/pages/light/common.js:231
    Fr http://mycoder:21000/en-US/static/@cdong-local-build.5/build/pages/light/common.js:223
    pl http://mycoder:21000/en-US/static/@cdong-local-build.5/build/pages/light/common.js:223
    Qs http://mycoder:21000/en-US/static/@cdong-local-build.5/build/pages/light/common.js:223
    Xs http://mycoder:21000/en-US/static/@cdong-local-build.5/build/pages/light/common.js:223
    Nl http://mycoder:21000/en-US/static/@cdong-local-build.5/build/pages/light/common.js:223
    Fl http://mycoder:21000/en-US/static/@cdong-local-build.5/build/pages/light/common.js:223
    tl http://mycoder:21000/en-US/static/@cdong-local-build.5/build/pages/light/common.js:223
    Fl http://mycoder:21000/en-US/static/@cdong-local-build.5/build/pages/light/common.js:223
    render http://mycoder:21000/en-US/static/@cdong-local-build.5/build/pages/light/common.js:223
    default http://mycoder:21000/en-US/static/@cdong-local-build.5/build/pages/light/common.js:1743
    showSource http://mycoder:21000/en-US/static/@A55AC748A91CE86F786EE5DD13E52C55DF8AC1DCEDDC4B835A02AF7C59732881.5/build/pages/light/show_source.js:1{noformat}",20/Mar/25 4:25 PM;712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0;[~accountid:712020:35e6e1f3-3bd2-4276-bb4b-27c98f7d797b] We came across this user experience issue while manually testing the Show Source enhancement. Could someone from the UI team please review this issue and prioritize it for the Q-release if it is a minor fix? Thanks!,09/Apr/25 4:55 PM;6053a57786b0dd007187fe97;Updated to add cloud versions as this affects Cloud versions as well. Adding [~accountid:557058:89ace195-adc2-4f25-9924-ded7c8f10bb6] to include this in Cloud release notes Known Issues,"15/Apr/25 2:40 AM;712020:d54fb228-3416-49bb-b513-a91063c50662;the issue is located on the page {{/show_source}}. UI codebase:

* script entry point: {{/search_mrsparkle/exposed/js/routers/ShowSource.jsx}}
* where the error happens: {{/search_mrsparkle/exposed/js/views/show_source/model.jsx}}  (line 65)",15/Apr/25 9:45 AM;6053a3ed90f288007008c49b;Moved the ticket to the End User Experience backlog.,"05/Jun/25 11:11 PM;613254346fa73c006a9e37be;[Hem Aravind|https://cd.splunkdev.com/haeswaran] mentioned this issue in [a merge request|https://cd.splunkdev.com/eui/splunkcore-web-ui/-/merge_requests/5545] of [enterprise-core-ui / splunkcore-web-ui|https://cd.splunkdev.com/eui/splunkcore-web-ui] on branch [bugfix/SPL-273098-show-source|https://cd.splunkdev.com/eui/splunkcore-web-ui/-/tree/bugfix/SPL-273098-show-source]:{quote}Draft: SPL-273098 [PUBLIC] ""Show Source"" UI returns a javascript error when it should return a 404 error{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,09/Jun/25 10:02 AM
MachineTypeFilter doesn't work when DS works in clustered mode - applications are getting wrongfully updated into clients when performing OS filtering in serverclass,SPL-270345,3798775,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,Manikanthreddy Kudumula (C),6389243e9e48f2b9a6157dad,Manikanthreddy Kudumula (C),6389243e9e48f2b9a6157dad,05/Feb/25 7:05 AM,25/Aug/25 10:20 AM,,01/Jul/25 5:30 AM,10.0.x,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.x,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0.x,9.2.8,9.3.6,9.4.4,,,,,Deployment Server,,,,,0,Backport_Approved,dwest_reviewed,emea_support:approved,SP:Not-DP,support_reviewed,,,"*_Summary:_* _The customer is deploying apps from the Deployment server to deployment clients using the MachineTypeFiltering in serverclass but the apps are deploying to both Windows and Linux servers._

*_Customer Impact:_* _Due to this the apps are getting wrongfully installed on the servers where they would not persist._

*_Description:_* _Describe what the customer was trying to achieve when they first noticed the problem._

* _How long has the customer been tracking this issue and what is the frequency? Has the customer made any changes to their environment just before the issue started?_
After upgrading the Splunk to 9.4.0 version they were facing this issue
* _What errors/warnings/behaviors are being reported? Be specific and provide evidence._
N/A
* _What is the expectation when this problem is resolved?_
The OS filtration must be done correctly and deployed to the correct OS
* _If the customer and/or Support engineer tried any mitigation, provide ALL of them and their outcomes._
We tried checking over the call by creating a “raitec_app1_case_test“ serverclass and observed the same behavior 

*_Problem Analysis_*

* _In your own words, describe support's analysis of the problem._
When applying MachineTypeFilter  in server class the app will be deployed to the clients based on the OS level. Suppose we mention the server class to deploy the apps in linux but the apps are deployed to windows machine.

 For Example [serverClass:linux_server:app:grz_linux_server_base-infos] 
machineTypesFilter = linux-* ..>>>= is frequently deployed to multiple Windows hosts.

 [serverClass:win-server_domino_prod_common:app:grz_win-server_domino_prod_common] 
is also found on several hosts where it is not supposed to be. 

At the same time, the following applications are absent but should be deployed: [serverClass:windows_server:app:grz_windows_server_base-infos] [serverClass:windows_server:app:grz_server_events] [serverClass:windows_server:app:Splunk_TA_microsoft-iis] 
The following hosts have confirmed issues: 
AZURECONN9911 - W2016 
AZUREDSC7901 - W2022 
AZUREDSC9901 - W2022 
AZURECONN9921 - W2016 
AZURECONN1101 - W2016 
AZURECONN9901 - W2016 
AZURETASK9901 - W2022.
* _Provide logs and accompanying data with reference to the above confluence page and all the relevant documentation based on your analysis to support your observation and theories._
This was a known issue and it was present on 9.2.x, 9.3.0 , but as per previous JIRA it was fixed on 9.2.2 and 9.3.1. But my customer is on 9.4.0.

[https://docs.splunk.com/Documentation/Splunk/9.2.2/ReleaseNotes/Fixedissues#Distributed_deployment.2C_forwarder.2C_deployment_server_issues|https://docs.splunk.com/Documentation/Splunk/9.2.2/ReleaseNotes/Fixedissues#Distributed_deployment.2C_forwarder.2C_deployment_server_issues]
* _Provide links to diags either in Splunkbot OR attached via Google Drive._
The Diag files where the MachineType Filter Configured in serverclass:

2 Deployment server:
[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-51case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-01-08_12-50-12.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-51case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-01-08_12-50-12.tar.gz]
[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-81case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-01-08_12-50-00.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-81case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-01-08_12-50-00.tar.gz]

Windows server diag file:
[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-93case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC9901-2025-01-08.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-93case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC9901-2025-01-08.tar.gz]
[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-33case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC7901-2025-01-08.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-33case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC7901-2025-01-08.tar.gz]

The diag files with *no “MachineTypeFilter”* configured in serverclass:

2 Deployment servers diag files:
[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-90case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-02-04_15-31-49.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-90case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-02-04_15-31-49.tar.gz]
[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-83case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-02-04_15-36-22.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-83case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-02-04_15-36-22.tar.gz]
Linux diag file:
[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-18case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-tcapp2152-2025-02-04.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-18case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-tcapp2152-2025-02-04.tar.gz]
Windows diag file:
[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-15case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-LNO9905-2025-02-04.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-15case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-LNO9905-2025-02-04.tar.gz]

* _If you include screenshots, please label them accordingly and link them as thumbnails here or in the comments section with explanations._
We got on a meeting and asked the customer to reproduce the issue, and then we created a test server class and the apps deployed in to both linux and windows machine.

!image-20250205-150919.png|width=338,height=116,alt=""image-20250205-150919.png""!

Windows machines where apps are updated for 60 minutes window


!image-20250205-150927.png|width=1501,height=226,alt=""image-20250205-150927.png""!



* _What is the ask from Engineering from a code-fix or product engineering standpoint?_
Code fix 


*_Workaround_*

We have requested the customer to not use MachineTypeFilter in server class and now they are not observing any issue.",,Abhisek Panda (C),Adrian Beściak,Automation for Jira,Beniamin Barret,Benoni John Aligi,Jakub Sotwin,Lukasz Bondyra,Manikanthreddy Kudumula (C),Niclas Andersson,,,,,,,,,,,,,,,,,,,,,,,,,,,61b6cf4f08e4e000697f87d1,712020:a5230362-acfe-487f-9235-36c30ff00ae3,557058:f58131cb-b67d-43c7-b30d-6b58d40bd077,712020:c45d9c5f-3c5e-46fd-8e0a-fef5e24f4dad,712020:ef8737ef-e3be-48c7-98dc-077af35b88d0,712020:7b3a83ef-896c-42c1-a06b-913b64551248,712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee,6389243e9e48f2b9a6157dad,6036b7996bc3f300699ce83a,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,SPL-280184,SPL-280185,SPL-280186,SPL-280187,,,,,,,,,,,,DMXAM-1192,SPL-252818,,,,,,,,,,,05/Feb/25 7:09 AM;57fe36df-e272-4e92-8ce1-1b2782fe7f71;image-20250205-150919.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7935413,05/Feb/25 7:09 AM;57fe36df-e272-4e92-8ce1-1b2782fe7f71;image-20250205-150927.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7935412,12/Feb/25 9:02 AM;82b49a91-d03b-419d-bd20-76d686ac332a;image-20250212-170017.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7956905,26/Mar/25 10:06 AM;82b49a91-d03b-419d-bd20-76d686ac332a;image-20250326-170510.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8049441,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Above,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7cf4baa0,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,7344000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jakub Sotwin,712020:7b3a83ef-896c-42c1-a06b-913b64551248,,Yes,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,Yes,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tue Jul 01 12:28:01 UTC 2025,true,33262a15-2583-4549-9d35-9080747b998e(33262a15-2583-4549-9d35-9080747b998e),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Agent Management,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,4.0,35.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,33262a15-2583-4549-9d35-9080747b998e(33262a15-2583-4549-9d35-9080747b998e),addon_com.codebarrel.addons.automation(addon_com.codebarrel.addons.automation),dec57fe5-8893-45a6-8870-4208fb08e962(dec57fe5-8893-45a6-8870-4208fb08e962),f77903ac-d151-4d9b-8c00-2c04b60c47ce(f77903ac-d151-4d9b-8c00-2c04b60c47ce),cfd3c2d6-52bd-406d-b760-8452b7f7693b(cfd3c2d6-52bd-406d-b760-8452b7f7693b),58b764e1-397e-4ccc-a2cd-08d516b099f3(58b764e1-397e-4ccc-a2cd-08d516b099f3),82b49a91-d03b-419d-bd20-76d686ac332a(82b49a91-d03b-419d-bd20-76d686ac332a),57fe36df-e272-4e92-8ce1-1b2782fe7f71(57fe36df-e272-4e92-8ce1-1b2782fe7f71),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Piotr Lenarczyk,712020:38f52430-cea6-4dec-8d56-45a83045e998,,,,,,,,,,,,,,Codefix,,GDI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,,,,,,,,,,,,No,,,,,,0|i001ke:zzzzzzzzzzzzz0zugzzzv,,,,,,,,,,,None,,None,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Rated,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Missed requirement in ERD: https://splunk.atlassian.net/wiki/spaces/PROD/pages/1078204367655/Deployment+Server+Scalability+ERD#DeploymentServerScalabilityERD-2.3.2.Replicatetheclientconnection
 
Fix: Extended connectionID with utsname and guid, as was decided in DACI: https://splunk.atlassian.net/wiki/spaces/PROD/pages/1078793437185/0059+DS+fix+MachineTypesFilter+not+honored+in+clustered+mode",,,,Code change,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"TD Bank, Canada",3799046,012400000005WzMAAU,P2,No,500KW00001rfjtqYAA,,Closed,Resolved,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev1-Blocker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FY25 Q4 S8,FY25 Q4 S9,FY25 Q4 S14,FY25 Q4 S15,FY25 Q4 S16,FY25 Q4 S17,FY25 Q4 S18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

No

• If available, will customer upgrade to fixed version?

YES

• If support is able to reproduce, share the setup.

N/A",,,,,,,,,,,,,,,,,,,,1.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,Standard,,,,,,,,,,,,,,,,2025-02-07 09:22:52.51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_1_*:*_0_*|*_1_*:*_1_*:*_70951216_*|*_3_*:*_2_*:*_8010592796_*|*_16785_*:*_2_*:*_6750399_*|*_12230_*:*_1_*:*_66924462_*|*_10001_*:*_2_*:*_352313663_*|*_11128_*:*_1_*:*_3649534387,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Adrian Beściak,,,,,,,,,,,,,,,2025-02-07 09:22:52.51,2025-02-07 09:22:52.51,,,,,,Adrian Beściak,712020:a5230362-acfe-487f-9235-36c30ff00ae3,,,,,,05/Feb/25 8:02 AM;6389243e9e48f2b9a6157dad;Reached out to developer over this Jira ([https://splunk.atlassian.net/browse/SPL-252818|https://splunk.atlassian.net/browse/SPL-252818|smart-link] ) which is having a same bug in 9.2.x version. They suggested to raise a new case for 9.4.0 version.,"07/Feb/25 1:22 AM;712020:c45d9c5f-3c5e-46fd-8e0a-fef5e24f4dad;[~accountid:6389243e9e48f2b9a6157dad] We will allocate some time to resolve this issue but we don’t have capacity to do it right now? How critical is this issue for client? Is there an option for workaround, for example if client could update his config to not rely on faulty {{MachineTypeFilter}} until we provide solution?",07/Feb/25 3:15 AM;6389243e9e48f2b9a6157dad;Hi [~accountid:712020:c45d9c5f-3c5e-46fd-8e0a-fef5e24f4dad] Customer is having 16000 plus clients and based on the machine type (OS) they are deploying apps by creating a serverclass and they mention that it is difficult to deploy apps. So we need someone to quickly look at this as the case is already escalated.,"07/Feb/25 6:50 AM;712020:7b3a83ef-896c-42c1-a06b-913b64551248;Thank you [~accountid:6389243e9e48f2b9a6157dad] for letting us know about increased customer priority for this case. This is on our ToDo issues list that we are working currently. As for nowe we are unable to allocate “engineer” as we are unfortunately facing 2 ongoing hot priority escalation currently. We hope to see them close in the next week and we will be start working on this issue.



In the meantime few clarification question

* Did you know if the fix from [https://splunk.atlassian.net/browse/SPL-252818|https://splunk.atlassian.net/browse/SPL-252818|smart-link] was implemented on customer and then was working? Or customer move from 9.1 to 9.4 directly skipping 9.2 and 9.3",07/Feb/25 8:12 AM;6389243e9e48f2b9a6157dad;[~accountid:712020:7b3a83ef-896c-42c1-a06b-913b64551248] The customer has updated from 9.3.2 version to 9.4 version. But the customer is using a build version which is provided by our dev team on one of the other case(*3545950*).,12/Feb/25 6:13 AM;712020:ef8737ef-e3be-48c7-98dc-077af35b88d0;Hi [~accountid:712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee] any update here ?,"12/Feb/25 7:02 AM;712020:37eb4184-f85f-4f64-91fa-da88565e5f21;Hi team, this P1 case [+3662887+|https://splunk.lightning.force.com/lightning/r/5005a00003JJN90AAH/view] have this [https://splunk.atlassian.net/browse/SPL-269535|https://splunk.atlassian.net/browse/SPL-269535|smart-link]  anyway, the engineers stated “Dev check further why machineTypesFilter = windows-* or whitelist.0 = cn* don’t work as expected” is being worked on this SPL, we will be monitoring this SPL to check the next steps, let us know if there is any additional information needed from customer","12/Feb/25 9:02 AM;712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee;[~accountid:712020:ef8737ef-e3be-48c7-98dc-077af35b88d0] I tried running various scenarios to reproduce this issue but all seems to work as expected so far, so I’m going to need another piece of information here:

- since this 9.4 build was provided by dev team, what is a commit/branch it was created from? Why it’s not an official one?
- how many DSes are there in the system and what’s the architecture? I can see there are 2 DSes working in clustered mode: _spldep9902_ and _spldep9903_, is there also a Master DS that these 2 DSes would connect to to get applications? A Load Balancer?
- which machine has this DNS “[https://deploymentserver.splunk.grz.at:8089|https://deploymentserver.splunk.grz.at:8089]”? Most of diags attached here list it as a DS to connect to.
- was _machineTypesFilter_ working as expected on 9.3.2? Were there any architectural changes between 9.3.2 and 9.4 (like configuring clustered mode)?
- There are 2 bundles of diags attached to this ticket. I can see that issue exists in _“The Diag files where the MachineType Filter Configured in serverclass”_ diags - I can see linux app _“grz_linux_server_base-infos”_ in [+AZUREDSC9901+|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-93case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC9901-2025-01-08.tar.gz] machine. I don’t see this problem in _“The diag files with_ *_no “MachineTypeFilter”_* _configured in serverclass”_ diags - is there any issue with deployment in these diags?
- What was the exact scenario when making tests together with a customer over a call with _“raitec_app1_case_test”_?
Thank you in advance!


[~accountid:712020:37eb4184-f85f-4f64-91fa-da88565e5f21] In [https://splunk.atlassian.net/browse/SPL-269535|https://splunk.atlassian.net/browse/SPL-269535|smart-link]  
_machineTypesFilter_ works as expected, according to below comment there:

!image-20250212-170017.png|width=946,height=66,alt=""image-20250212-170017.png""!

 ","12/Feb/25 9:19 AM;712020:37eb4184-f85f-4f64-91fa-da88565e5f21;Hi [~accountid:712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee], thanks for the clarification, we will stop monitoring this SPL while we confirm the current next steps from dev team on [https://splunk.atlassian.net/browse/SPL-269535|https://splunk.atlassian.net/browse/SPL-269535|smart-link]  ","13/Feb/25 9:09 AM;6389243e9e48f2b9a6157dad;Hi [~accountid:712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee] 

Here is the answers for your queries,

since this 9.4 build was provided by dev team, what is a commit/branch it was created from? Why it’s not an official one?
[https://splunk.atlassian.net/browse/SPL-263634|https://splunk.atlassian.net/browse/SPL-263634|smart-link], 

* how many DSes are there in the system and what’s the architecture? I can see there are 2 DSes working in clustered mode: _spldep9902_ and _spldep9903_, is there also a Master DS that these 2 DSes would connect to to get applications? A Load Balancer?
No, there is no Master DS for this but between the Deployment server and the deployment clients there is a load balancer.
* which machine has this DNS “[https://deploymentserver.splunk.grz.at:8089|https://deploymentserver.splunk.grz.at:8089/]”? Most of diags attached here list it as a DS to connect to.
Load balancer.
* was _machineTypesFilter_ working as expected on 9.3.2? Were there any architectural changes between 9.3.2 and 9.4 (like configuring clustered mode)?
They have noticed this issue in August means they have observed the issue in 9.3.2 as well.
* There are 2 bundles of diags attached to this ticket. I can see that issue exists in _“The Diag files where the MachineType Filter Configured in serverclass”_ diags - I can see linux app _“grz_linux_server_base-infos”_ in [+AZUREDSC9901+|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-93case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC9901-2025-01-08.tar.gz] machine. I don’t see this problem in _“The diag files with_ *_no “MachineTypeFilter”_* _configured in serverclass”_ diags - is there any issue with deployment in these diags?

In the shared diag files, the old and the new diag files are persist the MachineTypefilter is mentioned in old diag files for reference and the new diag files also have MachineTypeFilter but for testing purpose we didn’t configure the MachineTypeFilter in one serverclass “_raitec_app1_case_test_“ and it is deployed successfully.
* What was the exact scenario when making tests together with a customer over a call with _“raitec_app1_case_test”_?
Just want to confirm that the issue is either with the serverclass or with the MachineTypeFilter.","14/Feb/25 7:53 AM;712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee;[~accountid:6389243e9e48f2b9a6157dad] Thank you for your responses!
I ran today another bunch of tests and all of them work correctly, I was not able to reproduce customer’s problems.

In order to understand this issue, I’m going to ask you to reproduce it with a customer with debug logs enabled on both DSes:

{noformat}ClientSessionsManager
DSManager
Serverclass
DSClientFilter{noformat}

Please record this session and send us a video, along with new diags (or at least describe steps in a detailed way).

Following up previous questions - so there are only 2 clustered DSes in this system?","26/Feb/25 8:41 AM;6389243e9e48f2b9a6157dad;Hi [~accountid:712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee] The customer has kept 4 components in Debug mode; however, they have not achieved successful results during the app deployment. In contrast, when they switched to Debug mode for only three components, they were able to gather the desired results.  

I will have a meeting with the customer tomorrow. If debugging with all four components proves unfeasible, I will proceed with the three components, although I am uncertain which specific three components' debug logs are necessary.  

Could you please specify which three debug logs are required?","27/Feb/25 4:37 AM;712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee;[~accountid:6389243e9e48f2b9a6157dad] thank you for the update.

{quote}The customer has kept 4 components in Debug mode; however, they have not achieved successful results during the app deployment{quote}

Do I understand correctly, that with 4 components in Debug mode, apps were deployed correctly, and with 3 components in Debug mode they were deployed incorrectly?

That’s a surprising behavior, but actually it might turn to be helpful to compare logs between these 2 situations. Please record this session and send logs for both scenarios - with 4 components and with 3 components. 

ClientSessionsManager logs don’t have to be enabled.
DSManager is highly desirable (in case of reproduction difficulties can be disabled too as a last resort)
Serverclass - must have
DSClientFilter - must have
","27/Feb/25 6:54 AM;6389243e9e48f2b9a6157dad;Hi [~accountid:712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee] Thanks for your response,

>> Do I understand correctly, that with 4 components in Debug mode, apps were deployed correctly,

No, when they enabled the Debug for 4 components they are observing timedout error while deploying apps but observed that the apps were deployed into windows machine instead of linux machine. 

When they enabled Debug for 3 any components the apps were deployed successfully and observed that the apps were deployed into windows machine instead of linux machine. 

In both scenarios the apps were wrongly pushed when using server class but keeping debug logs for 3 components works to get the diag files for further analysis.

Please let me know if you have any queries","27/Feb/25 8:08 AM;712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee;Thank you for explanation, please disable _ClientSessionsManager_ then and keep 3 other components enabled, and provide a recording w/ logs, it should contain all we need for further analysis.","07/Mar/25 2:13 AM;6389243e9e48f2b9a6157dad;Hi [~accountid:712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee] 

We had a Zoom meeting with the customer and we asked the customer to keep the 3 components in debug mode and then restarted the service, for only one deployment server spldep9902 it is successful, the deployment from spldep9903 with DSClientFilter set to DEBUG failed.
We have tried many times to collect the video proof but every time the deployment of app is failed/timedout. Due to a pre-scheduled meeting for the customer they took into offline and have performed without recording a video, so we don’t have a video recording for this.

* Short summary of the test -
we have 2 deployment apps with props file where some comment is added.
1 is for testing machinetypesfilter on Windows and 1 for linux.
* raitec_win_machinetypesfilter_test
* raitec_lin_machinetypesfilter_test

These are their respective serverClasses:
### machineTypesFilter Test###
[serverClass:raitec_linux_test]
machineTypesFilter=linux-x86_64
restartSplunkd=true
whitelist.0 = *
[serverClass:raitec_linux_test:app:raitec_lin_machinetypesfilter_test]
restartIfNeeded = true

[serverClass:raitec_windows_test]
machineTypesFilter=windows-x64
restartSplunkd=true
whitelist.0 = *
[serverClass:raitec_windows_test:app:raitec_win_machinetypesfilter_test]
restartIfNeeded = true

Results:
As we could see after the successful deployment from spldep9902, the raitec_lin_machinetypesfilter_test app is frequently downloaded by several windows hosts.

Customer shared the diag files from 1 Deployment server 2 windows machines:

[https://downloadsvc.splunk.com/download/splunk/03-06-2025/uploadsvc-76case3641593-03-06-2025-USER-0030b000028J1oXAAS-diag-VRBO80204-2025-03-06.tar.gz|https://downloadsvc.splunk.com/download/splunk/03-06-2025/uploadsvc-76case3641593-03-06-2025-USER-0030b000028J1oXAAS-diag-VRBO80204-2025-03-06.tar.gz]
[https://downloadsvc.splunk.com/download/splunk/03-06-2025/uploadsvc-10case3641593-03-06-2025-USER-0030b000028J1oXAAS-diag-ADONISNP2101-2025-03-06.tar.gz|https://downloadsvc.splunk.com/download/splunk/03-06-2025/uploadsvc-10case3641593-03-06-2025-USER-0030b000028J1oXAAS-diag-ADONISNP2101-2025-03-06.tar.gz]

Deployment server diag:-

[https://downloadsvc.splunk.com/download/splunk/03-06-2025/uploadsvc-65case3641593-03-06-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-03-06_13-51-18.tar.gz|https://downloadsvc.splunk.com/download/splunk/03-06-2025/uploadsvc-65case3641593-03-06-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-03-06_13-51-18.tar.gz]

Please let me know if you need any other information on this.",11/Mar/25 12:01 AM;712020:ef8737ef-e3be-48c7-98dc-077af35b88d0;Hi [~accountid:712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee] any update ?,"11/Mar/25 9:52 AM;712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee;[~accountid:712020:ef8737ef-e3be-48c7-98dc-077af35b88d0] We’re investigating this, having some lucky guesses already but need to dig more and confirm.","11/Mar/25 10:00 AM;712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee;[~accountid:6389243e9e48f2b9a6157dad] Thank you for providing these diags.
Additional question - what load balancing mechanism is used there, round robin, sticky sessions or sth else?

And please confirm - is the issue related *only* to incorrect apps on *windows* machines (deploying linux ones and not deploying to windows ones) or do *linux* machines also exhibit unexpected behavior?

On ADONISNP2101 agent I can see alternating logs for installing/uninstalling apps:
{{Installing app=raitec_win_machinetypesfilter}}
{{Removing app=raitec_lin_machinetypesfilter_test}}

And

{{Installing app=raitec_lin_machinetypesfilter_test}}
{{Removing app=raitec_win_machinetypesfilter_test}}

I’m not sure if they result from customer reproducing this issue or the situation in the system is unstable, meaning that one time apps are deployed as expected and another time they are not?
Please let me know.","13/Mar/25 6:34 AM;712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee;[~accountid:712020:ef8737ef-e3be-48c7-98dc-077af35b88d0] [~accountid:6389243e9e48f2b9a6157dad] I confirm that there’s a defect related to machineTypesFilter processing in DS clustered mode, present since 9.2.
I will follow up with detailed analysis and potential workaround steps (if any) when I complete analysis.","13/Mar/25 8:32 AM;712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee;[~accountid:712020:ef8737ef-e3be-48c7-98dc-077af35b88d0] [~accountid:6389243e9e48f2b9a6157dad] 
Please find detailed analysis here [https://docs.google.com/document/d/1hOWUmeJOHeeAhD9V4-iS4sxi0y8_U9y6ZFzj0VpW_ck/edit?usp=sharing|https://docs.google.com/document/d/1hOWUmeJOHeeAhD9V4-iS4sxi0y8_U9y6ZFzj0VpW_ck/edit?usp=sharing|smart-link] ","17/Mar/25 3:06 AM;712020:7b3a83ef-896c-42c1-a06b-913b64551248;I would like to confirm that this is a defect on the DS code side as mentioned below. The current plan is to work on a fix within the SE 10.0 code line.



Further backporting will be done later in the game. The final decision on which releases will receive the fix will be made according to the go/backport policy, the complexity and risk of the merge, and the projected timeline for the duration of support for each release.



For the maintenance release cycle, the EARLIEST delivery date is defined by the release train and the current prediction is

* {{release/cobalt-9.2.7}} 
* {{release/duranium-9.3.5}}
* {{release/europium-9.4.3}} ",18/Mar/25 4:18 AM;712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee;The fix path and action items for development[https://splunk.atlassian.net/wiki/spaces/PROD/pages/1078793437185/0059+DS+fix+MachineTypesFilter+not+honored+in+clustered+mode|https://splunk.atlassian.net/wiki/spaces/PROD/pages/1078793437185/0059+DS+fix+MachineTypesFilter+not+honored+in+clustered+mode|smart-link] ,"26/Mar/25 9:38 AM;6389243e9e48f2b9a6157dad;Hi [~accountid:712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee] I could see that the case status is on waiting for customer, can you please let me know if you need any information that we can request/get from the customer?","26/Mar/25 10:06 AM;712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee;[~accountid:6389243e9e48f2b9a6157dad] Not at all. The status is changing daily between Waiting for Customer and Customer Update, I’ve no idea why. 

From my perspective there’s no need for more information from a customer.

!image-20250326-170510.png|width=666,height=871,alt=""image-20250326-170510.png""!",28/Mar/25 2:39 PM;712020:7b3a83ef-896c-42c1-a06b-913b64551248;Switch state to in progress as next item is a fix from our team. ,09/Apr/25 6:43 AM;6389243e9e48f2b9a6157dad;Hi [~accountid:712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee]  Any update on the fix please?,"09/Apr/25 8:36 AM;712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee;[~accountid:6389243e9e48f2b9a6157dad] Thank you for actively monitoring this. The work item has been prepared in team’s backlog, it is queued to be picked up when there’s team capacity.","17/Apr/25 7:48 AM;6389243e9e48f2b9a6157dad;Hi [~accountid:712020:9780c402-3909-4f0e-8ec3-8f7bbb0933ee]  Could you please provide any updates on this case? Additionally, is there an anticipated timeline for implementing a fix? 

Any information could be helpful.

Thanks in advance!","18/Apr/25 5:27 AM;712020:7b3a83ef-896c-42c1-a06b-913b64551248;Work is not finished yet. Work will be done on develop with merge to Flubber branch. We will also backport this to Europium but as currently 9.4.2 is on hard freeze it will be part of 9.4.3.



We do not plan backporting to 9.2 and 9.3. ","30/Jun/25 9:17 AM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:712020:a5230362-acfe-487f-9235-36c30ff00ae3] has submitted a backport request to [~accountid:712020:7b3a83ef-896c-42c1-a06b-913b64551248] [~accountid:712020:38f52430-cea6-4dec-8d56-45a83045e998]



","30/Jun/25 9:42 AM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:712020:7b3a83ef-896c-42c1-a06b-913b64551248] has Approved the backport.  

[~accountid:712020:a5230362-acfe-487f-9235-36c30ff00ae3], please proceed with the backport","30/Jun/25 3:34 PM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:712020:7b3a83ef-896c-42c1-a06b-913b64551248] has Approved the backport.  

[~accountid:712020:a5230362-acfe-487f-9235-36c30ff00ae3], please proceed with the backport","01/Jul/25 1:26 AM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:712020:a5230362-acfe-487f-9235-36c30ff00ae3] since you have selected to document this in the release notes, please ensure that the Jira's title is clearly written and hide any internal terms or code names in the title by surrounding them with square brackets.","01/Jul/25 5:28 AM;712020:a5230362-acfe-487f-9235-36c30ff00ae3;Fix has been delivered and backported to sustain branches. It should be available in SE 9.2.8, 9.3.6, 9.4.4 and 10.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3850786,DMXAM-1300,DS Maintenance ,Done,01/Jul/25 5:30 AM
"Scheduled email exports of large dashboards compress images to approximately 1440 x 960 pixels, leading to blurry PDFs.",SPL-270271,3797600,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,David Ferris,712020:60e266ef-1d8c-4b26-a206-003e04d18f59,David Ferris,712020:60e266ef-1d8c-4b26-a206-003e04d18f59,David Ferris,712020:60e266ef-1d8c-4b26-a206-003e04d18f59,04/Feb/25 1:10 PM,28/Aug/25 2:45 PM,,,10.0.2503.x,10.0.x,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,develop,Lindt_Tick(9.1.2312.100),Milkyway(9.2.2403.100),Nutella,Nutella(9.2.2406.107),Oreo,Pocky,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise Dashboards,,,,,0,,,,,,,,"*Observed Behavior:*

For Studio dashboards with large dimensions the quality of scheduled exports are severely reduced and can become unusable in terms of the visibility of data and text. See `large-dashboard-dark.png` inside of our integration snapshots in the enterprise dashboards repo for an example of this quality reduction.

*Expected Behavior:* 

The exported dashboard via scheduled export should have consistent quality and usability as a result across all dimensions. Users should be able to produce a single usable PDF export via scheduled export for dashboards with large dimensions.

*Steps To Reproduce:*

1. Create a studio dashboard and modify the canvas dimensions to something like 1440x6000 and enable the `Actual size` display mode
2. Fill the canvas with various visualizations
3. Send a test email to yourself via the scheduled export dialog inside the dashboard
4. Observe that the PDF content is blurred and due to resolution compression",,Adarsh KR (C),Andrew Brown,Benny Shi,David Ferris,Scot Corrie,Teshika Holmes,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,712020:fa2809c5-7f19-421d-8f3f-fedb3f09adc3,557058:37b4518c-e757-440f-87b7-181f5f425e80,6053a7b82f452d006f829331,712020:60e266ef-1d8c-4b26-a206-003e04d18f59,6036b7a14623c60069c04029,626f919e66ad530069d24c70,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,,,0,0,,,SPL-272088,SPL-272085,SPL-272082,SPL-272331,SPL-272323,SPL-272334,SPL-274413,SPL-284327,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Above,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@67acb54b,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,2246400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thu Aug 28 21:45:49 UTC 2025,true,27dd4d0d-8579-40ea-ad9b-09c0c62a35ff(27dd4d0d-8579-40ea-ad9b-09c0c62a35ff),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,User Experience YVR - Enterprise Dashboards,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,"Pre-conditions (If any):

Steps to reproduce:
1. 
2. 

Expected:

Actual:

Browser:
OS:",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,d6850ade-7208-4068-abf3-ba401b4d73f6(d6850ade-7208-4068-abf3-ba401b4d73f6),andrewb(andrewb),bshi(bshi),cdeel(JIRAUSER43916),6b2dff0a-8d18-4adc-8906-425b20ad5c7e(6b2dff0a-8d18-4adc-8906-425b20ad5c7e),scorrie(scorrie),27dd4d0d-8579-40ea-ad9b-09c0c62a35ff(27dd4d0d-8579-40ea-ad9b-09c0c62a35ff),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,End User Experience,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,No,,,,,,0|i014qf:,,,,,,,"This ticket was previously closed as it was being associated with investigation/planning work related to these improvements.

However, since this ticket is linked to this issue in the documentation under the 'Known Issues' it should not be closed until all improvements have been made and this issue is 'fixed'. Re-opening this ticket and created a linked SPIKE ticket to track the previously completed investigation/planning efforts.",,,,None,,None,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,David Ferris,712020:60e266ef-1d8c-4b26-a206-003e04d18f59,,,,,,,,,,,,,,,,,,,Not Rated,,,,,,,,,,,,,,,,,,,,,,,,,,,,"As a part of the PDF creation process, studio dashboard dimensions are coerced/compressed down to a resolution of approximately 1440 x 960 pixels. This is done in order to ensure the dashboard can be fully displayed within the generated PDF.

As a result, dashboards with large dimensions are compressed more dramatically leading to a blurry appearance to the dashboard content within the exported PDF. 

Note: This only applies to scheduled exports in Dashboard Studio and not on-demand export. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ent-Dash-FY25-Q5-S3,Ent-Dash-Prioritized-Backlog,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2025-03-24 15:00:32.457,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Reduce the dimensions of the dashboard, or split up large dashboards into separate smaller dashboards. Scheduled export compresses the studio dashboard to a resolution of approximately 1440 x 960 pixels before it is screenshotted for the PDF. Reducing the dimensions of the dashboard closer to this resolution should improve the visibility and quality of the export. If you split the dashboard into smaller dashboards, and schedule them to export separately, this effectively reduces the dimensions of each dashboard and improves the quality of each exported PDF.",,,,,,,,,,2025-03-24 15:00:32.457,2025-03-24 15:00:32.457,,,,,,David Ferris,712020:60e266ef-1d8c-4b26-a206-003e04d18f59,,,,,,03/Mar/25 5:19 PM;712020:60e266ef-1d8c-4b26-a206-003e04d18f59;Investigation document containing the fix implementation plan with follow up item tracking: [https://docs.google.com/document/d/14DlpqrYZE-iJbtEXK9dvYZP1VDFkEvDotFmFjcVOeOQ/edit?tab=t.0#heading=h.ci3qwwwwxrs7|https://docs.google.com/document/d/14DlpqrYZE-iJbtEXK9dvYZP1VDFkEvDotFmFjcVOeOQ/edit?tab=t.0#heading=h.ci3qwwwwxrs7|smart-link] ,03/Mar/25 6:38 PM;712020:60e266ef-1d8c-4b26-a206-003e04d18f59;Test plan: [https://docs.google.com/spreadsheets/d/1QaBHEWgg5Zagv4FRY3oPLU8rI_ROUYtTDMZ8Ede2cRY/edit?gid=0#gid=0|https://docs.google.com/spreadsheets/d/1QaBHEWgg5Zagv4FRY3oPLU8rI_ROUYtTDMZ8Ede2cRY/edit?gid=0#gid=0|smart-link] ,"24/Mar/25 8:00 AM;712020:fa2809c5-7f19-421d-8f3f-fedb3f09adc3;[~accountid:712020:60e266ef-1d8c-4b26-a206-003e04d18f59] 
Could you please let me know the ETA on On-Prem release version, as I could see backport has been done.

I have a similar case, customer wants the ETA of the release
TY! ","27/Mar/25 5:46 AM;712020:fa2809c5-7f19-421d-8f3f-fedb3f09adc3;[~accountid:712020:60e266ef-1d8c-4b26-a206-003e04d18f59] 
Could you please let me know the ETA for the On-Prem release version? I noticed that backport has been included.

I have a similar case where the customer is also requesting the ETA for the release. Your assistance would be greatly appreciated.
TY!",08/Apr/25 7:02 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:712020:60e266ef-1d8c-4b26-a206-003e04d18f59] can you provide the fix version (or versions) for this bug? It’s marked fixed but we don’t know in which version(s) the fix will reach customers. ,"08/Apr/25 9:04 AM;712020:60e266ef-1d8c-4b26-a206-003e04d18f59;Hi [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80], apologies for the confusion on this but this ticket was initially filed to be addressed as a bug however upon investigating this it was determined that the scope of work to improve this would be larger than a single ticket and as such this ticket was used more as a SPIKE. The implementation/fix tickets are linked with this ticket and as a result the improvements for this have been planned but are currently not available.



Apologies for the confusion on this as I can see this ticket is being used to track in the release notes Known Issues. I can re-open this ticket and create a linked SPIKE ticket instead to track the completed investigation/planning work that this ticket was previously relating to in order to avoid future confusion on this.","08/Apr/25 12:14 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;Sounds good, thank you [~accountid:712020:60e266ef-1d8c-4b26-a206-003e04d18f59] ","08/May/25 4:55 PM;6053a7b82f452d006f829331;Hi [~accountid:712020:fa2809c5-7f19-421d-8f3f-fedb3f09adc3], this is not a customer issue but internal bug, if you believe this one is related to one of the customer problems, please create a new customer issue Jira type and follow the SOP process to have it reviewed first.  Please do not attach the SFDC case to any internal bug ticket. Thanks","30/Jun/25 1:48 PM;626f919e66ad530069d24c70;Hello [~accountid:712020:60e266ef-1d8c-4b26-a206-003e04d18f59] IHAC who is using Dashboard studio and gets the same error/issue that is being reported in this SPL. Do I have to create another SPL and attached to here? Or can I attach to the SFDC case no to here? 

Teshika ","30/Jun/25 3:43 PM;712020:60e266ef-1d8c-4b26-a206-003e04d18f59;Hi [~accountid:626f919e66ad530069d24c70], this is not a customer issue but an internal bug and as such no SFDC case should be attached to it. If you believe the customer issue is related to this please create a new *customer issue* Jira type and follow the SOP process to have it reviewed first after which the on-call engineer from our team will be able to provide triage and investigation. Thank you.","28/Jul/25 2:12 PM;626f919e66ad530069d24c70;[~accountid:712020:60e266ef-1d8c-4b26-a206-003e04d18f59] I have spoken with Scot on this issue and he stated to attach this SPL to the SFDC case that I currently have. That there is no need to create a new SPL and attach to this one. 

Teshika ","30/Jul/25 8:31 AM;712020:60e266ef-1d8c-4b26-a206-003e04d18f59;Hi [~accountid:626f919e66ad530069d24c70], if I may just push back on this slightly, this is an internal bug ticket which our team is using to track the long term fix/improvements to scheduled exports generally to better handle large dashboards as such it is not related to any one specific customer issue and will not be able to be closed until we have completed work on improving quality all around. 

In addition this Jira is a bug ticket whereas customer tickets should be filed specifically as a ‘Customer Issue' and go through the SOP review process so that our team can correctly track it and triage it as such. Attaching the SFDC to this Jira will make it very difficult for us to track the customer issue especially since it is possible that other customers will report this issue until it is addressed. I would strongly recommend to create a separate Jira for the specific issue your customer is facing as a 'Customer Issue’ and link the SFDC to that ticket instead, that way it can go through the SOP process and our team can triage to our current on-call engineer appropriately. Thank you very much.



David Ferris","05/Aug/25 7:35 AM;6036b90eac6e4e0069e93755;[~accountid:6036b7a14623c60069c04029] [~accountid:712020:60e266ef-1d8c-4b26-a206-003e04d18f59] 

What is the best path moving forward here? It seems both parties are giving conflicting information and want to make sure we have this documented correctly while we are waiting on a resolution from dev. 

Does a new SPL need to be created as a “customer issue” ticket or can we in-fact use this for tracking? ","21/Aug/25 1:12 PM;6036b7a14623c60069c04029;[~accountid:6036b90eac6e4e0069e93755] - The can be converted to a “Customer Issue”.

Click on three dots ( . . .) in upper right, and move to ""Customer Issue"", keep clicking “Next”, make it “Review by Support” (status), and then “Confirm”.

We are strictly adhering to the SOP, which should be followed for this ticket. (Normally this is done by the TSE, but since [~accountid:712020:60e266ef-1d8c-4b26-a206-003e04d18f59] submitted the ticket, he might have to work with the TSE, or at least just complete the basics). 

Follow the below (on my site: [+https://go/scot)+|https://go/scot)]

h3. *_SPL Creation & Troubleshooting_*

Please see the _""_[+_Submitting Quality JIRAs_+|https://www.splunkcoach.com/new/ui/learner#/1790784227539270962?series=1831348295550361174]_""_ QTP training for a session on the SPL SOP.

The above training video explains the SPL SOP in detail, and might be easier to follow for some people than the ""SPL SOP"", below.

This is the current [+SPL SOP+|https://splunk.atlassian.net/wiki/spaces/HSSR/pages/188770819883/Creating+Splunk+Core+Product+Jiras]. 

_The “_*_20250723.SPL Walkthrough.mp4_*_” video on my site is helpful as well._

Feel free to reach out to me if you have questions on the SOP.","21/Aug/25 1:28 PM;712020:60e266ef-1d8c-4b26-a206-003e04d18f59;Hi [~accountid:6036b90eac6e4e0069e93755] [~accountid:6036b7a14623c60069c04029], Just to clarify on this again, this ticket is an internal bug which I have created for my team to track work related to us improving this area. As such it should *not* have an SFDC case attached to it or be converted to a customer issue.

If a customer is experiencing quality issues with their Scheduled Exports please create a *separate* Customer Issue ticket and follow the SOP process as Scot has provided above. You may link this ticket in your Customer Issue to identify this as related but please do not attach any SFDC directly to this ticket or convert this ticket to a “Customer Issue“. Thank you.","21/Aug/25 1:47 PM;6036b90eac6e4e0069e93755;Hey [~accountid:6036b7a14623c60069c04029] [~accountid:712020:60e266ef-1d8c-4b26-a206-003e04d18f59] 

Thank you both, that sets the path forward clearly.



 I’ll let that information cascade to [~accountid:626f919e66ad530069d24c70]so an SPL can be raised. 

[~accountid:626f919e66ad530069d24c70] Please see below:

“If a customer is experiencing quality issues with their Scheduled Exports please create a *separate* Customer Issue ticket and follow the SOP process as Scot has provided above. You may link this ticket (SPL-270271) in your Customer Issue to identify this as related but please do not attach any SFDC directly to this ticket or convert this ticket to a “Customer Issue“. Thank you.”","28/Aug/25 2:45 PM;626f919e66ad530069d24c70;[~accountid:712020:60e266ef-1d8c-4b26-a206-003e04d18f59] I have not converted this SPL and I have created an SPL and it is linked here. 

Teshika",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3885790,SPL-274502,[Skittles] Scheduled Export Quality Improvements,To Do,08/Apr/25 11:07 AM
Cascading bundle replication stuck: indexer peer going down during replication can block future replications if it comes back up,SPL-269946,3788247,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Ryan Chan,6278d03db249c0006f94eb49,Yevhen Shylo,61e678e308c1f7006a1704fc,Yevhen Shylo,61e678e308c1f7006a1704fc,30/Jan/25 5:19 AM,08/May/25 4:43 PM,,13/Mar/25 1:50 PM,9.1.0(Beryllium),9.1.1,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.4.0(Europium),9.4.1,9.4.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.1.10,9.2.7,9.3.5,9.4.3,,,,,Bundle Replication Enhancement,Search Infra - Bundle Management,,,,0,Backport_Approved,emea_support:approved,sinfra:cos:infra_issue,sinfra:sx:bundle_replication_failure,SP:Analytics,support_reviewed,udhanabalan_review,"*_Summary:_* 
Intermittently Cascading replication is being stuck due to one indexer

*_Customer Impact:_* 
_Knowledge bundle replication is halted and lookups are not being updated on the IDX layer_

*_Description:_* 
Customer with a huge IDX enviornemt ~300 indexers, is periodically seeing that the knowledge bundle replication seems to stop. in th elogs on the Captain they see that it is due to the fact that the previous replication is still in effect:
{{01-30-2025 09:09:40.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.}}

This will continue endlessly until Captain is restarted.
This seems to be a continuation of [https://splunk.atlassian.net/browse/SPL-239645|https://splunk.atlassian.net/browse/SPL-239645|smart-link]  but on the version 9.2.4
*_Problem Analysis_*:

Replication seems to be stuck in the endless loop of restarting replication but stopping it, because one of the indexers is still ongoing replication:

{noformat}01-30-2025 09:09:40.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Trigger replication with reason=""async replication allowed""{noformat}

{noformat}01-30-2025 09:09:40.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Replication still in progress for peer with uri=https://10.186.25.75:8089 and status=1.{noformat}

{noformat}01-30-2025 09:09:40.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.{noformat}

{noformat}01-30-2025 09:09:40.380 +0000 DEBUG BundleReplicatorThread [1458992 BundleReplicatorThread] - Waiting to try again after cooldown{noformat}

{noformat}01-30-2025 09:09:41.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Trigger replication with reason=""async replication allowed""{noformat}

{noformat}01-30-2025 09:09:41.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Replication still in progress for peer with uri=https://10.186.25.75:8089 and status=1.{noformat}

{noformat}01-30-2025 09:09:41.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.{noformat}

{noformat}01-30-2025 09:09:41.380 +0000 DEBUG BundleReplicatorThread [1458992 BundleReplicatorThread] - Waiting to try again after cooldown{noformat}


When looking at the diag of the Indexer in question, no logs are seen regarding the attempt.

Creation of new knowledge bundles is also paused:

[https://drive.google.com/file/d/1cRY-Yr0snOWMXskrlB3-RkbOi9GXVpjA/view?usp=sharing|https://drive.google.com/file/d/1cRY-Yr0snOWMXskrlB3-RkbOi9GXVpjA/view?usp=sharing|smart-link]
 
Because of the version ""cascade_plan_replication_retry_fast"" in distsearch.conf is already set to true  by default. But it seems like replication is having an issue because it does not “Fail“ but stays “in progress“, however, when checking bundle replication status, all nodes are marked as successful(with affected one not being on the list due to cascading):
[https://drive.google.com/file/d/1bfuuR_Idj6JTM3lLgWw9efcK6J9D3b5i/view?usp=sharing|https://drive.google.com/file/d/1bfuuR_Idj6JTM3lLgWw9efcK6J9D3b5i/view?usp=sharing|smart-link] 

The issue is seen very intermittently. Customer saw a big drop in occurrences after the 9.2.4 upgrade and even stopped observing issue during ney year freeze, but it started re-occurring since.

Fresh diags are available on google drive:
SHC:
[https://drive.google.com/file/d/1Bd19lA-woq-MzkM0SXTQc-7mQDAOGNr0/view?usp=sharing|https://drive.google.com/file/d/1Bd19lA-woq-MzkM0SXTQc-7mQDAOGNr0/view?usp=sharing|smart-link] 
IDX:
[https://drive.google.com/file/d/1-MjpbYnWbsBiWGkt9YwywRxT3hHQktd5/view?usp=sharing|https://drive.google.com/file/d/1-MjpbYnWbsBiWGkt9YwywRxT3hHQktd5/view?usp=sharing|smart-link] 

And also on SplunkBOT:
SHC:
[https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=fd9e1cea-4ebd-4bf4-b208-1f45e34b4c62&case_numbertok=3637549&hosttok=gz-zu2appplv201&case_filetok=gz-zu2appplv201-sh_-20250130-121456-C96MUFV4&found_anontok=0&job_typetok=classic|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=fd9e1cea-4ebd-4bf4-b208-1f45e34b4c62&case_numbertok=3637549&hosttok=gz-zu2appplv201&case_filetok=gz-zu2appplv201-sh_-20250130-121456-C96MUFV4&found_anontok=0&job_typetok=classic]
IDX:
[https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=cc144e51-6e78-4d83-85ce-c4f169af50cb&case_numbertok=3637549&hosttok=gx-zu2pssplv093&case_filetok=gx-zu2pssplv093-idx_-20250130-121243-vS9FwYiT&found_anontok=0&job_typetok=classic|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=cc144e51-6e78-4d83-85ce-c4f169af50cb&case_numbertok=3637549&hosttok=gx-zu2pssplv093&case_filetok=gx-zu2pssplv093-idx_-20250130-121243-vS9FwYiT&found_anontok=0&job_typetok=classic]

*_Workaround_*

_Restart SHC Captain or trigger Captain re-election_",,Automation for Jira,Daniel Sánchez,Hongxun Liu,Jessica Kulak,Laquiche Cosby,Mike Sakahara,Niclas Andersson,Ryan Chan,User known,Wojciech Szumal,Yevhen Shylo,,,,,,,,,,,,,,,,,,,,,,,,,557058:f58131cb-b67d-43c7-b30d-6b58d40bd077,712020:36270c16-e891-42f0-abcd-5c4682637382,6053a3fb81b82500685ced67,6036b87618537600702416e1,6036b889d416ea0070072c72,5b18609873c23f253a5f8a44,6036b7996bc3f300699ce83a,6278d03db249c0006f94eb49,unknown,6036b7f5d3fc7c0068099fdb,61e678e308c1f7006a1704fc,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,SPL-272632,SPL-272633,SPL-272634,SPL-272740,,,,,,SPL-274537,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Above,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7b4375c0,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,PWC,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,11923200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 
SH Cluster that runs searches across hundreds of indexers

o If available, provide Splunk topology diagram, host name with IP mapping.
N/A",,,,,,,,,,"o What errors are being reported?
01-30-2025 09:09:40.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.",,,,,,,,,,,,,,,,,,,,,,,,,,"o What is the expectation when this problem is not there?
Expectation is for replication not to be halted because of one indexer",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• How long has the customer been tracking this issue and how frequent this issue is being hit?
Issue been encountered on-and off for a long time. Since the latest upgrade it occurs once a week/two",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thu May 08 23:42:34 UTC 2025,true,3980ff27-77f1-4495-af7e-c9bf44a2136e(3980ff27-77f1-4495-af7e-c9bf44a2136e),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Infrastructure - SH Management,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,addon_com.codebarrel.addons.automation(addon_com.codebarrel.addons.automation),e14d2c5b-738b-4131-bfbe-af152165a05f(e14d2c5b-738b-4131-bfbe-af152165a05f),hliu(hliu),jkulak(jkulak),lcosby(lcosby),3980ff27-77f1-4495-af7e-c9bf44a2136e(3980ff27-77f1-4495-af7e-c9bf44a2136e),57a32282-17fc-4c85-bd05-1eff0252fa8f(57a32282-17fc-4c85-bd05-1eff0252fa8f),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Codefix,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,No,,,,,,0|iex7un:,,,,,,,,,,,None,,None,None,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Rated,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PwC,3637549,012400000005WzMAAU,P2,Yes,5005a00003165WeAAI,,Closed,Resolved - Work Around,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3637549,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SearchInfra-SHM-FY25Q3-S3,SearchInfra-SHM-FY25Q3-S1,SearchInfra-SHM-FY25Q3-S2,SearchInfra-SHM-FY25Q3-S4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment
Not reproducible

• If available, will customer upgrade to fixed version?
Yes

• If support is able to reproduce, share the setup.
N/A",,,,,,,,,,12.0,,,,,,,,,,,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,On Prem Premium,,,,,,,,,,,,,,,,2025-01-30 17:56:20.257,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_1_*:*_0_*|*_1_*:*_1_*:*_468337_*|*_3_*:*_1_*:*_2085097267_*|*_16785_*:*_1_*:*_1897_*|*_12230_*:*_1_*:*_238126511_*|*_10001_*:*_1_*:*_124537116_*|*_11128_*:*_1_*:*_1642,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ryan Chan,,,,,"If cascading bundle replication is stuck, restart the captain or switch captaincy",,,,,,,,,,2025-01-30 17:56:20.257,2025-01-30 17:56:20.257,,,,,,,,,,,,,"30/Jan/25 9:56 AM;6053a3fb81b82500685ced67;[~accountid:6278d03db249c0006f94eb49] ,

Can you help triage this ticket?

Thanks,

Hongxun

cc [~accountid:616539acc669a60069cf7d1e] ",30/Jan/25 10:54 AM;6278d03db249c0006f94eb49;[~accountid:61e678e308c1f7006a1704fc] are all SH and IDX on the same version? just want to check if there is a potential mismatch here,"31/Jan/25 12:57 AM;61e678e308c1f7006a1704fc;[~accountid:6278d03db249c0006f94eb49] , as you can see from the diags attached, at least both SHC and indexer in question are of the same version(but also different indexers are being the culprits so issue is not isolated to only one)
I can clarify this if it is needed, but considering it’s PWC, all of their enviornment should be on the same version, especially considering that they have upgraded to 9.2.4 specifically do adress replication issues","05/Feb/25 4:49 PM;6278d03db249c0006f94eb49;(Reposting from slack)

It seems the issue is one indexer peer (10.186.25.75:8089) went down during replication and its status was never cleared (which was set to in-progress). Then, the peer came back up later with the same status, which blocks replication because splunk thinks an older replication is still in progress.Indexer replicating

{noformat}01-30-2025 03:17:46.011 +0000 DEBUG CascadingReplicationTransaction [1458992 BundleReplicatorThread] - Cascading replication txn for planId=4DD8F298-FFD5-475E-86F4-94B93E89F2BD has headers=x-splunk-dist-search-peername: 9773CC81-A867-43AE-A118-493815B5343C\nAuthorization: Splunk mlmYnUmlkzYEhwBHcu5Cguz6MIxXGpgpPDaggMBBxIurzpj4rMx2cmtgnd3FNWI322FC^NNkubqlcotO2VBF9K^hC5VoIQxk8QO47nLZhP9Pf1qeeN\n filePath=/opt/splunk/var/run/splunk/cascade_plans/9773CC81-A867-43AE-A118-493815B5343C/4DD8F298-FFD5-475E-86F4-94B93E89F2BD/plan.json uri=https://10.186.25.75:8089//services/replication/cascading/upload/plan/4DD8F298-FFD5-475E-86F4-94B93E89F2BD{noformat}

Indexer goes down

{noformat}01-30-2025 03:17:59.478 +0000 INFO  Shutdown [2886 Shutdown] - Shutdown complete in 27.94 seconds{noformat}

Indexer comes back up

{noformat}01-30-2025 03:25:46.755 +0000 INFO  loader [1506 MainThread] - Splunkd starting (build c103a21bb11d).{noformat}

Replication blocked

{noformat}01-30-2025 03:26:58.976 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Replication still in progress for peer with uri=https://10.186.25.75:8089 and status=1.{noformat}

So the next step is probably to investigate whether or not we will need to implement a code fix to clear the status when the peer goes down or when it comes back up and needs to be added back to the replication plan.",12/Feb/25 8:23 AM;6036b87618537600702416e1;[~accountid:6278d03db249c0006f94eb49] [~accountid:6053a3fb81b82500685ced67] checking in if there are any updates on this. Thanks.,12/Feb/25 12:56 PM;6278d03db249c0006f94eb49;[~accountid:6036b87618537600702416e1] Currently trying to reproduce this issue on my end to get a better idea of what needs to be fixed,12/Feb/25 3:04 PM;6036b889d416ea0070072c72;[~accountid:6278d03db249c0006f94eb49] Thanks for the update. Any additional findings or info you can share would be helpful. We have a call with the customer tomorrow morning. ,"12/Feb/25 5:22 PM;6278d03db249c0006f94eb49;This looks like another edge case, whereas the previous one the customer ran into was fixed in 9.2.4. We are considering a couple different ways to fix this, but in the meantime, we do need to try reproducing this issue. Will update you on anything we find",17/Feb/25 10:37 AM;6036b87618537600702416e1;[~accountid:6278d03db249c0006f94eb49] Is there an update we can share with the customer? We will be meeting with them tomorrow morning. Thanks.,17/Feb/25 5:29 PM;6278d03db249c0006f94eb49;Copying over the discussion from slack: [https://splunk.slack.com/archives/C083B5E0Y5C/p1739818152617789|https://splunk.slack.com/archives/C083B5E0Y5C/p1739818152617789|smart-link] ,"24/Feb/25 8:56 AM;712020:36270c16-e891-42f0-abcd-5c4682637382;Hi [~accountid:6278d03db249c0006f94eb49] , is there any update on this? what are the next steps at this point? thanks in advance! ","05/Mar/25 10:57 AM;6278d03db249c0006f94eb49;Currently have an MR open with the fix, which is still in review with the team. When the fix is merged, it will be backported to 9.2 and other previous releases. ",11/Mar/25 5:44 AM;6036b87618537600702416e1;[~accountid:6278d03db249c0006f94eb49] I see the posted branch cut date for 9.2.6 being 3/13. Will we be able to confirm at that point if this will be included in 9.2.6?,"11/Mar/25 1:33 PM;6278d03db249c0006f94eb49;[~accountid:6036b87618537600702416e1] yes we should be able to get this merged and backported by 3/13. I will let you know if anything changes, thanks","13/Mar/25 1:21 AM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:6278d03db249c0006f94eb49] has Approved the backport.  

[~accountid:6278d03db249c0006f94eb49], please proceed with the backport","13/Mar/25 10:04 AM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:6053a3fb81b82500685ced67] has Approved the backport.  

[~accountid:6278d03db249c0006f94eb49], please proceed with the backport","13/Mar/25 1:58 PM;6278d03db249c0006f94eb49;[~accountid:6036b87618537600702416e1] fix is now merged in develop, and has been backported to 9.1, 9.2, and 9.3","08/May/25 4:42 PM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:6278d03db249c0006f94eb49] since you have selected to document this in the release notes, please ensure that the Jira's title is clearly written and hide any internal terms or code names in the title by surrounding them with square brackets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3492888,SPL-257466,RFS Knowledge Bundle Replication Improvements,Done,13/Mar/25 1:50 PM
"Workload Management monitoring console dashboard shows memory usage exceeds the configured memory limit and the ""CPU used (# of cores)"" shows ""0""",SPL-268110,3735739,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,Mateusz Juda,614d2968f1c17400684bde7f,Shannon Bray,62b962f2118b20bee2baab7d,Shannon Bray,62b962f2118b20bee2baab7d,16/Dec/24 2:46 PM,30/Jul/25 4:24 PM,,,10.0.x,9.1.0(Beryllium),9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Workload Management,,,,,0,Reviewer-sylim,support-reviewed,,,,,,"As discussed in Slack: [https://splunk.slack.com/archives/C9Q6F6X37/p1730203376037169?thread_ts=1729746970.265219&cid=C9Q6F6X37|https://splunk.slack.com/archives/C9Q6F6X37/p1730203376037169?thread_ts=1729746970.265219&cid=C9Q6F6X37|smart-link] 

The customer has attempted to use Splunk's workload management by setting a search pool with a 1%, 2% or 5% memory limit:

{{etc/apps/search/local/workload_pools.conf}}

{noformat}[workload_pool:limit_user_pool]
category = search
cpu_weight = 10
default_category_pool = 0
mem_weight = 1
#mem_weight = 2
#mem_weight = 5{noformat}

However, the monitoring console dashboard {{Resource Usage > Workload Management > Workload Management Activity: Instance}} shows that the limit_user_pool memory usage exceeds the limit of 225MB, and all the ""CPU used (# of cores)"" shows ""0""

!image-20241216-225251.png|width=1836,height=665,alt=""image-20241216-225251.png""!

Output of {{| rest splunk_server=QCLMSMNSH01 /services/server/status/resource-usage/splunk-processes}}that was run on the Splunk Monitoring Console instance:

[^QCLMSMNSH01_splunk-processes.csv]



+*Customer Environment*+

8 x Indexer 9.1.0.2 (cluster) <- 8 x Search Head 9.1.0.2

Cluster Manager 9.1.0.2

Distributed Monitoring Console 9.1.0.2

+*Diags from customer:*+

qclmsmndp01: monitoring console ([google drive|https://drive.google.com/file/d/1-5tBnopi2SKjFpSwuMNRhCvXt3VRGpVB/view?usp=drive_link] [splunkbot|https://splunkbot.splunk.com/en-US/app/SplunkBOT/overview?indextok=0014000000pgi7zaat&job_idtok=bf987edd-b93d-4815-894e-b691ef141af5&case_numbertok=3633637&hosttok=qclmsmndp02&case_filetok=qclmsmndp02-sh_-20241216-222357-bE4bpkpr&found_anontok=0&job_typetok=classic&form.mount.tok=raw&form.mount.filter=&form.network.tok=raw&form.network.filter=&form.interfaces.filter=&form.cpuInfo.tok=raw&form.cpuInfo.filter=&form.processes.tok=raw&form.processes.filter=&form.ulimit.tok=raw&form.ulimit.filter=&form.memory.tok=raw&form.memory.filter=&form.kvstore.filter=&form.searchPeerBundlesDir.filter=&form.sinkholeDir.filter=&form.authDir.filter=])
qclmsmnsh02: search head captain ([google drive|https://drive.google.com/file/d/1-2G5JzqdwfXdq7vU3pRg97rlRgIgrqbm/view?usp=drive_link] [splunkbot|https://splunkbot.splunk.com/en-US/app/SplunkBOT/overview?indextok=0014000000pgi7zaat&job_idtok=ff7b687e-9958-49b9-bece-51c12a7a265f&case_numbertok=3633637&hosttok=qclmsmnsh02&case_filetok=qclmsmnsh02-sh_-20241216-222438-Jft6weIa&found_anontok=0&job_typetok=classic])
qclmsmnid02: indexer ([google drive|https://drive.google.com/file/d/1-1r1Z3k1k7QKhGcZAVaOdChgu3D4HN4y/view?usp=drive_link] [splunkbot|https://splunkbot.splunk.com/en-US/app/SplunkBOT/overview?indextok=0014000000pgi7zaat&job_idtok=03994c75-9cc4-463f-9b55-d25da93dc796&case_numbertok=3633637&hosttok=qclmsmnid02&case_filetok=qclmsmnid02-idx_-20241216-222338-oCHA7Pgr&found_anontok=0&job_typetok=classic])

+*Business Impact:*+

Splunk workload management is implemented to prevent runaway searches and Out of Memory conditions on the Splunk instances. According to the Splunk docs [https://docs.splunk.com/Documentation/Splunk/9.3.2/Workloads/Keyconcepts|https://docs.splunk.com/Documentation/Splunk/9.3.2/Workloads/Keyconcepts] ""the workload pool can only use memory up to the limit for which it has been configured.""

However a review of the ""Workload Management Activity: Instance"" dashboard shows that the ""Memory Used"" by the Workload Pools are significantly more than the configured ""Memory Limit"" and CPU usage is always at ""0"".

Based on this dashboard workload management is not enforcing memory limits correctly and it is unclear if CPU usage is being managed at all. As a result high-priority searches could be unexpectedly terminated due to OOM conditions, or the Splunk process itself could crash, leading to an outage which should have been avoidable.

+*The Customer would like to know:*+

# Why the memory usage exceeds the limit? (According to document [https://docs.splunk.com/Documentation/Splunk/9.3.2/Workloads/Keyconcepts|https://docs.splunk.com/Documentation/Splunk/9.3.2/Workloads/Keyconcepts] “the workload pool can only use memory up to the limit for which it has been configured. ”)
# Why no data for the CPU usage?",,Sarah Cheng,Shannon Bray,Sung Lim,User known,User known,Wiriadi Wangsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5f18fb3a5ee2c30023bde612,62b962f2118b20bee2baab7d,6036b832f8c057007083c0e3,unknown,unknown,6036b7ad4623c60069c040b7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16/Dec/24 3:00 PM;853d1b79-9eec-49ba-8a75-8942dac0584c;QCLMSMNSH01_splunk-processes.csv;https://splunk.atlassian.net/rest/api/3/attachment/content/7824265,16/Dec/24 2:55 PM;853d1b79-9eec-49ba-8a75-8942dac0584c;image-20241216-225251.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7824249,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Above,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@77f52c2f,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,8553600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tue Jun 17 00:41:04 UTC 2025,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,WLM,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,2.0,24.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,frankl(JIRAUSER52506),mjuda(JIRAUSER52333),scheng(scheng),853d1b79-9eec-49ba-8a75-8942dac0584c(853d1b79-9eec-49ba-8a75-8942dac0584c),sylim(sylim),wwangsa(wwangsa),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,No,,,,,,0|i02fw7:,,,,,,,,,,,None,,None,None,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Rated,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,The Hong Kong Jockey Club,3633637,012400000005WzMAAU,P2,Yes,5005a0000315zjmAAA,,Closed,Resolved - Work Around,Standard,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

# On Search Head create Workload Management Pool {{$SPLUNK_HOME/etc/apps/search/local/workload_pools.conf}}

{noformat}[workload_pool:limit_user_pool]
category = search
cpu_weight = 10
default_category_pool = 0
mem_weight = 2{noformat}

# Create {{$SPLUNK_HOME/etc/apps/search/local/workload_rules.conf}}

{noformat}[workload_rule:test_wlm]
predicate = user=test_wlm
user_message = Test Mesage
workload_pool = limit_user_pool

[workload_rules_order]
rules = test_wlm{noformat}

# Run search as user {{test_wlm}} 
# Check Monitoring Console {{Resource Usage > Workload Management > Workload Management Activity: Instance}}

• If available, will customer upgrade to fixed version?

yes

• If support is able to reproduce, share the setup.

*Nova Instance*

SH: [http://10.202.37.19:8000/|http://10.202.37.19:8000/]

DMC: [http://10.202.35.82:8000/|http://10.202.35.82:8000/]

Username: {{test_wlm}}

Password: default Nova Splunk admin password

username: admin

Password: default Nova Splunk admin password",,,,,,,,,,,,,,,,,,,,,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,Standard,,,,,,,,,,,,,,,,2024-12-17 04:43:01.776,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mateusz Juda,,,,,Set the Memory Limit % value to 100% for each workload pool.,,,,,,,,,,2024-12-17 04:43:01.776,2024-12-17 04:43:01.776,,,,,,,,,,,,,"16/Dec/24 8:43 PM;6036b832f8c057007083c0e3;# Review the Jira Summary {color:#36B37E}*[ PASS ]*{color} 
# Review Accuracy and Completeness {color:#36B37E}*[ PASS ]*{color}  
# Review the Description Field {color:#36B37E}*[ PASS ]*{color} 

+Comments to engineering+

[~accountid:614d2968f1c17400684bde7f] Here’s the Jira you requested in a slack discussion.

+Comments to TSE+

*_Triaged_*

",16/Dec/24 8:47 PM;6036b832f8c057007083c0e3;[~accountid:614d2968f1c17400684bde7f]  as requested in the slack this is assigned to you. ,"15/Jan/25 11:24 PM;62b962f2118b20bee2baab7d;Hi [~accountid:614d2968f1c17400684bde7f] , I wanted to check if you have had a chance to look into this? 

Is there any update I can provide to the customer?",22/Jan/25 2:26 PM;62b962f2118b20bee2baab7d;Hi [~accountid:614d2968f1c17400684bde7f] the customer is following up on this issue. If you could please provide an update or ETA it would be greatly appreciated.,"28/Jan/25 10:21 PM;6154ccf378e5e400701deb04;Hi [~accountid:614d2968f1c17400684bde7f] ,

hope you are well. not sure if you have the bw to look at this or anyone from [~accountid:712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee] 's team can help to have a look too?

Thanks

Frank",29/Jan/25 2:59 AM;614d2968f1c17400684bde7f;Hi [~accountid:6154ccf378e5e400701deb04] and [~accountid:62b962f2118b20bee2baab7d] we are discussing the issue with PMs. Actually it is not a simple bug.,"29/Jan/25 3:01 PM;62b962f2118b20bee2baab7d;Thanks [~accountid:614d2968f1c17400684bde7f] for the update.

Can I confirm is this just how the data is being displayed in the monitoring console dashboard and Workload Management itself is working as expected? ",30/Jan/25 3:29 AM;614d2968f1c17400684bde7f;Unfortunately not 😕 ,"10/Feb/25 9:00 PM;6154ccf378e5e400701deb04;Hi [~accountid:614d2968f1c17400684bde7f] and [~accountid:6278cef523d61e006fc3b1df] ,

Hope we are well. I know it’s not an easy design change – not sure if we are targeting this in any sprints that we have in the near future?

Thanks

Frank",18/Feb/25 5:53 AM;614d2968f1c17400684bde7f;[~accountid:6154ccf378e5e400701deb04] and [~accountid:62b962f2118b20bee2baab7d] I shared with you doc called “WLM memory issue” (see the inbox). It describes what we see here under the hood. As you will see it is not a regular issue.,"25/Feb/25 8:06 PM;6154ccf378e5e400701deb04;Thanks [~accountid:614d2968f1c17400684bde7f]  for sharing the doc.

However, it’s been last modified since Jan 2025 and not sure if there are continuous discussions over this topic still? ","26/Feb/25 2:07 AM;614d2968f1c17400684bde7f;Yes, we discuss it internally, cc PM [~accountid:6278cef523d61e006fc3b1df] .","04/Mar/25 2:17 PM;6154ccf378e5e400701deb04;Thanks [~accountid:6278cef523d61e006fc3b1df] and [~accountid:614d2968f1c17400684bde7f] .

Do we know whether this WLM fundamental design issue affects our cloud releases too?","05/Mar/25 1:14 AM;614d2968f1c17400684bde7f;Yes, cloud it also affected per se, however it is less (or even not at all) impactful. It is because the cloud stacks by default have {{100%}} memory for all search pools. It means that actually the memory limit doesn’t matter, even if we move processes between the pools (all pools shares the same memory). Still there is a possibility of an impact when a stack has additional pools (or non-default settings), but this is rare - our docs doesn’t support that, so the differences are only when there was a special ask to change it.","12/Mar/25 7:09 PM;6154ccf378e5e400701deb04;Hi [~accountid:614d2968f1c17400684bde7f][~accountid:712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee] and [~accountid:6278cef523d61e006fc3b1df] ,

Hope we are well. I know probably you all are very busy with discussing this issue – and i see this [https://docs.google.com/document/d/19kr-us600tmsviQmG6WoZjyKUiZ0Ah54SpH5fnLe3S0/edit?tab=t.0#heading=h.6w120iu8vdyc|https://docs.google.com/document/d/19kr-us600tmsviQmG6WoZjyKUiZ0Ah54SpH5fnLe3S0/edit?tab=t.0#heading=h.6w120iu8vdyc|smart-link]  doc has not been updating since Jan. Not sure if this issue is still under our radar?

Thanks,","25/Mar/25 6:35 PM;6154ccf378e5e400701deb04;Hi [~accountid:614d2968f1c17400684bde7f][~accountid:712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee] and [~accountid:6278cef523d61e006fc3b1df] ,

Hope you are well. 

still no luck seeing any progress from [https://docs.google.com/document/d/19kr-us600tmsviQmG6WoZjyKUiZ0Ah54SpH5fnLe3S0/edit?tab=t.0#heading=h.6w120iu8vdyc|https://docs.google.com/document/d/19kr-us600tmsviQmG6WoZjyKUiZ0Ah54SpH5fnLe3S0/edit?tab=t.0#heading=h.6w120iu8vdyc|smart-link]  – do we need any formal escalation on this to help push this moving forward? 

Could we open a doc bug to temporarily correct this and document this behaviour for customer’s awareness if we are not yet ready to fix it?

CC: [~accountid:5f18fb3a5ee2c30023bde612] ","25/Mar/25 6:43 PM;5f18fb3a5ee2c30023bde612;[~accountid:62b962f2118b20bee2baab7d] Could we please provide the business impact of the customer on this issue? Is it more of cosmetic issue or business critical?

[~accountid:614d2968f1c17400684bde7f] and team, understand this is a complex issue due to the initial design, would appreciate if you can confirm if this is treated as a bug and provide some sort of timeline or any workaround possible so that we can at least offer a relief to the customer while informing them this will take time to get fixed.",27/Mar/25 5:04 PM;62b962f2118b20bee2baab7d;Business impact has been added to the description.,"08/Jun/25 10:46 PM;6036b7ad4623c60069c040b7;Dear All,

+ [~accountid:614d2968f1c17400684bde7f] [~accountid:62b962f2118b20bee2baab7d] 

I've read [Mateusz's superb documentation|https://docs.google.com/document/d/19kr-us600tmsviQmG6WoZjyKUiZ0Ah54SpH5fnLe3S0/edit?tab=t.0]. From Mat's explanation, it looks really like a feature limitation to me (and could I suggest we frame it that way?)

Because WLM uses cgroups (v1 or v2) and in cgroups, charge transfer is either deprecated or not supported, imho, we need to have that stated in our public doc.

That is:

* We do not recommend enabling automatic transfer of workload from one workload pool to another at all.
* If customer wishes to do this, please proceed at their own risk. he Monitoring Console (MC) will report incorrect memory usage and there may be unexpected OOM.
* To ensure accurate MC reporting and avoid unexpected OOM, please do not enable automatic transfer of workload.


My question is if there is no load transfer from one pool to another at all, does the MC reporting look correct?","08/Jun/25 10:48 PM;6036b7ad4623c60069c040b7;If you guys are ok with my suggestion, I will raise a docguild Jira.

In the meantime, this is what the customer is up to:
[https://splunk.slack.com/archives/C08KN0XC869/p1748919058210559|https://splunk.slack.com/archives/C08KN0XC869/p1748919058210559|smart-link] 

* Regarding
** *Option 1:* Use {{mem_weight = 100%}} (disables memory limits, allows move rules) - this is not applicable in the customer environment
** *Option 2:* Don’t use move rules; assign searches to correct pool at start (memory is tracked properly) - the customer did apply the rules and monitor for this week.  They will have update by end of this week","10/Jun/25 3:00 AM;614d2968f1c17400684bde7f;[~accountid:6036b7ad4623c60069c040b7] 

{quote}My question is if there is no load transfer from one pool to another at all, does the MC reporting look correct?{quote}

Yes, it should be fine. At least I haven’t noticed anything wrong then.","15/Jun/25 7:00 PM;62b962f2118b20bee2baab7d;Hi [~accountid:614d2968f1c17400684bde7f] again thank you for your assistance with this.

Can I just confirm regarding the placement rules, the customer was using the following placement rule with no “runtime” in the testing but this also resulted in the memory usage exceeding the configured memory limit:

{noformat}[workload_rule:test_wlm]
predicate = user=test_wlm
user_message = Test Mesage
workload_pool = limit_user_pool{noformat}



Can I just double check that [option 2|https://splunk.slack.com/archives/C08KN0XC869/p1747231276317779?thread_ts=1747193696.699019&cid=C08KN0XC869] is valid as from the testing it does not seem to be working.

{noformat}Do not use move rules. A search has to be assigned to a pool at the beginning of its execution (placement rules) and has to stay there. No move, so the memory accounting is fine.{noformat}",16/Jun/25 4:43 AM;614d2968f1c17400684bde7f;[~accountid:62b962f2118b20bee2baab7d] That’s surprising tbh! Do we have screen shots and diags? What is the architecture of the deployment? ,"16/Jun/25 5:41 PM;62b962f2118b20bee2baab7d;[~accountid:614d2968f1c17400684bde7f]I think it might be a false alarm. I tried to repro it but couldn’t and I can confirm that WLM is kicking in as expected to kill the search when memory limit is reached when using a rule with {{predicate = user=test_wlm}}

After speaking with the customer yesterday, they will be setting the search pools to 100% memory as per the updated docs and then   adjusting the Search Category memory limit as needed to help prevent OOM.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Dec/24 2:46 PM
rsh does not return the error string to the fsh,SPL-267899,3730214,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Fixed,Krithik Acharya,6278cff98dd5f3006854762c,Nir Frenkel,6053a293009fee00694bc4e6,Nir Frenkel,6053a293009fee00694bc4e6,11/Dec/24 11:39 PM,30/Jul/25 4:26 PM,,29/Apr/25 12:47 PM,9.1.2308.200(KitKat_Tock),9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Lindt_Tock(9.1.2312.200),Milkyway(9.2.2403.100),Nutella,Oreo,Pocky,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Federation,,,,,0,,,,,,,,"# create a federated dataset that points to the data model {{internal_server}} on the {{rsh}}
# DO NOT create the lookup {{host.csv}} on the {{rsh}}  ( nor the {{fsh}} ) .run the following qeury

 3. run the following query :

{noformat}|tstats  prestats=true count from datamodel=federated:remote_internal_server by host | lookup host.csv host |stats count by host_description{noformat}

Since the lookup {{host.csv}} does not exist on the {{rsh}} the query fails on the {{rsh}}… however on the {{fsh}} we see the following message : 



{noformat}Failed to find the type of remote federated dataset. (error accessing remoteProvider='https://10.241.180.84:8089/servicesNS/user01/search/search/parser?output_mode=json', statusCode='400', description='Bad Request' while trying to get query type for query=' | tstats prestats=true summariesonly=false allow_old_summaries=false count FROM datamodel=internal_server BY host | lookup host.csv host | stats count by host_description' from deployment providerName='remote_deployment_1').
{noformat}

This is happening since the parser endpoint fails ( most likely in the eval process ).

We need to see if we can pass from the {{rsh}} to the {{fsh}} the right error which is actually 

{noformat}Error in 'lookup' command: Could not construct lookup 'host.csv, host'. See search.log for more details.
The lookup table 'host.csv' does not exist or is not available.
The search job has failed due to an error. You may be able view the job in the Job Inspector.{noformat}

!Screenshot 2024-12-11 at 11.44.50 PM.png|width=2047,height=671,alt=""Screenshot 2024-12-11 at 11.44.50 PM.png""!





!Screenshot 2024-12-11 at 11.45.19 PM.png|width=1208,height=593,alt=""Screenshot 2024-12-11 at 11.45.19 PM.png""!",,Jason New,User known,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:6ef22d56-cccf-4d8b-bf57-481513ee7c61,unknown,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,SPL-276684,,,,,,,,,,,,,,,,,,,,,,,,,,,11/Dec/24 11:47 PM;nfrenkel;Screenshot 2024-12-11 at 11.44.50 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7813944,11/Dec/24 11:47 PM;nfrenkel;Screenshot 2024-12-11 at 11.45.19 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7813945,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Above,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3e43f20f,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,10800000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Giri Basava,62a6ed3b9cd13c0068aea7f7,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives

List of individuals to include in a review of this incident

h1. Dependent Service Incident ID

(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers

h1. What Happened?

h1. What Did We Do Well?

h1. What Do We Need to Improve?

h1. Dogfood Our Own Observability Products

||*Product*||*Use*||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.

h3. Please share feedback about our products during this incident.

h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items

JIRAs should be linked to this ticket",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thu May 22 00:02:10 UTC 2025,true,adallas(adallas),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Federation,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,2.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,"Pre-conditions (If any):

Steps to reproduce:
1. 
2. 

Expected:

Actual:

Browser:
OS:",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,jnew(jnew),33934e6a-2fe2-4201-878c-68ba60bcf314(33934e6a-2fe2-4201-878c-68ba60bcf314),nfrenkel(JIRAUSER43952),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Krithik Acharya,6278cff98dd5f3006854762c,,,,,,,,,,,,,,,,,,0|ieol5b:,,,,,,,,,,,None,,None,None,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,Krithik Acharya,6278cff98dd5f3006854762c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MITRE,3549694,012400000005WzCAAU,P4,No,5005a00003146qyAAA,,Open,Pending Confirmation,Standard,Tier2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Datafed-FY25Q4-S6,Datafed-FY25Q4-S7,Datafed-FY25Q3-S1,Datafed-FY25Q3-S2,Datafed-FY25Q3-S3,Datafed-FY25Q3-S4,Datafed-FY25Q3-S5,Datafed-FY25Q3-S7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2025-05-08 16:30:17.818,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_1_*:*_0_*|*_3_*:*_3_*:*_3803930609_*|*_10001_*:*_3_*:*_4573060160_*|*_10039_*:*_1_*:*_2294188013,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Krithik Acharya,,,,,,,,,,,,,,,2025-05-08 16:30:17.818,2025-05-08 16:30:17.818,,,,,,Krithik Acharya,6278cff98dd5f3006854762c,,,,,,"08/May/25 9:30 AM;557058:6ef22d56-cccf-4d8b-bf57-481513ee7c61;Hi[~accountid:6053a293009fee00694bc4e6],  Can this improvement include the SID for the search so that it is clear which search has failed.  The scenario would be that a scheduled alert, relying on a RSH, starts failing because a lookup is deleted on the RSH.  In this scenario the proposed enhancement for the log format from the RSH would not be very helpful for identifying which Fed search has started generating parser errors.  If the SID from the FSH or ideally both is included then that would make troubleshooting a lot easier for our customers as they link this with audit log to see the name, owner, app and type of search that is failing.","08/May/25 11:19 PM;6053a293009fee00694bc4e6;Hi [~accountid:557058:6ef22d56-cccf-4d8b-bf57-481513ee7c61], thanks for the great idea for improvement. We actually return the {{rsh_sid}} from the {{fsh}} to the {{rsh}} for internal usage but we don’t expose it. I will add a ticket and we can add this to the product ","21/May/25 5:02 PM;6053a293009fee00694bc4e6;Hi [~accountid:557058:6ef22d56-cccf-4d8b-bf57-481513ee7c61] 

We have noticed that you are worried about scheduled searches…. so we understand from that you are not really interested in the UI error message …. so for that we have the {{rsh_sid}} in the audit.log of the {{fsh}} : 

{noformat}05-21-2025 22:56:46.811 +0000 INFO  AuditLogger - Audit:[timestamp=05-21-2025 22:56:46.812, user=admin, action=search, info=completed, search_id='1747868020.74', has_error_warn=false, fully_completed_search=true, total_run_time=163.84, event_count=2771534, result_count=10
00, available_count=1000, scan_count=2771534, drop_count=0, exec_time=1747868020, api_et=1747866180.000000000, api_lt=1747868020.000000000, api_index_et=N/A, api_index_lt=N/A, search_et=1747785000.000000000, search_lt=1747785600.000000000, is_realtime=0, savedsearch_name=
"""", search_type=ad-hoc, search_startup_time=""55"", is_prjob=true, is_spl2_search=false, is_flex_search=false, scenarios="""", rate_limit_retry_enabled=false, dispatch_artifact_bytes=2060288, status_csv_bytes=4096, is_fss3=false, is_fsasl=false, cpu_time=209.91, acceleration_
id=""A6AAA8FB-40A5-49E5-AFEA-675616F2F074_search_admin_a085580605a52ee6"", app=""search"", provenance=""UI:Search"", mode=""historical"", workload_pool=standard_perf, is_proxied=false, searched_buckets=22, eliminated_buckets=0, considered_events=2866595, total_slices=91706, decom
pressed_slices=15137, duration.command.search.index=824, invocations.command.search.index.bucketcache.hit=22, duration.command.search.index.bucketcache.hit=0, invocations.command.search.index.bucketcache.miss=0, duration.command.search.index.bucketcache.miss=0, invocation
s.command.search.index.bucketcache.error=0, duration.command.search.rawdata=2658, invocations.command.search.rawdata.bucketcache.hit=21, duration.command.search.rawdata.bucketcache.hit=0, invocations.command.search.rawdata.bucketcache.miss=0, duration.command.search.rawda
ta.bucketcache.miss=0, invocations.command.search.rawdata.bucketcache.error=0, sourcetype_count__turbine=2771534, roles='admin+power+user', search='search index=federated:reg-data earliest=1747785000 latest=1747785600', incomplete_bucket_maps='false', is_federated_search=
1, is_streaming_phase_only=1, is_transparent_mode=0, rsh_sid_0=""rsh1:1747868020.1909"", is_fsh_remote_search=0]{noformat}

Please see {{rsh_sid_0=""rsh1:1747868020.1909""}}

Is that good enough for you ?

Regarding to your request about the queries that fail in the {{parser}} phase there is not yet a search generated on the {{rsh}} ( [~accountid:5e9626c70fe27d0c0edf862a] and [~accountid:712020:a3572d76-3379-4f58-b9d9-84ba84ae8cf2]  found that ) …. so we can’t add that id ( that doesn’t exist yet ) to the {{fsh}} .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,29/Apr/25 12:47 PM
Search with mcatalog command returns missing metrics when used with append=t and last index is not valid,SPL-264529,3651081,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,Yuan Shen,611be82846c6b500717e747c,Yuan Shen,611be82846c6b500717e747c,Yuan Shen,611be82846c6b500717e747c,15/Oct/24 2:01 PM,30/Jul/25 4:26 PM,,,10.0.x,9.1.0,9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Federation,,,,,0,,,,,,,,"Search with {{mcatalog append=t}} and the last index is invalid, search results miss some metrics from the first valid index.  Event count is a lot less too.



{noformat}| mcatalog prestats=t values(_dims) WHERE index=_metrics | mcatalog prestats=t append=t values(_dims) WHERE index=invalid_metrics_index | stats values(_dims) {noformat}

The above search returns ~10 results. But there are some inconsistent results seen every time a search is run. 


If only run search on index=_metrics, the search returns 61 results consistently.

{noformat}| mcatalog prestats=t values(_dims) WHERE index=_metrics | stats values(_dims) {noformat}



But if I add a valid index name after the invalid name, such as {{index=_invalid_index OR index=_metrics}}, I see full results again, like below


{noformat}| mcatalog prestats=t values(_dims) WHERE index=_metrics | mcatalog prestats=t append=t values(_dims) WHERE index=invalid_index OR index=_metrics | stats values(_dims){noformat}



!Missing_results_from_append_invalid_index.png|width=1775,height=574,alt=""Missing_results_from_append_invalid_index.png""!



!Full_results_from_single_index_search.png|width=1759,height=955,alt=""Full_results_from_single_index_search.png""!",,Ankit Jain,Giri Basava,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a8b6686bf5007045d71b,62a6ed3b9cd13c0068aea7f7,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-261801,,,,,,,,,,,,,,,,,,,,15/Oct/24 2:19 PM;yuans;Full_results_from_single_index_search.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7650896,15/Oct/24 2:20 PM;yuans;Missing_results_from_append_invalid_index.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7650899,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Above,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@56ba053f,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,System,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,9072000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives

List of individuals to include in a review of this incident

h1. Dependent Service Incident ID

(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers

h1. What Happened?

h1. What Did We Do Well?

h1. What Do We Need to Improve?

h1. Dogfood Our Own Observability Products

||*Product*||*Use*||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.

h3. Please share feedback about our products during this incident.

h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items

JIRAs should be linked to this ticket",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tue Jun 10 20:32:14 UTC 2025,true,adallas(adallas),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Federation,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,2.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,"Pre-conditions (If any):

Steps to reproduce:
1. 
2. 

Expected:

Actual:

Browser:
OS:",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ankitj(ankitj),9657acd1-4db3-482a-a217-b59b5d5c075f(9657acd1-4db3-482a-a217-b59b5d5c075f),yuans(JIRAUSER50024),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|ifbuwf:,,,,,,,,,,,None,,None,None,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuan Shen,611be82846c6b500717e747c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2024-10-16 17:31:52.645,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Affected -> can_not_fix (no fix),NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2024-10-16 17:31:52.645,2024-10-16 17:31:52.645,,,,,,Yuan Shen,611be82846c6b500717e747c,,,,,,"16/Oct/24 10:31 AM;6053a8b6686bf5007045d71b;I did some analysis and it seems the {{MetricSearchStatsProcessor}} is handling the {{append=t}} case incorrectly.

Here, it would cause the processor that is working with non-existent index to set the {{info._query_finished}} prematurely while the first processor (that had valid index) was still producing results:

{noformat}void MetricStatsProcessor::execute(SearchResultsFiles &results, SearchResultsInfo &info)
{
    assert(_prestats);

    if (!info._realtime || (_backfill && !_backfill_done)) {
        if (!_pTStatsProc.valid()) {
            if (!_matchNothing) {
                info.addMessage(&gLogger, ""METRICS:TSTATS_PROC_INVALID"");
            }
            if (info._realtime) {
                _backfill_done = true;
            }
            else {
                info._query_finished = true;       // <---------------- CAUSES A FOLLOWING PROCESSOR IN APPEND MODE
                                                   // TO PREMATURELY INDICATE FINISHED STATUS
                return;
            }
        }
    ...
}{noformat}



The fix should be easy to not indicate the FINISHED STATUS unless the previous processor is done producing results:

{noformat}void MetricStatsProcessor::execute(SearchResultsFiles &results, SearchResultsInfo &info)
{
    assert(_prestats);

    if (!info._realtime || (_backfill && !_backfill_done)) {
        // DO NOT PROCEED UNTIL PREVIOUS PROCESSOR IS DONE
        if (_append && (!info._query_finished || !results.mem().empty()) {
            info._query_finished = false;
            return;
        }
  
        if (!_pTStatsProc.valid()) {
            if (!_matchNothing) {
                info.addMessage(&gLogger, ""METRICS:TSTATS_PROC_INVALID"");
            }
            if (info._realtime) {
                _backfill_done = true;
            }
            else {
                info._query_finished = true;
                return;
            }
        }
    ...
}{noformat}



Cc: [~accountid:611be82846c6b500717e747c] [~accountid:6053a57786b0dd007187fe97] ","16/Oct/24 10:45 PM;611be82846c6b500717e747c;[~accountid:6053a8b6686bf5007045d71b] I tried the following change, there are still results missing in the {{append=t }}case.  Just to confirm, it is 
{{if ((_append && !info._query_finished) || !results.mem().empty())}}

I do see it goes into this {{if}} condition multiple times.  

Been in the on-boarding training, so work is a bit delayed.

Thanks,
Yuan",10/Jun/25 1:32 PM;62a6ed3b9cd13c0068aea7f7;[~accountid:611be82846c6b500717e747c] a reminder to check with Ankit and identify the next steps ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Nov/24 2:33 PM
[9.3.0] Upgrade removes thruput processor and group=per_(source|sourcetype|index|host)_thruput in metrics.log for universalforwarders.,SPL-263518,3626810,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Harendra Rawat,6053a6d694d7b90069f83e4f,Siddharth Parui (C),61b6cf4d21f381006710960c,Siddharth Parui (C),61b6cf4d21f381006710960c,27/Sep/24 2:18 PM,30/Jul/25 4:24 PM,,18/Nov/24 6:31 PM,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0.x,,,,,,,,Pipeline Enhancements,Universal Forwarder,,,,0,enterprise-ui-reviewed,Reviewer-BrianOsburn,,,,,,"h3. Summary: 

Universal Forwarders post upgrading to version 9.3.0 loses visibility in {{metrics.logs}} for the group - {{group=per_sourcetype_thruput}}

h3. Customer impact: 

Universal Forwarders no longer report a vital part of logs, limiting the abilities of their monitoring dashboards.


Description/Problem Analysis:

* Upgrade to Universal Forwarder 9.3.0 causes the {{group=per_(source|sourcetype|index|host)_thruput}} under the {{metrics.log}} to be not logged.
* This is visible for universal forwarders that were/are being upgraded from 9.2.2 either via custom script or manual upgrade following the doc - [https://docs.splunk.com/Documentation/Splunk/9.3.1/Installation/AboutupgradingREADTHISFIRST|https://docs.splunk.com/Documentation/Splunk/9.3.1/Installation/AboutupgradingREADTHISFIRST|smart-link]  
* Before the upgrade, the group logs relevant metrics logs and pushes them to the universal forwarders, but this stops after the 9.3.0 upgrade as evident in the below screenshot -
(Date range - before the upgrade)
!image-20240930-132341.png|width=97.9820796879928%,alt=""image-20240930-132341.png""!
(Date range - after the upgrade performed at 27 Sept -01:00 AM GMT+05:30)
!image-20240930-132709.png|width=97.9820796879928%,alt=""image-20240930-132709.png""!
* We have tested this to be positive for any OS installation where the Splunk-Forwarder services are upgraded to 9.3.0.
*- Windows UF -* 
*Hostname:* *windernaut-UF-02*
*IP: 10.202.21.189* 
*Username: Splunker* 
*Passwd: W\!n_5up3rn0va\!*
*Splunk_username: admin*
*Splunk_userpass: admin@123*

*- Linux UF -* 
*Hostname: sleekylinux-UF-02*
*IP: 10.202.35.115*
*Username: Splunker*
*Passwd: splk*
*Splunk_username: admin*
*Splunk_userpass: admin@123*

* Diags for Splunk Universal Forwarders during our testing are dropped in the drive share here - 
Internal testing diags - [https://drive.google.com/drive/folders/1pzunRB6X3BOod_ezHpKKTVBK99PO8nKP?usp=drive_link|https://drive.google.com/drive/folders/1pzunRB6X3BOod_ezHpKKTVBK99PO8nKP?usp=drive_link|smart-link]  
Customer-provided diags -   [https://drive.google.com/drive/folders/1jVXCtGF_8i4XviZrVruCnuidFXZGanbG?usp=sharing|https://drive.google.com/drive/folders/1jVXCtGF_8i4XviZrVruCnuidFXZGanbG?usp=sharing|smart-link] ",,Aalok Mehta,Andrew Brown,Brian Osburn,Daniel West,Doron Keller,Gina Wesley,Harendra Rawat,Iuri Chaer,Izabela Stepek,Jo Hornsby,Oscar Montero (C),Prasanna Joshi (C),Richard Huber,Rock Baek,Siddharth Parui (C),Simon Boateng,srv -jira-gitlabci,Sung Lim,Victor Ebken,,,,,,,,,,,,,,,,,557058:5241047c-b73e-4d1b-9bee-9f9b7bcb9036,557058:37b4518c-e757-440f-87b7-181f5f425e80,6036b82dd3fc7c006809a25d,6036b7eb783a4600687c4be2,6053a38ef180c300675e6205,712020:be13152f-de3b-4eac-9001-1b9790174689,6053a6d694d7b90069f83e4f,6053a60f94d7b90069f835a9,712020:36069e23-50e0-42f3-a886-4464b9d9e8c4,557058:f87e2651-5132-4aed-8b96-f608ee969435,637f38d7a593cb822e945897,63a0b21578aabbefa9d12dd7,6036b80118537600702411d0,5f935739ddb0df006e91b422,61b6cf4d21f381006710960c,606d4d15c040cd0069d8bafa,62ec512f825fbfbfcff13ef5,6036b832f8c057007083c0e3,557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792,,,,,,,,,,,,,,,,,,0,,,,0,,,SPL-275142,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-266313,,,,,,,,,,,,,24/Oct/24 3:02 AM;dwest;Compare-9.2.3-vs-9.3.1.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7675652,26/Jun/25 1:46 PM;4db264c3-61c6-448b-8df6-cf01fd69ab0b;a4bf0f03-8a99-465d-bce7-61d82bce37a8.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8241984,30/Sep/24 6:30 AM;6364a585-6a04-4329-9f5b-916ee3f9e287;image-20240930-132341.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7606085,30/Sep/24 6:30 AM;6364a585-6a04-4329-9f5b-916ee3f9e287;image-20240930-132709.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7606084,16/Dec/24 10:22 AM;andrewb;image-20241216-182201.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7823478,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Above,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@76a0022a,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Splunk Cloud,,,,,,,,,Manpower INC.,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,13392000,,,,,,,,,,,,,,,,,,,,,,,,,Upgraded Version,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 
Upgraded universal forwaders from 9.2 to 9.3

o If available, provide Splunk topology diagram, host name with IP mapping.
N/a",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mon Apr 21 21:43:09 UTC 2025,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Noah Orchestration,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,5.0,34.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,amehta(amehta),abinkin(abinkin),andrewb(andrewb),1770fdde-ce02-4003-b41c-bd83649c1c11(1770fdde-ce02-4003-b41c-bd83649c1c11),bosburn(bosburn),dwest(dwest),doronk(doronk),d3c07cd6-2dc7-4d9e-87f9-8069c56b4e6d(d3c07cd6-2dc7-4d9e-87f9-8069c56b4e6d),hrawat(hrawat),ichaer(ichaer),8ff3a430-37aa-44ba-9264-d22f32b0c496(8ff3a430-37aa-44ba-9264-d22f32b0c496),8fa4b7c2-aed6-4191-9b73-1284d36b5e7a(8fa4b7c2-aed6-4191-9b73-1284d36b5e7a),402fff06-3ede-4e75-ad03-e6ab16a1e6da(402fff06-3ede-4e75-ad03-e6ab16a1e6da),4db264c3-61c6-448b-8df6-cf01fd69ab0b(4db264c3-61c6-448b-8df6-cf01fd69ab0b),rbaek(rbaek),6364a585-6a04-4329-9f5b-916ee3f9e287(6364a585-6a04-4329-9f5b-916ee3f9e287),sboateng(sboateng),06ac125d-c228-4c3f-b77f-a6149c059c6f(06ac125d-c228-4c3f-b77f-a6149c059c6f),,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|ie4og7:,,,,,,,,,,,None,,None,None,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,The Wawanesa Mutual Insurance Company,3782669,012400000005WzMAAU,P3,No,500KW00001puthuYAA,,Closed,Closed By Customer,Standard,Tier3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Spotlight FY25/Q2/1,Spotlight FY25/Q2/2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

- Install a UF on a version older than 9.3.0
- Configure the UF to send data to a Search-Head.
- Run the search - 

{noformat}index=_internal host=""<UF hostname>"" source=*metrics.log group=per_sourcetype_thruput{noformat}

- Confirm if you see data
- Upgrade the UF to 9.3.0
- Run the same search again and would not see data logged under - {{group=per_sourcetype_thruput}}

• If available, will customer upgrade to fixed version?
Yes

• If support is able to reproduce, share the setup.
Shared above.",,,,,,,,,,,,,,,,,,,,,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,Standard,,,,,,,,,,,,,,,,2024-09-27 22:56:06.542,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1_*:*_1_*:*_234534010_*|*_3_*:*_1_*:*_63588507_*|*_16785_*:*_2_*:*_576634049_*|*_6_*:*_1_*:*_0_*|*_12230_*:*_1_*:*_16525_*|*_10001_*:*_2_*:*_3636780751_*|*_11128_*:*_1_*:*_6117,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Harendra Rawat,,,,,"------ON UF ONLY-------
in default-mode.conf add following line<br/>

<code>#Turn off a processor<br/>
[pipeline:indexerPipe]<br/>
disabled_processors= index_thruput, indexer, indexandforward, latencytracker, diskusage, signing,tcp-output-generic-processor, syslog-output-generic-processor, http-output-generic-processor, stream-output-processor, s2soverhttpoutput, destination-key-processor</code>


<!--Splunkers: Ignore the markup tags in the workaround above. They're necessary to make the information display properly in public known issues.  Original workaround text without markup is below.

#Turn off a processor
[pipeline:indexerPipe]
disabled_processors= index_thruput, indexer, indexandforward, latencytracker, diskusage, signing,tcp-output-generic-processor, syslog-output-generic-processor, http-output-generic-processor, stream-output-processor, s2soverhttpoutput, destination-key-processor

-->",,,,,,,,,,2024-09-27 22:56:06.542,2024-09-27 22:56:06.542,,,,,,Harendra Rawat,6053a6d694d7b90069f83e4f,,,,,,"27/Sep/24 3:56 PM;712020:e4d13320-a98f-4b22-b346-ab5bba37f293;Hi [~accountid:61b6cf4d21f381006710960c] , this is logged under Admin Experience, I don’t see any details listed in this ticket. Can you please provide details to this ticket, so we can be sure which ProductBacklogArea & Mission Team it belongs to?",30/Sep/24 5:52 AM;61b6cf4d21f381006710960c;Extremely sorry for this [~accountid:712020:e4d13320-a98f-4b22-b346-ab5bba37f293] - We are working on getting all details added and should have it all ready by today’s EOD (US hours). ,30/Sep/24 7:30 AM;6036b82dd3fc7c006809a25d;Looks good to me - moving to the GDI team / Data Edge,01/Oct/24 8:45 AM;712020:e4d13320-a98f-4b22-b346-ab5bba37f293;[~accountid:61b6cf4d21f381006710960c] No worries at all! Wanted to make sure it went to the right spot 🙂 Thank you!,"08/Oct/24 8:22 AM;637f38d7a593cb822e945897;Hello team, I have a similar issue with a customer related to this (Case: *3569266*). UFs that I did not upgrade have generated 350000 to 580000 per_sourcetype_thruput events since Sept 13. The UFs that were upgraded have generated between 7 and 11. That is a 99.998% reduction in the number of *_thruput events. I hope you can help further and prioritize this.","23/Oct/24 2:17 AM;606d4d15c040cd0069d8bafa;[~accountid:6053af3886b0dd0071886e09] [~accountid:6053b41990f2880070097bd9] can we please have this ticket triaged & assigned to an engineer, there are multiple customers impacted & as I understand it, this has the potential to impact many more. [~accountid:712020:03e4dad8-c710-4dc1-9093-f7200b2b5b6c] ","23/Oct/24 2:59 AM;6036b7eb783a4600687c4be2;To speed things up a bit, please find test system here.
Standalone Indexer/SH 9.3.1 [http://10.202.35.194/|http://10.202.35.194/] admin : Chang3d!
UF9.2.3 10.202.5.130 splunker : splk  Splunk is admin : Chang3d!
UF9.3.1 10.202.3.220 splunker : splk  Splunk is admin : Chang3d!
Nothing altered, defaults out of the box.

index=_internal metrics  index=_internal NOT host=so1 group=per_host_thruput OR group=per_sourcetype_thruput OR group=per_index_thruput | timechart  count by host

[http://10.202.35.194:8000/en-GB/app/search/search?sid=1729677518.33|http://10.202.35.194:8000/en-GB/app/search/search?sid=1729677518.33]",23/Oct/24 11:06 AM;6053b41990f2880070097bd9;is this still an active issue?  SFDC is closed,"23/Oct/24 11:20 AM;5f935739ddb0df006e91b422;[~accountid:6053b24d86b0dd0071889031] 

Could you help us find the right team for this ticket?  We think this is for framework team. ","23/Oct/24 11:38 AM;63a0b21578aabbefa9d12dd7;Hi [~accountid:6053b41990f2880070097bd9] yes this is an active issue,, the case which got closed was other one which had similar issue. Raised by [~accountid:637f38d7a593cb822e945897] ","23/Oct/24 11:56 AM;63a0b21578aabbefa9d12dd7;This is the  active SFDC case: *3561825,* for which we raised this SPL","24/Oct/24 2:55 AM;6036b7eb783a4600687c4be2;[~accountid:6053b41990f2880070097bd9] There are other customer cases we can attach to this Jira to keep it open.
Please, we are able to reproduce behaviour.   See my comment below.

I think customers rely on these as arguably the 3 most important metrics from a UF.","24/Oct/24 2:59 AM;6036b7eb783a4600687c4be2;

[http://10.202.35.194:8000/en-GB/app/search/search?sid=1729763876.345|http://10.202.35.194:8000/en-GB/app/search/search?sid=1729763876.345].  admin Chang3d!

!Compare-9.2.3-vs-9.3.1.png|width=2718,height=984,alt=""Compare-9.2.3-vs-9.3.1.png""!",24/Oct/24 3:51 AM;6053b41990f2880070097bd9;[~accountid:6036b7eb783a4600687c4be2] for a resolution we need to reach out to the metrics team as suggested below.,"24/Oct/24 5:15 AM;6036b7eb783a4600687c4be2;[~accountid:6053b41990f2880070097bd9] Sorry for the gap in my knowledge here, this is the source of truth for routing to Dev…
[https://splunk.atlassian.net/wiki/spaces/CLOUDOPS/pages/34357217407/Cloud+Escalation+Policy#CloudEscalationPolicy-DeveloperObservability-MetricsPlatformTeam|https://splunk.atlassian.net/wiki/spaces/CLOUDOPS/pages/34357217407/Cloud+Escalation+Policy#CloudEscalationPolicy-DeveloperObservability-MetricsPlatformTeam|smart-link]    
No mention of Metrics on here unless it is for ‘Developer Observability - Metrics Platform Team’…. Is there a person you know who I can ping, or just #metrics on slack? ","24/Oct/24 8:26 AM;606d4d15c040cd0069d8bafa;Hi [~accountid:6053b41990f2880070097bd9] echoing [~accountid:6036b7eb783a4600687c4be2] below, do you have a contact/channel/team we can reach out to re the metrics team?","25/Oct/24 6:36 AM;6036b7eb783a4600687c4be2;Checking on Slack here [https://splunk.slack.com/archives/C015QJ897Q8/p1729862543855479|https://splunk.slack.com/archives/C015QJ897Q8/p1729862543855479|smart-link] 
Metrics gave it a look here [https://splunk.slack.com/archives/C8T2ZBT6Z/p1729838165640249|https://splunk.slack.com/archives/C8T2ZBT6Z/p1729838165640249|smart-link] ","26/Oct/24 5:53 AM;6053a6d694d7b90069f83e4f;Workaround
[https://community.splunk.com/t5/Getting-Data-In/Missing-per-thruput-metrics-on-9-3-x-Universal-forwarders/m-p/702841#M116237|https://community.splunk.com/t5/Getting-Data-In/Missing-per-thruput-metrics-on-9-3-x-Universal-forwarders/m-p/702841#M116237|smart-link] ","28/Oct/24 8:33 AM;6053a38ef180c300675e6205;Assigning to [~accountid:712020:9f15d6cb-b2a8-47b0-afd6-10a916bdfdbd] and the Foundations team based on discussions elsewhere. 

[~accountid:712020:9f15d6cb-b2a8-47b0-afd6-10a916bdfdbd] I understand that this issue will also affect upgrades of SH and IDX, not only UF so these should be tested as well. ",28/Oct/24 10:26 AM;63a0df33d3aeefa4054033d6;Hi [~accountid:6053a6d694d7b90069f83e4f] We suggested the workaround to the customer however they do not want to proceed with that and are waiting for a fix. Do we have any ETA for the fix/Upcoming release. Could you please elaborate a bit so that we can get back to the customer.,29/Oct/24 2:04 AM;557058:5241047c-b73e-4d1b-9bee-9f9b7bcb9036;Can this be added to the release notes?,"14/Nov/24 9:08 AM;62ec512f825fbfbfcff13ef5;This comment is auto-generated to inform you that the Jira severity level has been updated to align with the SFDC priority.
 If there is more than one support case linked, it will be set to align with the highest priority of open cases. If all cases are closed, then it will align to the highest priority of all closed cases. Please refer to the [P&T Customer Issues SLO|https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k] or contact [#help-customer-slo|https://slack.com/app_redirect?channel=C02KZJ51P4H]","15/Nov/24 7:09 AM;712020:36069e23-50e0-42f3-a886-4464b9d9e8c4;After investigating this issue we know that

* for UF two pipelines (indexerPipe and parsing) are registering the same metrics/callbacks (for thruput:thruput and for thruput:idxSummary) groups. In version 9.2.3 MetricsManager was overriding those, but in version 9.3.0 it is ignored and it does not allow to override already registered metric.
* We found this message in PipelineComponent.cpp:

{noformat}bool PipelineComponent::findPsetWithLeastThruput(unsigned& pset_num)
{
    bool found_processor = false;
    std::vector<Pipeline *> pipelines;
    //Only one of the thruput processor is enabled on any splunk instance
    //indexerPipe, index_thruput
    //parsing, thruput
    //remotequeueoutput, remote_thruput{noformat}
And for some reason it seems that for UF we have {indexerPipe, index_thruput} and {parsing, thruput} at the same time

So the workaround works because it is disabling {indexerPipe, index_thruput} process and leaves {parsing, thruput} . It seems there is nothing to do in MetricsManger and someone with domain/pipeline knowledge can decide whether the problem should be solved by changes in config files or somewhere else in the code.","18/Nov/24 5:52 AM;6053a60f94d7b90069f835a9;[~accountid:6053a6d694d7b90069f83e4f], I was thinking about this and my initial opinion is that having {{index_thruput}} enabled by default on UF is a bug. Do you know if there's a reason for it to be there?

I scanned through Skynet and found a few other metrics entries which have been affected, I’ll look into those to see what to do about them, but this specific one looks to me like a longstanding bug in our UF {{default-mode.conf}}.",18/Nov/24 11:45 AM;6053a6d694d7b90069f83e4f;[~accountid:6053a60f94d7b90069f835a9] it was an issue even where it was reporting 99% correctly with {{on collision last wins}}. Just the longstanding issue got exposed now with {{on collision first wins}}. I am fixing it now. We need both.,"18/Nov/24 11:59 AM;6053a60f94d7b90069f835a9;Cool, thanks 😉. I’m still not 100% sure on whether we should revert to the last one winning… I guess it’s the best way of avoiding more headache while we look into the collisions that we have. The right thing would be to assert, collisions are bugs and we’re definitely not catching them in-house without crashes 😕. Sucks that we’ve ignored those bugs for so long, using an assertion with the thing having been out there for so long sounds too risky.","18/Nov/24 12:27 PM;6053a6d694d7b90069f83e4f;With new behaviour where first metrics registration wins exposed a long
standing UF thruput metrics reporting.

On UF all unparsed events are pushed from parsingqueue to tcpout.
Thruput is reported by parsing thruput processor.

On Intermediate UF where IUF receives data fully parsed data(HFs/some UFs/third party etc), fully parsed data skips parsingQueue and lands
directly into indexqueue. Thruput is reported by indexing thruput processor.","18/Nov/24 12:29 PM;6053a6d694d7b90069f83e4f;[~accountid:6053a60f94d7b90069f835a9] let’s assert() and UT will pick the asserts. Have metrics owners fix. Because with collision, even though metrics manager is only calling back one, all the non-reporting modules are still  generating metrics without getting reported. ","18/Nov/24 12:32 PM;712020:be13152f-de3b-4eac-9001-1b9790174689;Team, my customer has been trying the suggested workaround and it has not worked.  Is there any case of the workaround just not working at all?  I provided the diags. for case 3614244.  Is there anything else we can do for them to fix this?",18/Nov/24 12:35 PM;6053a6d694d7b90069f83e4f;I would suggest verify if customer indeed applied workaround correctly and restarted UF.,"18/Nov/24 6:34 PM;6053a6d694d7b90069f83e4f;*Workaround:*
*------ON UF ONLY-------* 
*default-mode.conf*  add following


{noformat}#Turn off a processor 
[pipeline:indexerPipe] 
disabled_processors= index_thruput, indexer, indexandforward, latencytracker, diskusage, signing,tcp-output-generic-processor, syslog-output-generic-processor, http-output-generic-processor, stream-output-processor, s2soverhttpoutput, destination-key-processor{noformat}","16/Dec/24 10:22 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;Apologies, but I removed the # from before {{#Turn off a processor }}in the workaround field. I hope it’ll be understandable by Splunkers who look at that field in this Jira. This issues is included in the public release notes as a known issue, and the # was interpreted in the markup as a numbered list, which would be more confusing to customers. 



!image-20241216-182201.png|width=399,height=311,alt=""image-20241216-182201.png""!","18/Apr/25 3:43 AM;6053a6d694d7b90069f83e4f;{color:#ff5630}*Note:*{color} As a side effect of this issue, {color:#ff5630}maxKbps{color}(limits.conf) will also be impacted as it requires thruput metrics to function.","21/Apr/25 2:43 PM;6053a6d694d7b90069f83e4f;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] additional regression is detected. maxKbps is not functioning now since it needs thruput metrics. Is it possible to update it in known issues?
See details [https://splunk.slack.com/archives/C0880RC1GKB/p1744365636695339?thread_ts=1744365536.116819&cid=C0880RC1GKB|https://splunk.slack.com/archives/C0880RC1GKB/p1744365636695339?thread_ts=1744365536.116819&cid=C0880RC1GKB|smart-link] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,18/Nov/24 6:31 PM
CLONE - telemetry-metric local endpoint getting 401 errors,SPL-262543,3605233,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Fixed,Nicholas Siu,6270485ee51c620070bccce7,Nicholas Siu,6270485ee51c620070bccce7,Nicholas Siu,6270485ee51c620070bccce7,11/Sep/24 9:55 AM,30/Jul/25 4:24 PM,,11/Sep/24 9:57 AM,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Lindt_Tick(9.1.2312.100),Lindt_Tock(9.1.2312.200),Nutella,Oreo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0.x,Pocky,,,,,,,Search - Telemetry,,,,,0,enterprise-ui,,,,,,,"Customer is reporting 401s from the /telemetry-metric endpoint with {{/servicesNS/None/None/telemetry-metric}} appearing in the splunkd logsInvestigation:

* expanded the time window for the search of the warning logs and the partial results show that this issue is happening before lindt (confirmed up to kitkat thus far)
* upon further investigation of the logs, the routes with ""issues"" are later passing with a new sessionkey, meaning that this is not a route specific issue but a sessionkey issue
* when there is an invalid session key, the parameters app and owner have the appropriate values meaning the UI is properly passing the parameters to the endpoint
* not sure why {{None}} is being logged on the splunkd side but I would assume the backend is overriding it off of the session key?

Ask: We need assistance from SANDI Team to investigate the 401 errors.

h1. Summary

Increased 401 responses from the /telemetry-metric endpoint due to missing app and namespace ({{/servicesNS/None/None/telemetry-metric}}).

* The telemetry is non-critical and does not affect any product/feature functionality

No workarounds ","GCP classic stack
SHC, ES SH, IDM, ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,SPL-261455,,,,,,,,,,,,,,,,,,,,11/Sep/24 9:55 AM;1d21b593-19a5-4261-b9c5-c12554c4d65e;windowC.log;https://splunk.atlassian.net/rest/api/3/attachment/content/7552319,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7f880e2,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,https://splunk.atlassian.net/browse/CINC-57740,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,Standard,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Splunk Cloud,,,,,,,,,zions,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,32572800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 

What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives

List of individuals to include in a review of this incident

h1. Dependent Service Incident ID

(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers

h1. What Happened?

h1. What Did We Do Well?

h1. What Do We Need to Improve?

h1. Dogfood Our Own Observability Products

||*Product*||*Use*||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.

h3. Please share feedback about our products during this incident.

h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items

JIRAs should be linked to this ticket",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2024-09-11 16:55:08.119,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sandl Global OnCall,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1d21b593-19a5-4261-b9c5-c12554c4d65e(1d21b593-19a5-4261-b9c5-c12554c4d65e),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Codefix,,End User Experience,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Nicholas Siu,6270485ee51c620070bccce7,,,,,,,,,,,,,,,,,,0|ie6tdb:,,,,,,,,,,,None,,None,None,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3532309,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 [https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs|https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs]”?

*  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 [https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs|https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs]”?

*  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data [https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection|https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection]

*  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

*  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 

* What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.",,,Cloud Premium,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_158869,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Affected -> can_not_fix (no fix),NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,11/Sep/24 9:57 AM
"Splunk to Splunk federated searches do not utilize the dispatch.index_earliest and dispatch.index_latest  parameters in the saved search configuration when the search is dispatched to the remote search head, leading to incorrect results",SPL-262259,3597625,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Fixed,Krithik Acharya,6278cff98dd5f3006854762c,Mahak Shah,63055da649a5c6754d90fc7b,Mahak Shah,63055da649a5c6754d90fc7b,05/Sep/24 2:55 PM,06/Aug/25 1:13 PM,,30/Jan/25 9:35 AM,9.1.2308.200(KitKat_Tock),9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Lindt_Tock(9.1.2312.200),Milkyway(9.2.2403.100),Nutella,Oreo,Pocky,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0.2503.x,,,,,,,,Search Federation,,,,,1,,,,,,,,"see [https://splunk.slack.com/archives/C01VA6P7TD4/p1724961650680989|https://splunk.slack.com/archives/C01VA6P7TD4/p1724961650680989|smart-link] 



Federated Searches doesn't utilize the dispatch.index_earliest and dispatch.index_latest  params coming from the saved searches configuration when dispatching the search on the RSH.


{noformat}[Test - CloudDispatchBug]
cron_schedule = */15 * * * *
dispatch.earliest_time = -1d@d
dispatch.index_earliest = -20m@m
dispatch.index_latest = -5m@m
dispatch.latest_time = +1d@d
search = index=test source=""Test - Generate Test Data for Bug"" \
| eval event=_raw\
| rex field=_raw ""id=\""(?<id>[^\""]+)\""""\
| table _time, source, event, id\
| collect addtime=true index=mr-stuff-test{noformat}



When configured this way where {{index_earliest}} and {{index_latest}} are configs in the saved search. It is not dispatched to the RSH. See code:

{noformat}DistributedSearchResultCollectionManager::setupFederatedTransaction{noformat}

{noformat}   // TODO: In the case of an all time search et and lt = 0.0
    // Adding this check since default endpoint behavior without et and lt specified is an ALL TIME search
    if(info._search_lt > info._search_et) {
        postData << ""earliest_time"";
        postData << '=';
        postData << info._search_et;
        postData << '&';

        postData << ""latest_time"";
        postData << '=';
        postData << info._search_lt;
        postData << '&';
    }
    else if(info._search_lt.isZero() && !info._search_et.isZero())
    {
        // Case where user has provided only earliest_time and no latest_time.
        // Not passing latest_time so that the RSH can determine the default
        // value to keep in line with regular search behavior (SPL-241502)
        postData << ""earliest_time"";
        postData << '=';
        postData << info._search_et;
        postData << '&';
    }{noformat}

This needs to be fixed.

Workaround: 
These configs can be added as a part of SPL query. In this way these parameters are sent to the RSH. See 
[https://docs.splunk.com/Documentation/Splunk/9.3.0/SearchReference/SearchTimeModifiers#List_of_time_modifiers|https://docs.splunk.com/Documentation/Splunk/9.3.0/SearchReference/SearchTimeModifiers#List_of_time_modifiers] 

{noformat}Test - CloudDispatchBug]
cron_schedule = */15 * * * *
dispatch.earliest_time = -1d@d
dispatch.latest_time = +1d@d
search = index=test _index_earliest=-20m@m _index_latest=-5m@m source=""Test - Generate Test Data for Bug"" \
| eval event=_raw\
| rex field=_raw ""id=\""(?<id>[^\""]+)\""""\
| table _time, source, event, id\
| collect addtime=true index=mr-stuff-test{noformat}",,Andrew Brown,Doug Ewald,Giri Basava,Krithik Acharya,Macy Cronkrite,Mahak Shah,Nithin Krishna Reghunathan,Samuel Curtis,Teshika Holmes,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,622f94911f014e0069cc209b,62a6ed3b9cd13c0068aea7f7,6278cff98dd5f3006854762c,557058:a421b77e-b3c1-4bb3-813f-af8ade9c3659,63055da649a5c6754d90fc7b,712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0,61d406430586a20069486775,626f919e66ad530069d24c70,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Above,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4454ae3e,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,5270400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Giri Basava,62a6ed3b9cd13c0068aea7f7,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thu Jul 24 17:57:41 UTC 2025,true,addon_com.servicerocket.jira.salesforce(addon_com.servicerocket.jira.salesforce),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Federation,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,30.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Minor,High,,,"Pre-conditions (If any):

Steps to reproduce:
1. 
2. 

Expected:

Actual:

Browser:
OS:",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),2517d7b5-dd6f-46b7-a1c3-74b705d1cf52(2517d7b5-dd6f-46b7-a1c3-74b705d1cf52),9657acd1-4db3-482a-a217-b59b5d5c075f(9657acd1-4db3-482a-a217-b59b5d5c075f),33934e6a-2fe2-4201-878c-68ba60bcf314(33934e6a-2fe2-4201-878c-68ba60bcf314),1c4423df-41d5-4b57-977f-c8ba27c273c2(1c4423df-41d5-4b57-977f-c8ba27c273c2),dc729936-08dc-4a23-8bff-df98279dc85b(dc729936-08dc-4a23-8bff-df98279dc85b),e5ef8fd6-eb0e-4c63-ad75-de6ba4cd3898(e5ef8fd6-eb0e-4c63-ad75-de6ba4cd3898),27dd4d0d-8579-40ea-ad9b-09c0c62a35ff(27dd4d0d-8579-40ea-ad9b-09c0c62a35ff),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Krithik Acharya,6278cff98dd5f3006854762c,,,,,,,,,,,,,,,,,,0|i015r3:,,,,,,,,,,,None,,None,None,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,Krithik Acharya,6278cff98dd5f3006854762c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MITRE,3549694,012400000005WzMAAU,P4,No,5005a00003146qyAAA,,Closed,Resolved - Work Around,Standard,Tier2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,https://splunk.lightning.force.com/lightning/r/Case/5005a00003146qyAAA/view,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Datafed-FY25Q4-S1,Datafed-FY25Q4-S2,Datafed-FY25Q4-S4,Datafed-FY25Q4-S6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2024-09-11 14:51:50.242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_1_*:*_0_*|*_3_*:*_1_*:*_1149749272_*|*_10001_*:*_1_*:*_4569_*|*_10039_*:*_1_*:*_5943737632,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Krithik Acharya,,,,,"These parameters can be added as a part of the search string, using the <code>_index_earliest</code> and <code>_index_latest</code> time modifiers. This will send the parameters correctly to the remote search head. See 
[https://docs.splunk.com/Documentation/Splunk/latest/SearchReference/SearchTimeModifiers#List_of_time_modifiers List of time modifiers] in the ''Search Reference''.",,,,,,,,,,2024-09-11 14:51:50.242,2024-09-11 14:51:50.242,,,,,,Krithik Acharya,6278cff98dd5f3006854762c,,,,,,"11/Sep/24 7:51 AM;622f94911f014e0069cc209b;[~accountid:63055da649a5c6754d90fc7b] , do you have any updates on this issue? Customer is asking for an update but I see nothing in the support case (3549694) or this Jira ","11/Sep/24 8:22 AM;61d406430586a20069486775;[~accountid:62a6ed3b9cd13c0068aea7f7] I located your information from [https://splunk.atlassian.net/wiki/spaces/CLOUDOPS/pages/34357217407/Cloud+Escalation+Policy|https://splunk.atlassian.net/wiki/spaces/CLOUDOPS/pages/34357217407/Cloud+Escalation+Policy|smart-link]  . Can you please verify that this should be with your team? If so, can you please confirm next steps for this issue?

cc: [~accountid:622f94911f014e0069cc209b] ",11/Sep/24 8:59 AM;62a6ed3b9cd13c0068aea7f7;[~accountid:61d406430586a20069486775] yes. Have you worked with the customer to explore the workaround?,11/Sep/24 9:01 AM;61d406430586a20069486775;[~accountid:626f919e66ad530069d24c70] can you please confirm the information below from [~accountid:62a6ed3b9cd13c0068aea7f7] ?,"01/Oct/24 12:28 PM;626f919e66ad530069d24c70;[~accountid:63055da649a5c6754d90fc7b] & [~accountid:62a6ed3b9cd13c0068aea7f7] - I am asking if there is an update for this SPL. I’ve looked in the release notes and the workaround is not working for the customer either. Thanks, Teshika ","01/Oct/24 1:26 PM;63055da649a5c6754d90fc7b;Hi [~accountid:626f919e66ad530069d24c70] , This workaround was tested by me and worked in my local set up, Are we sure there isn’t any other bug. Could you elaborate more on the issue the customer is facing? 
Also tagging our current on-call if needed [~accountid:61324ce86a4c09006ac5ead2] ","01/Oct/24 1:43 PM;626f919e66ad530069d24c70;[~accountid:63055da649a5c6754d90fc7b] - which fix are you referring to? I can give this information to the customer and find out if it works for them. 

Teshika ","01/Oct/24 1:48 PM;63055da649a5c6754d90fc7b;The one in the ticket description:




Workaround: 
These configs can be added as a part of SPL query. In this way these parameters are sent to the RSH. See 
[https://docs.splunk.com/Documentation/Splunk/9.3.0/SearchReference/SearchTimeModifiers#List_of_time_modifiers|https://docs.splunk.com/Documentation/Splunk/9.3.0/SearchReference/SearchTimeModifiers#List_of_time_modifiers]

{noformat}Test - CloudDispatchBug]
cron_schedule = */15 * * * *
dispatch.earliest_time = -1d@d
dispatch.latest_time = +1d@d
search = index=test _index_earliest=-20m@m _index_latest=-5m@m source=""Test - Generate Test Data for Bug"" \
| eval event=_raw\
| rex field=_raw ""id=\""(?<id>[^\""]+)\""""\
| table _time, source, event, id\
| collect addtime=true index=mr-stuff-test{noformat}","23/Oct/24 6:28 AM;61d406430586a20069486775;[~accountid:626f919e66ad530069d24c70] Can you please confirm that the workaround below was provided to the customer from [~accountid:63055da649a5c6754d90fc7b] ?

[~accountid:63055da649a5c6754d90fc7b] Can you provide some insight into next steps for a long term fix for this issue? Do we have a timeline for introduction into a sprint?

CC- [~accountid:6036b7a06bc3f300699ce8a9] ","06/Nov/24 8:23 AM;61d406430586a20069486775;Hello[~accountid:63055da649a5c6754d90fc7b] [~accountid:62a6ed3b9cd13c0068aea7f7] , can you please provide some insight into next steps for a long term fix for this issue? Do we have a timeline for introduction into a sprint?","06/Nov/24 3:06 PM;62a6ed3b9cd13c0068aea7f7;[~accountid:61d406430586a20069486775] this is not priortized. I am adding our PM here for input [~accountid:712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0] . In addition, I don’t see if the workaround was tried by the customer. Can you or [~accountid:626f919e66ad530069d24c70] confirm?","07/Nov/24 6:15 AM;61d406430586a20069486775;Thanks [~accountid:62a6ed3b9cd13c0068aea7f7] , the customer has confirmed that the workaround is effective. However, noted the following:

”However, this is a product bug, I shouldn't have to specify the time modifiers in my search and instead should be able to use a documented feature as part of the conf file. When is the bug going to be fixed? I need the bug fixed so that we can close out this ticket.”

Any assistance to get it prioritized and placed into the pipeline would be much appreciated.

CC - [~accountid:712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0] [~accountid:63055da649a5c6754d90fc7b] ","07/Nov/24 4:48 PM;712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0;[~accountid:61d406430586a20069486775]  Glad to know that, workaround was effective. thanks for the update!
- Yes, I agree with the feedback from customer. We will have to work on long term fix even though the interim workaround did temporarily solve the problem. 
- Will prioritize this customer request and shall sync up with [~accountid:62a6ed3b9cd13c0068aea7f7] / [~accountid:63055da649a5c6754d90fc7b] to explore the permanent fix for this problem.",12/Nov/24 1:41 PM;712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0;[~accountid:61d406430586a20069486775] We have prioritized this bug fix request from customer and targeting this for our upcoming Q-release. We will keep you posted once we have an ETA to share with the customer. thanks for bringing this up!,"12/Nov/24 1:48 PM;712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0;[~accountid:62a6ed3b9cd13c0068aea7f7] As discussed, I’ve re-assigned this bug fix request to [~accountid:6278cff98dd5f3006854762c] for next steps and request the team to prioritize this item for our upcoming sprint. thanks!","13/Nov/24 4:58 AM;61d406430586a20069486775;Thanks [~accountid:712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0] , looking forward to it! Please let me know if there is anything we can do to support.","26/Nov/24 3:45 PM;6278cff98dd5f3006854762c;Some findings from investigation after implementing code updates:

Here’s the configuration for the saved search in {{savedsearches.conf}}:

{noformat}[Simple_Internal_Search]
action.email.useNSSubject = 1
action.webhook.enable_allowlist = 0
alert.track = 0
cron_schedule = * * * * *
dispatch.earliest_time = -15m
dispatch.latest_time = now
dispatch.index_earliest = -20m@m
dispatch.index_latest = -5m@m
enableSched = 1
schedule_priority = highest
search = index=_internal | stats count | rename count as ""Event Count from _internal"" | noop log_debug=*{noformat}

*On FSH search.log:*
(DistributedSearchResultCollectionManager::setupFederatedTransaction)

{noformat}11-26-2024 21:24:00.978 DEBUG DistributedSearchResultCollectionManager [452568 searchOrchestrator] - KRITHIK - Initializing index time parameters: _search_et=1732655340.000000, _search_lt=1732656240.000000
11-26-2024 21:24:00.979 DEBUG DistributedSearchResultCollectionManager [452568 searchOrchestrator] - KRITHIK - Initializing index time parameters: _index_et=0.000000, _index_lt=0.000000
11-26-2024 21:24:00.979 DEBUG DistributedSearchResultCollectionManager [452568 searchOrchestrator] - KRITHIK - Initializing index time parameters: _indextime_api_et=1732655040.000000, _indextime_api_lt=1732655940.000000{noformat}

These values are derived from *dispatch.index_earliest* and *dispatch.index_latest*. These values are updated in the *Post Data* sent to the RSH.

*On RSH search.log:*
(BatchSearch::initSearch)

{noformat}11-26-2024 21:24:01.498 INFO  BatchSearch [452660 localCollectorThread] -  BatchSearch is initialized for indexes = {_internal}, et = 1732655340.000000000, lt = 1732656240.000000000, index_et = 1732655040.000000000, index_lt = 1732655940.000000000, noRead = FALSE{noformat}

These match the values specified in the saved search configuration. The {{BatchSearch::initSearch}} method initializes a search by setting up the relevant arguments required to execute the search.

h3. *Note:*

When the saved search runs as a scheduled job (via {{cron_schedule}}) or via {{| savedsearch Simple_Internal_Search}} , {{_indextime_api_et}} and {{_indextime_api_lt}} are correctly updated based on {{dispatch.index_earliest}} and {{dispatch.index_latest}}.


{noformat}11-26-2024 21:48:02.100 DEBUG DistributedSearchResultCollectionManager [513633 searchOrchestrator] - KRITHIK - Initializing index time parameters: _search_et=1732568400.000000, _search_lt=1732657681.000000
11-26-2024 21:48:02.100 DEBUG DistributedSearchResultCollectionManager [513633 searchOrchestrator] - KRITHIK - Initializing index time parameters: _index_et=0.000000, _index_lt=0.000000
11-26-2024 21:48:02.100 DEBUG DistributedSearchResultCollectionManager [513633 searchOrchestrator] - KRITHIK - Initializing index time parameters: _indextime_api_et=1732656480.000000, _indextime_api_lt=1732657380.000000{noformat}

But clicking on {{Run}} in the UI (Settings → Searches, reports, and alerts) of the savedsearch doesn't:

{noformat}11-26-2024 21:50:56.366 DEBUG DistributedSearchResultCollectionManager [520807 searchOrchestrator] - KRITHIK - Initializing index time parameters: _search_et=1732656955.000000, _search_lt=1732657855.000000
11-26-2024 21:50:56.366 DEBUG DistributedSearchResultCollectionManager [520807 searchOrchestrator] - KRITHIK - Initializing index time parameters: _index_et=0.000000, _index_lt=0.000000
11-26-2024 21:50:56.366 DEBUG DistributedSearchResultCollectionManager [520807 searchOrchestrator] - KRITHIK - Initializing index time parameters: _indextime_api_et=0.000000, _indextime_api_lt=0.000000{noformat}


This is because latter way doesn’t execute the search as a savedsearch with the savedsearch properties.",04/Dec/24 6:31 AM;61d406430586a20069486775;[~accountid:6278cff98dd5f3006854762c] Wanted to check in and see if there were any updates to the resolution timeline? TIA!,"04/Dec/24 9:19 AM;6278cff98dd5f3006854762c;[~accountid:61d406430586a20069486775] 

It will be available in the next release (Q release)!

Thanks","10/Dec/24 8:21 AM;626f919e66ad530069d24c70;[~accountid:6278cff98dd5f3006854762c] when is the next release? I saw that Nutella updates have been paused due to some issues. 

Teshika ",10/Dec/24 8:48 AM;712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0;[~accountid:626f919e66ad530069d24c70] We are targeting this for our upcoming Q release (ETA around  June 2025),26/Feb/25 7:37 AM;622f94911f014e0069cc209b;[~accountid:712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0] is the mid March timeframe still on track? ,26/Feb/25 8:44 AM;712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0;[~accountid:61d406430586a20069486775] - This will be available in Q-release. ETA for Q-release would be around June 2025. Thanks!,28/Apr/25 12:20 PM;61d406430586a20069486775;[~accountid:712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0] Are we still on track for a June release?,"28/Apr/25 12:56 PM;712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0;Yes, that’s right! cc [~accountid:6278cff98dd5f3006854762c] ",02/Jul/25 8:14 AM;61d406430586a20069486775;[~accountid:712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0] What fix version is this release in?,07/Jul/25 8:29 AM;61d406430586a20069486775;[~accountid:626f919e66ad530069d24c70] [~accountid:712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0] The customer wants to know which version has the fix. Can you please provide this information?,07/Jul/25 9:52 AM;712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0;[~accountid:6278cff98dd5f3006854762c]  - Can you please confirm the fix version here?,"07/Jul/25 11:32 AM;6278cff98dd5f3006854762c;[~accountid:712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0] [~accountid:61d406430586a20069486775] 
This fix will be available from Splunk Enterprise version {{10.0.2503.0}} (QualityStreet), which is targeted for GA the week of July 28, 2025.",24/Jul/25 10:57 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:6278cff98dd5f3006854762c] please move 10.0.2503.x out of the affects versions list and into the fix version field. It’s important to have accurate information in these fields. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,30/Jan/25 9:35 AM
[non MVP] Revert does not revert versions associated to permission changes,SPL-260620,3567424,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,,,,Lizzy Li,6053a69a37065a0069999691,Lizzy Li,6053a69a37065a0069999691,12/Aug/24 9:27 AM,30/Jul/25 4:24 PM,,,10.0.2503.x,10.0.x,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Oreo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise Dashboards,,,,,0,,,,,,,,"A new version is created when a dashboard’s permissions changes. Currently, you cannot revert the permissions changes. Workaround is to manually change permissions. 

*AC*

* Allow users to revert permissions changes ",,Andrew Brown,Lizzy Li,User known,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,6053a69a37065a0069999691,unknown,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4e4416fd,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,5270400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thu Jul 24 17:23:29 UTC 2025,true,adallas(adallas),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,User Experience YVR - Enterprise Dashboards,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),kgorzynski(JIRAUSER52213),elizabethl(elizabethl),mimin(JIRAUSER46512),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,End User Experience,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|ie1ijz:,,,,,,,,,,,None,,None,None,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ent-Dash-Prioritized-Backlog,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2024-08-12 22:14:52.828,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not_affected,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2024-08-12 22:14:52.828,2024-08-12 22:14:52.828,,,,,,,,,,,,,12/Aug/24 9:28 AM;6053a69a37065a0069999691;[~accountid:6053b2b981b82500685d9561] another known issue for Oreo/9.4,12/Aug/24 3:14 PM;6053b2b981b82500685d9561;Added to the running list: [https://docs.google.com/document/d/130E14iTbPSvoIg4nV4OnWLg5km4PpEr7HIWsWSqo77Q/edit?usp=sharing|https://docs.google.com/document/d/130E14iTbPSvoIg4nV4OnWLg5km4PpEr7HIWsWSqo77Q/edit?usp=sharing|smart-link] ,"23/Jul/25 11:49 AM;61436d20e7c3280070a1979f;[~accountid:6053a69a37065a0069999691] is this still a known issue for QualityStreet? I’m assuming it still isn’t fixed, and belongs in the Known issues release notes for 10.0.2503. If that’s not correct, please let me know, so I can remove it from 10.0.2503 release notes. Thanks! ","23/Jul/25 12:25 PM;6053a69a37065a0069999691;Hi [~accountid:61436d20e7c3280070a1979f] it’s still a known issue. At this time though, we aren’t prioritizing fixing it. So I wonder if we should continue to doc it as a “known issue” or if we should put it in a list of limitations instead? ",23/Jul/25 4:07 PM;61436d20e7c3280070a1979f;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] do you have any other recommendations for this issue besides just continuing to carry it forward in Known issues? Do you want to move it to the Dashboard docs or leave it where it is in Release notes? ,"24/Jul/25 10:23 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;It’ll take more time to move this from known issues into an appropriate place in the docs. I don’t see moving it as a priority. I don’t object if someone feels strongly about doing it, but I won’t spend time on it",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3567427,SPL-260621,Dashboards - Version History v2,To Do,12/Aug/24 9:27 AM
Health Report for destination output issues show Last 50 detailed logs in Indexer Cluster nodes but not in Search Head or Cluster Manager,SPL-258394,3512102,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,,,Felix Jiang,6053bea406cbba006a0eca0c,Felix Jiang,6053bea406cbba006a0eca0c,28/Jun/24 5:35 PM,30/Jul/25 4:25 PM,,,10.0.x,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Health Report,,,,,0,,,,,,,,"Health Report Issues as posted in [https://docs.google.com/presentation/d/1OZF2cfbBXIXVbegRqmr908GGJVB7HFQe/edit?disco=AAABQeZaxKs|https://docs.google.com/presentation/d/1OZF2cfbBXIXVbegRqmr908GGJVB7HFQe/edit?disco=AAABQeZaxKs|smart-link] 

The health report details page seems to be behaving differently in an index cluster. On the search head and the CM, in distributed mode, we don't see the last 50 messages. The logs are only displayed if I log into the UI on an indexer, which a customer would rarely do.

This is specifically for logs related to File System or S3 output issues. Not sure if it also occurs for other types of logs. See images posted in [https://splunk.slack.com/archives/C0797JHL7SM/p1719482578774679|https://splunk.slack.com/archives/C0797JHL7SM/p1719482578774679|smart-link] ",,Andrew Brown,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@65d51ad9,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,36115200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thu Aug 01 16:18:39 UTC 2024,true,adallas(adallas),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Frameworks - Spotlight,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),fjiang(JIRAUSER47701),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|ifb5kv:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IngestActions-FY25Q2-S5,IngestActions-FY25Q3-S1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2024-07-17 00:47:39.04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2024-07-17 00:47:39.04,2024-07-17 00:47:39.04,,,,,,,,,,,,,16/Jul/24 5:47 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:6053bea406cbba006a0eca0c] is the affects version 9.2 or 9.3? I can make it show up for 9.2.x and 9.3.0 if that’s desired. The version in Jira today for 9.3.0 GA is just {{Duranium}}. We will update it after GA to 9.3.0(Duranium).,"17/Jul/24 8:52 AM;6053bea406cbba006a0eca0c;Hey [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] upon further reflection, this has always been the case since we first released an “S3 output health report indicator” which was 9.0.X I believe cc [~accountid:6053a57445a3bb006816a45e] . Given that would you recommend just adding documentation in the appropriate health report / ingest actions sections to point to the user that logs will only show up on indexers for output indicators (as opposed to listing as known issue)?","17/Jul/24 5:10 PM;6053bea406cbba006a0eca0c;After discussing with Stephen, we decided this belongs in the known issues section. 9.0.1+ and 9.0.2208+ are the affect versions [https://splunk.atlassian.net/browse/SPL-217661|https://splunk.atlassian.net/browse/SPL-217661|smart-link] ","17/Jul/24 5:25 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;Thanks, Felix and Stephen. 9.0.x is EOL and I don’t think we need this published for Cloud customers. I’ll add it to known issues for 9.1.0+ on Enterprise","01/Aug/24 9:18 AM;6053bea406cbba006a0eca0c;Hey [~accountid:6053c3de2f452d006f83c9e4] , this seems like a generic health report limitation (last 50 logs that show up on IDXC HRs don’t show up in the SH HR). Can this be prioritized in your backlog with fields adjusted accordingly?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3097925,SPL-243359,"

Ingest Actions UI Tech Debt",To Do,28/Jun/24 5:35 PM
"""| timechart count"" search is causing Splunk to crash with ""Crashing thread: searchOrchestrator""",SPL-255514,3447962,Bug,In Progress,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,Shihan Zhang,712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77,Illia Demydchenkov,6234374e62dc1e006803b2f4,Illia Demydchenkov,6234374e62dc1e006803b2f4,09/May/24 5:51 AM,24/Sep/25 7:41 AM,,,10.0.x,9.0.1,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.1.0,9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search,,,,,0,dwest_reviewed,emea_support:approved,support_reviewed,,,,,"Splunk crashes when executing the following search:

{noformat}| timechart count{noformat}

Splunk instance is not stopped, but the following crash log is generated:

{noformat}[build 050c9bca8588] 2024-05-06 11:27:07
Received fatal signal 11 (Segmentation fault) on PID 228293.
 Cause:
   No memory mapped at address [0x0000000000000000].
 Crashing thread: searchOrchestrator
 Registers:
    RIP:  [0x0000000000000000] ?
    RDI:  [0x00007FD620D5A200]
    RSI:  [0x0000563B0345AE34]
    RBP:  [0x00007FD61A7FD6A0]
    RSP:  [0x00007FD61A7FB908]
    RAX:  [0x0000563B059E9418]
    RBX:  [0x00007FD61A7FBA50]
    RCX:  [0x00007FD61A7FB928]
    RDX:  [0x0000000000000001]
    R8:  [0x00007FD61A7FB730]
    R9:  [0x00007FD61A7FB640]
    R10:  [0x0000563B05B51A70]
    R11:  [0x0000000000000000]
    R12:  [0x0000000000000001]
    R13:  [0x00007FD61A7FBA50]
    R14:  [0x00007FD620C0D3C0]
    R15:  [0x00007FD61A7FB9A0]
    EFL:  [0x0000000000010206]
    TRAPNO:  [0x000000000000000E]
    ERR:  [0x0000000000000014]
    CSGSFS:  [0x002B000000000033]
    OLDMASK:  [0x0000000000000000]

 OS: Linux
 Arch: x86-64

 Backtrace (PIC build):
 Linux / Q100SPUL4537 / 4.12.14-122.194-default / #1 SMP Mon Feb 12 14:50:38 UTC 2024 (dcbe7be) / x86_64
 /etc/SuSE-release: SUSE Linux Enterprise Server 12 (x86_64)
 glibc version: 2.22
 glibc release: stable
 MAP: 563b0000a000-563b05927000 r-xp 00000000 fe:0d 2729522                    /usd/as16544a/soft/splunk/bin/splunkd
 MAP: 563b05928000-563b05a69000 r--p 0591d000 fe:0d 2729522                    /usd/as16544a/soft/splunk/bin/splunkd
 MAP: 563b05a69000-563b05a71000 rw-p 05a5e000 fe:0d 2729522                    /usd/as16544a/soft/splunk/bin/splunkd
 MAP: 563b05a71000-563b05b52000 rw-p 00000000 00:00 0 
 MAP: 7fd605df5000-7fd605df6000 ---p 00000000 00:00 0 
 MAP: 7fd605df6000-7fd605ff6000 rw-p 00000000 00:00 0 
 MAP: 7fd6069fb000-7fd6069fc000 ---p 00000000 00:00 0 
 MAP: 7fd6069fc000-7fd606bfc000 rw-p 00000000 00:00 0 
 MAP: 7fd606bfc000-7fd606bfd000 ---p 00000000 00:00 0 
 MAP: 7fd606bfd000-7fd606dfd000 rw-p 00000000 00:00 0 
 MAP: 7fd606dfd000-7fd606dfe000 ---p 00000000 00:00 0 
 MAP: 7fd606dfe000-7fd606ffe000 rw-p 00000000 00:00 0 
 MAP: 7fd607400000-7fd60a600000 rw-p 00000000 00:00 0 
 MAP: 7fd60a800000-7fd60aa00000 rw-p 00000000 00:00 0 
 MAP: 7fd60abf5000-7fd60abf6000 ---p 00000000 00:00 0 
 MAP: 7fd60abf6000-7fd60adf6000 rw-p 00000000 00:00 0 
 MAP: 7fd60b000000-7fd60ba00000 rw-p 00000000 00:00 0 
 MAP: 7fd60c200000-7fd60e200000 rw-p 00000000 00:00 0 
 MAP: 7fd60e200000-7fd60ea00000 rw-p 00000000 00:00 0 
 MAP: 7fd60ea00000-7fd60f200000 rw-p 00000000 00:00 0 
 MAP: 7fd60f200000-7fd610000000 rw-p 00000000 00:00 0 
 MAP: 7fd610000000-7fd610600000 rw-p 00000000 00:00 0 
 MAP: 7fd610600000-7fd610e00000 rw-p 00000000 00:00 0 
 MAP: 7fd610ffb000-7fd610ffc000 ---p 00000000 00:00 0 
 MAP: 7fd610ffc000-7fd6111fc000 rw-p 00000000 00:00 0 
 MAP: 7fd611200000-7fd615800000 rw-p 00000000 00:00 0 
 MAP: 7fd6159ff000-7fd615a00000 ---p 00000000 00:00 0 
 MAP: 7fd615a00000-7fd615c00000 rw-p 00000000 00:00 0 
 MAP: 7fd615c00000-7fd616400000 rw-p 00000000 00:00 0 
 MAP: 7fd616400000-7fd617200000 rw-p 00000000 00:00 0 
 MAP: 7fd617200000-7fd617a00000 rw-p 00000000 00:00 0 
 MAP: 7fd617a00000-7fd617e00000 rw-p 00000000 00:00 0 
 MAP: 7fd617ffc000-7fd617ffd000 ---p 00000000 00:00 0 
 MAP: 7fd617ffd000-7fd6181fd000 rw-p 00000000 00:00 0 
 MAP: 7fd618200000-7fd618400000 rw-p 00000000 00:00 0 
 MAP: 7fd618600000-7fd618c00000 rw-p 00000000 00:00 0 
 MAP: 7fd618ffc000-7fd618ffd000 ---p 00000000 00:00 0 
 MAP: 7fd618ffd000-7fd6191fd000 rw-p 00000000 00:00 0 
 MAP: 7fd619200000-7fd61a400000 rw-p 00000000 00:00 0 
 MAP: 7fd61a5fe000-7fd61a5ff000 ---p 00000000 00:00 0 
 MAP: 7fd61a5ff000-7fd61a7ff000 rw-p 00000000 00:00 0 
 MAP: 7fd61a800000-7fd61ae00000 rw-p 00000000 00:00 0 
 MAP: 7fd61ae00000-7fd61b400000 rw-p 00000000 00:00 0 
 MAP: 7fd61b5ff000-7fd61b600000 ---p 00000000 00:00 0 
 MAP: 7fd61b600000-7fd61b800000 rw-p 00000000 00:00 0 
 MAP: 7fd61b800000-7fd61bc00000 rw-p 00000000 00:00 0 
 MAP: 7fd61bdff000-7fd61be00000 ---p 00000000 00:00 0 
 MAP: 7fd61be00000-7fd61c000000 rw-p 00000000 00:00 0 
 MAP: 7fd61c000000-7fd61c200000 rw-p 00000000 00:00 0 
 MAP: 7fd61c3fd000-7fd61c3fe000 ---p 00000000 00:00 0 
 MAP: 7fd61c3fe000-7fd61c5fe000 rw-p 00000000 00:00 0 
 MAP: 7fd61c600000-7fd61cc00000 rw-p 00000000 00:00 0 
 MAP: 7fd61cdff000-7fd61ce00000 ---p 00000000 00:00 0 
 MAP: 7fd61ce00000-7fd61d000000 rw-p 00000000 00:00 0 
 MAP: 7fd61d000000-7fd61d200000 rw-p 00000000 00:00 0 
 MAP: 7fd61d3ff000-7fd61d400000 ---p 00000000 00:00 0 
 MAP: 7fd61d400000-7fd61d600000 rw-p 00000000 00:00 0 
 MAP: 7fd61d600000-7fd61d800000 rw-p 00000000 00:00 0 
 MAP: 7fd61d9ff000-7fd61da00000 ---p 00000000 00:00 0 
 MAP: 7fd61da00000-7fd61dc00000 rw-p 00000000 00:00 0 
 MAP: 7fd61dc00000-7fd61e400000 rw-p 00000000 00:00 0 
 MAP: 7fd61e5ff000-7fd61e600000 ---p 00000000 00:00 0 
 MAP: 7fd61e600000-7fd61ea00000 rw-p 00000000 00:00 0 
 MAP: 7fd61ea00000-7fd61ee00000 rw-p 00000000 00:00 0 
 MAP: 7fd61ee00000-7fd620600000 rw-p 00000000 00:00 0 
 MAP: 7fd6207ff000-7fd620800000 ---p 00000000 00:00 0 
 MAP: 7fd620800000-7fd620a00000 rw-p 00000000 00:00 0 
 MAP: 7fd620a00000-7fd620e00000 rw-p 00000000 00:00 0 
 MAP: 7fd620ffe000-7fd620fff000 ---p 00000000 00:00 0 
 MAP: 7fd620fff000-7fd6211ff000 rw-p 00000000 00:00 0 
 MAP: 7fd6211ff000-7fd621200000 ---p 00000000 00:00 0 
 MAP: 7fd621200000-7fd621400000 rw-p 00000000 00:00 0 
 MAP: 7fd621400000-7fd621600000 rw-p 00000000 00:00 0 
 MAP: 7fd6217ff000-7fd621800000 ---p 00000000 00:00 0 
 MAP: 7fd621800000-7fd621a00000 rw-p 00000000 00:00 0 
 MAP: 7fd621a00000-7fd621e00000 rw-p 00000000 00:00 0 
 MAP: 7fd621fff000-7fd622000000 ---p 00000000 00:00 0 
 MAP: 7fd622000000-7fd622200000 rw-p 00000000 00:00 0 
 MAP: 7fd622200000-7fd624a00000 rw-p 00000000 00:00 0 
 MAP: 7fd624bff000-7fd624c00000 ---p 00000000 00:00 0 
 MAP: 7fd624c00000-7fd624e00000 rw-p 00000000 00:00 0 
 MAP: 7fd624e00000-7fd625000000 rw-p 00000000 00:00 0 
 MAP: 7fd6251ff000-7fd625200000 ---p 00000000 00:00 0 
 MAP: 7fd625200000-7fd625400000 rw-p 00000000 00:00 0 
 MAP: 7fd625400000-7fd625a00000 rw-p 00000000 00:00 0 
 MAP: 7fd625a6f000-7fd625a8f000 rwxp 00000000 00:00 0 
 MAP: 7fd625a9f000-7fd625aaf000 rwxp 00000000 00:00 0 
 MAP: 7fd625abf000-7fd625acf000 rwxp 00000000 00:00 0 
 MAP: 7fd625aef000-7fd625bff000 rwxp 00000000 00:00 0 
 MAP: 7fd625bff000-7fd625c00000 ---p 00000000 00:00 0 
 MAP: 7fd625c00000-7fd625e00000 rw-p 00000000 00:00 0 
 MAP: 7fd625e00000-7fd626800000 rw-p 00000000 00:00 0 
 MAP: 7fd626806000-7fd6268f6000 rwxp 00000000 00:00 0 
 MAP: 7fd6268f6000-7fd626918000 r-xp 00000000 00:2a 6950247                    /lib64/libgcc_s.so.1
 MAP: 7fd626918000-7fd626919000 r--p 00021000 00:2a 6950247                    /lib64/libgcc_s.so.1
 MAP: 7fd626919000-7fd62691a000 rw-p 00022000 00:2a 6950247                    /lib64/libgcc_s.so.1
 MAP: 7fd626920000-7fd626930000 rwxp 00000000 00:00 0 
 MAP: 7fd626930000-7fd626c00000 rwxp 00000000 00:00 0 
 MAP: 7fd626c00000-7fd627000000 rw-p 00000000 00:00 0 
 MAP: 7fd627003000-7fd627004000 ---p 00000000 00:00 0 
 MAP: 7fd627004000-7fd62700d000 rw-p 00000000 00:00 0 
 MAP: 7fd62700d000-7fd62700e000 ---p 00000000 00:00 0 
 MAP: 7fd62700e000-7fd62702e000 rwxp 00000000 00:00 0 
 MAP: 7fd62702e000-7fd627033000 rw-p 00000000 00:00 0 
 MAP: 7fd627033000-7fd6271ce000 r-xp 00000000 00:2a 5870486                    /lib64/libc-2.22.so
 MAP: 7fd6271ce000-7fd6271d2000 r--p 0019b000 00:2a 5870486                    /lib64/libc-2.22.so
 MAP: 7fd6271d2000-7fd6271d4000 rw-p 0019f000 00:2a 5870486                    /lib64/libc-2.22.so
 MAP: 7fd6271d4000-7fd6271d8000 rw-p 00000000 00:00 0 
 MAP: 7fd6271d8000-7fd6272d3000 r-xp 00000000 00:2a 5870494                    /lib64/libm-2.22.so
 MAP: 7fd6272d3000-7fd6272d4000 r--p 000fb000 00:2a 5870494                    /lib64/libm-2.22.so
 MAP: 7fd6272d4000-7fd6272d5000 rw-p 000fc000 00:2a 5870494                    /lib64/libm-2.22.so
 MAP: 7fd6272d5000-7fd627589000 r-xp 00000000 fe:0d 2671333                    /usd/as16544a/soft/splunk/lib/libcrypto.so.1.0.0
 MAP: 7fd627589000-7fd62758a000 ---p 002b4000 fe:0d 2671333                    /usd/as16544a/soft/splunk/lib/libcrypto.so.1.0.0
 MAP: 7fd62758a000-7fd6275a8000 r--p 002b4000 fe:0d 2671333                    /usd/as16544a/soft/splunk/lib/libcrypto.so.1.0.0
 MAP: 7fd6275a8000-7fd6275b6000 rw-p 002d2000 fe:0d 2671333                    /usd/as16544a/soft/splunk/lib/libcrypto.so.1.0.0
 MAP: 7fd6275b6000-7fd6275bb000 rw-p 00000000 00:00 0 
 MAP: 7fd6275bb000-7fd627627000 r-xp 00000000 fe:0d 2672828                    /usd/as16544a/soft/splunk/lib/libssl.so.1.0.0
 MAP: 7fd627627000-7fd627628000 ---p 0006c000 fe:0d 2672828                    /usd/as16544a/soft/splunk/lib/libssl.so.1.0.0
 MAP: 7fd627628000-7fd62762c000 r--p 0006c000 fe:0d 2672828                    /usd/as16544a/soft/splunk/lib/libssl.so.1.0.0
 MAP: 7fd62762c000-7fd627632000 rw-p 00070000 fe:0d 2672828                    /usd/as16544a/soft/splunk/lib/libssl.so.1.0.0
 MAP: 7fd627632000-7fd62764a000 r-xp 00000000 00:2a 5870526                    /lib64/noelision/libpthread-2.22.so
 MAP: 7fd62764a000-7fd62764b000 r--p 00017000 00:2a 5870526                    /lib64/noelision/libpthread-2.22.so
 MAP: 7fd62764b000-7fd62764c000 rw-p 00018000 00:2a 5870526                    /lib64/noelision/libpthread-2.22.so
 MAP: 7fd62764c000-7fd627650000 rw-p 00000000 00:00 0 
 MAP: 7fd627650000-7fd6277c8000 r-xp 00000000 fe:0d 2670637                    /usd/as16544a/soft/splunk/lib/libsqlite3.so.0.8.6
 MAP: 7fd6277c8000-7fd6277cb000 r--p 00177000 fe:0d 2670637                    /usd/as16544a/soft/splunk/lib/libsqlite3.so.0.8.6
 MAP: 7fd6277cb000-7fd6277d1000 rw-p 0017a000 fe:0d 2670637                    /usd/as16544a/soft/splunk/lib/libsqlite3.so.0.8.6
 MAP: 7fd6277d1000-7fd6277d3000 rw-p 00000000 00:00 0 
 MAP: 7fd6277d3000-7fd6277e4000 r-xp 00000000 fe:0d 2670643                    /usd/as16544a/soft/splunk/lib/libbz2.so.1.0.3
 MAP: 7fd6277e4000-7fd6277e5000 ---p 00011000 fe:0d 2670643                    /usd/as16544a/soft/splunk/lib/libbz2.so.1.0.3
 MAP: 7fd6277e5000-7fd6277e6000 r--p 00011000 fe:0d 2670643                    /usd/as16544a/soft/splunk/lib/libbz2.so.1.0.3
 MAP: 7fd6277e6000-7fd6277e7000 rw-p 00012000 fe:0d 2670643                    /usd/as16544a/soft/splunk/lib/libbz2.so.1.0.3
 MAP: 7fd6277e7000-7fd627893000 r-xp 00000000 fe:0d 2672904                    /usd/as16544a/soft/splunk/lib/libarchive.so.13.6.2
 MAP: 7fd627893000-7fd627896000 r--p 000ab000 fe:0d 2672904                    /usd/as16544a/soft/splunk/lib/libarchive.so.13.6.2
 MAP: 7fd627896000-7fd627897000 rw-p 000ae000 fe:0d 2672904                    /usd/as16544a/soft/splunk/lib/libarchive.so.13.6.2
 MAP: 7fd627897000-7fd627898000 rw-p 00000000 00:00 0 
 MAP: 7fd627898000-7fd62789a000 r-xp 00000000 00:2a 5870492                    /lib64/libdl-2.22.so
 MAP: 7fd62789a000-7fd62789b000 r--p 00002000 00:2a 5870492                    /lib64/libdl-2.22.so
 MAP: 7fd62789b000-7fd62789c000 rw-p 00003000 00:2a 5870492                    /lib64/libdl-2.22.so
 MAP: 7fd62789c000-7fd62789d000 rw-p 00000000 00:00 0 
 MAP: 7fd62789d000-7fd6278f2000 r-xp 00000000 fe:0d 2670642                    /usd/as16544a/soft/splunk/lib/libxmlsec1-openssl.so.1.2.24
 MAP: 7fd6278f2000-7fd6278f3000 ---p 00055000 fe:0d 2670642                    /usd/as16544a/soft/splunk/lib/libxmlsec1-openssl.so.1.2.24
 MAP: 7fd6278f3000-7fd6278f6000 r--p 00055000 fe:0d 2670642                    /usd/as16544a/soft/splunk/lib/libxmlsec1-openssl.so.1.2.24
 MAP: 7fd6278f6000-7fd6278f7000 rw-p 00058000 fe:0d 2670642                    /usd/as16544a/soft/splunk/lib/libxmlsec1-openssl.so.1.2.24
 MAP: 7fd6278f7000-7fd6278f8000 rw-p 00000000 00:00 0 
 MAP: 7fd6278f8000-7fd627973000 r-xp 00000000 fe:0d 2670640                    /usd/as16544a/soft/splunk/lib/libxmlsec1.so.1.2.24
 MAP: 7fd627973000-7fd627974000 ---p 0007b000 fe:0d 2670640                    /usd/as16544a/soft/splunk/lib/libxmlsec1.so.1.2.24
 MAP: 7fd627974000-7fd627976000 r--p 0007b000 fe:0d 2670640                    /usd/as16544a/soft/splunk/lib/libxmlsec1.so.1.2.24
 MAP: 7fd627976000-7fd627978000 rw-p 0007d000 fe:0d 2670640                    /usd/as16544a/soft/splunk/lib/libxmlsec1.so.1.2.24
 MAP: 7fd627978000-7fd6279ca000 r-xp 00000000 fe:0d 2670614                    /usd/as16544a/soft/splunk/lib/libxslt.so.1.1.34
 MAP: 7fd6279ca000-7fd6279cb000 ---p 00052000 fe:0d 2670614                    /usd/as16544a/soft/splunk/lib/libxslt.so.1.1.34
 MAP: 7fd6279cb000-7fd6279cc000 r--p 00052000 fe:0d 2670614                    /usd/as16544a/soft/splunk/lib/libxslt.so.1.1.34
 MAP: 7fd6279cc000-7fd6279cd000 rw-p 00053000 fe:0d 2670614                    /usd/as16544a/soft/splunk/lib/libxslt.so.1.1.34
 MAP: 7fd6279cd000-7fd6279cf000 rw-p 00000000 00:00 0 
 MAP: 7fd6279cf000-7fd627b84000 r-xp 00000000 fe:0d 2672827                    /usd/as16544a/soft/splunk/lib/libxml2.so.2.9.10
 MAP: 7fd627b84000-7fd627b85000 ---p 001b5000 fe:0d 2672827                    /usd/as16544a/soft/splunk/lib/libxml2.so.2.9.10
 MAP: 7fd627b85000-7fd627b8d000 r--p 001b5000 fe:0d 2672827                    /usd/as16544a/soft/splunk/lib/libxml2.so.2.9.10
 MAP: 7fd627b8d000-7fd627b8f000 rw-p 001bd000 fe:0d 2672827                    /usd/as16544a/soft/splunk/lib/libxml2.so.2.9.10
 MAP: 7fd627b8f000-7fd627b90000 rw-p 00000000 00:00 0 
 MAP: 7fd627b90000-7fd627c52000 r-xp 00000000 fe:0d 2672906                    /usd/as16544a/soft/splunk/lib/libpcre2-8.so.0.11.0
 MAP: 7fd627c52000-7fd627c53000 ---p 000c2000 fe:0d 2672906                    /usd/as16544a/soft/splunk/lib/libpcre2-8.so.0.11.0
 MAP: 7fd627c53000-7fd627c54000 r--p 000c2000 fe:0d 2672906                    /usd/as16544a/soft/splunk/lib/libpcre2-8.so.0.11.0
 MAP: 7fd627c54000-7fd627c55000 rw-p 000c3000 fe:0d 2672906                    /usd/as16544a/soft/splunk/lib/libpcre2-8.so.0.11.0
 MAP: 7fd627c55000-7fd627c5c000 r-xp 00000000 00:2a 5870520                    /lib64/librt-2.22.so
 MAP: 7fd627c5c000-7fd627c5d000 r--p 00006000 00:2a 5870520                    /lib64/librt-2.22.so
 MAP: 7fd627c5d000-7fd627c5e000 rw-p 00007000 00:2a 5870520                    /lib64/librt-2.22.so
 MAP: 7fd627c5e000-7fd627c5f000 rw-p 00000000 00:00 0 
 MAP: 7fd627c64000-7fd627c65000 rw-p 00000000 00:00 0 
 MAP: 7fd627c65000-7fd627c75000 rwxp 00000000 00:00 0 
 MAP: 7fd627c75000-7fd627caa000 r-xp 00000000 fe:0d 2670618                    /usd/as16544a/soft/splunk/lib/libbson-1.0.so.0.0.0
 MAP: 7fd627caa000-7fd627cab000 ---p 00035000 fe:0d 2670618                    /usd/as16544a/soft/splunk/lib/libbson-1.0.so.0.0.0
 MAP: 7fd627cab000-7fd627cae000 r--p 00035000 fe:0d 2670618                    /usd/as16544a/soft/splunk/lib/libbson-1.0.so.0.0.0
 MAP: 7fd627cae000-7fd627caf000 rw-p 00038000 fe:0d 2670618                    /usd/as16544a/soft/splunk/lib/libbson-1.0.so.0.0.0
 MAP: 7fd627caf000-7fd627cb5000 rw-p 00000000 00:00 0 
 MAP: 7fd627cb5000-7fd627d49000 r-xp 00000000 fe:0d 2671334                    /usd/as16544a/soft/splunk/lib/libmongoc-1.0.so.0.0.0
 MAP: 7fd627d49000-7fd627d4a000 r--p 00093000 fe:0d 2671334                    /usd/as16544a/soft/splunk/lib/libmongoc-1.0.so.0.0.0
 MAP: 7fd627d4a000-7fd627d4d000 rw-p 00094000 fe:0d 2671334                    /usd/as16544a/soft/splunk/lib/libmongoc-1.0.so.0.0.0
 MAP: 7fd627d4d000-7fd627d67000 r-xp 00000000 fe:0d 2670649                    /usd/as16544a/soft/splunk/lib/libz.so.1.2.11
 MAP: 7fd627d67000-7fd627d68000 ---p 0001a000 fe:0d 2670649                    /usd/as16544a/soft/splunk/lib/libz.so.1.2.11
 MAP: 7fd627d68000-7fd627d69000 r--p 0001a000 fe:0d 2670649                    /usd/as16544a/soft/splunk/lib/libz.so.1.2.11
 MAP: 7fd627d69000-7fd627d6a000 rw-p 0001b000 fe:0d 2670649                    /usd/as16544a/soft/splunk/lib/libz.so.1.2.11
 MAP: 7fd627d6a000-7fd627d6b000 rw-p 00000000 00:00 0 
 MAP: 7fd627d6b000-7fd627d6c000 r-xp 00000000 fe:0d 2672846                    /usd/as16544a/soft/splunk/lib/libdlstub.so.1.0.0
 MAP: 7fd627d6c000-7fd627d6d000 r--p 00000000 fe:0d 2672846                    /usd/as16544a/soft/splunk/lib/libdlstub.so.1.0.0
 MAP: 7fd627d6d000-7fd627d6e000 rw-p 00001000 fe:0d 2672846                    /usd/as16544a/soft/splunk/lib/libdlstub.so.1.0.0
 MAP: 7fd627d6e000-7fd627db7000 r-xp 00000000 fe:0d 2670645                    /usd/as16544a/soft/splunk/lib/libjemalloc.so.2
 MAP: 7fd627db7000-7fd627dba000 r--p 00048000 fe:0d 2670645                    /usd/as16544a/soft/splunk/lib/libjemalloc.so.2
 MAP: 7fd627dba000-7fd627dbb000 rw-p 0004b000 fe:0d 2670645                    /usd/as16544a/soft/splunk/lib/libjemalloc.so.2
 MAP: 7fd627dbb000-7fd627dbc000 rw-p 00000000 00:00 0 
 MAP: 7fd627dbc000-7fd627dd7000 r-xp 00000000 fe:0d 2672843                    /usd/as16544a/soft/splunk/lib/libdlwrapper.so.1.0.0
 MAP: 7fd627dd7000-7fd627dd9000 r--p 0001a000 fe:0d 2672843                    /usd/as16544a/soft/splunk/lib/libdlwrapper.so.1.0.0
 MAP: 7fd627dd9000-7fd627dda000 rw-p 0001c000 fe:0d 2672843                    /usd/as16544a/soft/splunk/lib/libdlwrapper.so.1.0.0
 MAP: 7fd627dda000-7fd627ddb000 rw-p 00000000 00:00 0 
 MAP: 7fd627ddb000-7fd627dfc000 r-xp 00000000 00:2a 5870478                    /lib64/ld-2.22.so
 MAP: 7fd627dfc000-7fd627dfd000 r--p 00021000 00:2a 5870478                    /lib64/ld-2.22.so
 MAP: 7fd627dfd000-7fd627dfe000 rw-p 00022000 00:2a 5870478                    /lib64/ld-2.22.so
 MAP: 7fd627dfe000-7fd627dff000 rw-p 00000000 00:00 0 
 MAP: 7ffe2a434000-7ffe2a455000 rw-p 00000000 00:00 0                          [stack]
 MAP: 7ffe2a494000-7ffe2a497000 r--p 00000000 00:00 0                          [vvar]
 MAP: 7ffe2a497000-7ffe2a499000 r-xp 00000000 00:00 0                          [vdso]
 MAP: ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]
Last errno: 0
Threads running: 12
Runtime: 1095574.175207s
argv: [splunkd -h 17.242.27.240 -p 8089 restart]
Process renamed: [splunkd pid=16318] splunkd -h 17.242.27.240 -p 8089 restart [process-runner]
Process renamed: [splunkd pid=16318] search --id=1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB --maxbuckets=0 --ttl=600 --maxout=500000 --maxtime=8640000 --lookups=1 --reduce_freq=10 --user=j902080 --pro --roles=fi_cap_waf-koord:fi_cloud_sr_fi_cloud_mkp_app-aaa_etaps:fi_cloud_sr_fi_cloud_mkp_app-aaa_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aaa_prod:fi_cloud_sr_fi_cloud_mkp_app-aaa_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-adrui_etaps:fi_cloud_sr_fi_cloud_mkp_app-adrui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-adrui_prod:fi_cloud_sr_fi_cloud_mkp_app-adrui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-apv_etaps:fi_cloud_sr_fi_cloud_mkp_app-apv_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-apv_prod:fi_cloud_sr_fi_cloud_mkp_app-apv_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aspm_etaps:fi_cloud_sr_fi_cloud_mkp_app-aspm_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aspm_prod:fi_cloud_sr_fi_cloud_mkp_app-aspm_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ats_etaps:fi_cloud_sr_fi_cloud_mkp_app-ats_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ats_prod:fi_cloud_sr_fi_cloud_mkp_app-ats_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aue_etaps:fi_cloud_sr_fi_cloud_mkp_app-aue_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aue_prod:fi_cloud_sr_fi_cloud_mkp_app-aue_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-avgui_etaps:fi_cloud_sr_fi_cloud_mkp_app-avgui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-avgui_prod:fi_cloud_sr_fi_cloud_mkp_app-avgui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cas_etaps:fi_cloud_sr_fi_cloud_mkp_app-casa_etaps:fi_cloud_sr_fi_cloud_mkp_app-casa_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-casa_prod:fi_cloud_sr_fi_cloud_mkp_app-casa_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cbs_etaps:fi_cloud_sr_fi_cloud_mkp_app-cbs_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cbs_prod:fi_cloud_sr_fi_cloud_mkp_app-cbs_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cfgkui_etaps:fi_cloud_sr_fi_cloud_mkp_app-cfgkui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cfgkui_prod:fi_cloud_sr_fi_cloud_mkp_app-cfgkui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cmd_etaps:fi_cloud_sr_fi_cloud_mkp_app-cmd_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cmd_prod:fi_cloud_sr_fi_cloud_mkp_app-cmd_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-dkkw_prod:fi_cloud_sr_fi_cloud_mkp_app-dkkw_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-epi_etaps:fi_cloud_sr_fi_cloud_mkp_app-epi_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fiss_etaps:fi_cloud_sr_fi_cloud_mkp_app-fiss_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fiss_prod:fi_cloud_sr_fi_cloud_mkp_app-fiss_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_etaps:fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_etaps:fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_prod:fi_cloud_sr_fi_cloud_mkp_app-fkpsep_etaps:fi_cloud_sr_fi_cloud_mkp_app-fkpsep_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsep_prod:fi_cloud_sr_fi_cloud_mkp_app-fkpsep_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-gbsui_etaps:fi_cloud_sr_fi_cloud_mkp_app-gbsui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-gbsui_prod:fi_cloud_sr_fi_cloud_mkp_app-gbsui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ikdui_etaps:fi_cloud_sr_fi_cloud_mkp_app-ikdui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ikdui_prod:fi_cloud_sr_fi_cloud_mkp_app-ikdui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-imoui_etaps:fi_cloud_sr_fi_cloud_mkp_app-imoui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-imoui_prod:fi_cloud_sr_fi_cloud_mkp_app-imoui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ipk_etaps:fi_cloud_sr_fi_cloud_mkp_app-ipk_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-kwitt_etaps:fi_cloud_sr_fi_cloud_mkp_app-kwitt_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-lap_etaps:fi_cloud_sr_fi_cloud_mkp_app-lap_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-lap_prod:fi_cloud_sr_fi_cloud_mkp_app-lqrui_etaps:fi_cloud_sr_fi_cloud_mkp_app-lqrui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-lst_etaps:fi_cloud_sr_fi_cloud_mkp_app-lst_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-lst_prod:fi_cloud_sr_fi_cloud_mkp_app-lst_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-mkg_etaps:fi_cloud_sr_fi_cloud_mkp_app-mkg_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-mkg_prod:fi_cloud_sr_fi_cloud_mkp_app-mkg_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-mprui_etaps:fi_cloud_sr_fi_cloud_mkp_app-mprui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-mprui_prod:fi_cloud_sr_fi_cloud_mkp_app-mprui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-npl_etaps:fi_cloud_sr_fi_cloud_mkp_app-npl_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-npl_prod:fi_cloud_sr_fi_cloud_mkp_app-npl_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-onb-hub_etaps:fi_cloud_sr_fi_cloud_mkp_app-onb-hub_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-onb-hub_prod:fi_cloud_sr_fi_cloud_mkp_app-onb-hub_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-onbhub_etaps:fi_cloud_sr_fi_cloud_mkp_app-onbhub_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-onbhub_prod:fi_cloud_sr_fi_cloud_mkp_app-ptbo_etaps:fi_cloud_sr_fi_cloud_mkp_app-ptbo_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ptbo_prod:fi_cloud_sr_fi_cloud_mkp_app-ptbo_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-tbxui_etaps:fi_cloud_sr_fi_cloud_mkp_app-tbxui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-tbxui_prod:fi_cloud_sr_fi_cloud_mkp_app-tbxui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-und_etaps:fi_cloud_sr_fi_cloud_mkp_app-und_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-und_prod:fi_cloud_sr_fi_cloud_mkp_app-und_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-zbm_prod:fi_cloud_sr_fi_cloud_mkp_app-zgm_etaps:fi_cloud_sr_fi_cloud_mkp_app-zgm_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-zgm_prod:fi_cloud_sr_fi_cloud_mkp_app-zgm_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-zoo_etaps:fi_cloud_sr_fi_cloud_mkp_app-zoo_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-zoo_prod:fi_cloud_sr_fi_cloud_mkp_app-zoo_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_bapicontainer_etaps:fi_cloud_sr_fi_cloud_mkp_fgcenter_etaps:fi_cloud_sr_fi_cloud_mkp_idhfrontendcontainer_etaps:fi_cloud_sr_fi_cloud_mkp_innovate_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_mkpbapi_etaps:fi_cloud_sr_fi_cloud_mkp_mkpcont_etaps:fi_cloud_sr_fi_cloud_mkp_mkpstationaer_etaps:fi_cloud_sr_fi_cloud_mkp_onb_etaps:fi_cloud_sr_fi_cloud_mkp_pfm_etaps:fi_cloud_sr_fi_cloud_mkp_pkp_etaps:fi_cloud_sr_fi_cloud_mkp_plattform_etaps:fi_cloud_sr_fi_cloud_mkp_plattform_etaps_unmasked:fi_cloud_sr_fi_cloud_mkp_plattform_prod:fi_cloud_sr_fi_cloud_mkp_plattform_prod_unmasked:fi_cloud_sr_fi_cloud_mkp_sbcontainer_etaps:fi_cloud_sr_fi_cloud_openshift-audit_etaps:fi_cloud_sr_fi_cloud_openshift-audit_siu:fi_cloud_sr_fi_cloud_openshift-metrics_etaps:fi_cloud_sr_fi_cloud_openshift-metrics_siu:fi_cloud_sr_fi_cloud_openshift-objects_etaps:fi_cloud_sr_fi_cloud_openshift-objects_siu:fi_cloud_sr_fi_cloud_openshift_etaps:fi_cloud_sr_fi_cloud_openshift_siu:fi_siem_app_fi_ui_openshift_ro:fi_siem_app_if:fi_siem_app_mkg:fi_siem_cap_cloud:fi_siem_cap_if:fi_siem_cap_mkg:fi_siem_sr_if_eial_dm:fi_siem_sr_if_eial_km:fi_siem_sr_if_kits_dm:fi_siem_sr_if_kits_km:fi_siem_sr_if_prod_dm:fi_siem_sr_if_prod_km:fi_siem_sr_if_slpp_dm:fi_siem_sr_if_slpp_km:fi_siem_sr_if_stat_eial_km:fi_siem_sr_if_stat_prod_km:fi_siem_sr_if_stat_slpp_km:fi_siem_sr_mkg_eial_dm:fi_siem_sr_mkg_kits_dm:fi_siem_sr_mkg_prod_dm:fi_siem_sr_mkg_slpp_dm

Regex JIT enabled

RE2 regex engine enabled

using CLOCK_MONOTONIC
Preforked process=0/17523: process_runtime_msec=208750, search=0/26886, search_runtime_msec=244, new_user=N, export_search=N, args_size=7299, completed_searches=7, user_changes=2, cache_rotations=7

Thread: ""searchOrchestrator"", did_join=1, ready_to_run=Y, main_thread=N, token=140557544318720
MutexByte: MutexByte-waiting={none}

====================Search Result information====================
SearchID:1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB
DispatchDir:/usd/as16544a/soft/splunk/var/run/splunk/dispatch/1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB
Internal Error:
Number of phases:0
=================================================================
-------------------- FILE CONTENTS OF ""/usd/as16544a/soft/splunk/var/run/splunk/dispatch/1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB/info.csv""
""_sid"",""_timestamp"",now,""_startTime"",""_endTime"",""_search_StartTime"",""_provenance"",""_rt_earliest"",""_rt_latest"",""_rtspan"",""_scan_count"",""_drop_count"",""_maxevents"",""_countMap"",""_search_StartUp_Spent"",""_columnOrder"",""_keySet"",""_indexes"",""_remoteServers"",""_max_remote_servers"",""_group_list"",""is_remote_sorted"",""rt_backfill"",""read_raw"",""sample_ratio"",""sample_seed"",""sample_exact"",""enable_event_stream"",""remote_log_download_mode"",""_default_group"",""_timeline_events_preview"",""_rtoptions"",""field_rendering"",""_query_finished"",""_request_finalization"",""_fully_completed_search"",""_auth_token"",""_splunkd_port"",""_splunkd_protocol"",""_splunkd_uri"",""prd_preview_mode"",""prd_preview_reducer_enabled"",""internal_only"",""summary_mode"",""summary_maxtimespan"",""summary_stopped"",""is_batch_mode"",""_retry_count"",""kv_store_settings"",""kv_store_additional_settings"",""_dsi_id"",""_base_lispy"",""_root_sid"",""_shp_id"",""_search"",""_remote_search"",""_reduce_search"",""_search_type"",""_datamodel_map"",""_optional_fields_json"",""_tstats_reduce"",""_tstats_search_filter"",""summary_id"",""required_tags_to_summarize"",""generation_id"",site,""bucket_map_id"",label,""savedsearch_label"",""is_saved_search"",""is_flex_search"",""is_shc_mode"",""is_cluster_slave"",""is_adhoc_proxied"",""search_can_be_event_type"",realtime,""indexed_realtime"",""indexed_realtime_offset"",""replay_speed"",""_rt_batch_retry"",""_read_buckets_since_startup"",""_ppc.app"",""_ppc.user"",""_ppc.bs"",""_bundle_version"",""_tz"",""_is_scheduled"",""_is_summary_index"",""_is_remote"",""_orig_search_head"",""_search_metrics"",""workload_pool"",""workload_info"",""_indexContainWildcard"",""workload_search_time_range"",""contains_delete_command"",""check_dangerous_command"",""_bs_thread_count"",""_bs_pipeline_identifier"",""_is_export"",""_exported_results"",""_is_keepalive"",""_rate_limit_retry_enabled"",""_results_format"",""_compression_algorithm"",""_force_compat_results"",""_execution_plan"",""phase_id"",""_report_tsidx_search_inspections"",""_is_federation_enabled"",""_fsh_streaming_phase_only"",""_fsh_sid"",""_is_fsh_remote_search"",""_fsh_server_name"",""_fsh_providers"",""_fsh_roles"",""_fsh_user"",""_remote_provider_name"",""_fsh_search_info"",""_use_fsh_ko"",""fsh_version"",""_fsh_guid""?""1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB"",""1714987626.890354000"",""1714987626.000000000"",""1714899600.000000000"",""1714987626.000000000"",""1714987626.879618000"",""UI:Search"","""","""","""",0,0,0,""duration.startup.configuration;119;duration.startup.handoff;75;invocations.startup.configuration;1;invocations.startup.handoff;1;"",0,"""","""","""","""",0,"""",0,0,1,1,0,0,1,disabledSavedSearches,""*"",0,"""","""",1,0,1,""RjCLM_Ka0AIeD3A9^ZsnayQPlUnjixEUg^eKSY90ibD105amTkAsi8fnnB6awdxvTwCxOWPxQF_0RfB_MmQbf^7zjyd^01cos9hlDTtXGt1W2dfxa19hpDb8WA2EJFL5pgx52Jz_^VVUQExHpIxyWIkmThLACBpglwmo1hV4FuKbwo"",8089,https,""https://17.242.27.240:8089"",0,0,0,all,"""",0,0,0,""external_kvstore;0;hosts;d100spul4607v111.d100.intern:8191\;q100spul4037v111.q100.intern:8191\;q100spul4537v111.q100.intern:8191\;d100spul4107v111.d100.intern:8191\;;local;17.242.27.240:8191;read_preference;9B484ECA-FEC0-49FF-82DE-436655BDDFCB;replica_set_name;splunkrs;sh_id;8AE98483-80B5-425E-8D47-AAAB98F3F35D;status;ready;"",""hosts_guids;28AA9C0F-66D8-4F58-8CE7-1101CC006CA2\;3690C9B3-DA4A-400A-A1B5-87BEBA648061\;9B484ECA-FEC0-49FF-82DE-436655BDDFCB\;F3CFEF34-E2A4-4AF5-9984-307A694C9646\;;"",0,"""","""",""8AE98483-80B5-425E-8D47-AAAB98F3F35D"",""| timechart count"","""","""","""","""",""{}"","""","""","""","""",0,default,0,"""","""",0,0,1,0,0,0,0,0,0,0,0,0,""fi_UI_openshift"",j902080,""$SPLUNK_ETC"",14908619050624449031,""### SERIALIZED TIMEZONE FORMAT 1.0;Y3208 NW 4C 4D 54;Y7200 YW 43 45 53 54;Y3600 NW 43 45 54;Y7200 YS 43 45 53 54;Y3600 NS 43 45 54;Y10800 YW 43 45 4D 54;Y10800 YS 43 45 4D 54;Y7200 YG 43 45 53 54;Y3600 NG 43 45 54;Y7200 YW 43 45 53 54;Y3600 NW 43 45 54;@-639010800 4;@323830800 7;@338950800 8;@354675600 7;@370400400 8;@386125200 7;@401850000 8;@417574800 7;@433299600 8;@449024400 7;@465354000 8;@481078800 7;@496803600 8;@512528400 7;@528253200 8;@543978000 7;@559702800 8;@575427600 7;@591152400 8;@606877200 7;@622602000 8;@638326800 7;@654656400 8;@670381200 7;@686106000 8;@701830800 7;@717555600 8;@733280400 7;@749005200 8;@764730000 7;@780454800 8;@796179600 7;@811904400 8;@828234000 7;@846378000 8;@859683600 7;@877827600 8;@891133200 7;@909277200 8;@922582800 7;@941331600 8;@954032400 7;@972781200 8;@985482000 7;@1004230800 8;@1017536400 7;@1035680400 8;@1048986000 7;@1067130000 8;@1080435600 7;@1099184400 8;@1111885200 7;@1130634000 8;@1143334800 7;@1162083600 8;@1174784400 7;@1193533200 8;@1206838800 7;@1224982800 8;@1238288400 7;@1256432400 8;@1269738000 7;@1288486800 8;@1301187600 7;@1319936400 8;@1332637200 7;@1351386000 8;@1364691600 7;@1382835600 8;@1396141200 7;@1414285200 8;@1427590800 7;@1445734800 8;@1459040400 7;@1477789200 8;@1490490000 7;@1509238800 8;@1521939600 7;@1540688400 8;@1553994000 7;@1572138000 8;@1585443600 7;@1603587600 8;@1616893200 7;@1635642000 8;@1648342800 7;@1667091600 8;@1679792400 7;@1698541200 8;@1711846800 7;@1729990800 8;@1743296400 7;@1761440400 8;@1774746000 7;@1792890000 8;@1806195600 7;@1824944400 8;@1837645200 7;@1856394000 8;@1869094800 7;@1887843600 8;@1901149200 7;@1919293200 8;@1932598800 7;@1950742800 8;@1964048400 7;@1982797200 8;@1995498000 7;@2014246800 8;@2026947600 7;@2045696400 8;@2058397200 7;@2077146000 8;@2090451600 7;@2108595600 8;@2121901200 7;@2140045200 8;@2153350800 9;@2172099600 10;@2184800400 9;@2203549200 10;@2216250000 9;@2234998800 10;@2248304400 9;@2266448400 10;@2279754000 9;@2297898000 10;@2311203600 9;@2329347600 10;@2342653200 9;@2361402000 10;@2374102800 9;@2392851600 10;@2405552400 9;@2424301200 10;@2437606800 9;@2455750800 10;@2469056400 9;@2487200400 10;@2500506000 9;@2519254800 10;@2531955600 9;@2550704400 10;@2563405200 9;@2582154000 10;@2595459600 9;@2613603600 10;@2626909200 9;@2645053200 10;@2658358800 9;F;$"",0,0,0,"""",""{""""ConsideredBuckets"""":0,""""EliminatedBuckets"""":0,""""ConsideredEvents"""":0,""""TotalSlicesInBuckets"""":0,""""DecompressedSlices"""":0,""""FieldMetadata_Events"""":"""""""",""""Partition"""":{}}"","""","""",0,"""",0,0,1,0,0,0,0,0,srs,none,0,classic,4294967295,0,0,0,"""",0,"""","""","""","""","""","""",0,"""",""""?
--------------------

====================Ending info.csv==============================
=================================================================
-------------------- LAST FEW LINE OF FILE ""/usd/as16544a/soft/splunk/var/run/splunk/dispatch/1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB/search.log""
 Last few lines of the file):
    05-06-2024 11:27:06.891 INFO  dispatchRunner [16318 MainThread] - Search process mode: preforked (reused process) (build 050c9bca8588).
    05-06-2024 11:27:06.910 INFO  dispatchRunner [16318 MainThread] - registering build time modules, count=1
    05-06-2024 11:27:06.910 INFO  dispatchRunner [16318 MainThread] - registering search time components of build time module name=vix
    05-06-2024 11:27:06.911 INFO  BundlesSetup [16318 MainThread] - Setup stats for /usd/as16544a/soft/splunk/etc: wallclock_elapsed_msec=119, cpu_time_used=0.118814, shared_services_generation=2, shared_services_population=1
    05-06-2024 11:27:06.960 INFO  AuthenticationProviderLDAP [16318 MainThread] - strategy=""V998DPV1-SH-IF"" has no valid value for 'ldap_negative_cache_timeout'. Defaulting to 86400
    05-06-2024 11:27:06.962 INFO  UserManagerPro [16318 MainThread] - Load authentication: forcing roles=""fi_cap_waf-koord, fi_cloud_sr_fi_cloud_mkp_app-aaa_etaps, fi_cloud_sr_fi_cloud_mkp_app-aaa_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aaa_prod, fi_cloud_sr_fi_cloud_mkp_app-aaa_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-adrui_etaps, fi_cloud_sr_fi_cloud_mkp_app-adrui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-adrui_prod, fi_cloud_sr_fi_cloud_mkp_app-adrui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-apv_etaps, fi_cloud_sr_fi_cloud_mkp_app-apv_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-apv_prod, fi_cloud_sr_fi_cloud_mkp_app-apv_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aspm_etaps, fi_cloud_sr_fi_cloud_mkp_app-aspm_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aspm_prod, fi_cloud_sr_fi_cloud_mkp_app-aspm_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ats_etaps, fi_cloud_sr_fi_cloud_mkp_app-ats_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ats_prod, fi_cloud_sr_fi_cloud_mkp_app-ats_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aue_etaps, fi_cloud_sr_fi_cloud_mkp_app-aue_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aue_prod, fi_cloud_sr_fi_cloud_mkp_app-aue_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-avgui_etaps, fi_cloud_sr_fi_cloud_mkp_app-avgui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-avgui_prod, fi_cloud_sr_fi_cloud_mkp_app-avgui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cas_etaps, fi_cloud_sr_fi_cloud_mkp_app-casa_etaps, fi_cloud_sr_fi_cloud_mkp_app-casa_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-casa_prod, fi_cloud_sr_fi_cloud_mkp_app-casa_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cbs_etaps, fi_cloud_sr_fi_cloud_mkp_app-cbs_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cbs_prod, fi_cloud_sr_fi_cloud_mkp_app-cbs_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cfgkui_etaps, fi_cloud_sr_fi_cloud_mkp_app-cfgkui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cfgkui_prod, fi_cloud_sr_fi_cloud_mkp_app-cfgkui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cmd_etaps, fi_cloud_sr_fi_cloud_mkp_app-cmd_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cmd_prod, fi_cloud_sr_fi_cloud_mkp_app-cmd_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-dkkw_prod, fi_cloud_sr_fi_cloud_mkp_app-dkkw_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-epi_etaps, fi_cloud_sr_fi_cloud_mkp_app-epi_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fiss_etaps, fi_cloud_sr_fi_cloud_mkp_app-fiss_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fiss_prod, fi_cloud_sr_fi_cloud_mkp_app-fiss_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_etaps, fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_etaps, fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_prod, fi_cloud_sr_fi_cloud_mkp_app-fkpsep_etaps, fi_cloud_sr_fi_cloud_mkp_app-fkpsep_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsep_prod, fi_cloud_sr_fi_cloud_mkp_app-fkpsep_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-gbsui_etaps, fi_cloud_sr_fi_cloud_mkp_app-gbsui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-gbsui_prod, fi_cloud_sr_fi_cloud_mkp_app-gbsui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ikdui_etaps, fi_cloud_sr_fi_cloud_mkp_app-ikdui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ikdui_prod, fi_cloud_sr_fi_cloud_mkp_app-ikdui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-imoui_etaps, fi_cloud_sr_fi_cloud_mkp_app-imoui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-imoui_prod, fi_cloud_sr_fi_cloud_mkp_app-imoui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ipk_etaps, fi_cloud_sr_fi_cloud_mkp_app-ipk_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-kwitt_etaps, fi_cloud_sr_fi_cloud_mkp_app-kwitt_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-lap_etaps, fi_cloud_sr_fi_cloud_mkp_app-lap_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-lap_prod, fi_cloud_sr_fi_cloud_mkp_app-lqrui_etaps, fi_cloud_sr_fi_cloud_mkp_app-lqrui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-lst_etaps, fi_cloud_sr_fi_cloud_mkp_app-lst_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-lst_prod, fi_cloud_sr_fi_cloud_mkp_app-lst_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-mkg_etaps, fi_cloud_sr_fi_cloud_mkp_app-mkg_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-mkg_prod, fi_cloud_sr_fi_cloud_mkp_app-mkg_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-mprui_etaps, fi_cloud_sr_fi_cloud_mkp_app-mprui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-mprui_prod, fi_cloud_sr_fi_cloud_mkp_app-mprui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-npl_etaps, fi_cloud_sr_fi_cloud_mkp_app-npl_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-npl_prod, fi_cloud_sr_fi_cloud_mkp_app-npl_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-onb-hub_etaps, fi_cloud_sr_fi_cloud_mkp_app-onb-hub_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-onb-hub_prod, fi_cloud_sr_fi_cloud_mkp_app-onb-hub_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-onbhub_etaps, fi_cloud_sr_fi_cloud_mkp_app-onbhub_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-onbhub_prod, fi_cloud_sr_fi_cloud_mkp_app-ptbo_etaps, fi_cloud_sr_fi_cloud_mkp_app-ptbo_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ptbo_prod, fi_cloud_sr_fi_cloud_mkp_app-ptbo_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-tbxui_etaps, fi_cloud_sr_fi_cloud_mkp_app-tbxui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-tbxui_prod, fi_cloud_sr_fi_cloud_mkp_app-tbxui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-und_etaps, fi_cloud_sr_fi_cloud_mkp_app-und_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-und_prod, fi_cloud_sr_fi_cloud_mkp_app-und_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-zbm_prod, fi_cloud_sr_fi_cloud_mkp_app-zgm_etaps, fi_cloud_sr_fi_cloud_mkp_app-zgm_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-zgm_prod, fi_cloud_sr_fi_cloud_mkp_app-zgm_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-zoo_etaps, fi_cloud_sr_fi_cloud_mkp_app-zoo_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-zoo_prod, fi_cloud_sr_fi_cloud_mkp_app-zoo_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_bapicontainer_etaps, fi_cloud_sr_fi_cloud_mkp_fgcenter_etaps, fi_cloud_sr_fi_cloud_mkp_idhfrontendcontainer_etaps, fi_cloud_sr_fi_cloud_mkp_innovate_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_mkpbapi_etaps, fi_cloud_sr_fi_cloud_mkp_mkpcont_etaps, fi_cloud_sr_fi_cloud_mkp_mkpstationaer_etaps, fi_cloud_sr_fi_cloud_mkp_onb_etaps, fi_cloud_sr_fi_cloud_mkp_pfm_etaps, fi_cloud_sr_fi_cloud_mkp_pkp_etaps, fi_cloud_sr_fi_cloud_mkp_plattform_etaps, fi_cloud_sr_fi_cloud_mkp_plattform_etaps_unmasked, fi_cloud_sr_fi_cloud_mkp_plattform_prod, fi_cloud_sr_fi_cloud_mkp_plattform_prod_unmasked, fi_cloud_sr_fi_cloud_mkp_sbcontainer_etaps, fi_cloud_sr_fi_cloud_openshift-audit_etaps, fi_cloud_sr_fi_cloud_openshift-audit_siu, fi_cloud_sr_fi_cloud_openshift-metrics_etaps, fi_cloud_sr_fi_cloud_openshift-metrics_siu, fi_cloud_sr_fi_cloud_openshift-objects_etaps, fi_cloud_sr_fi_cloud_openshift-objects_siu, fi_cloud_sr_fi_cloud_openshift_etaps, fi_cloud_sr_fi_cloud_openshift_siu, fi_siem_app_fi_ui_openshift_ro, fi_siem_app_if, fi_siem_app_mkg, fi_siem_cap_cloud, fi_siem_cap_if, fi_siem_cap_mkg, fi_siem_sr_if_eial_dm, fi_siem_sr_if_eial_km, fi_siem_sr_if_kits_dm, fi_siem_sr_if_kits_km, fi_siem_sr_if_prod_dm, fi_siem_sr_if_prod_km, fi_siem_sr_if_slpp_dm, fi_siem_sr_if_slpp_km, fi_siem_sr_if_stat_eial_km, fi_siem_sr_if_stat_prod_km, fi_siem_sr_if_stat_slpp_km, fi_siem_sr_mkg_eial_dm, fi_siem_sr_mkg_kits_dm, fi_siem_sr_mkg_prod_dm, fi_siem_sr_mkg_slpp_dm""
    05-06-2024 11:27:06.962 INFO  UserManager [88054 RunDispatch] - Setting user context: splunk-system-user
    05-06-2024 11:27:06.962 INFO  UserManager [88054 RunDispatch] - Done setting user context: NULL -> splunk-system-user
    05-06-2024 11:27:06.964 INFO  UserManager [88054 RunDispatch] - Unwound user context: splunk-system-user -> NULL
    05-06-2024 11:27:06.964 INFO  UserManager [88054 RunDispatch] - Setting user context: j902080
    05-06-2024 11:27:06.964 INFO  UserManager [88054 RunDispatch] - Done setting user context: NULL -> j902080
    05-06-2024 11:27:06.965 INFO  dispatchRunner [88054 RunDispatch] - search context: user=""j902080"", app=""fi_UI_openshift"", bs-pathname=""/usd/as16544a/soft/splunk/etc""
    05-06-2024 11:27:06.966 INFO  SearchParser [88054 RunDispatch] - PARSING: | timechart count
    05-06-2024 11:27:06.966 INFO  dispatchRunner [88054 RunDispatch] - Search running in non-clustered mode
    05-06-2024 11:27:06.966 INFO  dispatchRunner [88054 RunDispatch] - SearchHeadInitSearchMs=3
    05-06-2024 11:27:06.966 INFO  dispatchRunner [88054 RunDispatch] - Executing the Search orchestrator and iterator model (dfs=false).
    05-06-2024 11:27:06.966 INFO  SearchOrchestrator [88054 RunDispatch] - SearchOrchestrator getting constructed
    05-06-2024 11:27:06.966 INFO  SearchOrchestrator [88054 RunDispatch] -  Initialized the SRI
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Initializing feature flags from config. feature_seed=2630698466
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=parallelreduce:enablePreview:true
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=search:search_retry:false
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=search:search_retry_realtime:false
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=parallelreduce:autoAppliedPercentage:false
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=subsearch:enableConcurrentPipelineProcessing:false
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=stats:allow_stats_v2:true
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=search_optimization::set_required_fields:stats:false
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=searchresults:srs2:false
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=search:read_final_results_from_timeliner:true
    05-06-2024 11:27:06.967 INFO  SearchOrchestrator [88054 RunDispatch] - Search feature_flags={""v"":1,""enabledFeatures"":[""parallelreduce:enablePreview"",""stats:allow_stats_v2"",""search:read_final_results_from_timeliner""],""disabledFeatures"":[""search:search_retry"",""search:search_retry_realtime"",""parallelreduce:autoAppliedPercentage"",""subsearch:enableConcurrentPipelineProcessing"",""search_optimization::set_required_fields:stats"",""searchresults:srs2""]}
    05-06-2024 11:27:06.967 INFO  ISplunkDispatch [88054 RunDispatch] - Not running in splunkd. Bundle replication not triggered.
    05-06-2024 11:27:06.967 INFO  SearchOrchestrator [88057 searchOrchestrator] - Initialzing the run time settings for the orchestrator.
    05-06-2024 11:27:06.967 INFO  UserManager [88057 searchOrchestrator] - Setting user context: j902080
    05-06-2024 11:27:06.967 INFO  UserManager [88057 searchOrchestrator] - Done setting user context: NULL -> j902080
    05-06-2024 11:27:06.967 INFO  AdaptiveSearchEngineSelector [88057 searchOrchestrator] - Search execution_plan=classic
    05-06-2024 11:27:06.967 INFO  SearchOrchestrator [88057 searchOrchestrator] - Creating the search DAG.
    05-06-2024 11:27:06.967 INFO  SearchParser [88057 searchOrchestrator] - PARSING: | timechart count
    05-06-2024 11:27:06.967 INFO  DispatchStorageManagerInfo [88057 searchOrchestrator] - Successfully created new dispatch directory for search job. sid=ea3a13baffe5525d_tmp dispatch_dir=/usd/as16544a/soft/splunk/var/run/splunk/dispatch/ea3a13baffe5525d_tmp
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200
    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (timechart) args: count, AS, count, by, _time
    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free
    05-06-2024 11:27:06.969 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false
    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (timechart) args: count, AS, count, by, _time
    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free
    05-06-2024 11:27:06.969 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false
    05-06-2024 11:27:06.969 INFO  SearchParser [88057 searchOrchestrator] - PARSING: bin _time span=rtspan | prestats count by _time
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200
    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (stats) args: count, by, _time
    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free
    05-06-2024 11:27:06.969 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false
    05-06-2024 11:27:06.969 INFO  DispatchThread [88057 searchOrchestrator] - BatchMode: allowBatchMode: 1, conf(1): 1, timeline/Status buckets(0):0, realtime(0):0, report pipe empty(0):0, reqTimeOrder(0):0, summarize(0):0, statefulStreaming(0):0
    05-06-2024 11:27:06.969 INFO  DispatchThread [88057 searchOrchestrator] - required fields list to add to remote search = _time,prestats_reserved_*,psrsvd_*
    05-06-2024 11:27:06.969 INFO  DispatchCommandProcessor [88057 searchOrchestrator] - summaryHash=2cdebc05dd9685a6 summaryId=8AE98483-80B5-425E-8D47-AAAB98F3F35D_fi_UI_openshift_j902080_2cdebc05dd9685a6 remoteSearch=prebin  _time span=rtspan  | prestats  count by _time
    05-06-2024 11:27:06.969 INFO  DispatchCommandProcessor [88057 searchOrchestrator] - summaryHash=NS7f796cf728faa99f summaryId=8AE98483-80B5-425E-8D47-AAAB98F3F35D_fi_UI_openshift_j902080_NS7f796cf728faa99f remoteSearch=prebin _time span=rtspan | prestats count by _time
    05-06-2024 11:27:06.969 INFO  DispatchThread [88057 searchOrchestrator] - Getting summary ID for summaryHash=NS7f796cf728faa99f
    05-06-2024 11:27:07.016 INFO  DispatchThread [88057 searchOrchestrator] - Did not find a usable summary_id, setting info._summary_mode=none, not modifying input summary_id=8AE98483-80B5-425E-8D47-AAAB98F3F35D_fi_UI_openshift_j902080_NS7f796cf728faa99f
    05-06-2024 11:27:07.016 INFO  SearchParser [88057 searchOrchestrator] - PARSING: | timechart count
    05-06-2024 11:27:07.016 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100
    05-06-2024 11:27:07.016 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200
    05-06-2024 11:27:07.016 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100
    05-06-2024 11:27:07.016 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200
    05-06-2024 11:27:07.016 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (timechart) args: count, AS, count, by, _time
    05-06-2024 11:27:07.016 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free
    05-06-2024 11:27:07.017 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count
    05-06-2024 11:27:07.017 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false
    05-06-2024 11:27:07.017 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (timechart) args: count, AS, count, by, _time
    05-06-2024 11:27:07.017 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free
    05-06-2024 11:27:07.017 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count
    05-06-2024 11:27:07.017 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false
    05-06-2024 11:27:07.017 INFO  AstOptimizer [88057 searchOrchestrator] - SrchOptMetrics optimize_toJson=0.000280740
    05-06-2024 11:27:07.017 INFO  SearchParser [88057 searchOrchestrator] - PARSING: | timechart count
    05-06-2024 11:27:07.017 INFO  FederatedProviderVisitor [88057 searchOrchestrator] - Federated Whole Search Remote Execution Enabled: false
    05-06-2024 11:27:07.017 INFO  AstOptimizer [88057 searchOrchestrator] - Data federation feature enabled: 0
    05-06-2024 11:27:07.017 INFO  ProjElim [88057 searchOrchestrator] - Black listed processors=[addinfo]
    05-06-2024 11:27:07.033 ERROR AstCommandNode [88057 searchOrchestrator] - AstOptimizerException. Cannot unlink the only command_node=timechart
    05-06-2024 11:27:07.033 INFO  ScopedTimer [88057 searchOrchestrator] - search.optimize 0.016420383

--------------------


====================Ending search.log============================

x86 CPUID registers:
         0: 00000016 756E6547 6C65746E 49656E69
         1: 00050654 07200800 7FFEFBFF BFEBFBFF
         2: 76036301 00F0B5FF 00000000 00C30000
         3: 00000000 00000000 00000000 00000000
         4: 00000000 00000000 00000000 00000000
         5: 00000040 00000040 00000003 00002020
         6: 00000077 00000002 00000009 00000000
         7: 00000000 00000000 00000000 00000000
         8: 00000000 00000000 00000000 00000000
         9: 00000000 00000000 00000000 00000000
         A: 07300404 00000000 00000000 00000603
         B: 00000000 00000000 0000004D 00000007
         C: 00000000 00000000 00000000 00000000
         D: 00000000 00000000 00000000 00000000
         E: 00000000 00000000 00000000 00000000
         F: 00000000 00000000 00000000 00000000
        10: 00000000 00000000 00000000 00000000
        11: 00000000 00000000 00000000 00000000
        12: 00000000 00000000 00000000 00000000
        13: 00000000 00000000 00000000 00000000
        14: 00000000 00000000 00000000 00000000
        15: 00000002 000000A8 00000000 00000000
        16: 00000834 00000E74 00000064 00000000
  80000000: 80000008 00000000 00000000 00000000
  80000001: 00000000 00000000 00000121 2C100800
  80000002: 65746E49 2952286C 6F655820 2952286E
  80000003: 6C6F4720 31362064 43203033 40205550
  80000004: 312E3220 7A484730 00000000 00000000
  80000005: 00000000 00000000 00000000 00000000
  80000006: 00000000 00000000 01006040 00000000
  80000007: 00000000 00000000 00000000 00000100
  80000008: 0000302E 00000000 00000000 00000000
terminating...{noformat}

Diagnostic file: [https://downloadsvc.splunk.com/download/splunk/05-06-2024/uploadsvc-60case3476794-05-06-2024-USER-0035a00002XKxqxAAD-diag-Q100SPUL4537-2024-05-06_12-05-45.tar.gz|https://downloadsvc.splunk.com/download/splunk/05-06-2024/uploadsvc-60case3476794-05-06-2024-USER-0035a00002XKxqxAAD-diag-Q100SPUL4537-2024-05-06_12-05-45.tar.gz]

Diagnostic file in SplunkBOT: [https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000wia6baac&job_idtok=63b74811-a6da-4201-8de6-94b987caf0f0&case_numbertok=3476794&hosttok=Q100SPUL4537&case_filetok=Q100SPUL4537-sh_-20240506-111713-iyoHE9rk&found_anontok=0&job_typetok=classic&form.mount.tok=raw&form.mount.filter=&form.network.tok=raw&form.network.filter=&form.interfaces.filter=&form.cpuInfo.tok=raw&form.cpuInfo.filter=&form.processes.tok=raw&form.processes.filter=&form.ulimit.tok=raw&form.ulimit.filter=&form.memory.tok=raw&form.memory.filter=&form.kvstore.filter=&form.searchPeerBundlesDir.filter=&form.sinkholeDir.filter=&form.authDir.filter=|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000wia6baac&job_idtok=63b74811-a6da-4201-8de6-94b987caf0f0&case_numbertok=3476794&hosttok=Q100SPUL4537&case_filetok=Q100SPUL4537-sh_-20240506-111713-iyoHE9rk&found_anontok=0&job_typetok=classic&form.mount.tok=raw&form.mount.filter=&form.network.tok=raw&form.network.filter=&form.interfaces.filter=&form.cpuInfo.tok=raw&form.cpuInfo.filter=&form.processes.tok=raw&form.processes.filter=&form.ulimit.tok=raw&form.ulimit.filter=&form.memory.tok=raw&form.memory.filter=&form.kvstore.filter=&form.searchPeerBundlesDir.filter=&form.sinkholeDir.filter=&form.authDir.filter=]

Crash dump: [https://downloadsvc.splunk.com/download/splunk/05-06-2024/uploadsvc-81case3476794-05-06-2024-USER-0035a00002XKxqxAAD-core-searchOrchestra-228293-1714987627.gz|https://downloadsvc.splunk.com/download/splunk/05-06-2024/uploadsvc-81case3476794-05-06-2024-USER-0035a00002XKxqxAAD-core-searchOrchestra-228293-1714987627.gz]

I understand that this might not be the intended use of timechart command, however it should not cause splunk to crash.

The log above is from customer environment on 9.0.6. In my lab environment I was able to reproduce the issue on 9.0.6, 9.1.4 and 9.2.0.1. 

The access to 9.2.0.1 environment and the steps to reproduce can be found below:

Splunk Instance: 10.202.11.171 | Linux credentials: splunker:splk | Splunk Credentials: admin:5up3rn0va

# Navigate to Apps > Search & Reporting ([http://10.202.11.171:8000/en-GB/app/search/search|http://10.202.11.171:8000/en-GB/app/search/search])
# Execute the following search: 
{noformat}| timechart count{noformat}
# Crash log is generated. To find it, run a new search: 
{noformat}index=_internal ""Crashing thread: searchOrchestrator""{noformat}",,Andrew Brown,Ben Zeigler,Brandon Fernandez,Carlos Garza Garza,Cindy Brown,Illia Demydchenkov,Jakub Nowak,Liuqing Yang,Ray Liu,Scott Calvert,Shihan Zhang,Sung Lim,User known,User known,User known,Weichao Duan,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,630489ffbcb35371090f2048,6036b887e2020c0070c2eba2,712020:8f391806-d205-4ec3-aa04-69f3c5235289,63315a9efedc6169aed6c7cd,6234374e62dc1e006803b2f4,712020:5e4dbf4d-8c68-4f01-a2f4-6f42f2f13547,712020:116391a8-fb85-45f4-b37a-1f14a50d58eb,6053a3b1f180c300675e6390,5ce46b40aee3080dc2f65b76,712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77,6036b832f8c057007083c0e3,unknown,unknown,unknown,6053a471695c3900707a5ed6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25/Aug/25 3:24 PM;bcc0bd7b-44e1-4b00-84cf-eeeccc45d8f8;image-20250825-222356.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8379597,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3ef62ddf,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"o Have they made any changes to their environment just before the issue started?
N/A",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,1759,,,,,,,,,,,,,,,,,,,,,,,,,Patched Version,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 
N/A

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,"o What errors are being reported?
[build 050c9bca8588] 2024-05-06 11:27:07
Received fatal signal 11 (Segmentation fault) on PID 228293.
 Cause:
   No memory mapped at address [0x0000000000000000].
 Crashing thread: searchOrchestrator
 Registers:
    RIP:  [0x0000000000000000] ?
    RDI:  [0x00007FD620D5A200]
    RSI:  [0x0000563B0345AE34]
    RBP:  [0x00007FD61A7FD6A0]
    RSP:  [0x00007FD61A7FB908]
    RAX:  [0x0000563B059E9418]
    RBX:  [0x00007FD61A7FBA50]
    RCX:  [0x00007FD61A7FB928]
    RDX:  [0x0000000000000001]
    R8:  [0x00007FD61A7FB730]
    R9:  [0x00007FD61A7FB640]
    R10:  [0x0000563B05B51A70]
    R11:  [0x0000000000000000]
    R12:  [0x0000000000000001]
    R13:  [0x00007FD61A7FBA50]
    R14:  [0x00007FD620C0D3C0]
    R15:  [0x00007FD61A7FB9A0]
    EFL:  [0x0000000000010206]
    TRAPNO:  [0x000000000000000E]
    ERR:  [0x0000000000000014]
    CSGSFS:  [0x002B000000000033]
    OLDMASK:  [0x0000000000000000]

 OS: Linux
 Arch: x86-64

 Backtrace (PIC build):
 Linux / Q100SPUL4537 / 4.12.14-122.194-default / #1 SMP Mon Feb 12 14:50:38 UTC 2024 (dcbe7be) / x86_64
 /etc/SuSE-release: SUSE Linux Enterprise Server 12 (x86_64)
 glibc version: 2.22
 glibc release: stable
 MAP: 563b0000a000-563b05927000 r-xp 00000000 fe:0d 2729522                    /usd/as16544a/soft/splunk/bin/splunkd
 MAP: 563b05928000-563b05a69000 r--p 0591d000 fe:0d 2729522                    /usd/as16544a/soft/splunk/bin/splunkd
 MAP: 563b05a69000-563b05a71000 rw-p 05a5e000 fe:0d 2729522                    /usd/as16544a/soft/splunk/bin/splunkd
 MAP: 563b05a71000-563b05b52000 rw-p 00000000 00:00 0 
 MAP: 7fd605df5000-7fd605df6000 ---p 00000000 00:00 0 
 MAP: 7fd605df6000-7fd605ff6000 rw-p 00000000 00:00 0 
 MAP: 7fd6069fb000-7fd6069fc000 ---p 00000000 00:00 0 
 MAP: 7fd6069fc000-7fd606bfc000 rw-p 00000000 00:00 0 
 MAP: 7fd606bfc000-7fd606bfd000 ---p 00000000 00:00 0 
 MAP: 7fd606bfd000-7fd606dfd000 rw-p 00000000 00:00 0 
 MAP: 7fd606dfd000-7fd606dfe000 ---p 00000000 00:00 0 
 MAP: 7fd606dfe000-7fd606ffe000 rw-p 00000000 00:00 0 
 MAP: 7fd607400000-7fd60a600000 rw-p 00000000 00:00 0 
 MAP: 7fd60a800000-7fd60aa00000 rw-p 00000000 00:00 0 
 MAP: 7fd60abf5000-7fd60abf6000 ---p 00000000 00:00 0 
 MAP: 7fd60abf6000-7fd60adf6000 rw-p 00000000 00:00 0 
 MAP: 7fd60b000000-7fd60ba00000 rw-p 00000000 00:00 0 
 MAP: 7fd60c200000-7fd60e200000 rw-p 00000000 00:00 0 
 MAP: 7fd60e200000-7fd60ea00000 rw-p 00000000 00:00 0 
 MAP: 7fd60ea00000-7fd60f200000 rw-p 00000000 00:00 0 
 MAP: 7fd60f200000-7fd610000000 rw-p 00000000 00:00 0 
 MAP: 7fd610000000-7fd610600000 rw-p 00000000 00:00 0 
 MAP: 7fd610600000-7fd610e00000 rw-p 00000000 00:00 0 
 MAP: 7fd610ffb000-7fd610ffc000 ---p 00000000 00:00 0 
 MAP: 7fd610ffc000-7fd6111fc000 rw-p 00000000 00:00 0 
 MAP: 7fd611200000-7fd615800000 rw-p 00000000 00:00 0 
 MAP: 7fd6159ff000-7fd615a00000 ---p 00000000 00:00 0 
 MAP: 7fd615a00000-7fd615c00000 rw-p 00000000 00:00 0 
 MAP: 7fd615c00000-7fd616400000 rw-p 00000000 00:00 0 
 MAP: 7fd616400000-7fd617200000 rw-p 00000000 00:00 0 
 MAP: 7fd617200000-7fd617a00000 rw-p 00000000 00:00 0 
 MAP: 7fd617a00000-7fd617e00000 rw-p 00000000 00:00 0 
 MAP: 7fd617ffc000-7fd617ffd000 ---p 00000000 00:00 0 
 MAP: 7fd617ffd000-7fd6181fd000 rw-p 00000000 00:00 0 
 MAP: 7fd618200000-7fd618400000 rw-p 00000000 00:00 0 
 MAP: 7fd618600000-7fd618c00000 rw-p 00000000 00:00 0 
 MAP: 7fd618ffc000-7fd618ffd000 ---p 00000000 00:00 0 
 MAP: 7fd618ffd000-7fd6191fd000 rw-p 00000000 00:00 0 
 MAP: 7fd619200000-7fd61a400000 rw-p 00000000 00:00 0 
 MAP: 7fd61a5fe000-7fd61a5ff000 ---p 00000000 00:00 0 
 MAP: 7fd61a5ff000-7fd61a7ff000 rw-p 00000000 00:00 0 
 MAP: 7fd61a800000-7fd61ae00000 rw-p 00000000 00:00 0 
 MAP: 7fd61ae00000-7fd61b400000 rw-p 00000000 00:00 0 
 MAP: 7fd61b5ff000-7fd61b600000 ---p 00000000 00:00 0 
 MAP: 7fd61b600000-7fd61b800000 rw-p 00000000 00:00 0 
 MAP: 7fd61b800000-7fd61bc00000 rw-p 00000000 00:00 0 
 MAP: 7fd61bdff000-7fd61be00000 ---p 00000000 00:00 0 
 MAP: 7fd61be00000-7fd61c000000 rw-p 00000000 00:00 0 
 MAP: 7fd61c000000-7fd61c200000 rw-p 00000000 00:00 0 
 MAP: 7fd61c3fd000-7fd61c3fe000 ---p 00000000 00:00 0 
 MAP: 7fd61c3fe000-7fd61c5fe000 rw-p 00000000 00:00 0 
 MAP: 7fd61c600000-7fd61cc00000 rw-p 00000000 00:00 0 
 MAP: 7fd61cdff000-7fd61ce00000 ---p 00000000 00:00 0 
 MAP: 7fd61ce00000-7fd61d000000 rw-p 00000000 00:00 0 
 MAP: 7fd61d000000-7fd61d200000 rw-p 00000000 00:00 0 
 MAP: 7fd61d3ff000-7fd61d400000 ---p 00000000 00:00 0 
 MAP: 7fd61d400000-7fd61d600000 rw-p 00000000 00:00 0 
 MAP: 7fd61d600000-7fd61d800000 rw-p 00000000 00:00 0 
 MAP: 7fd61d9ff000-7fd61da00000 ---p 00000000 00:00 0 
 MAP: 7fd61da00000-7fd61dc00000 rw-p 00000000 00:00 0 
 MAP: 7fd61dc00000-7fd61e400000 rw-p 00000000 00:00 0 
 MAP: 7fd61e5ff000-7fd61e600000 ---p 00000000 00:00 0 
 MAP: 7fd61e600000-7fd61ea00000 rw-p 00000000 00:00 0 
 MAP: 7fd61ea00000-7fd61ee00000 rw-p 00000000 00:00 0 
 MAP: 7fd61ee00000-7fd620600000 rw-p 00000000 00:00 0 
 MAP: 7fd6207ff000-7fd620800000 ---p 00000000 00:00 0 
 MAP: 7fd620800000-7fd620a00000 rw-p 00000000 00:00 0 
 MAP: 7fd620a00000-7fd620e00000 rw-p 00000000 00:00 0 
 MAP: 7fd620ffe000-7fd620fff000 ---p 00000000 00:00 0 
 MAP: 7fd620fff000-7fd6211ff000 rw-p 00000000 00:00 0 
 MAP: 7fd6211ff000-7fd621200000 ---p 00000000 00:00 0 
 MAP: 7fd621200000-7fd621400000 rw-p 00000000 00:00 0 
 MAP: 7fd621400000-7fd621600000 rw-p 00000000 00:00 0 
 MAP: 7fd6217ff000-7fd621800000 ---p 00000000 00:00 0 
 MAP: 7fd621800000-7fd621a00000 rw-p 00000000 00:00 0 
 MAP: 7fd621a00000-7fd621e00000 rw-p 00000000 00:00 0 
 MAP: 7fd621fff000-7fd622000000 ---p 00000000 00:00 0 
 MAP: 7fd622000000-7fd622200000 rw-p 00000000 00:00 0 
 MAP: 7fd622200000-7fd624a00000 rw-p 00000000 00:00 0 
 MAP: 7fd624bff000-7fd624c00000 ---p 00000000 00:00 0 
 MAP: 7fd624c00000-7fd624e00000 rw-p 00000000 00:00 0 
 MAP: 7fd624e00000-7fd625000000 rw-p 00000000 00:00 0 
 MAP: 7fd6251ff000-7fd625200000 ---p 00000000 00:00 0 
 MAP: 7fd625200000-7fd625400000 rw-p 00000000 00:00 0 
 MAP: 7fd625400000-7fd625a00000 rw-p 00000000 00:00 0 
 MAP: 7fd625a6f000-7fd625a8f000 rwxp 00000000 00:00 0 
 MAP: 7fd625a9f000-7fd625aaf000 rwxp 00000000 00:00 0 
 MAP: 7fd625abf000-7fd625acf000 rwxp 00000000 00:00 0 
 MAP: 7fd625aef000-7fd625bff000 rwxp 00000000 00:00 0 
 MAP: 7fd625bff000-7fd625c00000 ---p 00000000 00:00 0 
 MAP: 7fd625c00000-7fd625e00000 rw-p 00000000 00:00 0 
 MAP: 7fd625e00000-7fd626800000 rw-p 00000000 00:00 0 
 MAP: 7fd626806000-7fd6268f6000 rwxp 00000000 00:00 0 
 MAP: 7fd6268f6000-7fd626918000 r-xp 00000000 00:2a 6950247                    /lib64/libgcc_s.so.1
 MAP: 7fd626918000-7fd626919000 r--p 00021000 00:2a 6950247                    /lib64/libgcc_s.so.1
 MAP: 7fd626919000-7fd62691a000 rw-p 00022000 00:2a 6950247                    /lib64/libgcc_s.so.1
 MAP: 7fd626920000-7fd626930000 rwxp 00000000 00:00 0 
 MAP: 7fd626930000-7fd626c00000 rwxp 00000000 00:00 0 
 MAP: 7fd626c00000-7fd627000000 rw-p 00000000 00:00 0 
 MAP: 7fd627003000-7fd627004000 ---p 00000000 00:00 0 
 MAP: 7fd627004000-7fd62700d000 rw-p 00000000 00:00 0 
 MAP: 7fd62700d000-7fd62700e000 ---p 00000000 00:00 0 
 MAP: 7fd62700e000-7fd62702e000 rwxp 00000000 00:00 0 
 MAP: 7fd62702e000-7fd627033000 rw-p 00000000 00:00 0 
 MAP: 7fd627033000-7fd6271ce000 r-xp 00000000 00:2a 5870486                    /lib64/libc-2.22.so
 MAP: 7fd6271ce000-7fd6271d2000 r--p 0019b000 00:2a 5870486                    /lib64/libc-2.22.so
 MAP: 7fd6271d2000-7fd6271d4000 rw-p 0019f000 00:2a 5870486                    /lib64/libc-2.22.so
 MAP: 7fd6271d4000-7fd6271d8000 rw-p 00000000 00:00 0 
 MAP: 7fd6271d8000-7fd6272d3000 r-xp 00000000 00:2a 5870494                    /lib64/libm-2.22.so
 MAP: 7fd6272d3000-7fd6272d4000 r--p 000fb000 00:2a 5870494                    /lib64/libm-2.22.so
 MAP: 7fd6272d4000-7fd6272d5000 rw-p 000fc000 00:2a 5870494                    /lib64/libm-2.22.so
 MAP: 7fd6272d5000-7fd627589000 r-xp 00000000 fe:0d 2671333                    /usd/as16544a/soft/splunk/lib/libcrypto.so.1.0.0
 MAP: 7fd627589000-7fd62758a000 ---p 002b4000 fe:0d 2671333                    /usd/as16544a/soft/splunk/lib/libcrypto.so.1.0.0
 MAP: 7fd62758a000-7fd6275a8000 r--p 002b4000 fe:0d 2671333                    /usd/as16544a/soft/splunk/lib/libcrypto.so.1.0.0
 MAP: 7fd6275a8000-7fd6275b6000 rw-p 002d2000 fe:0d 2671333                    /usd/as16544a/soft/splunk/lib/libcrypto.so.1.0.0
 MAP: 7fd6275b6000-7fd6275bb000 rw-p 00000000 00:00 0 
 MAP: 7fd6275bb000-7fd627627000 r-xp 00000000 fe:0d 2672828                    /usd/as16544a/soft/splunk/lib/libssl.so.1.0.0
 MAP: 7fd627627000-7fd627628000 ---p 0006c000 fe:0d 2672828                    /usd/as16544a/soft/splunk/lib/libssl.so.1.0.0
 MAP: 7fd627628000-7fd62762c000 r--p 0006c000 fe:0d 2672828                    /usd/as16544a/soft/splunk/lib/libssl.so.1.0.0
 MAP: 7fd62762c000-7fd627632000 rw-p 00070000 fe:0d 2672828                    /usd/as16544a/soft/splunk/lib/libssl.so.1.0.0
 MAP: 7fd627632000-7fd62764a000 r-xp 00000000 00:2a 5870526                    /lib64/noelision/libpthread-2.22.so
 MAP: 7fd62764a000-7fd62764b000 r--p 00017000 00:2a 5870526                    /lib64/noelision/libpthread-2.22.so
 MAP: 7fd62764b000-7fd62764c000 rw-p 00018000 00:2a 5870526                    /lib64/noelision/libpthread-2.22.so
 MAP: 7fd62764c000-7fd627650000 rw-p 00000000 00:00 0 
 MAP: 7fd627650000-7fd6277c8000 r-xp 00000000 fe:0d 2670637                    /usd/as16544a/soft/splunk/lib/libsqlite3.so.0.8.6
 MAP: 7fd6277c8000-7fd6277cb000 r--p 00177000 fe:0d 2670637                    /usd/as16544a/soft/splunk/lib/libsqlite3.so.0.8.6
 MAP: 7fd6277cb000-7fd6277d1000 rw-p 0017a000 fe:0d 2670637                    /usd/as16544a/soft/splunk/lib/libsqlite3.so.0.8.6
 MAP: 7fd6277d1000-7fd6277d3000 rw-p 00000000 00:00 0 
 MAP: 7fd6277d3000-7fd6277e4000 r-xp 00000000 fe:0d 2670643                    /usd/as16544a/soft/splunk/lib/libbz2.so.1.0.3
 MAP: 7fd6277e4000-7fd6277e5000 ---p 00011000 fe:0d 2670643                    /usd/as16544a/soft/splunk/lib/libbz2.so.1.0.3
 MAP: 7fd6277e5000-7fd6277e6000 r--p 00011000 fe:0d 2670643                    /usd/as16544a/soft/splunk/lib/libbz2.so.1.0.3
 MAP: 7fd6277e6000-7fd6277e7000 rw-p 00012000 fe:0d 2670643                    /usd/as16544a/soft/splunk/lib/libbz2.so.1.0.3
 MAP: 7fd6277e7000-7fd627893000 r-xp 00000000 fe:0d 2672904                    /usd/as16544a/soft/splunk/lib/libarchive.so.13.6.2
 MAP: 7fd627893000-7fd627896000 r--p 000ab000 fe:0d 2672904                    /usd/as16544a/soft/splunk/lib/libarchive.so.13.6.2
 MAP: 7fd627896000-7fd627897000 rw-p 000ae000 fe:0d 2672904                    /usd/as16544a/soft/splunk/lib/libarchive.so.13.6.2
 MAP: 7fd627897000-7fd627898000 rw-p 00000000 00:00 0 
 MAP: 7fd627898000-7fd62789a000 r-xp 00000000 00:2a 5870492                    /lib64/libdl-2.22.so
 MAP: 7fd62789a000-7fd62789b000 r--p 00002000 00:2a 5870492                    /lib64/libdl-2.22.so
 MAP: 7fd62789b000-7fd62789c000 rw-p 00003000 00:2a 5870492                    /lib64/libdl-2.22.so
 MAP: 7fd62789c000-7fd62789d000 rw-p 00000000 00:00 0 
 MAP: 7fd62789d000-7fd6278f2000 r-xp 00000000 fe:0d 2670642                    /usd/as16544a/soft/splunk/lib/libxmlsec1-openssl.so.1.2.24
 MAP: 7fd6278f2000-7fd6278f3000 ---p 00055000 fe:0d 2670642                    /usd/as16544a/soft/splunk/lib/libxmlsec1-openssl.so.1.2.24
 MAP: 7fd6278f3000-7fd6278f6000 r--p 00055000 fe:0d 2670642                    /usd/as16544a/soft/splunk/lib/libxmlsec1-openssl.so.1.2.24
 MAP: 7fd6278f6000-7fd6278f7000 rw-p 00058000 fe:0d 2670642                    /usd/as16544a/soft/splunk/lib/libxmlsec1-openssl.so.1.2.24
 MAP: 7fd6278f7000-7fd6278f8000 rw-p 00000000 00:00 0 
 MAP: 7fd6278f8000-7fd627973000 r-xp 00000000 fe:0d 2670640                    /usd/as16544a/soft/splunk/lib/libxmlsec1.so.1.2.24
 MAP: 7fd627973000-7fd627974000 ---p 0007b000 fe:0d 2670640                    /usd/as16544a/soft/splunk/lib/libxmlsec1.so.1.2.24
 MAP: 7fd627974000-7fd627976000 r--p 0007b000 fe:0d 2670640                    /usd/as16544a/soft/splunk/lib/libxmlsec1.so.1.2.24
 MAP: 7fd627976000-7fd627978000 rw-p 0007d000 fe:0d 2670640                    /usd/as16544a/soft/splunk/lib/libxmlsec1.so.1.2.24
 MAP: 7fd627978000-7fd6279ca000 r-xp 00000000 fe:0d 2670614                    /usd/as16544a/soft/splunk/lib/libxslt.so.1.1.34
 MAP: 7fd6279ca000-7fd6279cb000 ---p 00052000 fe:0d 2670614                    /usd/as16544a/soft/splunk/lib/libxslt.so.1.1.34
 MAP: 7fd6279cb000-7fd6279cc000 r--p 00052000 fe:0d 2670614                    /usd/as16544a/soft/splunk/lib/libxslt.so.1.1.34
 MAP: 7fd6279cc000-7fd6279cd000 rw-p 00053000 fe:0d 2670614                    /usd/as16544a/soft/splunk/lib/libxslt.so.1.1.34
 MAP: 7fd6279cd000-7fd6279cf000 rw-p 00000000 00:00 0 
 MAP: 7fd6279cf000-7fd627b84000 r-xp 00000000 fe:0d 2672827                    /usd/as16544a/soft/splunk/lib/libxml2.so.2.9.10
 MAP: 7fd627b84000-7fd627b85000 ---p 001b5000 fe:0d 2672827                    /usd/as16544a/soft/splunk/lib/libxml2.so.2.9.10
 MAP: 7fd627b85000-7fd627b8d000 r--p 001b5000 fe:0d 2672827                    /usd/as16544a/soft/splunk/lib/libxml2.so.2.9.10
 MAP: 7fd627b8d000-7fd627b8f000 rw-p 001bd000 fe:0d 2672827                    /usd/as16544a/soft/splunk/lib/libxml2.so.2.9.10
 MAP: 7fd627b8f000-7fd627b90000 rw-p 00000000 00:00 0 
 MAP: 7fd627b90000-7fd627c52000 r-xp 00000000 fe:0d 2672906                    /usd/as16544a/soft/splunk/lib/libpcre2-8.so.0.11.0
 MAP: 7fd627c52000-7fd627c53000 ---p 000c2000 fe:0d 2672906                    /usd/as16544a/soft/splunk/lib/libpcre2-8.so.0.11.0
 MAP: 7fd627c53000-7fd627c54000 r--p 000c2000 fe:0d 2672906                    /usd/as16544a/soft/splunk/lib/libpcre2-8.so.0.11.0
 MAP: 7fd627c54000-7fd627c55000 rw-p 000c3000 fe:0d 2672906                    /usd/as16544a/soft/splunk/lib/libpcre2-8.so.0.11.0
 MAP: 7fd627c55000-7fd627c5c000 r-xp 00000000 00:2a 5870520                    /lib64/librt-2.22.so
 MAP: 7fd627c5c000-7fd627c5d000 r--p 00006000 00:2a 5870520                    /lib64/librt-2.22.so
 MAP: 7fd627c5d000-7fd627c5e000 rw-p 00007000 00:2a 5870520                    /lib64/librt-2.22.so
 MAP: 7fd627c5e000-7fd627c5f000 rw-p 00000000 00:00 0 
 MAP: 7fd627c64000-7fd627c65000 rw-p 00000000 00:00 0 
 MAP: 7fd627c65000-7fd627c75000 rwxp 00000000 00:00 0 
 MAP: 7fd627c75000-7fd627caa000 r-xp 00000000 fe:0d 2670618                    /usd/as16544a/soft/splunk/lib/libbson-1.0.so.0.0.0
 MAP: 7fd627caa000-7fd627cab000 ---p 00035000 fe:0d 2670618                    /usd/as16544a/soft/splunk/lib/libbson-1.0.so.0.0.0
 MAP: 7fd627cab000-7fd627cae000 r--p 00035000 fe:0d 2670618                    /usd/as16544a/soft/splunk/lib/libbson-1.0.so.0.0.0
 MAP: 7fd627cae000-7fd627caf000 rw-p 00038000 fe:0d 2670618                    /usd/as16544a/soft/splunk/lib/libbson-1.0.so.0.0.0
 MAP: 7fd627caf000-7fd627cb5000 rw-p 00000000 00:00 0 
 MAP: 7fd627cb5000-7fd627d49000 r-xp 00000000 fe:0d 2671334                    /usd/as16544a/soft/splunk/lib/libmongoc-1.0.so.0.0.0
 MAP: 7fd627d49000-7fd627d4a000 r--p 00093000 fe:0d 2671334                    /usd/as16544a/soft/splunk/lib/libmongoc-1.0.so.0.0.0
 MAP: 7fd627d4a000-7fd627d4d000 rw-p 00094000 fe:0d 2671334                    /usd/as16544a/soft/splunk/lib/libmongoc-1.0.so.0.0.0
 MAP: 7fd627d4d000-7fd627d67000 r-xp 00000000 fe:0d 2670649                    /usd/as16544a/soft/splunk/lib/libz.so.1.2.11
 MAP: 7fd627d67000-7fd627d68000 ---p 0001a000 fe:0d 2670649                    /usd/as16544a/soft/splunk/lib/libz.so.1.2.11
 MAP: 7fd627d68000-7fd627d69000 r--p 0001a000 fe:0d 2670649                    /usd/as16544a/soft/splunk/lib/libz.so.1.2.11
 MAP: 7fd627d69000-7fd627d6a000 rw-p 0001b000 fe:0d 2670649                    /usd/as16544a/soft/splunk/lib/libz.so.1.2.11
 MAP: 7fd627d6a000-7fd627d6b000 rw-p 00000000 00:00 0 
 MAP: 7fd627d6b000-7fd627d6c000 r-xp 00000000 fe:0d 2672846                    /usd/as16544a/soft/splunk/lib/libdlstub.so.1.0.0
 MAP: 7fd627d6c000-7fd627d6d000 r--p 00000000 fe:0d 2672846                    /usd/as16544a/soft/splunk/lib/libdlstub.so.1.0.0
 MAP: 7fd627d6d000-7fd627d6e000 rw-p 00001000 fe:0d 2672846                    /usd/as16544a/soft/splunk/lib/libdlstub.so.1.0.0
 MAP: 7fd627d6e000-7fd627db7000 r-xp 00000000 fe:0d 2670645                    /usd/as16544a/soft/splunk/lib/libjemalloc.so.2
 MAP: 7fd627db7000-7fd627dba000 r--p 00048000 fe:0d 2670645                    /usd/as16544a/soft/splunk/lib/libjemalloc.so.2
 MAP: 7fd627dba000-7fd627dbb000 rw-p 0004b000 fe:0d 2670645                    /usd/as16544a/soft/splunk/lib/libjemalloc.so.2
 MAP: 7fd627dbb000-7fd627dbc000 rw-p 00000000 00:00 0 
 MAP: 7fd627dbc000-7fd627dd7000 r-xp 00000000 fe:0d 2672843                    /usd/as16544a/soft/splunk/lib/libdlwrapper.so.1.0.0
 MAP: 7fd627dd7000-7fd627dd9000 r--p 0001a000 fe:0d 2672843                    /usd/as16544a/soft/splunk/lib/libdlwrapper.so.1.0.0
 MAP: 7fd627dd9000-7fd627dda000 rw-p 0001c000 fe:0d 2672843                    /usd/as16544a/soft/splunk/lib/libdlwrapper.so.1.0.0
 MAP: 7fd627dda000-7fd627ddb000 rw-p 00000000 00:00 0 
 MAP: 7fd627ddb000-7fd627dfc000 r-xp 00000000 00:2a 5870478                    /lib64/ld-2.22.so
 MAP: 7fd627dfc000-7fd627dfd000 r--p 00021000 00:2a 5870478                    /lib64/ld-2.22.so
 MAP: 7fd627dfd000-7fd627dfe000 rw-p 00022000 00:2a 5870478                    /lib64/ld-2.22.so
 MAP: 7fd627dfe000-7fd627dff000 rw-p 00000000 00:00 0 
 MAP: 7ffe2a434000-7ffe2a455000 rw-p 00000000 00:00 0                          [stack]
 MAP: 7ffe2a494000-7ffe2a497000 r--p 00000000 00:00 0                          [vvar]
 MAP: 7ffe2a497000-7ffe2a499000 r-xp 00000000 00:00 0                          [vdso]
 MAP: ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]
Last errno: 0
Threads running: 12
Runtime: 1095574.175207s
argv: [splunkd -h 17.242.27.240 -p 8089 restart]
Process renamed: [splunkd pid=16318] splunkd -h 17.242.27.240 -p 8089 restart [process-runner]
Process renamed: [splunkd pid=16318] search --id=1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB --maxbuckets=0 --ttl=600 --maxout=500000 --maxtime=8640000 --lookups=1 --reduce_freq=10 --user=j902080 --pro --roles=fi_cap_waf-koord:fi_cloud_sr_fi_cloud_mkp_app-aaa_etaps:fi_cloud_sr_fi_cloud_mkp_app-aaa_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aaa_prod:fi_cloud_sr_fi_cloud_mkp_app-aaa_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-adrui_etaps:fi_cloud_sr_fi_cloud_mkp_app-adrui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-adrui_prod:fi_cloud_sr_fi_cloud_mkp_app-adrui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-apv_etaps:fi_cloud_sr_fi_cloud_mkp_app-apv_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-apv_prod:fi_cloud_sr_fi_cloud_mkp_app-apv_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aspm_etaps:fi_cloud_sr_fi_cloud_mkp_app-aspm_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aspm_prod:fi_cloud_sr_fi_cloud_mkp_app-aspm_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ats_etaps:fi_cloud_sr_fi_cloud_mkp_app-ats_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ats_prod:fi_cloud_sr_fi_cloud_mkp_app-ats_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aue_etaps:fi_cloud_sr_fi_cloud_mkp_app-aue_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aue_prod:fi_cloud_sr_fi_cloud_mkp_app-aue_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-avgui_etaps:fi_cloud_sr_fi_cloud_mkp_app-avgui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-avgui_prod:fi_cloud_sr_fi_cloud_mkp_app-avgui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cas_etaps:fi_cloud_sr_fi_cloud_mkp_app-casa_etaps:fi_cloud_sr_fi_cloud_mkp_app-casa_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-casa_prod:fi_cloud_sr_fi_cloud_mkp_app-casa_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cbs_etaps:fi_cloud_sr_fi_cloud_mkp_app-cbs_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cbs_prod:fi_cloud_sr_fi_cloud_mkp_app-cbs_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cfgkui_etaps:fi_cloud_sr_fi_cloud_mkp_app-cfgkui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cfgkui_prod:fi_cloud_sr_fi_cloud_mkp_app-cfgkui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cmd_etaps:fi_cloud_sr_fi_cloud_mkp_app-cmd_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cmd_prod:fi_cloud_sr_fi_cloud_mkp_app-cmd_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-dkkw_prod:fi_cloud_sr_fi_cloud_mkp_app-dkkw_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-epi_etaps:fi_cloud_sr_fi_cloud_mkp_app-epi_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fiss_etaps:fi_cloud_sr_fi_cloud_mkp_app-fiss_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fiss_prod:fi_cloud_sr_fi_cloud_mkp_app-fiss_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_etaps:fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_etaps:fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_prod:fi_cloud_sr_fi_cloud_mkp_app-fkpsep_etaps:fi_cloud_sr_fi_cloud_mkp_app-fkpsep_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsep_prod:fi_cloud_sr_fi_cloud_mkp_app-fkpsep_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-gbsui_etaps:fi_cloud_sr_fi_cloud_mkp_app-gbsui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-gbsui_prod:fi_cloud_sr_fi_cloud_mkp_app-gbsui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ikdui_etaps:fi_cloud_sr_fi_cloud_mkp_app-ikdui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ikdui_prod:fi_cloud_sr_fi_cloud_mkp_app-ikdui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-imoui_etaps:fi_cloud_sr_fi_cloud_mkp_app-imoui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-imoui_prod:fi_cloud_sr_fi_cloud_mkp_app-imoui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ipk_etaps:fi_cloud_sr_fi_cloud_mkp_app-ipk_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-kwitt_etaps:fi_cloud_sr_fi_cloud_mkp_app-kwitt_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-lap_etaps:fi_cloud_sr_fi_cloud_mkp_app-lap_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-lap_prod:fi_cloud_sr_fi_cloud_mkp_app-lqrui_etaps:fi_cloud_sr_fi_cloud_mkp_app-lqrui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-lst_etaps:fi_cloud_sr_fi_cloud_mkp_app-lst_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-lst_prod:fi_cloud_sr_fi_cloud_mkp_app-lst_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-mkg_etaps:fi_cloud_sr_fi_cloud_mkp_app-mkg_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-mkg_prod:fi_cloud_sr_fi_cloud_mkp_app-mkg_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-mprui_etaps:fi_cloud_sr_fi_cloud_mkp_app-mprui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-mprui_prod:fi_cloud_sr_fi_cloud_mkp_app-mprui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-npl_etaps:fi_cloud_sr_fi_cloud_mkp_app-npl_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-npl_prod:fi_cloud_sr_fi_cloud_mkp_app-npl_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-onb-hub_etaps:fi_cloud_sr_fi_cloud_mkp_app-onb-hub_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-onb-hub_prod:fi_cloud_sr_fi_cloud_mkp_app-onb-hub_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-onbhub_etaps:fi_cloud_sr_fi_cloud_mkp_app-onbhub_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-onbhub_prod:fi_cloud_sr_fi_cloud_mkp_app-ptbo_etaps:fi_cloud_sr_fi_cloud_mkp_app-ptbo_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ptbo_prod:fi_cloud_sr_fi_cloud_mkp_app-ptbo_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-tbxui_etaps:fi_cloud_sr_fi_cloud_mkp_app-tbxui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-tbxui_prod:fi_cloud_sr_fi_cloud_mkp_app-tbxui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-und_etaps:fi_cloud_sr_fi_cloud_mkp_app-und_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-und_prod:fi_cloud_sr_fi_cloud_mkp_app-und_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-zbm_prod:fi_cloud_sr_fi_cloud_mkp_app-zgm_etaps:fi_cloud_sr_fi_cloud_mkp_app-zgm_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-zgm_prod:fi_cloud_sr_fi_cloud_mkp_app-zgm_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-zoo_etaps:fi_cloud_sr_fi_cloud_mkp_app-zoo_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-zoo_prod:fi_cloud_sr_fi_cloud_mkp_app-zoo_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_bapicontainer_etaps:fi_cloud_sr_fi_cloud_mkp_fgcenter_etaps:fi_cloud_sr_fi_cloud_mkp_idhfrontendcontainer_etaps:fi_cloud_sr_fi_cloud_mkp_innovate_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_mkpbapi_etaps:fi_cloud_sr_fi_cloud_mkp_mkpcont_etaps:fi_cloud_sr_fi_cloud_mkp_mkpstationaer_etaps:fi_cloud_sr_fi_cloud_mkp_onb_etaps:fi_cloud_sr_fi_cloud_mkp_pfm_etaps:fi_cloud_sr_fi_cloud_mkp_pkp_etaps:fi_cloud_sr_fi_cloud_mkp_plattform_etaps:fi_cloud_sr_fi_cloud_mkp_plattform_etaps_unmasked:fi_cloud_sr_fi_cloud_mkp_plattform_prod:fi_cloud_sr_fi_cloud_mkp_plattform_prod_unmasked:fi_cloud_sr_fi_cloud_mkp_sbcontainer_etaps:fi_cloud_sr_fi_cloud_openshift-audit_etaps:fi_cloud_sr_fi_cloud_openshift-audit_siu:fi_cloud_sr_fi_cloud_openshift-metrics_etaps:fi_cloud_sr_fi_cloud_openshift-metrics_siu:fi_cloud_sr_fi_cloud_openshift-objects_etaps:fi_cloud_sr_fi_cloud_openshift-objects_siu:fi_cloud_sr_fi_cloud_openshift_etaps:fi_cloud_sr_fi_cloud_openshift_siu:fi_siem_app_fi_ui_openshift_ro:fi_siem_app_if:fi_siem_app_mkg:fi_siem_cap_cloud:fi_siem_cap_if:fi_siem_cap_mkg:fi_siem_sr_if_eial_dm:fi_siem_sr_if_eial_km:fi_siem_sr_if_kits_dm:fi_siem_sr_if_kits_km:fi_siem_sr_if_prod_dm:fi_siem_sr_if_prod_km:fi_siem_sr_if_slpp_dm:fi_siem_sr_if_slpp_km:fi_siem_sr_if_stat_eial_km:fi_siem_sr_if_stat_prod_km:fi_siem_sr_if_stat_slpp_km:fi_siem_sr_mkg_eial_dm:fi_siem_sr_mkg_kits_dm:fi_siem_sr_mkg_prod_dm:fi_siem_sr_mkg_slpp_dm

Regex JIT enabled

RE2 regex engine enabled

using CLOCK_MONOTONIC
Preforked process=0/17523: process_runtime_msec=208750, search=0/26886, search_runtime_msec=244, new_user=N, export_search=N, args_size=7299, completed_searches=7, user_changes=2, cache_rotations=7

Thread: ""searchOrchestrator"", did_join=1, ready_to_run=Y, main_thread=N, token=140557544318720
MutexByte: MutexByte-waiting={none}

====================Search Result information====================
SearchID:1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB
DispatchDir:/usd/as16544a/soft/splunk/var/run/splunk/dispatch/1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB
Internal Error:
Number of phases:0
=================================================================
-------------------- FILE CONTENTS OF ""/usd/as16544a/soft/splunk/var/run/splunk/dispatch/1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB/info.csv""
""_sid"",""_timestamp"",now,""_startTime"",""_endTime"",""_search_StartTime"",""_provenance"",""_rt_earliest"",""_rt_latest"",""_rtspan"",""_scan_count"",""_drop_count"",""_maxevents"",""_countMap"",""_search_StartUp_Spent"",""_columnOrder"",""_keySet"",""_indexes"",""_remoteServers"",""_max_remote_servers"",""_group_list"",""is_remote_sorted"",""rt_backfill"",""read_raw"",""sample_ratio"",""sample_seed"",""sample_exact"",""enable_event_stream"",""remote_log_download_mode"",""_default_group"",""_timeline_events_preview"",""_rtoptions"",""field_rendering"",""_query_finished"",""_request_finalization"",""_fully_completed_search"",""_auth_token"",""_splunkd_port"",""_splunkd_protocol"",""_splunkd_uri"",""prd_preview_mode"",""prd_preview_reducer_enabled"",""internal_only"",""summary_mode"",""summary_maxtimespan"",""summary_stopped"",""is_batch_mode"",""_retry_count"",""kv_store_settings"",""kv_store_additional_settings"",""_dsi_id"",""_base_lispy"",""_root_sid"",""_shp_id"",""_search"",""_remote_search"",""_reduce_search"",""_search_type"",""_datamodel_map"",""_optional_fields_json"",""_tstats_reduce"",""_tstats_search_filter"",""summary_id"",""required_tags_to_summarize"",""generation_id"",site,""bucket_map_id"",label,""savedsearch_label"",""is_saved_search"",""is_flex_search"",""is_shc_mode"",""is_cluster_slave"",""is_adhoc_proxied"",""search_can_be_event_type"",realtime,""indexed_realtime"",""indexed_realtime_offset"",""replay_speed"",""_rt_batch_retry"",""_read_buckets_since_startup"",""_ppc.app"",""_ppc.user"",""_ppc.bs"",""_bundle_version"",""_tz"",""_is_scheduled"",""_is_summary_index"",""_is_remote"",""_orig_search_head"",""_search_metrics"",""workload_pool"",""workload_info"",""_indexContainWildcard"",""workload_search_time_range"",""contains_delete_command"",""check_dangerous_command"",""_bs_thread_count"",""_bs_pipeline_identifier"",""_is_export"",""_exported_results"",""_is_keepalive"",""_rate_limit_retry_enabled"",""_results_format"",""_compression_algorithm"",""_force_compat_results"",""_execution_plan"",""phase_id"",""_report_tsidx_search_inspections"",""_is_federation_enabled"",""_fsh_streaming_phase_only"",""_fsh_sid"",""_is_fsh_remote_search"",""_fsh_server_name"",""_fsh_providers"",""_fsh_roles"",""_fsh_user"",""_remote_provider_name"",""_fsh_search_info"",""_use_fsh_ko"",""fsh_version"",""_fsh_guid""?""1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB"",""1714987626.890354000"",""1714987626.000000000"",""1714899600.000000000"",""1714987626.000000000"",""1714987626.879618000"",""UI:Search"","""","""","""",0,0,0,""duration.startup.configuration;119;duration.startup.handoff;75;invocations.startup.configuration;1;invocations.startup.handoff;1;"",0,"""","""","""","""",0,"""",0,0,1,1,0,0,1,disabledSavedSearches,""*"",0,"""","""",1,0,1,""RjCLM_Ka0AIeD3A9^ZsnayQPlUnjixEUg^eKSY90ibD105amTkAsi8fnnB6awdxvTwCxOWPxQF_0RfB_MmQbf^7zjyd^01cos9hlDTtXGt1W2dfxa19hpDb8WA2EJFL5pgx52Jz_^VVUQExHpIxyWIkmThLACBpglwmo1hV4FuKbwo"",8089,https,""https://17.242.27.240:8089"",0,0,0,all,"""",0,0,0,""external_kvstore;0;hosts;d100spul4607v111.d100.intern:8191\;q100spul4037v111.q100.intern:8191\;q100spul4537v111.q100.intern:8191\;d100spul4107v111.d100.intern:8191\;;local;17.242.27.240:8191;read_preference;9B484ECA-FEC0-49FF-82DE-436655BDDFCB;replica_set_name;splunkrs;sh_id;8AE98483-80B5-425E-8D47-AAAB98F3F35D;status;ready;"",""hosts_guids;28AA9C0F-66D8-4F58-8CE7-1101CC006CA2\;3690C9B3-DA4A-400A-A1B5-87BEBA648061\;9B484ECA-FEC0-49FF-82DE-436655BDDFCB\;F3CFEF34-E2A4-4AF5-9984-307A694C9646\;;"",0,"""","""",""8AE98483-80B5-425E-8D47-AAAB98F3F35D"",""| timechart count"","""","""","""","""",""{}"","""","""","""","""",0,default,0,"""","""",0,0,1,0,0,0,0,0,0,0,0,0,""fi_UI_openshift"",j902080,""$SPLUNK_ETC"",14908619050624449031,""### SERIALIZED TIMEZONE FORMAT 1.0;Y3208 NW 4C 4D 54;Y7200 YW 43 45 53 54;Y3600 NW 43 45 54;Y7200 YS 43 45 53 54;Y3600 NS 43 45 54;Y10800 YW 43 45 4D 54;Y10800 YS 43 45 4D 54;Y7200 YG 43 45 53 54;Y3600 NG 43 45 54;Y7200 YW 43 45 53 54;Y3600 NW 43 45 54;@-639010800 4;@323830800 7;@338950800 8;@354675600 7;@370400400 8;@386125200 7;@401850000 8;@417574800 7;@433299600 8;@449024400 7;@465354000 8;@481078800 7;@496803600 8;@512528400 7;@528253200 8;@543978000 7;@559702800 8;@575427600 7;@591152400 8;@606877200 7;@622602000 8;@638326800 7;@654656400 8;@670381200 7;@686106000 8;@701830800 7;@717555600 8;@733280400 7;@749005200 8;@764730000 7;@780454800 8;@796179600 7;@811904400 8;@828234000 7;@846378000 8;@859683600 7;@877827600 8;@891133200 7;@909277200 8;@922582800 7;@941331600 8;@954032400 7;@972781200 8;@985482000 7;@1004230800 8;@1017536400 7;@1035680400 8;@1048986000 7;@1067130000 8;@1080435600 7;@1099184400 8;@1111885200 7;@1130634000 8;@1143334800 7;@1162083600 8;@1174784400 7;@1193533200 8;@1206838800 7;@1224982800 8;@1238288400 7;@1256432400 8;@1269738000 7;@1288486800 8;@1301187600 7;@1319936400 8;@1332637200 7;@1351386000 8;@1364691600 7;@1382835600 8;@1396141200 7;@1414285200 8;@1427590800 7;@1445734800 8;@1459040400 7;@1477789200 8;@1490490000 7;@1509238800 8;@1521939600 7;@1540688400 8;@1553994000 7;@1572138000 8;@1585443600 7;@1603587600 8;@1616893200 7;@1635642000 8;@1648342800 7;@1667091600 8;@1679792400 7;@1698541200 8;@1711846800 7;@1729990800 8;@1743296400 7;@1761440400 8;@1774746000 7;@1792890000 8;@1806195600 7;@1824944400 8;@1837645200 7;@1856394000 8;@1869094800 7;@1887843600 8;@1901149200 7;@1919293200 8;@1932598800 7;@1950742800 8;@1964048400 7;@1982797200 8;@1995498000 7;@2014246800 8;@2026947600 7;@2045696400 8;@2058397200 7;@2077146000 8;@2090451600 7;@2108595600 8;@2121901200 7;@2140045200 8;@2153350800 9;@2172099600 10;@2184800400 9;@2203549200 10;@2216250000 9;@2234998800 10;@2248304400 9;@2266448400 10;@2279754000 9;@2297898000 10;@2311203600 9;@2329347600 10;@2342653200 9;@2361402000 10;@2374102800 9;@2392851600 10;@2405552400 9;@2424301200 10;@2437606800 9;@2455750800 10;@2469056400 9;@2487200400 10;@2500506000 9;@2519254800 10;@2531955600 9;@2550704400 10;@2563405200 9;@2582154000 10;@2595459600 9;@2613603600 10;@2626909200 9;@2645053200 10;@2658358800 9;F;$"",0,0,0,"""",""{""""ConsideredBuckets"""":0,""""EliminatedBuckets"""":0,""""ConsideredEvents"""":0,""""TotalSlicesInBuckets"""":0,""""DecompressedSlices"""":0,""""FieldMetadata_Events"""":"""""""",""""Partition"""":{}}"","""","""",0,"""",0,0,1,0,0,0,0,0,srs,none,0,classic,4294967295,0,0,0,"""",0,"""","""","""","""","""","""",0,"""",""""?
--------------------

====================Ending info.csv==============================
=================================================================
-------------------- LAST FEW LINE OF FILE ""/usd/as16544a/soft/splunk/var/run/splunk/dispatch/1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB/search.log""
 Last few lines of the file):
    05-06-2024 11:27:06.891 INFO  dispatchRunner [16318 MainThread] - Search process mode: preforked (reused process) (build 050c9bca8588).
    05-06-2024 11:27:06.910 INFO  dispatchRunner [16318 MainThread] - registering build time modules, count=1
    05-06-2024 11:27:06.910 INFO  dispatchRunner [16318 MainThread] - registering search time components of build time module name=vix
    05-06-2024 11:27:06.911 INFO  BundlesSetup [16318 MainThread] - Setup stats for /usd/as16544a/soft/splunk/etc: wallclock_elapsed_msec=119, cpu_time_used=0.118814, shared_services_generation=2, shared_services_population=1
    05-06-2024 11:27:06.960 INFO  AuthenticationProviderLDAP [16318 MainThread] - strategy=""V998DPV1-SH-IF"" has no valid value for 'ldap_negative_cache_timeout'. Defaulting to 86400
    05-06-2024 11:27:06.962 INFO  UserManagerPro [16318 MainThread] - Load authentication: forcing roles=""fi_cap_waf-koord, fi_cloud_sr_fi_cloud_mkp_app-aaa_etaps, fi_cloud_sr_fi_cloud_mkp_app-aaa_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aaa_prod, fi_cloud_sr_fi_cloud_mkp_app-aaa_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-adrui_etaps, fi_cloud_sr_fi_cloud_mkp_app-adrui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-adrui_prod, fi_cloud_sr_fi_cloud_mkp_app-adrui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-apv_etaps, fi_cloud_sr_fi_cloud_mkp_app-apv_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-apv_prod, fi_cloud_sr_fi_cloud_mkp_app-apv_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aspm_etaps, fi_cloud_sr_fi_cloud_mkp_app-aspm_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aspm_prod, fi_cloud_sr_fi_cloud_mkp_app-aspm_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ats_etaps, fi_cloud_sr_fi_cloud_mkp_app-ats_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ats_prod, fi_cloud_sr_fi_cloud_mkp_app-ats_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aue_etaps, fi_cloud_sr_fi_cloud_mkp_app-aue_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aue_prod, fi_cloud_sr_fi_cloud_mkp_app-aue_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-avgui_etaps, fi_cloud_sr_fi_cloud_mkp_app-avgui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-avgui_prod, fi_cloud_sr_fi_cloud_mkp_app-avgui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cas_etaps, fi_cloud_sr_fi_cloud_mkp_app-casa_etaps, fi_cloud_sr_fi_cloud_mkp_app-casa_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-casa_prod, fi_cloud_sr_fi_cloud_mkp_app-casa_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cbs_etaps, fi_cloud_sr_fi_cloud_mkp_app-cbs_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cbs_prod, fi_cloud_sr_fi_cloud_mkp_app-cbs_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cfgkui_etaps, fi_cloud_sr_fi_cloud_mkp_app-cfgkui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cfgkui_prod, fi_cloud_sr_fi_cloud_mkp_app-cfgkui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cmd_etaps, fi_cloud_sr_fi_cloud_mkp_app-cmd_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cmd_prod, fi_cloud_sr_fi_cloud_mkp_app-cmd_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-dkkw_prod, fi_cloud_sr_fi_cloud_mkp_app-dkkw_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-epi_etaps, fi_cloud_sr_fi_cloud_mkp_app-epi_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fiss_etaps, fi_cloud_sr_fi_cloud_mkp_app-fiss_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fiss_prod, fi_cloud_sr_fi_cloud_mkp_app-fiss_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_etaps, fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_etaps, fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_prod, fi_cloud_sr_fi_cloud_mkp_app-fkpsep_etaps, fi_cloud_sr_fi_cloud_mkp_app-fkpsep_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsep_prod, fi_cloud_sr_fi_cloud_mkp_app-fkpsep_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-gbsui_etaps, fi_cloud_sr_fi_cloud_mkp_app-gbsui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-gbsui_prod, fi_cloud_sr_fi_cloud_mkp_app-gbsui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ikdui_etaps, fi_cloud_sr_fi_cloud_mkp_app-ikdui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ikdui_prod, fi_cloud_sr_fi_cloud_mkp_app-ikdui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-imoui_etaps, fi_cloud_sr_fi_cloud_mkp_app-imoui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-imoui_prod, fi_cloud_sr_fi_cloud_mkp_app-imoui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ipk_etaps, fi_cloud_sr_fi_cloud_mkp_app-ipk_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-kwitt_etaps, fi_cloud_sr_fi_cloud_mkp_app-kwitt_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-lap_etaps, fi_cloud_sr_fi_cloud_mkp_app-lap_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-lap_prod, fi_cloud_sr_fi_cloud_mkp_app-lqrui_etaps, fi_cloud_sr_fi_cloud_mkp_app-lqrui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-lst_etaps, fi_cloud_sr_fi_cloud_mkp_app-lst_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-lst_prod, fi_cloud_sr_fi_cloud_mkp_app-lst_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-mkg_etaps, fi_cloud_sr_fi_cloud_mkp_app-mkg_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-mkg_prod, fi_cloud_sr_fi_cloud_mkp_app-mkg_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-mprui_etaps, fi_cloud_sr_fi_cloud_mkp_app-mprui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-mprui_prod, fi_cloud_sr_fi_cloud_mkp_app-mprui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-npl_etaps, fi_cloud_sr_fi_cloud_mkp_app-npl_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-npl_prod, fi_cloud_sr_fi_cloud_mkp_app-npl_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-onb-hub_etaps, fi_cloud_sr_fi_cloud_mkp_app-onb-hub_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-onb-hub_prod, fi_cloud_sr_fi_cloud_mkp_app-onb-hub_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-onbhub_etaps, fi_cloud_sr_fi_cloud_mkp_app-onbhub_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-onbhub_prod, fi_cloud_sr_fi_cloud_mkp_app-ptbo_etaps, fi_cloud_sr_fi_cloud_mkp_app-ptbo_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ptbo_prod, fi_cloud_sr_fi_cloud_mkp_app-ptbo_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-tbxui_etaps, fi_cloud_sr_fi_cloud_mkp_app-tbxui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-tbxui_prod, fi_cloud_sr_fi_cloud_mkp_app-tbxui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-und_etaps, fi_cloud_sr_fi_cloud_mkp_app-und_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-und_prod, fi_cloud_sr_fi_cloud_mkp_app-und_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-zbm_prod, fi_cloud_sr_fi_cloud_mkp_app-zgm_etaps, fi_cloud_sr_fi_cloud_mkp_app-zgm_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-zgm_prod, fi_cloud_sr_fi_cloud_mkp_app-zgm_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-zoo_etaps, fi_cloud_sr_fi_cloud_mkp_app-zoo_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-zoo_prod, fi_cloud_sr_fi_cloud_mkp_app-zoo_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_bapicontainer_etaps, fi_cloud_sr_fi_cloud_mkp_fgcenter_etaps, fi_cloud_sr_fi_cloud_mkp_idhfrontendcontainer_etaps, fi_cloud_sr_fi_cloud_mkp_innovate_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_mkpbapi_etaps, fi_cloud_sr_fi_cloud_mkp_mkpcont_etaps, fi_cloud_sr_fi_cloud_mkp_mkpstationaer_etaps, fi_cloud_sr_fi_cloud_mkp_onb_etaps, fi_cloud_sr_fi_cloud_mkp_pfm_etaps, fi_cloud_sr_fi_cloud_mkp_pkp_etaps, fi_cloud_sr_fi_cloud_mkp_plattform_etaps, fi_cloud_sr_fi_cloud_mkp_plattform_etaps_unmasked, fi_cloud_sr_fi_cloud_mkp_plattform_prod, fi_cloud_sr_fi_cloud_mkp_plattform_prod_unmasked, fi_cloud_sr_fi_cloud_mkp_sbcontainer_etaps, fi_cloud_sr_fi_cloud_openshift-audit_etaps, fi_cloud_sr_fi_cloud_openshift-audit_siu, fi_cloud_sr_fi_cloud_openshift-metrics_etaps, fi_cloud_sr_fi_cloud_openshift-metrics_siu, fi_cloud_sr_fi_cloud_openshift-objects_etaps, fi_cloud_sr_fi_cloud_openshift-objects_siu, fi_cloud_sr_fi_cloud_openshift_etaps, fi_cloud_sr_fi_cloud_openshift_siu, fi_siem_app_fi_ui_openshift_ro, fi_siem_app_if, fi_siem_app_mkg, fi_siem_cap_cloud, fi_siem_cap_if, fi_siem_cap_mkg, fi_siem_sr_if_eial_dm, fi_siem_sr_if_eial_km, fi_siem_sr_if_kits_dm, fi_siem_sr_if_kits_km, fi_siem_sr_if_prod_dm, fi_siem_sr_if_prod_km, fi_siem_sr_if_slpp_dm, fi_siem_sr_if_slpp_km, fi_siem_sr_if_stat_eial_km, fi_siem_sr_if_stat_prod_km, fi_siem_sr_if_stat_slpp_km, fi_siem_sr_mkg_eial_dm, fi_siem_sr_mkg_kits_dm, fi_siem_sr_mkg_prod_dm, fi_siem_sr_mkg_slpp_dm""
    05-06-2024 11:27:06.962 INFO  UserManager [88054 RunDispatch] - Setting user context: splunk-system-user
    05-06-2024 11:27:06.962 INFO  UserManager [88054 RunDispatch] - Done setting user context: NULL -> splunk-system-user
    05-06-2024 11:27:06.964 INFO  UserManager [88054 RunDispatch] - Unwound user context: splunk-system-user -> NULL
    05-06-2024 11:27:06.964 INFO  UserManager [88054 RunDispatch] - Setting user context: j902080
    05-06-2024 11:27:06.964 INFO  UserManager [88054 RunDispatch] - Done setting user context: NULL -> j902080
    05-06-2024 11:27:06.965 INFO  dispatchRunner [88054 RunDispatch] - search context: user=""j902080"", app=""fi_UI_openshift"", bs-pathname=""/usd/as16544a/soft/splunk/etc""
    05-06-2024 11:27:06.966 INFO  SearchParser [88054 RunDispatch] - PARSING: | timechart count
    05-06-2024 11:27:06.966 INFO  dispatchRunner [88054 RunDispatch] - Search running in non-clustered mode
    05-06-2024 11:27:06.966 INFO  dispatchRunner [88054 RunDispatch] - SearchHeadInitSearchMs=3
    05-06-2024 11:27:06.966 INFO  dispatchRunner [88054 RunDispatch] - Executing the Search orchestrator and iterator model (dfs=false).
    05-06-2024 11:27:06.966 INFO  SearchOrchestrator [88054 RunDispatch] - SearchOrchestrator getting constructed
    05-06-2024 11:27:06.966 INFO  SearchOrchestrator [88054 RunDispatch] -  Initialized the SRI
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Initializing feature flags from config. feature_seed=2630698466
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=parallelreduce:enablePreview:true
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=search:search_retry:false
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=search:search_retry_realtime:false
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=parallelreduce:autoAppliedPercentage:false
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=subsearch:enableConcurrentPipelineProcessing:false
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=stats:allow_stats_v2:true
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=search_optimization::set_required_fields:stats:false
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=searchresults:srs2:false
    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=search:read_final_results_from_timeliner:true
    05-06-2024 11:27:06.967 INFO  SearchOrchestrator [88054 RunDispatch] - Search feature_flags={""v"":1,""enabledFeatures"":[""parallelreduce:enablePreview"",""stats:allow_stats_v2"",""search:read_final_results_from_timeliner""],""disabledFeatures"":[""search:search_retry"",""search:search_retry_realtime"",""parallelreduce:autoAppliedPercentage"",""subsearch:enableConcurrentPipelineProcessing"",""search_optimization::set_required_fields:stats"",""searchresults:srs2""]}
    05-06-2024 11:27:06.967 INFO  ISplunkDispatch [88054 RunDispatch] - Not running in splunkd. Bundle replication not triggered.
    05-06-2024 11:27:06.967 INFO  SearchOrchestrator [88057 searchOrchestrator] - Initialzing the run time settings for the orchestrator.
    05-06-2024 11:27:06.967 INFO  UserManager [88057 searchOrchestrator] - Setting user context: j902080
    05-06-2024 11:27:06.967 INFO  UserManager [88057 searchOrchestrator] - Done setting user context: NULL -> j902080
    05-06-2024 11:27:06.967 INFO  AdaptiveSearchEngineSelector [88057 searchOrchestrator] - Search execution_plan=classic
    05-06-2024 11:27:06.967 INFO  SearchOrchestrator [88057 searchOrchestrator] - Creating the search DAG.
    05-06-2024 11:27:06.967 INFO  SearchParser [88057 searchOrchestrator] - PARSING: | timechart count
    05-06-2024 11:27:06.967 INFO  DispatchStorageManagerInfo [88057 searchOrchestrator] - Successfully created new dispatch directory for search job. sid=ea3a13baffe5525d_tmp dispatch_dir=/usd/as16544a/soft/splunk/var/run/splunk/dispatch/ea3a13baffe5525d_tmp
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200
    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (timechart) args: count, AS, count, by, _time
    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free
    05-06-2024 11:27:06.969 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false
    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (timechart) args: count, AS, count, by, _time
    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free
    05-06-2024 11:27:06.969 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false
    05-06-2024 11:27:06.969 INFO  SearchParser [88057 searchOrchestrator] - PARSING: bin _time span=rtspan | prestats count by _time
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200
    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (stats) args: count, by, _time
    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free
    05-06-2024 11:27:06.969 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count
    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false
    05-06-2024 11:27:06.969 INFO  DispatchThread [88057 searchOrchestrator] - BatchMode: allowBatchMode: 1, conf(1): 1, timeline/Status buckets(0):0, realtime(0):0, report pipe empty(0):0, reqTimeOrder(0):0, summarize(0):0, statefulStreaming(0):0
    05-06-2024 11:27:06.969 INFO  DispatchThread [88057 searchOrchestrator] - required fields list to add to remote search = _time,prestats_reserved_*,psrsvd_*
    05-06-2024 11:27:06.969 INFO  DispatchCommandProcessor [88057 searchOrchestrator] - summaryHash=2cdebc05dd9685a6 summaryId=8AE98483-80B5-425E-8D47-AAAB98F3F35D_fi_UI_openshift_j902080_2cdebc05dd9685a6 remoteSearch=prebin  _time span=rtspan  | prestats  count by _time
    05-06-2024 11:27:06.969 INFO  DispatchCommandProcessor [88057 searchOrchestrator] - summaryHash=NS7f796cf728faa99f summaryId=8AE98483-80B5-425E-8D47-AAAB98F3F35D_fi_UI_openshift_j902080_NS7f796cf728faa99f remoteSearch=prebin _time span=rtspan | prestats count by _time
    05-06-2024 11:27:06.969 INFO  DispatchThread [88057 searchOrchestrator] - Getting summary ID for summaryHash=NS7f796cf728faa99f
    05-06-2024 11:27:07.016 INFO  DispatchThread [88057 searchOrchestrator] - Did not find a usable summary_id, setting info._summary_mode=none, not modifying input summary_id=8AE98483-80B5-425E-8D47-AAAB98F3F35D_fi_UI_openshift_j902080_NS7f796cf728faa99f
    05-06-2024 11:27:07.016 INFO  SearchParser [88057 searchOrchestrator] - PARSING: | timechart count
    05-06-2024 11:27:07.016 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100
    05-06-2024 11:27:07.016 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200
    05-06-2024 11:27:07.016 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100
    05-06-2024 11:27:07.016 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200
    05-06-2024 11:27:07.016 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (timechart) args: count, AS, count, by, _time
    05-06-2024 11:27:07.016 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free
    05-06-2024 11:27:07.017 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count
    05-06-2024 11:27:07.017 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false
    05-06-2024 11:27:07.017 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (timechart) args: count, AS, count, by, _time
    05-06-2024 11:27:07.017 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free
    05-06-2024 11:27:07.017 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count
    05-06-2024 11:27:07.017 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false
    05-06-2024 11:27:07.017 INFO  AstOptimizer [88057 searchOrchestrator] - SrchOptMetrics optimize_toJson=0.000280740
    05-06-2024 11:27:07.017 INFO  SearchParser [88057 searchOrchestrator] - PARSING: | timechart count
    05-06-2024 11:27:07.017 INFO  FederatedProviderVisitor [88057 searchOrchestrator] - Federated Whole Search Remote Execution Enabled: false
    05-06-2024 11:27:07.017 INFO  AstOptimizer [88057 searchOrchestrator] - Data federation feature enabled: 0
    05-06-2024 11:27:07.017 INFO  ProjElim [88057 searchOrchestrator] - Black listed processors=[addinfo]
    05-06-2024 11:27:07.033 ERROR AstCommandNode [88057 searchOrchestrator] - AstOptimizerException. Cannot unlink the only command_node=timechart
    05-06-2024 11:27:07.033 INFO  ScopedTimer [88057 searchOrchestrator] - search.optimize 0.016420383

--------------------


====================Ending search.log============================

x86 CPUID registers:
         0: 00000016 756E6547 6C65746E 49656E69
         1: 00050654 07200800 7FFEFBFF BFEBFBFF
         2: 76036301 00F0B5FF 00000000 00C30000
         3: 00000000 00000000 00000000 00000000
         4: 00000000 00000000 00000000 00000000
         5: 00000040 00000040 00000003 00002020
         6: 00000077 00000002 00000009 00000000
         7: 00000000 00000000 00000000 00000000
         8: 00000000 00000000 00000000 00000000
         9: 00000000 00000000 00000000 00000000
         A: 07300404 00000000 00000000 00000603
         B: 00000000 00000000 0000004D 00000007
         C: 00000000 00000000 00000000 00000000
         D: 00000000 00000000 00000000 00000000
         E: 00000000 00000000 00000000 00000000
         F: 00000000 00000000 00000000 00000000
        10: 00000000 00000000 00000000 00000000
        11: 00000000 00000000 00000000 00000000
        12: 00000000 00000000 00000000 00000000
        13: 00000000 00000000 00000000 00000000
        14: 00000000 00000000 00000000 00000000
        15: 00000002 000000A8 00000000 00000000
        16: 00000834 00000E74 00000064 00000000
  80000000: 80000008 00000000 00000000 00000000
  80000001: 00000000 00000000 00000121 2C100800
  80000002: 65746E49 2952286C 6F655820 2952286E
  80000003: 6C6F4720 31362064 43203033 40205550
  80000004: 312E3220 7A484730 00000000 00000000
  80000005: 00000000 00000000 00000000 00000000
  80000006: 00000000 00000000 01006040 00000000
  80000007: 00000000 00000000 00000000 00000100
  80000008: 0000302E 00000000 00000000 00000000
terminating...",,,,,,,,,,,,,,,,,,,,,,,,,,"o What is the expectation when this problem is not there?
No crash when executing ""| timechart count"" search",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• How long has the customer been tracking this issue and how frequent this issue is being hit?
N/A",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"MW time:
Extension time:
Extension reasons:
Resolution:
CINC Channel:
CINC Jira:",,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wed Sep 24 14:41:19 UTC 2025,true,bfernandez(bfernandez),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Lifecycle,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,1.0,35.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),bb27173e-b711-4f6c-ad12-71bd061f2aa9(bb27173e-b711-4f6c-ad12-71bd061f2aa9),bfernandez(bfernandez),7833f9df-7e59-4388-a9f5-44e6336603a9(7833f9df-7e59-4388-a9f5-44e6336603a9),d10212ba-0277-47be-ab81-371c39a2baf5(d10212ba-0277-47be-ab81-371c39a2baf5),dawrobel(dwróbel),ddefa792-1613-4f6a-a05e-9e108782507a(ddefa792-1613-4f6a-a05e-9e108782507a),a9edebe9-9447-45ac-aad2-03fcb3e0af59(a9edebe9-9447-45ac-aad2-03fcb3e0af59),ea4b79b0-faf9-45c1-9064-f5f532639de5(ea4b79b0-faf9-45c1-9064-f5f532639de5),rliu(rliu),rwu(JIRAUSER48911),scalvert(scalvert),bcc0bd7b-44e1-4b00-84cf-eeeccc45d8f8(bcc0bd7b-44e1-4b00-84cf-eeeccc45d8f8),wduan(wduan),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|idkntr:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shihan Zhang,712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Salesforce,Inc.",3721752,012400000005WzCAAU,P2,No,500KW00001jrf6mYAA,,Open,Waiting on Splunk,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SearchLifecycle-FY26Q1-S3,SearchLifecycle-FY26Q1-S4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

Splunk Instance: 10.202.11.171 | Linux credentials: splunker:splk | Splunk Credentials: admin:5up3rn0va

# Navigate to Apps > Search & Reporting ([http://10.202.11.171:8000/en-GB/app/search/search|http://10.202.11.171:8000/en-GB/app/search/search])
# Execute the following search: 
{noformat}| timechart count{noformat}
# Crash log is generated. To find it, run a new search: 
{noformat}index=_internal ""Crashing thread: searchOrchestrator""{noformat}

• If available, will customer upgrade to fixed version?

yes

• If support is able to reproduce, share the setup.

Splunk Instance: 10.202.11.171 | Linux credentials: splunker:splk | Splunk Credentials: admin:5up3rn0va",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,On Prem Premium,,,,,,,,,,,,,,,,2024-05-10 10:27:05.542,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yoya Zhang,,,,,"When using a search with only the timechart command in a search:

| timechart count

Splunk instance will crash with ""Crashing thread: searchOrchestrator"". Currently there is no workaround other than not using this search string",,,,,,,,,,2024-05-10 10:27:05.542,2024-05-10 10:27:05.542,,,,,,Shihan Zhang,712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77,,,,,,"10/May/24 3:27 AM;712020:5e4dbf4d-8c68-4f01-a2f4-6f42f2f13547;Adding {{Search Pipeline}} mission team as the crash occurs after running a search query
Action performed as part of sandi-global-oncall
cc. [~accountid:5d826b3e36a52a0da790e367] ",10/May/24 12:56 PM;6053a3b1f180c300675e6390;I just wanted to clarify that this crash was caused by an improperly-formed SPL ({{| timechart count}}) and it wouldn’t block any customers. ,"13/May/24 6:25 AM;6234374e62dc1e006803b2f4;Hi [~accountid:6053a3b1f180c300675e6390],

You are correct, this is not the intended use of timechart command, however it should not cause splunk to crash. As the crash is happening every time this command is used, I would expect some kind of Warning message informing about improperly-formed SPL, not a crash.

The customer has shared similar concerns:

_Apart from not using that search again, it would be great if Splunk were a bit more robust, so it cannot be crashed by a malformed search._ 

Thanks! ",13/May/24 10:05 AM;6053a3b1f180c300675e6390;Agreed. We need to fix this crash even though it’s not blocking customers. ,"20/May/24 1:12 PM;712020:8f391806-d205-4ec3-aa04-69f3c5235289;{noformat}(gdb) backtrace
#0  AstCommandNode::unlink (this=0x7ffff57fb3a0) at /home/cgarzagarza/git/main/src/search/ast/optimizers/AstCommandNode.cpp:374
#1  0x000055555ca9d444 in AstOptimizationDoc::modify_ast_document (this=0x7ffff57fb8e0, node_to_visit=0x7ffff529ae00, mod_hint=..., valid_sources_for_traversal=...)
    at /home/cgarzagarza/git/main/src/search/ast/optimizers/AstOptimizer.cpp:470
#2  0x000055555ca9ddf3 in AstOptimizationDoc::implPreOrderDFS (this=0x7ffff57fb8e0, ast_cmd_node=0x7ffff529ae00, visitor=...) at /home/cgarzagarza/git/main/src/search/ast/optimizers/AstOptimizer.cpp:552
#3  0x000055555ca9e028 in AstOptimizationDoc::accept (this=0x7ffff57fb8e0, visitor=...) at /home/cgarzagarza/git/main/src/search/ast/optimizers/AstOptimizer.cpp:596
#4  0x000055555ca9ce40 in AstOptimizationDoc::executeVisitorHelper (this=0x7ffff57fb8e0, visitor_list=...) at /home/cgarzagarza/git/main/src/search/ast/optimizers/AstOptimizer.cpp:404
#5  0x000055555ca9c6bf in AstOptimizationDoc::postOptimize (this=0x7ffff57fb8e0) at /home/cgarzagarza/git/main/src/search/ast/optimizers/AstOptimizer.cpp:357
#6  0x000055555ca9bd93 in AstOptimizationDoc::_optimize (this=0x7ffff57fb8e0) at /home/cgarzagarza/git/main/src/search/ast/optimizers/AstOptimizer.cpp:270
#7  0x000055555ca9edc4 in AstOptimizationDoc::optimize (this=0x7ffff57fb8e0, optimizations_to_run=0x0) at /home/cgarzagarza/git/main/src/search/ast/optimizers/AstOptimizer.cpp:731
#8  0x000055555c56ae42 in SearchPhaseGenerator::optimize (this=0x7ffff52b4280, doc=...) at /home/cgarzagarza/git/main/src/search/dispatch/SearchPhaseGenerator.cpp:1617
#9  0x000055555c56c194 in SearchPhaseGenerator::createSingleSourcedPhases (this=0x7ffff52b4280) at /home/cgarzagarza/git/main/src/search/dispatch/SearchPhaseGenerator.cpp:1818
#10 0x000055555c560fe0 in SearchPhaseGenerator::createSearchPhases (this=0x7ffff52b4280, r=0x7ffff52227e8) at /home/cgarzagarza/git/main/src/search/dispatch/SearchPhaseGenerator.cpp:158
#11 0x000055555c584d44 in SearchOrchestrator::executeDefaultMode (this=0x7ffff62b5000) at /home/cgarzagarza/git/main/src/search/dispatch/SearchOrchestrator.cpp:584
#12 0x000055555c5843b7 in SearchOrchestrator::main (this=0x7ffff62b5000) at /home/cgarzagarza/git/main/src/search/dispatch/SearchOrchestrator.cpp:473
#13 0x000055555e69ef31 in Thread::_callMainAndDiscardTerminateException (this=0x7ffff62b5000) at /home/cgarzagarza/git/main/src/basicutil/Thread.cpp:2575
#14 0x000055555e69f37c in Thread::callMain (arg=0x7ffff62b5000) at /home/cgarzagarza/git/main/src/basicutil/Thread.cpp:2646
#15 0x00007ffff7bdf609 in start_thread (arg=<optimized out>) at pthread_create.c:477
#16 0x00007ffff72a1353 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95
{noformat}","10/Jun/24 6:26 AM;6036b82bac6e4e0069e92d92;Hi [~accountid:6053a3b1f180c300675e6390] , [~accountid:712020:8f391806-d205-4ec3-aa04-69f3c5235289] - any further updates on this jira? Support needs to get an update out to the customer, being able to set some kind of expectation with them about this issue would be a positive message. Thanks for any updates that you can share? ",26/Jun/24 10:54 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;I just received a question about the status of this one from a customer (Vodafone) via doc feedback.,06/Aug/24 12:20 PM;712020:8f391806-d205-4ec3-aa04-69f3c5235289;Passing to current OPS champ,"07/Feb/25 11:52 AM;5ce46b40aee3080dc2f65b76;Reviewed during customer issue meeting. The one thing to point out is that the search is not valid, but it still shouldn’t cause a crash.","13/Feb/25 9:50 AM;6053a471695c3900707a5ed6;Some notes from bug scrub meeting:

# Firstly better to verify if the issue is still there in latest release (bug was found 9.2)
# Fix might be needed in:
## Handler: throw an exception when the search request starts with non-generating cmd
## Phase generation: add check on searches starting with non-generating cmd","06/May/25 3:39 PM;630489ffbcb35371090f2048;I just associated SFDC case *3721752* to this SPL, this crash appears to also be trigged with the dc() function.

In order to test this, you will need to ensure you have anything other than a wildcard in the parenthesis.
In the linked SFDC case, the customer ran the following search which triggered a crash:

| timechart span=1d dc(clientIP) as Views

I was able to replicate this using the same instructions found in Steps to Reproduce, on Splunk v9.1.5 (which the customer is running) as well as v9.1.7, v9.1.9, and v9.2.6.
This does not appear to affect v9.3.1, but no other v9.3.0+ versions have been tested.","13/May/25 1:17 PM;6087a44df558240070f3795b;Hi [~accountid:712020:116391a8-fb85-45f4-b37a-1f14a50d58eb],

-Based upon Ben’s update below, would you happen to know if this issue is resolved in versions higher than 9.3.1? If yes, then the list of affected versions should be updated. Thanks.-

Correction: Ben updated in case *3721752* that he was able to replicate the crash in version 9.3.1 and later versions. The customer was advised not to use the command this way, but this has been a known issue for a while and the admins have little control on what customers enter in their search.  When can this be fixed?  Thank you.","14/May/25 7:48 AM;630489ffbcb35371090f2048;After further testing, I’ve found that this appears to affect the more recent versions as well, including version 9.3.1.
The v9.3.1 server I tested on is my homelab Splunk server… I have no idea why it isn’t displaying the same behavior as it is a fairly basic deployment.

On freshly deployed v9.3.1 servers the behavior is displayed, additionally, the same behavior is displayed on all newer versions of Splunk as well.",23/May/25 10:18 AM;63315a9efedc6169aed6c7cd;Hi [~accountid:712020:116391a8-fb85-45f4-b37a-1f14a50d58eb] is there any update on this? Thank you!,03/Jun/25 12:52 PM;630489ffbcb35371090f2048;[~accountid:5d826b3e36a52a0da790e367] and [~accountid:61324d282aa2800068a42013] - Can we get an update on this? Salesforce requested this Jira case be escalated in SFDC case *3721752*. ,"03/Jun/25 1:07 PM;712020:116391a8-fb85-45f4-b37a-1f14a50d58eb;Hey [~accountid:630489ffbcb35371090f2048] ,

What’s the priority of this ticket, has it been an escalated P2? I’m currently fully occupied with other escalated P2s. If this one is also an escalated p2, I can prioritize it. Otherwise, I may not have the bandwidth to take a look right now.

cc: [~accountid:61324d282aa2800068a42013] 

Thanks ","03/Jun/25 2:32 PM;630489ffbcb35371090f2048;[~accountid:712020:116391a8-fb85-45f4-b37a-1f14a50d58eb] The case is currently a P2, and it was a P1 previously as the search head that experienced a crash with ""Crashing thread: searchOrchestrator"" became non-responsive until the customer performed a reboot.","03/Jun/25 3:11 PM;712020:116391a8-fb85-45f4-b37a-1f14a50d58eb;Hey [~accountid:630489ffbcb35371090f2048] ,

Got it, I’ll try to get to this ticket ASAP, but I’m currently handling several escalated P2s, so I can’t prioritize this one just yet.

Thanks",25/Jul/25 12:45 PM;630489ffbcb35371090f2048;[~accountid:712020:116391a8-fb85-45f4-b37a-1f14a50d58eb] Any chance you’ve been able to make any progress on this case?,"29/Jul/25 2:31 PM;712020:116391a8-fb85-45f4-b37a-1f14a50d58eb;Hey [~accountid:630489ffbcb35371090f2048],

I’ll have some bandwidth to work on this starting today and continuing into next week.

Thanks","14/Aug/25 4:25 PM;630489ffbcb35371090f2048;Hi [~accountid:712020:116391a8-fb85-45f4-b37a-1f14a50d58eb], any luck with this issue?","22/Aug/25 2:24 PM;712020:116391a8-fb85-45f4-b37a-1f14a50d58eb;Hey team,

Having no bandwidth to deal with this ticket, need to hand off to the current on-call rotations. cc: [~accountid:627ad5719311100068a01a3f] , [~accountid:712020:cb7dd6ed-2701-49ed-a3ba-0dc95f61ee43] , [~accountid:712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77] 

Notes:

This crash is reproducible and impacts multiple versions. It doesn't occur with all non-generating commands, {{stats}} and {{sort}} work fine in my testing, but the issue appears specifically with {{timechart}} and {{chart}}.","22/Aug/25 2:51 PM;712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77;Hi [~accountid:630489ffbcb35371090f2048] , I am this rotation’s oncall engineer and I will take look into it next Monday. Will keep you posted!","25/Aug/25 3:24 PM;712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77;Hi [~accountid:630489ffbcb35371090f2048] ,

I do some investigation today, it seems to crash on {{AstCommandNode::unlink()}}, because it is trying to unlink the only command_node=timechart.

!image-20250825-222356.png|width=100%,alt=""image-20250825-222356.png""!

The system does do the check(check if it is the only command_node) and throw the exception, therefore my hypothesis is that there’s no proper catches for the exception, but I still need to verify it based on some more research, should have some updates tomorrow and will keep you posted!","25/Aug/25 3:46 PM;630489ffbcb35371090f2048;[~accountid:712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77] Thank you for the update, sounds like this means there will be slightly less work - the check is already in place, just need the system to actually handle the exception appropriately! ","28/Aug/25 8:47 AM;712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77;I did some further research into this problem, after the Exception is thrown, the system calls destructor method of {{AstOptimizationDoc}}, in which it will delete resources(like memory) consumed, and the crashes happen in this destructor method.

I will do more checks to see why it crashes.","04/Sep/25 2:03 PM;712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77;Hi [~accountid:630489ffbcb35371090f2048] , I make some further research into it: [https://splunk.atlassian.net/wiki/spaces/~7120207b9cf8ba3c434bbaadca9638a4082f77/pages/1078928509066/SPL-255514|https://splunk.atlassian.net/wiki/spaces/~7120207b9cf8ba3c434bbaadca9638a4082f77/pages/1078928509066/SPL-255514|smart-link]  and document my findings here, currently it seems like the system crashes because one pointer(_root) is destructed twice and the reason might be the destructor are not updated to match with previous change to unique_ptr…But that hasn’t been thoroughly verified yet and I will do further research into it before applying the code fix.

CC: [~accountid:6053a471695c3900707a5ed6] ","04/Sep/25 2:07 PM;630489ffbcb35371090f2048;[~accountid:712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77] 

I do not have access to that space, however I am sure it would go over my head. Thank you for the update!",17/Sep/25 1:46 PM;630489ffbcb35371090f2048;[~accountid:712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77] Any chance you have made further progress since your last update?,"18/Sep/25 9:05 AM;712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77;[~accountid:630489ffbcb35371090f2048] Yes…I have started my work on the code fix, we have several directions for how to fix it, I have finished coding for one but will still need to discuss with other experienced engineers to see which directions work best for us, this week I may focus on another ticket first, so this one should have some updates in next week.",18/Sep/25 11:52 AM;6036b887e2020c0070c2eba2;[~accountid:712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77] thank you for the update. I will notify the customer on progress made,"18/Sep/25 11:56 AM;712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77;[~accountid:6036b887e2020c0070c2eba2] Thanks! Just a quick check, I see this ticket is now a bug type in our system and the SFDC case related with it has been closed🤔 But I see you mentioned that you will notify customers, does that mean the SFDC case still open and the type for this ticket isn’t correct?",18/Sep/25 11:58 AM;6036b887e2020c0070c2eba2;[~accountid:712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77] I am watching over the case for [~accountid:630489ffbcb35371090f2048] while he is OOO. The case for Salesforce is still open currently. SFDC:[3721752|https://splunk.lightning.force.com/lightning/r/Case/500KW00001jrf6mYAA/view],22/Sep/25 10:21 AM;6036b887e2020c0070c2eba2;[~accountid:712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77] Let me know if there is anything to share with the customer. Ben is back tomorrow and will take back the case after that. ,24/Sep/25 7:41 AM;6036b887e2020c0070c2eba2;[~accountid:712020:7b9cf8ba-3c43-4bba-adca-9638a4082f77] I wanted to check in to see how discussions went internally on the code fix with the other engineers.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,In Progress,04/Sep/25 1:56 PM
[release notes] CLONE - CIDR match for tstats with ipv6 addresses isn't supported,SPL-254077,3409083,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Withdrawn,Netra Inamdar,6053c32c06cbba006a0efaf1,Kristina Gorzynski,61436d20e7c3280070a1979f,Kristina Gorzynski,61436d20e7c3280070a1979f,12/Apr/24 4:23 PM,14/Aug/25 5:02 PM,,14/Aug/25 5:02 PM,10.0.2503.x,10.0.x,9.0.2303.100(Icebreaker-Tick),9.0.2305.200 (JuicyFruitTock),9.1.0,9.1.1,9.1.10,9.1.2,9.1.2308.103(KitKat_Tick_Patch_3),9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Lindt_Tick_Patch_1(9.1.2312.101),Lindt_Tick_Patch2(9.1.2312.102),Lindt_Tick_Patch3(9.1.2312.103),Milkyway(9.2.2403.100),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search,,,,,0,cfit_reviewed,,,,,,,"Customer reported that whenever they’re running tstats with a subsearch inputlookup query containing a where targeting an ipv6 value, they’re getting the following error:

{noformat}Error in 'TsidxStats': WHERE clause is not an exact query{noformat}

Example:

{noformat}| tstats summariesonly=t count sum(Web.bytes_out) as bytes_out from datamodel=Web where sourcetype=xyz  NOT
[ inputlookup ip_lookup
| fields src
| where src=""2001:181b:c9ab:ec50:cd6a:423b:215b:f442/128""
| rename src AS Web.src] by Web.src{noformat}

When running the exact same query, against the exact same lookup, but with ipv4 value instead, the search completes successfully.

Example:

{noformat}| tstats summariesonly=t count sum(Web.bytes_out) as bytes_out from datamodel=Web where sourcetype=xyz  NOT
[ inputlookup ip_lookup
| fields src
| where src=""10.84.67.78/32""
| rename src AS Web.src] by Web.src{noformat}

This is easily reproducible. I don’t believe the search logs are very useful in this situation, but I attached them anyway","[https://skynet-search.splunkcloud.com/en-GB/app/cloudops/stack_overview?form.stack=socdap|https://skynet-search.splunkcloud.com/en-GB/app/cloudops/stack_overview?form.stack=socdap]

[https://skynet-search.splunkcloud.com/en-GB/app/cloudops/stack_overview?form.stack=cs-support|https://skynet-search.splunkcloud.com/en-GB/app/cloudops/stack_overview?form.stack=cs-support]",Andrew Brown,John Reed,Macy Cronkrite,User known,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,6053a82bf180c300675e972d,557058:a421b77e-b3c1-4bb3-813f-af8ade9c3659,unknown,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,SPL-241370,,,,,,,,,,,,,,,,,,,,12/Apr/24 4:23 PM;kgorzynski;ipv4.log;https://splunk.atlassian.net/rest/api/3/attachment/content/7050420,12/Apr/24 4:23 PM;kgorzynski;ipv6.log;https://splunk.atlassian.net/rest/api/3/attachment/content/7050421,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@591ca3ea,,,,,N/A,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"o Have they made any changes to their environment just before the issue started?
No",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Splunk Cloud,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,10022400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 
https://skynet-search.splunkcloud.com/en-GB/app/cloudops/stack_overview?form.stack=socdap

o If available, provide Splunk topology diagram, host name with IP mapping.
https://skynet-search.splunkcloud.com/en-GB/app/cloudops/stack_overview?form.stack=socdap",,,,,,,,,,"o What errors are being reported?
Error in 'TsidxStats': WHERE clause is not an exact query",,,,,,,,,,,,,No,,,,,,,,,,,,,"o What is the expectation when this problem is not there?
Search works for both ipv4 and ipv6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• How long has the customer been tracking this issue and how frequent this issue is being hit?
Always",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives

List of individuals to include in a review of this incident

h1. Dependent Service Incident ID

(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers

h1. What Happened?

h1. What Did We Do Well?

h1. What Do We Need to Improve?

h1. Dogfood Our Own Observability Products

||*Product*||*Use*||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.

h3. Please share feedback about our products during this incident.

h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items

JIRAs should be linked to this ticket",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Fri May 30 17:48:55 UTC 2025,true,ea4b79b0-faf9-45c1-9064-f5f532639de5(ea4b79b0-faf9-45c1-9064-f5f532639de5),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Pipeline,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,2.0,10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),johnr(johnr),kgorzynski(JIRAUSER52213),ninamdar(JIRAUSER45991),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Netra Inamdar,6053c32c06cbba006a0efaf1,,,,,,,,,,,,,,,,,,0|idf027:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Netra Inamdar,6053c32c06cbba006a0efaf1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Nestlé Operational Services Worldwide S.A.,3247349,012400000005WzCAAU,P3,No,5005a00002YR1ZRAA1,,Open,Waiting on Splunk,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3247349,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment
Log into test stack - https://sh-i-08152164d3f607fd4.cs-support.splunkcloud.com/
(if you don't have access, let me know, I'll create you an account)
 
Run search for ipv4 (works):
| tstats summariesonly=t count sum(Web.bytes_out) as bytes_out from datamodel=Web where sourcetype=xyz  NOT
[ inputlookup ip_lookup
| fields src
| where src=""10.84.67.78/32""
| rename src AS Web.src] by Web.src

Run the same search for ipv6 (doesn't work)
| tstats summariesonly=t count sum(Web.bytes_out) as bytes_out from datamodel=Web where sourcetype=xyz  NOT
[ inputlookup ip_lookup
| fields src
| where src=""2001:181b:c9ab:ec50:cd6a:423b:215b:f442/128""
| rename src AS Web.src] by Web.src 

• If available, will customer upgrade to fixed version?
Yes

• If support is able to reproduce, share the setup.
See above",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?
N/A

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?
Yes

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection
N/A

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.
Search

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 
N/A

 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
Bug is fixed",,,,,,,,,,,,,,,,,,,2024-08-22 21:17:19.284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4_*:*_1_*:*_0_*|*_5_*:*_1_*:*_11464945205_*|*_10039_*:*_1_*:*_1443969,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Run,,"The <code>tstats</code> command currently doesn’t filter events with CIDR match on fields that contain IPv6 addresses. Running <code>tstats</code> searches containing IPv6 addresses might result in the following error indicating that the addresses are treated as non-exact queries:

<div class=""samplecode"">
<pre>
Error in 'TsidxStats': WHERE clause is not an exact query
</pre>
</div>",,,,,,,,,,2024-08-22 21:17:19.284,2024-08-22 21:17:19.284,,,,,,Netra Inamdar,6053c32c06cbba006a0efaf1,,,,,,"12/Apr/24 4:32 PM;61436d20e7c3280070a1979f;Release note automation picks up the text in the workaround section and adds it to [Known issues|https://docs.splunk.com/Documentation/Splunk/9.1.2/ReleaseNotes/Knownissues] for Splunk Enterprise. 

For applicable [cloud release notes|https://docs.splunk.com/Documentation/SplunkCloud/9.0.2303/ReleaseNotes/Issues#Version_9.0.2303], I added the following to the Known issues.  

||*Date filed or added*||*Issue number*||*Description*||
|2024-4-12|SPL-254077|CIDR match for tstats with ipv6 addresses isn't supported.
The {{tstats}} command currently doesn’t filter events with CIDR match on fields that contain IPv6 addresses. Running {{tstats}} searches containing IPv6 addresses might result in the following error indicating that the addresses are treated as non-exact queries:
{noformat}Error in 'TsidxStats': WHERE clause is not an exact query{noformat}|",12/Apr/24 4:47 PM;61436d20e7c3280070a1979f;[~accountid:6053c32c06cbba006a0efaf1]Let me know if these release notes need changes. Close this ticket if you're satisfied with my fix. Thanks! ,"20/May/24 1:25 PM;61436d20e7c3280070a1979f;Since this issue still isn’t fixed, I”ve added Milkyway to the *Affects versions*, so that the issue is carried forward into [Milkyway Known issues relnotes|https://docs.splunk.com/Documentation/SplunkCloud/9.2.2403/ReleaseNotes/Issues]. ","22/Aug/24 2:17 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;It’s showing fixed now, so I’ll assume it’s fixed in Nutella. ",22/Aug/24 3:04 PM;61436d20e7c3280070a1979f;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] where do you see that this issue has been fixed? ,"22/Aug/24 4:18 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:61436d20e7c3280070a1979f] I was relying on the overall Jira status: Resolved > Fixed. 

I assumed that was not the status when you considered it for Milkyway back in May and that the status changed sometime between now and then. But I see in the history that you yourself changed the status from Unresolved to Resolved back in April. Oh, rats, that’s the problem with having Jiras that track release notes instead of Jiras that track bugs. Did you mean that this Jira is Fixed because it’s published in the list of known issues?

When you said “Since this issue still isn’t fixed, I”ve added Milkyway to the *Affects versions*,” which issue were you referring to.","22/Aug/24 5:00 PM;61436d20e7c3280070a1979f;🤯  [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] , sorry for the confusion! Yes, you’re correct, I set the ticket to resolved to indicate that ipv4 only support is documented in Known issues relnotes. But, the bug (no support for ipv6) is still unresolved. 

I’ve asked [~accountid:6053a82bf180c300675e972d]  to clarify whether Eng is planning to fix support for ipv6. Until it’s fixed, I suppose we have to keep carrying forward this ipv4 only Known issue into future release notes, which is a pain. ","23/Aug/24 9:17 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;Thanks, Kristina. Since this is the bug number that we include in the known issues, it would be clearer to leave it unresolved until the product issue is resolved. If that’a a big problem for your Jira boards, we can live with the current situation. It just requires anyone who checks on the Jira to understand that it’s an abstraction. 

I’ll undo the changes I made that moved this to fixed issues in the SCP release notes","27/Aug/24 6:47 PM;6053a82bf180c300675e972d;We do plan to add support for IPv6, but not until late in FY25 at the earliest, or even FY26. It is not currently in our SRR for this year. ",30/May/25 10:48 AM;6053c32c06cbba006a0efaf1;This is the ticket for adding IPv6 support - [https://splunk.atlassian.net/browse/SPL-250308|https://splunk.atlassian.net/browse/SPL-250308|smart-link]  but it is not being worked on yet. We can close this Jira and track changes in [https://splunk.atlassian.net/browse/SPL-277369|https://splunk.atlassian.net/browse/SPL-277369|smart-link] when it is picked up.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,14/Aug/25 5:02 PM
standard-mode federated search should alert ( and block the search ) when it is run in real time,SPL-253757,3396923,Bug,Resolved,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Brooke Raschke,712020:73ddc4da-9d6d-4655-ac18-678b0963cda1,Nir Frenkel,6053a293009fee00694bc4e6,Nir Frenkel,6053a293009fee00694bc4e6,05/Apr/24 4:39 PM,16/Sep/25 12:59 PM,,24/Jul/25 3:38 PM,10.0.x,9.0.0(Aurum),9.1.0(Beryllium),9.2.0(Cobalt),9.3.0(Duranium),9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.1.2507.x,,,,,,,,Search Federation,,,,,0,,,,,,,,"As of now we don’t alert the user if they  are using {{standard}} mode while using realtime search. 

We should alert the user in the UI AND fail the whole search so that the user gets only full result of his search",,Emily Cheung,srv- ssc-gitlab,Steven Castroverde,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a74994d7b90069f8435f,613254346fa73c006a9e37be,6053b637311e270068e41394,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-253755,,,,,,,,,,,,,,,,,,,,29/May/25 6:27 PM;nfrenkel;Screenshot 2025-05-29 at 6.26.53 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8185696,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@41a0b2cd,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,10022400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives

List of individuals to include in a review of this incident

h1. Dependent Service Incident ID

(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers

h1. What Happened?

h1. What Did We Do Well?

h1. What Do We Need to Improve?

h1. Dogfood Our Own Observability Products

||*Product*||*Use*||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.

h3. Please share feedback about our products during this incident.

h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items

JIRAs should be linked to this ticket",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Fri May 30 18:49:42 UTC 2025,true,nfrenkel(JIRAUSER43952),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Federation,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,1.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,b3388b99-76c7-45a5-a35b-d02de257d464(b3388b99-76c7-45a5-a35b-d02de257d464),echeung(echeung),nfrenkel(JIRAUSER43952),srv-ssc-gitlab(srv-ssc-gitlab),scastroverde(scastroverde),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Brooke Raschke,712020:73ddc4da-9d6d-4655-ac18-678b0963cda1,,,,,,,,,,,,,,,,,,0|iddkz3:,,,,,,,resolved in [https://splunk.atlassian.net/browse/SPL-237796?atlOrigin=eyJpIjoiNGUxMTVmOGY5NWIxNDFkYmI2ZDU2Y2IxNGM3OGIwODAiLCJwIjoiamlyYS1zbGFjay1pbnQifQ|https://splunk.atlassian.net/browse/SPL-237796?atlOrigin=eyJpIjoiNGUxMTVmOGY5NWIxNDFkYmI2ZDU2Y2IxNGM3OGIwODAiLCJwIjoiamlyYS1zbGFjay1pbnQifQ],,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,Brooke Raschke,712020:73ddc4da-9d6d-4655-ac18-678b0963cda1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Datafed-FY25Q4-S4,Datafed-FY25Q4-S7,datafed-FY26Q1-S1,Next Priority,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2024-04-17 20:50:23.039,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_1_*:*_0_*|*_3_*:*_1_*:*_1625244149_*|*_10001_*:*_1_*:*_66614_*|*_10039_*:*_1_*:*_37486652081,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2024-04-17 20:50:23.039,2024-04-17 20:50:23.039,,,,,,Brooke Raschke,712020:73ddc4da-9d6d-4655-ac18-678b0963cda1,,,,,,17/Apr/24 1:50 PM;613254346fa73c006a9e37be;[Steven Castroverde|https://cd.splunkdev.com/scastroverde] mentioned this issue in [a merge request|https://cd.splunkdev.com/cloudworks/monitoring/cmc-app/-/merge_requests/1516] of [cloudworks / monitoring / cmc-app|https://cd.splunkdev.com/cloudworks/monitoring/cmc-app] on branch [sc/SPL-253757|https://cd.splunkdev.com/cloudworks/monitoring/cmc-app/-/tree/sc/SPL-253757]:{quote}SPL-253757: [WL dashboard] Create timespan utility for tabbed components{quote},"05/Sep/24 2:28 PM;6053a74994d7b90069f8435f;Removing the Backport_Approved label, which I think was erroneously cloned to this ticket.","29/May/25 6:27 PM;6053a293009fee00694bc4e6;[~accountid:6053b637311e270068e41394] I see that you had an MR that is linked to this ticket Which is really strange….. Since your MR is not really related to this ticket in any way…..I was wondering why you picked this SPL for your MR

[https://cd.splunkdev.com/cloudworks/monitoring/cmc-app/-/merge_requests/1516|https://cd.splunkdev.com/cloudworks/monitoring/cmc-app/-/merge_requests/1516]

!Screenshot 2025-05-29 at 6.26.53 PM.png|width=1435,height=123,alt=""Screenshot 2025-05-29 at 6.26.53 PM.png""!",30/May/25 8:25 AM;6053b637311e270068e41394;That was a typo on my end the original ticket was [https://splunk.atlassian.net/browse/SPL-253727|https://splunk.atlassian.net/browse/SPL-253727|smart-link] ,30/May/25 11:49 AM;6053a293009fee00694bc4e6;Thanks [~accountid:6053b637311e270068e41394] ! I will remove your MR from this ticket then 🙂 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3396922,SPL-253756,[FS-S2S] ITSI integration for Splunk Federated Search,Done,24/Jul/25 3:38 PM
transparent-mode federated search should alert ( and block the search ) when it is run in real time,SPL-253755,3396919,Bug,Resolved,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Brooke Raschke,712020:73ddc4da-9d6d-4655-ac18-678b0963cda1,Nir Frenkel,6053a293009fee00694bc4e6,Nir Frenkel,6053a293009fee00694bc4e6,05/Apr/24 4:32 PM,16/Sep/25 12:59 PM,,02/Jul/25 9:04 AM,9.0.0(Aurum),9.1.0(Beryllium),9.2.0(Cobalt),9.3.0(Duranium),9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.1.2507.x,,,,,,,,Search Federation,,,,,0,psr-release-flubber,,,,,,,"As of now we don’t alert the user if they are using {{transparent}} mode while using realtime search.

In transparent mode we should show an alert on the UI and not send the search to the RSH only ( but execute it locally )



Some more information : 

[https://help.splunk.com/en/splunk-enterprise/search/search-manual/9.2/search-and-report-in-real-time/about-real-time-searches-and-reports|https://help.splunk.com/en/splunk-enterprise/search/search-manual/9.2/search-and-report-in-real-time/about-real-time-searches-and-reports|smart-link] 

* search-realtime is based on getting data directly from the indexing pipeline…. and the plan is to not support it in the future….. hence we don’t want to add support for it for federated search
* indexed-realtime gives you basically the same thing but with slightly more latency

I believe you can use {{bool shouldExecuteRTWindow() const { return _realtime && (!_rt_earliest.empty() || !_rt_latest.empty()); }}}  in {{src/framework/SearchResultsInfo.h}} to know if the search is realtime search.

cc [~accountid:712020:73ddc4da-9d6d-4655-ac18-678b0963cda1] .",,Andrew Brown,Brooke Raschke,Emily Cheung,Giri Basava,Macy Cronkrite,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,712020:73ddc4da-9d6d-4655-ac18-678b0963cda1,6053a74994d7b90069f8435f,62a6ed3b9cd13c0068aea7f7,557058:a421b77e-b3c1-4bb3-813f-af8ade9c3659,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-253757,,,,,,,,,,,,,,,,,,,,,,,,,,,28/May/25 4:33 PM;sukruthk;image-20250528-233104.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8182719,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@22583454,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,5270400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives

List of individuals to include in a review of this incident

h1. Dependent Service Incident ID

(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers

h1. What Happened?

h1. What Did We Do Well?

h1. What Do We Need to Improve?

h1. Dogfood Our Own Observability Products

||*Product*||*Use*||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.

h3. Please share feedback about our products during this incident.

h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items

JIRAs should be linked to this ticket",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thu Jul 24 19:12:22 UTC 2025,true,nfrenkel(JIRAUSER43952),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Federation,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,1.0,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),b3388b99-76c7-45a5-a35b-d02de257d464(b3388b99-76c7-45a5-a35b-d02de257d464),echeung(echeung),9657acd1-4db3-482a-a217-b59b5d5c075f(9657acd1-4db3-482a-a217-b59b5d5c075f),nfrenkel(JIRAUSER43952),sukruthk(JIRAUSER50500),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Brooke Raschke,712020:73ddc4da-9d6d-4655-ac18-678b0963cda1,,,,,,,,,,,,,,,,,,0|iddky7:,,,,,,,resolved in [https://splunk.atlassian.net/browse/SPL-237796?atlOrigin=eyJpIjoiNGUxMTVmOGY5NWIxNDFkYmI2ZDU2Y2IxNGM3OGIwODAiLCJwIjoiamlyYS1zbGFjay1pbnQifQ|https://splunk.atlassian.net/browse/SPL-237796?atlOrigin=eyJpIjoiNGUxMTVmOGY5NWIxNDFkYmI2ZDU2Y2IxNGM3OGIwODAiLCJwIjoiamlyYS1zbGFjay1pbnQifQ],,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,Brooke Raschke,712020:73ddc4da-9d6d-4655-ac18-678b0963cda1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Datafed-FY25Q4-S4,Datafed-FY25Q4-S2,Datafed-FY25Q4-S7,datafed-FY26Q1-S1,Next Priority,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2024-09-05 21:28:42.222,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_1_*:*_0_*|*_3_*:*_1_*:*_690673701_*|*_10001_*:*_1_*:*_5553_*|*_10039_*:*_1_*:*_36796277173,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2024-09-05 21:28:42.222,2024-09-05 21:28:42.222,,,,,,Brooke Raschke,712020:73ddc4da-9d6d-4655-ac18-678b0963cda1,,,,,,"05/Sep/24 2:28 PM;6053a74994d7b90069f8435f;Removing the Backport_Approved label, which I think was erroneously cloned to this ticket.","28/May/25 4:33 PM;611be68028ae75006ac4afda;Here is another instance where we are seeing an unrelated error or no alert about the federated search run in real time. 
Topology: Onprem-to-Cloud
FSH - [http://10.224.67.146:8000/|http://10.224.67.146:8000/]
admin/Chang3d!
Remote providers adding in transparent mode -
[https://console.splunkcloud.systems/stg/stack/rshc-cloud-noah/status|https://console.splunkcloud.systems/stg/stack/rshc-cloud-noah/status|smart-link] 
[https://console.splunkcloud.systems/stg/stack/rsh-cloud-classic/status|https://console.splunkcloud.systems/stg/stack/rsh-cloud-classic/status|smart-link] 

The federated search is being run on a onprem node which has two remote providers added in transparent mode.
This error appears for any real time search run on the onprem stack. 

{noformat}[rsh-cloud-classic,rshc-cloud-noah,sh-i-061b6831c007baebf.rshc-cloud-noah.stg.splunkcloud.com,sh-i-06d728877965dd891.rsh-cloud-classic.stg.splunkcloud.com] Unknown error for indexer: [sh-i-061b6831c007baebf.rshc-cloud-noah.stg.splunkcloud.com:ingest_pipe=0] . Search Results might be incomplete. Try running your search again. If you see this error repeatedly, open a case with Splunk Support. {noformat}

!image-20250528-233104.png|width=1912,height=187,alt=""image-20250528-233104.png""!

Can we handle this in a better way by blocking the federated search or showing a alert saying federated search not supported with real time search. Thanks",29/May/25 10:46 AM;62a6ed3b9cd13c0068aea7f7;[~accountid:712020:73ddc4da-9d6d-4655-ac18-678b0963cda1] this is a simple yet impactful startup task for you. ,24/Jul/25 10:52 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;This is included in the public list of known and fixed issues. If it’s fixed please provide a fix version for customers. [~accountid:712020:73ddc4da-9d6d-4655-ac18-678b0963cda1] ,24/Jul/25 12:00 PM;712020:73ddc4da-9d6d-4655-ac18-678b0963cda1;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] I’ve updated the version in the ticket,24/Jul/25 12:12 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:712020:73ddc4da-9d6d-4655-ac18-678b0963cda1] perfect thank you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3396922,SPL-253756,[FS-S2S] ITSI integration for Splunk Federated Search,Done,02/Jul/25 9:04 AM
CLONE - [release note] Issue with Splunk Enterprise versions 9.1.x through 9.4.x when http proxy is used,SPL-253690,3395538,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Harendra Rawat,6053a6d694d7b90069f83e4f,Kristina Gorzynski,61436d20e7c3280070a1979f,Kristina Gorzynski,61436d20e7c3280070a1979f,04/Apr/24 6:13 PM,30/Jul/25 4:24 PM,,27/Jan/25 8:22 PM,10.0.x,9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0.2503.x,10.0.x,,,,,,,Search,Upgrade,,,,0,AxpTriaged,cfit_reviewed,,,,,,"Splunk was installed on the 9.1.1 version on a basic environment with one search head and indexer and a proxy configured in between them. From the search head Under Distributed search>>search peers we can the indexer was connected and healthy. But when we run a simple search it returns the error ""Received error from proxy server"". 

Now, Splunk version 9.0.4 was installed on the same instance using Docker. After the successful installation of the 9.0.4 version the health status of the indexer remained healthy and when we ran a basic it returned the results with no errors.

Please note that there were no changes in the configuration while changing the versions.

List of proxies tested by the customer:

* Squid 4.15 on RHEL
* Squid 3.5.7 in LAB-2
* Squid 6.4 in LAB-2
* Envoyproxy 1.27.1
* Cntlm 0.92.3
* Bluecoat Proxy 6.7.5 SWG

Please find the search logs and DIAGs below

----

UPDATE 3/21/24:
We were able to reproduce a very similar behavior to the customer in our internal labs.

The issue was not observed on 9.0.4 but was seen on both 9.1.1 and 9.2.


*a.) WORKING: On 9.0.4 setup with SH and Indexer:* 

In the search head search.log we can see that we make a call to the “indexer” ipaddress {{10.202.9.10:8089}} to the {{/services/streams/search}} endpoint. 

*_Here in this version, we do get the results from the search query._**From search.log without error:*

{noformat}03-21-2024 08:05:01.921 DEBUG SearchResultParser [1513664 DistributedSearchResultCollectorThread] - https://10.202.9.10:8089/services/streams/search?sh_sid=1711008300.322 The protocol header is [splunk 9.0.4,124991,7]
03-21-2024 08:05:01.921 DEBUG SearchResultParser [1513664 DistributedSearchResultCollectorThread] - extraction successful header bytes left 124991 body_bytes 7
03-21-2024 08:05:01.921 DEBUG SearchResultTransaction [1513664 DistributedSearchResultCollectorThread] - Received data for peer=https://10.202.9.10:8089/services/streams/search?sh_sid=1711008300.322 of size=32768{noformat}


In the access.log from the squid proxy server we observe ""200"" for these requests and also we are getting results from the search query:
/var/log/squid/access.log:

{noformat}1711008143.104  57763 10.202.9.191 TCP_TUNNEL/200 2319609 CONNECT 10.202.9.10:8089 - HIER_DIRECT/10.202.9.10 -{noformat}


*Under server.conf for SH and Indexer*

{noformat}[proxyConfig]
http_proxy = http://10.202.20.39:8000{noformat}

*NOTE:* 10.202.9.191 (SH for 9.0.4), 10.202.9.10 (IDX for 9.0.4), 10.202.20.39 (Proxy server)*Test Instance Proxy server:*

{noformat}For CLI:
ssh://splunker@10.202.20.39:22
Username: splunker
Password: splk{noformat}

*Test Instance for SH 9.0.4:*

{noformat}For UI:
http://10.202.9.191:8000/
User : admin
Password: 5up3rn0va

For CLI: 
ssh://splunker@10.202.9.191
User: splunker
Password: splk{noformat}

*Test Instance for IDX 9.0.4:*

{noformat}On CLI:
ssh://splunker@10.202.9.10:22
User: splunker
Password: splk

For UI:
http://10.202.9.10:8000/
User : admin
Password: 5up3rn0va{noformat}

---------------------------------------------------------------------------------------------------------------------------------------------
*b.) NON-WORKING: On 9.1.1 setup with SH and Indexer(Non-working):* Here we don't get any results for our specific search query, and do get the following error on the UI:

{noformat}The following error(s) and caution(s) occurred while the search ran. Therefore, search results might be incomplete. Hide errors.
Received error from proxy server: Socket error: Resource temporarily unavailable
Unknown error for indexer: [idx1] . Search Results might be incomplete. If this occurs frequently, check on the peer.{noformat}

!image (10).png|width=1351,height=280!

In the search head search.log we can see that we make a call to the proxy server {{10.202.20.39}} to the the endpoint {{/services/streams/search}} where in version 9.04 we made a call to the indexer IP on this endpoint.

*On the search.log:*

{noformat}03-21-2024 08:10:16.473 ERROR SearchResultTransaction [124962 DistributedSearchResultCollectorThread] - Got status 502 from https://10.202.20.39:8089/services/streams/search?sh_sid=1711008616.267
......
03-21-2024 08:10:16.473 ERROR SearchResultParser [124962 DistributedSearchResultCollectorThread] - HTTP error status message from https://10.202.20.39:8089/services/streams/search?sh_sid=1711008616.267: Received error from proxy server:{noformat}



In the access.log from the squid proxy server we observe ""503"" for these requests:
*/var/log/squid/access.log:*

{noformat}1711008891.640  27979 10.202.20.217 TCP_TUNNEL/200 3366 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -
1711008891.640  28009 10.202.20.217 TCP_TUNNEL/200 3736 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -
1711008891.640  28003 10.202.20.217 TCP_TUNNEL/200 4476 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -
1711008891.640  27996 10.202.20.217 TCP_TUNNEL/200 3736 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -
1711008891.640  27991 10.202.20.217 TCP_TUNNEL/200 3819 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -
1711008891.640  27984 10.202.20.217 TCP_TUNNEL/200 2831 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -
1711008894.943    133 10.202.9.191 TCP_TUNNEL/200 6198 CONNECT beam.scs.splunk.com:443 - HIER_DIRECT/13.225.142.55 -
1711008898.765      0 10.202.20.217 NONE/503 0 CONNECT 10.202.20.39:8089 - HIER_NONE/- -
1711008898.779      0 10.202.20.217 NONE/503 0 CONNECT 10.202.20.39:8089 - HIER_NONE/- -
1711008898.784      0 10.202.20.217 NONE/503 0 CONNECT 10.202.20.39:8089 - HIER_NONE/- -{noformat}


*_We observe the same behavior as 9.1.1 and in 9.2.0.1 as well._* Under server.conf for SH and Indexer we point to the same proxy server as for 9.0.4 test above:

{noformat}[proxyConfig]
http_proxy = http://10.202.20.39:8000{noformat}

*Note:* 10.202.20.217 (IP of SH 9.1.1), 10.202.21.187 (IP of IDX 9.1.1), 10.202.20.39 (proxy server)*Test Instance Proxy server:*

{noformat}For CLI:
URL: ssh://splunker@10.202.20.39:22
Username: splunker
Password: splk{noformat}

*Test Instance SH 9.1.1:*

{noformat}http://10.202.20.217:8000/
User : admin
Password: 5up3rn0va

For CLI:
User: splunker
Password: splk{noformat}

*Test Instance IDX 9.1.1:*

{noformat}http://10.202.21.187:8000/
User : admin
Password: 5up3rn0va

For CLI:
User: splunker
Password: splk{noformat}


We have attached SH and IDX diags from our test to the jira. The environment is also available to access.

PLEASE NOTE, the customer saw the same behavior but their proxy server returned 502 errors and not 503 but I suspect this is related to how their network is setup differently to ours internal tests.",,Andrew Brown,Harendra Rawat,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,6053a6d694d7b90069f83e4f,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,DOCGUILD-30595,,,,,,,,,,,,,,,,,,,SPL-247255,,,,,,,,,,,,,,,,,,,,04/Apr/24 6:13 PM;kgorzynski;9.0.4 --sh 1E851B7D-0212-43D0-9D22-68409B0B26FA.tar.gz;https://splunk.atlassian.net/rest/api/3/attachment/content/7023789,04/Apr/24 6:13 PM;kgorzynski;9.0.4--IDX --86F859DE-B379-480D-B2DC-18E6423BE9DB.tar.gz;https://splunk.atlassian.net/rest/api/3/attachment/content/7023790,04/Apr/24 6:13 PM;kgorzynski;9.1.1 IDX 3EA56288-FCF5-48AE-9DA4-A060C7A83F1C (2).tar.gz;https://splunk.atlassian.net/rest/api/3/attachment/content/7023791,04/Apr/24 6:13 PM;kgorzynski;9.1.1 SH C8C3C3F4-6C41-41A9-AEED-1E8B7A6BFC13.tar.gz;https://splunk.atlassian.net/rest/api/3/attachment/content/7023792,04/Apr/24 6:13 PM;kgorzynski;9.1.1 debug search.log;https://splunk.atlassian.net/rest/api/3/attachment/content/7023793,04/Apr/24 6:13 PM;kgorzynski;Changin version to 9.0.4 from 9.1.1.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023794,04/Apr/24 6:13 PM;kgorzynski;CleanShot 2024-04-01 at 15.53.02-20240401-225328.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023795,04/Apr/24 6:13 PM;kgorzynski;CleanShot 2024-04-01 at 15.53.54-20240401-225416.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023796,04/Apr/24 6:13 PM;kgorzynski;CleanShot 2024-04-01 at 16.02.39-20240401-230303.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023797,04/Apr/24 6:13 PM;kgorzynski;CleanShot 2024-04-01 at 16.03.36-20240401-230351.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023798,04/Apr/24 6:13 PM;kgorzynski;CleanShot 2024-04-01 at 16.11.41-20240401-231205.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023799,04/Apr/24 6:13 PM;kgorzynski;CleanShot 2024-04-01 at 16.15.01-20240401-231531.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023800,04/Apr/24 6:13 PM;kgorzynski;Healthy after changing version to 9.0.4.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023801,04/Apr/24 6:13 PM;kgorzynski;Screenshot 2023-11-17 205020.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023802,04/Apr/24 6:13 PM;kgorzynski;debug search.log for 9.0.4 setup.txt;https://splunk.atlassian.net/rest/api/3/attachment/content/7023803,04/Apr/24 6:13 PM;kgorzynski;error after search in 9.1.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023804,04/Apr/24 6:13 PM;kgorzynski;image (10).png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023805,04/Apr/24 6:13 PM;kgorzynski;image (18).png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023806,04/Apr/24 6:13 PM;kgorzynski;image (19).png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023807,04/Apr/24 6:13 PM;kgorzynski;image (35).png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023808,04/Apr/24 6:13 PM;kgorzynski;search returning reults in 9.0.4.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7023809,04/Apr/24 6:13 PM;kgorzynski;squid.conf;https://splunk.atlassian.net/rest/api/3/attachment/content/7023810,04/Apr/24 6:13 PM;kgorzynski;uploadsvc-66case3334358-11-16-2023-USER-0033300001nm2mUAAQ-911-search.log;https://splunk.atlassian.net/rest/api/3/attachment/content/7023811,04/Apr/24 6:13 PM;kgorzynski;uploadsvc-91case3334358-11-16-2023-USER-0033300001nm2mUAAQ-904-search.log;https://splunk.atlassian.net/rest/api/3/attachment/content/7023812,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@438a6c0a,,,,,N/A,,,,Diag file,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"o Have they made any changes to their environment just before the issue started?
Changing the version of splunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,5788800,,,,,,,,,,,,,,,,,,,,,,,,,Upgraded Version,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 

What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 
one search head and indexer connected through proxy

o If available, provide Splunk topology diagram, host name with IP mapping.",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,"o What is the expectation when this problem is not there?
The search head should connect to the Indexer through proxy without any issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• How long has the customer been tracking this issue and how frequent this issue is being hit?
Everytime",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives

List of individuals to include in a review of this incident

h1. Dependent Service Incident ID

(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers

h1. What Happened?

h1. What Did We Do Well?

h1. What Do We Need to Improve?

h1. Dogfood Our Own Observability Products

||*Product*||*Use*||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.

h3. Please share feedback about our products during this incident.

h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items

JIRAs should be linked to this ticket",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Fri Jul 18 17:56:51 UTC 2025,true,adallas(adallas),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Pipeline,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,24.0,12.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),hrawat(hrawat),kgorzynski(JIRAUSER52213),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|iddctb:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Harendra Rawat,6053a6d694d7b90069f83e4f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Airbus Defence and Space,3334358,012400000005WzCAAU,P2,Yes,5005a00002fi2TgAAI,,Open,Waiting on Customer,Standard,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3334358,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

N/A

• If available, will customer upgrade to fixed version?

Yes

• If support is able to reproduce, share the setup.

N/A",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,Standard,,,,,,,,,,,,,,,,2024-08-29 21:38:00.398,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1_*:*_1_*:*_136038_*|*_3_*:*_1_*:*_25757805877_*|*_6_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_604703,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Harendra Rawat,,,,,"An enhancement was introduced to the search process in Splunk Enterprise version 9.1.1 that optimizes searches by using the peer's IP address instead of querying DNS for the target peers. In certain cases, such as if an http_proxy is specified in the server.conf file, the enhancement causes the search head to use the proxy server IP address instead of the search peer's IP address. As a result, the IP address of the proxy, instead of the originating peer, is logged and utilized in the peers.csv file during search operations, causing the following error message to display in Splunk Web: ""Received error from proxy server"".
<br>
To disable the new DNS query optimization and eliminate the error, add the following setting to the distsearch.conf file:

<br>
<div class=""samplecode""><pre>
[distributedSearch]
useIPAddrAsHost=false
</pre></div>",,,,,,,,,,2024-08-29 21:38:00.398,2024-08-29 21:38:00.398,,,,,,Harendra Rawat,6053a6d694d7b90069f83e4f,,,,,,"29/Aug/24 2:38 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:61436d20e7c3280070a1979f] can you take a look at the actual Jira and decide if we can mark this as fixed, and if so in which version(s)? The reporter of the original issue says it’s fixed and asked Harendra to close it, but I’m not sure how or where it was fixed. Thank you.","30/Aug/24 9:23 AM;61436d20e7c3280070a1979f;According to [~accountid:6053a6d694d7b90069f83e4f] , this bug still hasn’t been fixed. ","30/Aug/24 9:33 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;Thanks, then we’ll continue to carry it in known issues. I’ll add the latest round of release numbers","09/Jan/25 11:50 AM;61436d20e7c3280070a1979f;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] it looks like this issue still hasn’t been fixed. Please continue to carry forward this Known issue for on prem releases: 

* SPL-253690, SPL-247255   Issue with Splunk Enterprise version 9.1.x and 9.2.0 when connected to proxy server

I added Flubber to the Affects versions. 



[~accountid:6053a6d694d7b90069f83e4f] please let us know if you have any new info about this issue. Thx!",24/Jan/25 5:08 PM;6053a6d694d7b90069f83e4f;Fixing it. MR ready [https://cd.splunkdev.com/splcore/main/-/merge_requests/79196|https://cd.splunkdev.com/splcore/main/-/merge_requests/79196],28/Jan/25 9:28 AM;61436d20e7c3280070a1979f;Added this ticket to Fixed issues release notes for Quality Street as hidden text. See  [https://splunk.atlassian.net/browse/SPL-247255|https://splunk.atlassian.net/browse/SPL-247255|smart-link] . ,"11/Jul/25 1:41 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;Customer comment / complaint about the issue description. DOCGUILD-30595

{quote}For the entry Issue with Splunk Enterprise versions 9.1.x through 9.4.x when http proxy is used

We hit the issue in case 3648606 BUT we don't have http proxy set in server.conf

This known issues section could be rewritten. Thanks{quote}","14/Jul/25 1:26 PM;61436d20e7c3280070a1979f;I slacked [~accountid:6053a6d694d7b90069f83e4f] to figure out how this release note can be modified to also apply in the customer’s case (without HTTP proxy in server.conf).

[~accountid:6053a6d694d7b90069f83e4f]  Do you have any advice on how to rewrite this known issue given customer feedback in [DOCGUILD-30595|https://splunk.atlassian.net/browse/DOCGUILD-30595]? The customer didn't have an HTTP proxy set up in server.conf and still encountered the issue. I can't see the case 3648606 that the customer references to understand what caused their issue. 

My suggestion: Change {{If an http_proxy is specified in the server.conf file, the enhancement causes the originating peer's IP address to fail to resolve.}} to something like: {{In certain cases, such as if an http_proxy is specified in the server.conf file, the enhancement causes the originating peer's IP address to fail to resolve.}}

What do you think?

Here is the current release note:

||{color:#ffffff}Date filed{color}||{color:#ffffff}Issue number{color}||{color:#ffffff}Description{color}||
|2024-04-04|SPL-253690, SPL-247255|Issue with Splunk Enterprise versions 9.1.x through 9.4.x when http proxy is used
Workaround:
An enhancement was introduced to the search process in Splunk Enterprise version 9.1.1 that optimizes searches by using the peer's IP address instead of querying DNS for the target peers. If an http_proxy is specified in the server.conf file, the enhancement causes the originating peer's IP address to fail to resolve. As a result, the IP address of the proxy, instead of the originating peer, is logged and utilized in the peers.csv file during search operations, causing the following error message to display in Splunk Web: ""Received error from proxy server"".
\\
To disable the new DNS query optimization and eliminate the error, add the following setting to the distsearch.conf file:
\\
{noformat}[distributedSearch]
useIPAddrAsHost=false{noformat}|

Thank you! ","14/Jul/25 2:13 PM;6053a6d694d7b90069f83e4f;{{If an http_proxy is specified in the server.conf file, the enhancement causes Search Head to use proxy server ip address instead of search peer's IP address.}}
As a result, the IP address of the proxy, instead of the search peer, is logged and utilized in the peers.csv file during search operations, causing the following error message to display in Splunk Web: ""Received error from proxy server"".","16/Jul/25 5:10 PM;61436d20e7c3280070a1979f;[~accountid:6053a6d694d7b90069f83e4f] I just want to make sure I understand what you're suggesting for the text changes for SPL-253690. Is this what you want the Known Issue release note to say? Note that the customer in DOCGUILD-30595 said they ran into this issue even though they didn't have an http proxy set in server.conf. 

{quote}An enhancement was introduced to the search process in Splunk Enterprise version 9.1.1 that optimizes searches by using the peer's IP address instead of querying DNS for the target peers. In certain cases, such as if an http_proxy is specified in the server.conf file, the enhancement causes the search head to use the proxy server IP address instead of the search peer's IP address. As a result, the IP address of the proxy, instead of the originating peer, is logged and utilized in the peers.csv file during search operations, causing the following error message to display in Splunk Web: ""Received error from proxy server"".{quote}","18/Jul/25 9:15 AM;6053a6d694d7b90069f83e4f;[~accountid:61436d20e7c3280070a1979f] 
I am not aware of new issue or don’t have any context.


{quote}Note that the customer in DOCGUILD-30595 said they ran into this issue even though they didn't have an http proxy set in server.conf.{quote}",18/Jul/25 10:56 AM;61436d20e7c3280070a1979f;[~accountid:6053a6d694d7b90069f83e4f] Do you have any visibility into “Case 3648606” mentioned in [https://splunk.atlassian.net/browse/DOCGUILD-30595|https://splunk.atlassian.net/browse/DOCGUILD-30595|smart-link] ? I think including “In certain cases…” will give us enough flexibility to cover other instances where this error message might come up. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,27/Jan/25 8:22 PM
You might receive a Bulletin message in Splunk Web from indexers and indexer cluster members that indicates a security risk warning for the allowed e-mail domains list for alert actions.,SPL-253367,3388377,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Delivered,Malcolm Moore,557058:0cc20ab8-5f1d-4593-afd8-ec7e8404a9ce,Ajeet Kumar,5bc64d495b8ba5286cb9abf4,Ajeet Kumar,5bc64d495b8ba5286cb9abf4,01/Apr/24 9:38 AM,30/Jul/25 4:26 PM,,11/Feb/25 11:49 PM,9.1.10,9.1.6,9.1.7,9.1.8,9.1.9,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.3.x,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0.x,Pocky,,,,,,,Admin - CLI,Search Infra - Alert Management,,,,0,,,,,,,,"[https://splunk.atlassian.net/browse/VULN-14772|https://splunk.atlassian.net/browse/VULN-14772|smart-link] , we need to alert customer when email domain list is not used, and print it to console on splunk start. 

This should be done only once on startup. Maybe within loader.cpp",,Adarsh KR (C),Ajeet Kumar,Andrew Brown,Gaurav Gupta,Malcolm Moore,srv -jira-gitlabci,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,712020:fa2809c5-7f19-421d-8f3f-fedb3f09adc3,5bc64d495b8ba5286cb9abf4,557058:37b4518c-e757-440f-87b7-181f5f425e80,6053ac5ae394c30069cb0d7f,557058:0cc20ab8-5f1d-4593-afd8-ec7e8404a9ce,62ec512f825fbfbfcff13ef5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,144000,0,,0%,144000,0,,,,,,,,,,,VULN-14772,,,,,,,,,,,SPL-251833,,,,,,,,,,DOCGUILD-27528,,,,,,,,,,01/Apr/24 9:38 AM;ajkumar;image-20240328-203058.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7008472,01/Apr/24 9:38 AM;ajkumar;image-20240328-203114.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7008473,01/Apr/24 9:38 AM;ajkumar;image-20240331-214347.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7008474,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4a4a82ab,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,19353600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives

List of individuals to include in a review of this incident

h1. Dependent Service Incident ID

(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers

h1. What Happened?

h1. What Did We Do Well?

h1. What Do We Need to Improve?

h1. Dogfood Our Own Observability Products

||*Product*||*Use*||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.

h3. Please share feedback about our products during this incident.

h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items

JIRAs should be linked to this ticket",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wed Feb 12 01:19:55 UTC 2025,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Infrastructure - Scheduler,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,3.0,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,d6850ade-7208-4068-abf3-ba401b4d73f6(d6850ade-7208-4068-abf3-ba401b4d73f6),ajkumar(ajkumar),andrewb(andrewb),gaurav(gaurav),malmoore(malmoore),06ac125d-c228-4c3f-b77f-a6149c059c6f(06ac125d-c228-4c3f-b77f-a6149c059c6f),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Documentation improvement - Customer Facing,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|idc8kv:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,Malcolm Moore,557058:0cc20ab8-5f1d-4593-afd8-ec7e8404a9ce,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MINISTRY OF CULTURE,3620830,012400000005WzMAAU,P3,No,5005a0000315hX8AAI,,Closed,Resolved,Standard,Tier4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,4.0,,,,,,,,,,,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2024-11-12 17:20:52.027,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_9257016055,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The following error appears:

<div class=samplecode><pre>
Security risk warning: Found an empty value for 'allowedDomainList' in the alert_actions.conf configuration file. If you do not configure this setting, then users can send email alerts with search results to any domain. You can add values for 'allowedDomainList' either in the alert_actions.conf file or in Server Settings > Email Settings > Email Domains in Splunk Web.
</pre></div>

You might receive this message multiple times, once for every indexer or indexer cluster member that is in your environment. It can happen any time that an indexer or indexer cluster member restarts.

If you receive this Bulletin message from an indexer or indexer cluster member, you can safely dismiss it without further action.
If you receive the message from or while logged into a search head or search head cluster member, then the potential security risk that it indicates is valid. To address the problem, configure the list of internet domains to which the search head can send emails as part of alert actions, as described in Configure email notification for your Splunk instance in the Alerting Manual.",,,,,,,,,,2024-11-12 17:20:52.027,2024-11-12 17:20:52.027,,,,,,Malcolm Moore,557058:0cc20ab8-5f1d-4593-afd8-ec7e8404a9ce,,,,,,"01/Apr/24 9:45 AM;5bc64d495b8ba5286cb9abf4;[~accountid:557058:0cc20ab8-5f1d-4593-afd8-ec7e8404a9ce] 

Based on our zoom meeting, I have created this ticket for documentation updates. I assume, this will also be good for release notes. Please feel free to update any other fields as needed.

Thanks

CC: [~accountid:6053ac5ae394c30069cb0d7f] [~accountid:6053b06b2f452d006f82f5a7] [~accountid:6053b70081b82500685dc4cb] ",12/Nov/24 9:20 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:557058:0cc20ab8-5f1d-4593-afd8-ec7e8404a9ce] this in-progress ticket might be the resolution also to [https://splunk.atlassian.net/browse/DOCGUILD-27528|https://splunk.atlassian.net/browse/DOCGUILD-27528|smart-link],"20/Nov/24 9:09 AM;62ec512f825fbfbfcff13ef5;This comment is auto-generated to inform you that the Jira severity level has been updated to align with the SFDC priority.
 If there is more than one support case linked, it will be set to align with the highest priority of open cases. If all cases are closed, then it will align to the highest priority of all closed cases. Please refer to the [P&T Customer Issues SLO|https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k] or contact [#help-customer-slo|https://slack.com/app_redirect?channel=C02KZJ51P4H]","26/Nov/24 5:56 AM;712020:fa2809c5-7f19-421d-8f3f-fedb3f09adc3;Hi [~accountid:557058:0cc20ab8-5f1d-4593-afd8-ec7e8404a9ce]  I have a customer receiving the same warning, can you let me know the fix version of this issue. ",26/Nov/24 4:12 PM;557058:0cc20ab8-5f1d-4593-afd8-ec7e8404a9ce;[~accountid:5bc64d495b8ba5286cb9abf4] can you provide the versions in which this issue was fixed.,"27/Nov/24 11:32 AM;5bc64d495b8ba5286cb9abf4;[~accountid:557058:0cc20ab8-5f1d-4593-afd8-ec7e8404a9ce] 



For cloud customers - Customers should not be receiving warnings from Indexers/CM  (since M release). In Pocky we have some enhancements around same area captured here (see [https://splunk.atlassian.net/browse/SPL-263080|https://splunk.atlassian.net/browse/SPL-263080|smart-link]  

For on prem customers -Please see this enhancement [https://splunk.atlassian.net/browse/SPL-263080|https://splunk.atlassian.net/browse/SPL-263080|smart-link]  

[https://splunk.atlassian.net/browse/SPL-263080|https://splunk.atlassian.net/browse/SPL-263080|smart-link]  has the fix versions.",10/Feb/25 11:43 PM;6053ac5ae394c30069cb0d7f;[~accountid:5bc64d495b8ba5286cb9abf4]  and [~accountid:557058:0cc20ab8-5f1d-4593-afd8-ec7e8404a9ce] I guess we can close this ticket now?,"11/Feb/25 5:19 PM;5bc64d495b8ba5286cb9abf4;[~accountid:6053ac5ae394c30069cb0d7f] yes, we can close it. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,11/Feb/25 11:50 PM
Compression method for Filesystem destination is immutable once created since changing it leads to a corrupt file.,SPL-250849,3323495,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,,Sarah Harun,6053bbd186b0dd007188fb63,Marcel Widjaja,6053a57445a3bb006816a45e,Marcel Widjaja,6053a57445a3bb006816a45e,09/Feb/24 1:38 PM,30/Jul/25 4:24 PM,,,10.0.x,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Unreleased,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ingest Actions,,,,,0,pre_merge_ia_fs,,,,,,,"Setup a FS destination with {{compression=none}}, then setup a HEC input to send events to be routed to FS destinations.  After sending the a few events, in splunk web, edit FS destination and change the compression to gzip.  Let a few more events to be ingested.

After following the sequence above, the resulting file contains some events that are uncompressed (raw) and the later part of the file will contains compressed data. So, effectively we have a mix of uncompressed and compressed event in the same file

{{outputs.conf}}

{noformat}[rfs:testfs]
compression = gzip   <=== initially set it to none, then change it to gzip
description = Routing to /tmp/testfs
dropEventsOnUploadError = false
format = ndjson
format.json.index_time_fields = true
format.ndjson.index_time_fields = true
fs.appendToFileUntilSizeMB = 1
fs.timeBeforeClosingFileSecs = 10
partitionBy = day
path = file:///tmp/testfs{noformat}",,Andrew Brown,Lin Zhao,Marcel Widjaja,User known,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,712020:fed73088-02f5-40a8-be1e-14093e5a8055,6053a57445a3bb006816a45e,unknown,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@1c986d6d,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TBD,,,,,,,,,,TBD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,24192000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tue Dec 17 17:42:10 UTC 2024,true,adallas(adallas),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Edge,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),csarkar(JIRAUSER48010),fjiang(JIRAUSER47701),711f1569-76f7-4583-8695-cf7f05c456bc(711f1569-76f7-4583-8695-cf7f05c456bc),mwidjaja(mwidjaja),sharun(sharun),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cust Managed Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|id2u8n:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,No Template,,,,,,,,,,,,,,,,,,,,,,,,,,,,Marcel Widjaja,6053a57445a3bb006816a45e,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2024-03-04 16:33:42.26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NONE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2024-03-04 16:33:42.26,2024-03-04 16:33:42.26,,,,,,Marcel Widjaja,6053a57445a3bb006816a45e,,,,,,04/Mar/24 8:33 AM;712020:fed73088-02f5-40a8-be1e-14093e5a8055;We will block changing compression type for Duranium release. The underlying issue will be fixed after release.,"23/Apr/24 10:17 AM;6053a57445a3bb006816a45e;Received agreement with [~accountid:6053bea406cbba006a0eca0c] to defer this ticket until we understand more about customer use case for changing compression method after destination was created.   In the meantime, if the need to do this ever arise, one can simply remove and recreate the destination to achieve the same thing.","26/Jun/24 9:39 AM;6053bea406cbba006a0eca0c;* Edited the title to be more customer-readable.
* Added “Document as: include in release notes” so that it bubbles up as a known issue

cc:[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] [~accountid:6053bbd186b0dd007188fb63] ","26/Jun/24 4:57 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;When affected versions are added to the Affects Versions field, this bug will start to appear in the known issues list for the corresponding versions. The automation has been a bit inconsistent recently, so whoever adds Affects Versions, feel free to ping me to force an update to the docs",26/Jun/24 5:13 PM;6053bea406cbba006a0eca0c;Hey [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] - the affected version is 9.3.0 but that field value is not available yet,"26/Jun/24 5:28 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;Got it. We’re using {{Duranium}} until GA and then it’ll be changed to {{9.3.0(Duranium)}}. I’ve tagged it and will ensure it shows up in the right place. Thanks, Felix","17/Dec/24 9:42 AM;6053bdfdf180c300675f8a2b;[~accountid:712020:31338875-d17b-4573-bdb6-73bbb1f7cdc2] , [~accountid:6053bea406cbba006a0eca0c] , Please confirm if we still need to fix this gap. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3609381,SPL-262728,[Post-Release] Ingest Action File System Destination,To Do,23/Apr/24 10:14 AM
FS-StandardMode : Standalone sub-search with HEAD doesn't return any results,SPL-249666,3290971,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,,,Kannamraju Patchamatla,61324ce86a4c09006ac5ead2,Kannamraju Patchamatla,61324ce86a4c09006ac5ead2,18/Jan/24 10:19 AM,30/Jul/25 4:25 PM,,,10.0.x,9.0.0(Aurum),9.0.2305.100(JuicyFruitTick),9.1.0(Beryllium),9.1.2308.100(KitKat_Tick),9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Ice Breaker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Federation,,,,,0,found_by_federated_search_sli,,,,,,,"*CMD  :  [search index=federated:internal | head 5]*

  Above search command doesn't return any results even though the index has valid data .

!image-20240118-182310.png|width=1321,height=483!",,Zhipeng Yang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5e9626c70fe27d0c0edf862a,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,,,0,0,,,,,,,,,,,,,,,,,,,,,,SPL-244551,,,,,,,,,,,,,,,,,,,,18/Jan/24 10:23 AM;kpatchamatla;image-20240118-182310.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6756193,16/Apr/25 11:15 AM;zhipengy;image-20250416-181431.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8093560,16/Apr/25 11:15 AM;zhipengy;image-20250416-181502.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8093561,16/Apr/25 11:15 AM;zhipengy;image-20250416-181536.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8093559,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@b0fffb8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Gaurav Jaiswal,Vinay Manivel,5d833d323065f00d32c4ea15,613257abdae0a20071b1453c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,13824000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wed Apr 16 18:15:41 UTC 2025,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Federation,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,4.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,kpatchamatla(JIRAUSER50549),zhipengy(zhipengy),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|icy34f:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Boris Chen,Christina Noren,Matt Green,Rachel Perkins,61324249ac61dc00697cf056,613be989f6070d006b917dca,5af8dc294b719848347ec02c,5af5d6c28a1abd1427953558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Macquarie Group Services Australia Pty Ltd,3278407,012400000005WzMAAU,P2,No,5005a00002T3VOQAA3,,Closed,Resolved - Work Around,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,16.0,,,,,,,,,,,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 [https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs|https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs]”?

*  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 [https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs|https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs]”?

*  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data [https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection|https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection]

*  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

*  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 

* What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.",,,,,,,,,,,,,,,,,,,2025-04-16 18:15:41.704,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2025-04-16 18:15:41.704,2025-04-16 18:15:41.704,,,,,,,,,,,,,"16/Apr/25 11:15 AM;5e9626c70fe27d0c0edf862a;So append works:

!image-20250416-181431.png|width=1691,height=1120,alt=""image-20250416-181431.png""!

But I got warning for standalone subsearch:

!image-20250416-181502.png|width=1700,height=456,alt=""image-20250416-181502.png""!

Version of Splunk:

!image-20250416-181536.png|width=549,height=545,alt=""image-20250416-181536.png""!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2786239,PSRT-4525,(Data Federation) Enterprise Security Benchmarking,To Do,18/Jan/24 10:19 AM
"[PUBLIC] Federated Search, Enterprise --> Cloud configuration: Performance degradation increases when the number of indexers increases in the RSH ",SPL-244248,3120741,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,,,Cuong Dong,6053a57786b0dd007187fe97,Cuong Dong,6053a57786b0dd007187fe97,05/Sep/23 4:49 PM,30/Jul/25 4:25 PM,,,10.0.x,8.2.0 GA (Scootaloo),9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.2209.3,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.0.x,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Federation,,,,,0,Account_Escalated,cfit_reviewed,cislo-p2-medium,Escalation,,,,"Summary:

For Distributed Searches, each connection to an indexer has a DistributedSearchResultCollectorThread to receive data, handle it, and add to the job queue. On the other hand, Federated Searches have only one connection to RSH and one thread to add to the job queue. This could be a bottle neck for Federated Search:

# If DistributedSearchResultCollectorThread is slow to add to the job queue, we have many idle worker threads (default is 5)
# All data are sent through a connection. If 1 connection is slow, multiple connections could benefit from parallelism like Distributed Search. (This depends on the network; in our testing environment, the connection speed is excellent and this is never encountered in testing.)

Potential solutions:

* In this workaround [https://splunk.atlassian.net/browse/SPL-239298?focusedCommentId=13187070|https://splunk.atlassian.net/browse/SPL-239298?focusedCommentId=13187070|smart-link] , we reduce the worker threads to reduce thread contention, but this impacts all searches. For a short-term fix, we could add a new setting {{max_workers_searchparser_federated}} so that Federated Search can use a different setting without impacting other searches.
* For an enhancement to address the problem 1 above, we could look into adding more threads to process the data received from RSH and add to the job queue.
* To address the problem 2 above, it’s a bigger change to re-architect FSH to use multiple connections to query RSH. ","FSH is an onprem Splunk Enterprise version 9.0.0. (No onprem indexers)

RSH is Splunk Cloud on release 9.0.2209.4.",Cuong Dong,Matthew Ness,User known,User known,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a57786b0dd007187fe97,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,unknown,unknown,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-246469,,,,,,,,,,,SPL-239298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3bea3955,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Splunk Cloud,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,62726400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 
FSH - Splunk Enterprise 9.0.0
RSH - Splunk Cloud 9.0.2209.4 (AWS)

o If available, provide Splunk topology diagram, host name with IP mapping.
FSH hostname - splunk-hybrid-search.arlocloud.com",,,,,,,,,,"o What errors are being reported?
Federated search is taking much longer to complete or is failing with: Socket error during transaction. ReadWrite Error",,,,,,,,,,,,,No,,,,,,,,,,,,,"o What is the expectation when this problem is not there?
The Federated Search completes faster. The expectation is not to complete as fast as in Cloud, but also not with 3 to 4 hours difference.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• How long has the customer been tracking this issue and how frequent this issue is being hit?
30 days.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thu Sep 28 21:03:07 UTC 2023,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Federation,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,9.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,cdong(cdong),lbosquezgonzalez(JIRAUSER51601),mness(mness),nfrenkel(JIRAUSER43952),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|ic9xkf:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cuong Dong,6053a57786b0dd007187fe97,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Arlo Technologies, Inc.",3173137,012400000005WzCAAU,P2,No,5005a00002HTHEdAAP,,Open,Waiting on Customer,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3173137,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment.

issue is reproducible in the customer onprem SH (FSH) (Discussed in a zoom with the customer and Nir Frenkel)

• If available, will customer upgrade to fixed version?
yes
• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?
Used https://splunk.atlassian.net/wiki/spaces/HSSR/pages/188770819883/Creating+Splunk+Core+Product+Jiras

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?
Use https://splunk.atlassian.net/wiki/spaces/HSSR/pages/188770819883/Creating+Splunk+Core+Product+Jiras

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection
diags and dispatch directory will be attached to the case.

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 
Verified federated.conf on the FSH. Customer is using Transparent mode. Connection is successful.
Account configured in the federated.conf is under Admin role in the RSH and contains the fsh capability.
The same search, with a shorter timeframe, works as expected.
There are no local indexers in the customer onprem side.

 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
Understand why the federated search takes much longer to complete compared to the time it takes in the RSH - same search and same time frame. ",,,Standard,,,,,,,,,,,,,,,,2023-09-07 18:15:48.821,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"One possible workaround is to use a more efficient query. For example, use ""| tstats count where index=main by splunk_server"" instead of ""index=main | stats count by splunk_server"".


Another workaround is to change the max_workers_searchparser setting to a value lower than its default.

<note>Use this workaround if you are using your Splunk Enterprise federated search head (FSH) instance only for running federated searches. This workaround might affect non-federated searches.</note>

On the Splunk Enterprise FSH, follow these steps:

<ol><li>Create limits.conf in a local/ folder.</li>
<li>Set the <code>max_workers_searchparser</code> setting to a number lower than its default (1 or 2). For more information about this setting see the ''Admin Manual''. </li>
<li>Test which setting value provides a better performance. </li></ol>",,,,,,,,,,2023-09-07 18:15:48.821,2023-09-07 18:15:48.821,,,,,,Cuong Dong,6053a57786b0dd007187fe97,,,,,,"07/Sep/23 11:15 AM;6126b3d8ec0a83006a42d40a;[~accountid:6053a57786b0dd007187fe97] thanks for doing this analysis! A few follow-up questions: Is this an issue that can affect any customer? Or is there something specific about the [configuration of this customer|https://splunk.atlassian.net/browse/SPL-239298] that triggers this behavior?

This will help us prioritize this issue. ","07/Sep/23 12:17 PM;6053a57786b0dd007187fe97;This issue could affect any customer. However,  2 possible factors that affect Arlo:

* Arlo has 50+ servers. This increases the data volume enough to make a single DistributedSearchResultCollectorThread busy and make it obvious the difference between having 1 thread vs. multiple threads writing to the queue. For the typical 3-indexer config we often use for testing, the difference is not much (we tested different numbers of servers in this comment [https://splunk.atlassian.net/browse/SPL-239298?focusedCommentId=12969523|https://splunk.atlassian.net/browse/SPL-239298?focusedCommentId=12969523|smart-link] )
* The query is also a factor. If a query sends a lot of data like the query Arlo uses, it hits this bottleneck. We use {{| tstats count where index=prod by splunk_server}} as a workaround to get the same results without sending a lot of data . If most customers tend to avoid heavy-volume queries, they don’t see this issue often.",12/Sep/23 3:23 PM;6053a57786b0dd007187fe97;Should this be marked as a known issue if this is deferred? [~accountid:557058:89ace195-adc2-4f25-9924-ded7c8f10bb6] [~accountid:6053a293009fee00694bc4e6] [~accountid:6126b3d8ec0a83006a42d40a] [~accountid:62a6ed3b9cd13c0068aea7f7] ,12/Sep/23 3:58 PM;6053a293009fee00694bc4e6;I believe so [~accountid:6053a57786b0dd007187fe97] . And you can add the workaround that they might try when appropropriate ( if the sh is only used for federated searches ).,12/Sep/23 5:05 PM;6126b3d8ec0a83006a42d40a;[~accountid:6053a57786b0dd007187fe97]  I agree with the known issue. I think the description should be “performance degradation increases when the number of indexers increase in the RSH”. ,"22/Sep/23 10:11 AM;557058:89ace195-adc2-4f25-9924-ded7c8f10bb6;[~accountid:6053a57786b0dd007187fe97] If you want to mark this as a known issue, you[ need to convert this Jira into a bug|https://splunk.atlassian.net/wiki/spaces/PROD/pages/313635214478/How+to+add+or+remove+an+issue+from+the+automated+release+notes]. And you need to put all of the affected versions in the *Affects versions* field (or leave it alone, if this _only_ affects 9.0.2209.3, which I doubt to be the case). ",28/Sep/23 10:43 AM;557058:89ace195-adc2-4f25-9924-ded7c8f10bb6;[~accountid:6053a57786b0dd007187fe97] I’ve converted this ticket to a bug so it can be set up as a known issue. Can you please fill out the *Affected versions* field with the Enterprise and Cloud versions affected by this bug and put the workaround mentioned in the description into the *Workaround* field. Workarounds appear in the Known issues lists: [https://docs.splunk.com/Documentation/Splunk/9.1.1/ReleaseNotes/Knownissues#Federated_search_issues|https://docs.splunk.com/Documentation/Splunk/9.1.1/ReleaseNotes/Knownissues#Federated_search_issues|smart-link] ,28/Sep/23 12:27 PM;6053a57786b0dd007187fe97;Updated Affected Versions and Workaround fields.,"28/Sep/23 2:03 PM;557058:89ace195-adc2-4f25-9924-ded7c8f10bb6;I redid the title of the ticket to make it clear this applies only to Enterprise → Cloud FS configurations,  and reorganized the workaround so it’s a little easier to follow. Added markup so it’ll be formatted nicely in the Known Issues lists….

Workaround text in known issues should be written as if you’re talking to the user. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,05/Sep/23 4:49 PM
[PUBLIC] [DMA] The UI trigger for summary rebuild doesn't work for some accelerated data models that have no root-event dataset and have a reporting command in first root search dataset,SPL-242301,3070218,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,,Ankit Jain,6053a8b6686bf5007045d71b,Ankit Jain,6053a8b6686bf5007045d71b,Ankit Jain,6053a8b6686bf5007045d71b,21/Jul/23 11:00 AM,30/Jul/25 4:26 PM,,,10.0.x,9.0.2209.2(Hersheys-Patch2),9.0.2303.100(Icebreaker-Tick),9.0.2303.200(Icebreaker Tock),9.0.2305.101 (Juicyfruit-tick-Patch1),9.0.2305.200 (JuicyFruitTock),9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Lindt_Tock(9.1.2312.200),,,,,,,,Data Model - UI,Search - Datamodel Acceleration,,,,0,cobalt-defer-accept,juicyfruit-defer-accept,kitkat-defer-accept,,,,,"The Data Models that do not have any root dataset of {{event}} type and have more than one root datasets of {{search}} type with the first root search dataset having a reporting command in the {{baseSearch}} , would lead to this issue.

Steps to reproduce:

# Create a Data Model that has multiple root search datasets and the first root search dataset having a reporting command. You can create one by uploading the provided JSON ({{test_internal_audit_logs.json)}}
!Screenshot 2023-07-21 at 11.56.28 AM.png|width=1904,height=743!
# Accelerate the Data Model. Wait for it to reach 100% completion
# Do a ""rebuild"" trigger on that Data Model through web-ui
# The “rebuild” trigger fails with the error below
!Screenshot 2023-07-21 at 11.57.53 AM.png|width=699,height=508!



The reason why above happens is explained below:

For instance, above Data Model {{test_internal_audit_logs}} has two root search datasets:

# fully_completed_searches
# failed_searches

The Data Model ID(s) in order for the above two datasets using nomenclature {{DM_<app>_<datamodel>.<object_id>}}

will be in the same order:

# DM_search_test_internal_audit_logs
# DM_search_test_internal_audit_logs.failed_searches

Notice above that the first root search dataset doesn’t have the object-id ({{fully_completed_searches}})

Now, the Web-UI Rebuild trigger issues a {{DELETE}} on the very first root-event or root-search dataset. The first root dataset cannot be accelerated since it has a reporting command in baseSearch constraint:

{noformat}""baseSearch"": ""index=\""_audit\"" action=\""search\"" info=\""completed\"" fully_completed_search=\""true\"" | stats count BY action, event_count, scan_count, result_count, search_et, search_lt, exec_time, host, search_type""{noformat}

It can be seen by issuing a {{DELETE}} using curl like below:

{noformat}curl -ku ""admin"" -X DELETE https://127.0.0.1:8089/services/admin/summarization/tstats:DM_search_test_internal_audit_logs?count=0
Enter host password for user 'admin':
<?xml version=""1.0"" encoding=""UTF-8""?>
<response>
  <messages>
    <msg type=""ERROR"">Cannot find saved search for summary id=DM_search_test_internal_audit_logs.</msg>
  </messages>
</response>{noformat}



Now, the second dataset which is actually accelerated can’t be rebuild from Web UI and needs to be done through curl request below:

{noformat}curl -ku ""admin"" -X DELETE https://127.0.0.1:8089/services/admin/summarization/tstats:DM_search_test_internal_audit_logs.failed_searches?count=0
{noformat}",,Ankit Jain,Suketu Shah,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a8b6686bf5007045d71b,6053a66586b0dd007188092d,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SOLNESS-36603,,,,,,,,SOLNESS-47221,,,,,,,,,,,SPL-231558,,,,,,,,,,,,,,,,,,,,21/Jul/23 11:00 AM;ankitj;Screenshot 2023-03-09 at 10.02.06 AM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6139581,21/Jul/23 2:50 PM;ankitj;Screenshot 2023-07-13 at 4.56.25 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6140290,21/Jul/23 11:59 AM;ankitj;Screenshot 2023-07-21 at 11.56.28 AM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6139731,21/Jul/23 11:59 AM;ankitj;Screenshot 2023-07-21 at 11.57.53 AM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6139732,21/Jul/23 11:00 AM;ankitj;image-20230614-153021.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6139582,21/Jul/23 11:00 AM;ankitj;image-20230614-153037.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6139583,21/Jul/23 11:59 AM;ankitj;test_internal_audit_logs.json;https://splunk.atlassian.net/rest/api/3/attachment/content/6139733,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5a5a6634,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,System,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,67132800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tue Aug 08 21:20:20 UTC 2023,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,StructuredSearch,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,7.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ankitj(ankitj),suketus(suketus),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|ic25hj:,,,,,,,"The reason why above happens is explained below:

For instance, above Data Model test_internal_audit_logs has two root search datasets:
1. fully_completed_searches
2. failed_searches

The Data Model ID(s) in order for the above two datasets using nomenclature ""DM_<app>_<datamodel>.<object_id>""
will be in the same order:
1. DM_search_test_internal_audit_logs
2. DM_search_test_internal_audit_logs.failed_searches

Notice above that the first root search dataset doesn’t have the object-id (fully_completed_searches)

Now, the Web-UI Rebuild trigger issues a DELETE on the very first root-event or root-search dataset. The first root dataset cannot be accelerated since it has a reporting command in baseSearch constraint. Hence, the rebuild trigger can't find it.",,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Boris Chen,Christina Noren,Matt Green,Rachel Perkins,61324249ac61dc00697cf056,613be989f6070d006b917dca,5af8dc294b719848347ec02c,5af5d6c28a1abd1427953558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Godaddy.com, Inc.",3369119,012400000005WzMAAU,P3,No,5005a00002kZf9jAAC,,Closed,Resolved - Work Around,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1. Create a Data Model that has multiple root search datasets and the first root search dataset having a reporting command. You can create one by uploading the provided JSON (`test_internal_audit_logs.json`).

2. Accelerate the Data Model. Wait for it to reach 100% completion

3. Do a ""rebuild"" trigger on that Data Model through web-ui

4. The “rebuild” trigger fails with the error below",,,,,,,,,,5.0,,,,,,,,,,,,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2023-08-02 16:19:15.625,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The workaround is to change the Data Model definition to reorder the root search objects such that the root search object that can be accelerated is the very first one in the list.

For instance, for the provided `test_internal_audit_logs.json`, edit the JSON file on disk and move `failed_searches` dataset before `fully_completed_searches`.",,,,,,,,,,2023-08-02 16:19:15.625,2023-08-02 16:19:15.625,,,,,,,,,,,,,"21/Jul/23 2:50 PM;6053a8b6686bf5007045d71b;Created [https://splunk.atlassian.net/browse/SOLNESS-36603|https://splunk.atlassian.net/browse/SOLNESS-36603|smart-link] to fix the issue being reported from the build for {{Identity_management}} DMA

!Screenshot 2023-07-13 at 4.56.25 PM.png|width=538,height=392!",02/Aug/23 9:19 AM;6053a66586b0dd007188092d;[~accountid:6053a8b6686bf5007045d71b] Please file deferral request to move out of Juicyfruit Tock release since it was filed recently cc [~accountid:6053a47966c8790068386b56]  [~accountid:6053a6d394d7b90069f83e26] ,03/Aug/23 11:52 AM;6053a66586b0dd007188092d;[~accountid:6053a8b6686bf5007045d71b] to fill out the deferral request today.,"03/Aug/23 2:46 PM;6053a8b6686bf5007045d71b;|*Summary of Issue:*|The Data Models that do not have any root dataset of {{event}} type and have more than one root datasets of {{search}} type with the first root search dataset having a reporting command in the {{baseSearch}} , cannot be rebuild through the Data Models Web UI and give an error when attempting to rebuild.|
|*Workaround:*|The workaround is to change the Data Model definition to reorder the root search objects such that the root search object that can be accelerated is the very first one in the list.
For instance, Look at [https://splunk.atlassian.net/browse/SOLNESS-36603|https://splunk.atlassian.net/browse/SOLNESS-36603|smart-link]|
|*Frequency of Issue:*|The issue shows up in a very particular case (Data model definition has all non-event root datasets and the first root search dataset has a reporting command). Also, it has a valid workaround. It is also documented as known issue in Release Notes.|
|*Customer Impact:*|_Customers are not able to trigger “Rebuild” of the DMA through the Web UI._|
|*Regression:*|Yes. It started happening since Hersheys as a result of corresponding change that affected how the datasets are referenced in the REST handlers for summarization endpoints|
|*Reason for deferral:*|The bug requires a major overhaul of the Data Models Web UI and we were talking with UI team to get it into their backlog. But it seems it may not get prioritized and fixed any soon.
The issue shows up in a very particular case (Data model definition has all non-event root datasets and the first root search dataset has a reporting command). So far only one Data Model from ES app had such symptom and there’s already a [JIRA|https://splunk.atlassian.net/browse/SOLNESS-36603] to fix it within the datamodel definition in the app.
Also, it has a valid workaround. It is also documented as known issue in Release Notes.|
|*Target Fix timeframe/release:*|_L release_|","08/Aug/23 2:20 PM;6053a66586b0dd007188092d;Approved for Deferral as per 08/03/2023 bug deferral meeting. Hence, adding labels = kitkat-defer-accept and cobalt-defer-accept",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2573524,SPL-229533,[K Release] DMA supportability improvements,To Do,21/Jul/23 11:00 AM
The DELIMS setting or the kvdelim option may not be applied correctly when the k/v delim character appears 2 or more times in a field value,SPL-240774,3026234,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Won't Fix,Phil Wang,5d816271822c1b0c3a765315,Deepthi Joshi (C),63c6f7aed7f68827e6b5b21a,Deepthi Joshi (C),63c6f7aed7f68827e6b5b21a,09/Jun/23 1:50 AM,30/Jul/25 4:25 PM,,10/Nov/23 12:16 PM,10.0.2503.x,10.0.x,9.0.2209.4(Hersheys-Patch4),9.0.2303.203 (Icebreaker Tock-Patch3),9.0.2305.200 (JuicyFruitTock),9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.2308.100(KitKat_Tick),9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Lindt_Tick_Patch_1(9.1.2312.101),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search - Parser,,,,,0,,,,,,,,"*Subject:* DELIMS setting is not working as expected.

*Customer environment:*

1CM, 2 Search Heads (1 Adhoc & 1 ES), 21 indexers, 1 IDM

*Description:* 
Initially, the case was raised for Zscalar Technical Add-on, a developer-supported Add-on. But the error was similar when the customer created an app as a search app.

The customer is parsing the data, the URL field is not extracted completely.

The URL Field breaks after the first parameter and creates another field for the parameter after the first parameter.

Example

Original URL: url=[download.jword.jp/juc/control3.xml?ver=2.4.6.0&pid=cid0247-03&t=808951812|https://download.jword.jp/juc/control3.xml?ver=2.4.6.0&pid=cid0247-03&t=808951812]

Extracted URL:  url=[download.jword.jp/juc/control3.xml?ver=2.4.6.0|https://download.jword.jp/juc/control3.xml?ver=2.4.6.0]

and two other fields will be created,
pid=cid0247-03
t=808951812

*Troubleshooting:*

We have reproduced the issue on Splunk Cloud Instance.

# Logged in to Cloud Instance 
# Added the transform.conf setting as:
[transform_delims_test]
DELIMS = “\t” , “=”
# Added the props.conf setting as:
[delimstest]
KV_mode = none
REPORT-delims_test = transform_delims_test
category = testing
description = delimstest
# Created a new sourcetype: delimstest
# Created a new index: test
# Uploaded the sample logs provided by customer.
# Run the SPL query: index=""test"" sourcetype=""delimstest"" source=""zscalerDelimsDump.txt""
# The URL field is extracted with an incomplete URL.

Created the instance as Splunk Enterprise and Splunk Cloud, the issue is found similar.

*Splunk Enterprise Credentials:* [http://10.202.15.44:8000/en-US/app/launcher/home|http://10.202.15.44:8000/en-US/app/launcher/home] 

Username: test_user

password: splunker

*Splunk Cloud Credentials:* [https://support-test-splunk.stg.splunkcloud.com/en-US/app/launcher/home|https://support-test-splunk.stg.splunkcloud.com/en-US/app/launcher/home] 

username: test_user

password: splunker

*Sample logs:*

[^zscalerDelimsDump.txt]

*Screenshots of the Error*:

!3208073(1).png|width=1439,height=666!

!3208073.png|width=1437,height=651!



*Customer Expectation:*

The customer wants the URL field to be extracted completely when DELIMS is set",,Deepthi Joshi (C),Jonah Reuter,Kunal Kasalkar (C),Phil Wang,Prithiviraj Sairam (C),Shiraz Ashraf,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,63c6f7aed7f68827e6b5b21a,6053a5692f452d006f82793b,61d7bc8590cfd200712a3c95,5d816271822c1b0c3a765315,639d0f9f2aac389d02c2d7e4,61c97b397c6f980070da02c2,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-246933,,,,,,,,,,,,,,,15/Jun/23 7:27 AM;6b1603cd-45c4-458b-89fa-556172064243;3208073 (4e14cc3e-9a7b-4d99-87eb-4f6b518d3e07).png;https://splunk.atlassian.net/rest/api/3/attachment/content/5965659,14/Jun/23 6:15 AM;6b1603cd-45c4-458b-89fa-556172064243;3208073(1).png;https://splunk.atlassian.net/rest/api/3/attachment/content/5959391,14/Jun/23 6:15 AM;6b1603cd-45c4-458b-89fa-556172064243;3208073.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5959392,30/Jun/23 4:30 AM;6b1603cd-45c4-458b-89fa-556172064243;image-20230630-112032.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6036145,09/Jun/23 1:57 AM;6b1603cd-45c4-458b-89fa-556172064243;zscalerDelimsDump.txt;https://splunk.atlassian.net/rest/api/3/attachment/content/5937522,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5fea1d97,,,,,N/A,,,,Diag file,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"o Have they made any changes to their environment just before the issue started?
No",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Splunk Cloud,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,34992000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Description

Problem 

Opportunity 

Reference material 

Requirements

Out of scope / What we’re NOT doing  

Top 3 Assumptions 

Top 3 Known constraints/risks 
 
What Design will handoff 
UX designs for:   
UI designs for: 

Timing
Design Concepts review: 
Target handoff: 

Partners & Stakeholders
PM:  
Lead Eng: 
Docs: 
Scrum team: 
Other Stakeholders:",,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"o What is the customer environment for the issue? 
Splunk Cloud Version 9.0.2209.3 
o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,"o What errors are being reported?
N/A",,,,,,,,,,,,,No,,,,,,,,,,,,,"o What is the expectation when this problem is not there?
The URL field should extract properly",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• How long has the customer been tracking this issue and how frequent this issue is being hit?
Recently they found the issue",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wed Aug 14 16:32:35 UTC 2024,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Lifecycle,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,5.0,23.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6b1603cd-45c4-458b-89fa-556172064243(6b1603cd-45c4-458b-89fa-556172064243),jreuter(jreuter),kgorzynski(JIRAUSER52213),8e52ece8-88e4-4d61-b84c-ee871dfb27c4(8e52ece8-88e4-4d61-b84c-ee871dfb27c4),pwang(pwang),65b51b08-e43e-42ad-a9be-b981065b10f6(65b51b08-e43e-42ad-a9be-b981065b10f6),079e337c-c08b-42f7-b05b-63256fb62f3a(079e337c-c08b-42f7-b05b-63256fb62f3a),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|ibvbrj:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Phil Wang,5d816271822c1b0c3a765315,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Viseca Payment Services SA,3519269,012400000005WzMAAU,P2,No,5005a0000313OnpAAE,,Closed,Unresolved,Standard,Tier3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SearchLifecycle-FY24Q2-S6,SearchLifecycle-FY24Q3-S1,SearchLIfecycle-FY24Q3-S2,SearchLifecycle-FY24Q3-S3,SearchLifecycle-FY24Q3-S4,SearchLifecycle-FY24Q3-S5,SearchLifecycle-FY24Q3-S6,SearchLifecycle-FY24Q3-S7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment
The issue is reproducible, we created a 
1. Splunk Cloud instance
2. Created a new sourcetype -delimstest
3. Created a new index - test
4. uploaded the sample logs.
5. transform.conf setting
    [transform_delims_test]
    DELIMS = “\t” , “=”
6. props.conf setting
    [delimstest]
    KV_mode = none
    REPORT-delims_test = transform_delims_test
    category = testing
    description = delimstest

Test environment: 
• If available, will customer upgrade to fixed version?
Yes,

• If support is able to reproduce, share the setup.
   Splunk Enterprise Credentials: http://10.202.15.44:8000/en-US/app/launcher/home 
   Username: test_user
   password: splunker

    Splunk Cloud Credentials: https://support-test-splunk.stg.splunkcloud.com/en-US/app/launcher/home 
    username: test_user
    password: splunker",,,,,,,,,,,,,,,,,,,,,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?
N/A

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?
N/A

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection
N/A

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.
The customer is not able to extract URL field completely 

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
DELIMS Configuration needs to work as expected.

",,,Standard,,,,,,,,,,,,,,,,2023-06-14 09:04:28.947,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_1_*:*_0_*|*_3_*:*_1_*:*_769903299_*|*_10001_*:*_2_*:*_120137594_*|*_10039_*:*_2_*:*_2674880604,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Run,,"Perform field extractions by modifying your searches using other commands, such as the <code>rex</code> command or <code>eval</code> command. ",,,,,,,,,,2023-06-14 09:04:28.947,2023-06-14 09:04:28.947,,,,,,Phil Wang,5d816271822c1b0c3a765315,,,,,,"14/Jun/23 2:04 AM;61c97b397c6f980070da02c2;Hello [~accountid:63c6f7aed7f68827e6b5b21a]  All sections are complete, including the engineering/support template.  .TXT file was provided, but no screenshots. Please provide screen-shot(s) of the issue it helps the dev folks.  Given everything here is complete, I will move the case to triage state. 

Thanks,

 Shiraz.  ","14/Jun/23 6:18 AM;63c6f7aed7f68827e6b5b21a;Hello [~accountid:61c97b397c6f980070da02c2] I have attached the Screenshot of the issue.


Thank you!
","14/Jun/23 10:20 AM;6053a5692f452d006f82793b;{noformat}* The first set of quoted delimiters extracts the field/value pairs.
* The second set of quoted delimiters separates the field name from its
  corresponding value.{noformat}

Why would {{DELIMS = ""\t"" , ""=""}} work for something like {{url=download.jword.jp/juc/control3.xml?ver=2.4.6.0&pid=cid0247-03&t=808951812}}?

What does “completely” mean in “The customer wants the URL field to be extracted completely when DELIMS is set” (show me the expected results)?","15/Jun/23 7:27 AM;63c6f7aed7f68827e6b5b21a;The DELIMS = “\t” , “=” works fine for other field values, where the first set extracts the field and the second separates. But it is not working for the URL field.

Expected results are mentioned below under the actual URL

!3208073 (4e14cc3e-9a7b-4d99-87eb-4f6b518d3e07).png|width=1437,height=651!","15/Jun/23 9:26 AM;6053a5692f452d006f82793b;[~accountid:63c6f7aed7f68827e6b5b21a] I don’t understand. Is the underlined portion of the screenshot supposed to be correct since they are using regex? It’s still wrong as it should terminate at the question mark {{?}} and all other arguments are delineated by an ampersand {{&}}. 

What fields _does_ this work for? A {{\t}} is a tab. A {{=}} is an equals sign. How are you determining that field/value pairs are segregated by a tab here? I certainly don’t see any tabs in the full uri.. I do not think it is logical that this set of delimiters is going to do anything but split the url at the first {{=}}.

This is why we don’t use {{DELIMS}} for complex extractions like this; the segmentation is not consistent within the string. Here’s how we do it internally, using {{REGEX}} like the customer has done in the “correct” examples:

{noformat}REGEX = (?<uri>(?<uri_path>[^\s\?""]++)(?:\\?(?<uri_query>[^\s""]+))?){noformat}

Or a more complex examples from our defaults:

{noformat}REGEX = (?<uri>(?:\w++://(?<uri_domain>[^/\s]++))?(?<uri_path>(?<uri_root>/+(?:[^\s\?;=/]*+/+)*)(?<uri_file>[^\s\?;=?/]*+))(?:\?(?<uri_query>[^\s""]+))?){noformat}

There is absolutely no way you can accomplish this with {{DELIMS}} unless you use {{&}} in place of the {{\t}} but that is still wrong and will cause other issues. ","19/Jun/23 7:37 AM;61d7bc8590cfd200712a3c95;Hey [~accountid:6053a5692f452d006f82793b], 
Good day! I hope you’re doing well. 

So, can we consider from the below that for DELIMS it is complex to extract large strings? If yes, then nothing as such is documented. 



{noformat}DELIMS = <quoted string list>
* NOTE: This setting is only valid for search-time field extractions.
* IMPORTANT: If a value may contain an embedded unescaped double quote
  character, such as ""foo""bar"", use REGEX, not DELIMS. An escaped double
  quote (\"") is ok. Non-ASCII delimiters also require the use of REGEX.
* Optional. Use DELIMS in place of REGEX when you are working with ASCII-only
  delimiter-based field extractions, where field values (or field/value pairs)
  are separated by delimiters such as colons, spaces, line breaks, and so on.
* Sets delimiter characters, first to separate data into field/value pairs,
  and then to separate field from value.
* Each individual ASCII character in the delimiter string is used as a
  delimiter to split the event.
* Delimiters must be specified within double quotes (eg. DELIMS=""|,;"").
  Special escape sequences are \t (tab), \n (newline), \r (carriage return),
  \\ (backslash) and \"" (double quotes).
* When the event contains full delimiter-separated field/value pairs, you
  enter two sets of quoted characters for DELIMS:
* The first set of quoted delimiters extracts the field/value pairs.
* The second set of quoted delimiters separates the field name from its
  corresponding value.
* When the event only contains delimiter-separated values (no field names),
  use just one set of quoted delimiters to separate the field values. Then use
  the FIELDS setting to apply field names to the extracted values.
  * Alternately, Splunk software reads even tokens as field names and odd
    tokens as field values.
* Splunk software consumes consecutive delimiter characters unless you
  specify a list of field names.
* The following example of DELIMS usage applies to an event where
  field/value pairs are separated by '|' symbols and the field names are
  separated from their corresponding values by '=' symbols:
    [pipe_eq]
    DELIMS = ""|"", ""=""
* Default: """"{noformat}

refer doc: [https://docs.splunk.com/Documentation/Splunk/latest/Admin/Transformsconf#GLOBAL_SETTINGS|https://docs.splunk.com/Documentation/Splunk/latest/Admin/Transformsconf#GLOBAL_SETTINGS|smart-link] 

In sourcetype, by using the EXTRACT field [https://docs.splunk.com/Documentation/Splunk/9.0.5/Admin/Propsconf#Field_extraction_configuration|https://docs.splunk.com/Documentation/Splunk/9.0.5/Admin/Propsconf#Field_extraction_configuration|smart-link]  we had already used regex as a workaround. 

Now, the issue remains with DELIMS. Is there any limitation to the string characters where it won’t work?","20/Jun/23 8:47 AM;6053a5692f452d006f82793b;[~accountid:61d7bc8590cfd200712a3c95] OK, I think i misunderstood the description here. We always break the event on an ampersand {{&}} regardless of settings. I thought this might be due to segmenters, but the behavior is consistent even after removing {{&}} and {{?}} from the major segmenters. I would still suggest using REGEX as it will get you what you want.","20/Jun/23 3:39 PM;6053a5692f452d006f82793b;The issue here appears to be with the interaction between segmenters and delims. I can get what the customer expects (actually something more correct) with these configurations:

{panel:bgColor=#deebff}
props.conf

[delimstest]
SEGMENTATION = search
REPORT-delims_test = transform_delims_test
{panel}

{panel:bgColor=#deebff}
transforms.conf

[transform_delims_test]
DELIMS = ""\t?&"", ""=""
{panel}

With this we will break up the entire string after {{url=}} into its components and expose each key-value pair in the string of arguments and their values. I do not think that including the first parameter as part of the url is necessarily correct. I believe the customer is trying to get everything from {{url=}} to the first space as the “entire url”? I am not certain the proper combination of segmenters and delims that would produce the correct search time extraction that would accomplish the same thing as the regex {{url=[^\s]+}} as everything I have tried besides the above will stop at the ampersand ({{&}}). I do recall some issue from quite some time ago where we had some weird behavior with segmenters and GenericCleaner… eh but this was [https://splunk.atlassian.net/browse/SPL-145768|https://splunk.atlassian.net/browse/SPL-145768|smart-link] and only related to bucket replication when this is about search time extraction.

[~accountid:6036b79bf032740068734426] I would think this would go to the search team if triaged?

[~accountid:63c6f7aed7f68827e6b5b21a] perhaps you can ask the customer if the suggested method would work for their use case?","28/Jun/23 8:45 AM;63c6f7aed7f68827e6b5b21a;Hello [~accountid:6053a5692f452d006f82793b] 
My apologies for the delay in response.

I have informed the customer regarding the suggested method.  We will need to wait for them to respond back.

Thank you!","30/Jun/23 4:30 AM;63c6f7aed7f68827e6b5b21a;Hello [~accountid:6053a5692f452d006f82793b] 

The customer has modified the transforms as suggested. They have also run with KV_MODE=none. They see the slight difference in the URL field, but still, it's not fully extracting. 

Please find below the screenshot.

!image-20230630-112032.png|width=1561,height=535!","30/Jun/23 8:59 AM;6053a5692f452d006f82793b;I would say this is actually correct. A URL is the string up to the {{?}}. Everything after the {{?}} are parameters or arguments.

If you have this request:

{noformat}https://somedomain.com/parent/child/page?someparameter=somevalue&anotherparameter=anothervalue{noformat}

Then the URL is {{https://somedomain.com/parent/child/page}}and there are two parameters: {{someparameter=somevalue}} and {{anotherparameter=anothervalue}}. See section {{3.4. Query Component}} in [https://www.ietf.org/rfc/rfc2396.txt|https://www.ietf.org/rfc/rfc2396.txt] 

Regardless, I don’t think this is what the customer really wants (they want the entire uri) so there is something to explore here. I would triage this issue and await some feedback from the search team.","05/Jul/23 1:12 AM;63c6f7aed7f68827e6b5b21a;Hello
Sure, thank you and yes, the customer wants the entire URL.","13/Jul/23 10:28 AM;5d816271822c1b0c3a765315;I tend to agree with that {{customer wants the entire URL}}, therefore this ticket exposes a bug that has been there since 2007. 

In current code of {{KVTransformer.cpp}}, {{DELIMS}} is used to tokenize the source event (_raw). This seems wrong for the field {{url}} that contains the delim character {{=}} multiple times. As a result, the {{url}} key/value is NOT extracted by KVTransformer that uses {{DELIMS}}. Rather, the {{url}} key/value is extracted through the regular KVParser that does not honor {{DELIMS}}. Internally, KVParser users {{&}} as a separator {{KVCCLASS_SPLITTER_AMPERSAND}} and therefore the {{url}} value is truncated before {{&}}.

The correct code is that {{KVTransformer}} needs to split the URL string by the delim character, just one time. However, it’s a long-time bug, the code fix (to get the whole URL) can cause some side-effects that need further evaluation. ","18/Jul/23 3:52 AM;63c6f7aed7f68827e6b5b21a;Hello [~accountid:5d816271822c1b0c3a765315] 

Good Day! I hope you’re doing well.


Thank you for the update. I understand that this is long-time bug that needs some time to fix. Meanwhile, we need a “*Statement for the customer*” as the customer is asking for an update.


Regard,
Deepthi","18/Jul/23 8:12 AM;5d816271822c1b0c3a765315;Splunkdoc has already stated about this limitation {{If the delimiter appears in the value, that value is not extracted}}, per  [https://docs.splunk.com/Documentation/Splunk/9.1.0/SearchReference/Extract|https://docs.splunk.com/Documentation/Splunk/9.1.0/SearchReference/Extract|smart-link].

{noformat}kvdelim
Syntax: kvdelim=<string>
Description: A list of character delimiters that separate the key from the value. If the delimiter appears in the value, that value is not extracted. For example, if the delimiter is a colon ( : ) and a key-value pair is Referer: https://buttercupgames.com, the key-value pair is not extracted.{noformat}

It’s a known issue and unfortunately a design flaw. Since it’s a long-time bug, the fix can cause a breaking change to Splunk search. Therefore, it’s being under review.

CC [~accountid:6053a82bf180c300675e972d] [~accountid:6116e89eb5e18c00705f4c9c] ","21/Jul/23 10:45 AM;63c6f7aed7f68827e6b5b21a;Hello [~accountid:5d816271822c1b0c3a765315] 

Thank you for the information. I have updated this to the customer since it is a long-time bug customer agreed to the soft closure of the SFDC case.

Regards,

Deepthi","10/Nov/23 11:49 AM;5d816271822c1b0c3a765315;The fix of this issue is [https://cd.splunkdev.com/splcore/main/-/merge_requests/61796/|https://cd.splunkdev.com/splcore/main/-/merge_requests/61796/] and being fully discussed in the internal review meetings of search teams. The fix is not released considering the following factors. 

* this customer is the only one reporting this issue and has a workaround in place;
* the issue is old (> 10 years) and not new;
* the fix is a breaking change that impacts every customer potentially.

 We’ll document this ticket as an known product issue and wait for customer feedbacks.","10/Nov/23 12:16 PM;5d816271822c1b0c3a765315;A documentation ticket is created, [https://splunk.atlassian.net/browse/SPL-246933|https://splunk.atlassian.net/browse/SPL-246933|smart-link] , to document it as a known issue.
We'll wait for more customer feedback and decide next action.","05/Jan/24 9:42 AM;61436d20e7c3280070a1979f;Deleted from Workaround section:

{quote}
Customer added "" EXTRACT-foo=url=(?<actual_url>\\[^\s\\]_)\s_urlcategory "" in props.conf
and
in the Extract Field, they choose regular expressions  and added
url=(?<actual_url>\\[^\s\\]_)\s_urlcategory.
This workaround they have implemented in their production.{quote}","07/Aug/24 8:07 AM;639d0f9f2aac389d02c2d7e4;Hi Team !
Hi [~accountid:5d816271822c1b0c3a765315] 


I have a customer with a very similar issue on the On-prem env. Their version is *9.1.2*
For my customer, the DELIMS are not applied on the newly added field. They are not able to provide the source csv file though due to security reasons, but from what I observed, the field value contains a lot of spaces in the middle. Like,""JSJDR         DEIEEA     JE""   -----> This represents how the field value looked like. They are random values generated by an app and put into a csv file and ingested in the indexers. 

I will share you the differences in the transforms.conf 

transforms.conf
[bast_delim_fields]
DELIMS = "";""
FIELDS = ""message_id"",""date"",""event"",""bast_source"",""type_of_message"",""result"",""message_date"",""elapsed_time"",""action_code"",""customer_id"",""authorization_instance"",""credit_indicator"",""client_id"",""terminal_id"",""transaction_amount"",""currency"",""message_type_code"",""action_code_token"",""issuer_response_code"",""card_id"",""financial_meaning"",""acquiring_type"",""processing_code"",""acceptor_code"",""auth_appr_code"",""acceptor_id"",""gans_verarb"",""reference_number"",""auth_ref_num"",""schema"",""pos_data_code"",""consumer_category"",""acceptor_country""

*OLD version of the file was*
[bast_delim_fields]
DELIMS = "";""
FIELDS = ""message_id"",""date"",""event"",""bast_source"",""type_of_message"",""result"",""message_date"",""elapsed_time"",""action_code"",""customer_id"",""authorization_instance"",""credit_indicator"",""client_id"",""terminal_id"",""transaction_amount"",""currency"",""message_type_code"",""action_code_token"",""issuer_response_code"",""card_id"",""financial_meaning"",""acquiring_type"",""processing_code"",""acceptor_code"",""auth_appr_code"",""acceptor_id"",""gans_verarb"",""reference_number"",""auth_ref_num"",""schema"",""pos_data_code"",""consumer_category"" 

Even after creating the field extraction from the GUI, we saw that the field is not extracted. 
Also the customer pointed out that there was a field which was extracted that also had spaces in the middle but it was only one space. It looked like, “ADD GTT YTT” --Single space. 

The point is, the newly added field value is very random, and had lots of spaces and they had made the changes after 3 years the original was created. They had also made Splunk upgrades in those 3 years. 

Also we do not know the exact content of the source file as they will not share it due to security reasons. 



Please let me know your insights","07/Aug/24 11:23 AM;5d816271822c1b0c3a765315;[~accountid:639d0f9f2aac389d02c2d7e4] thanks for sharing the details.

The issue of this Jira ticket is casued by this statement {{If the delimiter appears in the value, that value is not extracted}}. Can you check the customer to see if the delimiter appears in the field value?

If possible, ask customer to provide a couple of fake events (removing the sensitive data). Is there any change on the source events?","09/Aug/24 5:43 AM;639d0f9f2aac389d02c2d7e4;[~accountid:5d816271822c1b0c3a765315] Hi, 

As per the customer, The delimiter does not appears in any other field value, below you can find a fake event given by the customer.

12345678-1234-1234-1234-123456789012;2024-08-09 13:35:19.052;Message Received;App Server;Authorization;;2024-08-09 13:35:18.912;0;000;1234567890123456;G;C;123456789;12345678;24.35;CHF;XX ;00 ;;123456******1234;A;M;000000;1234;123456;123456789;XXXX XX 1 A;123456789012;123456;5;;Retail;balb Filiale balblabla>blablablabl CH","14/Aug/24 9:32 AM;5d816271822c1b0c3a765315;If the delimiter does not appear in the value of any field, this issue is not related to this Jira ticket.

[~accountid:639d0f9f2aac389d02c2d7e4] can you open a new Jira ticket to investigate?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,10/Nov/23 12:16 PM
[PUBLIC] Inconsistency in displayed timezone in Dashboard Studio when using time range tokens,SPL-240750,3025162,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,As Designed,Caleb Dyck,6053a74606cbba006a0dc602,Grace Hu,61121472627b5600686bfb49,Grace Hu,61121472627b5600686bfb49,08/Jun/23 10:44 AM,30/Jul/25 4:24 PM,,01/Sep/23 11:15 AM,10.0.x,8.2.0 GA (Scootaloo),9.0.0(Aurum),9.0.2209(Hersheys),9.0.2303.100(Icebreaker-Tick),9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise Dashboards,,,,,0,dashboard-studio,enterprise-ui,Reviewer-ScotCorrie,support-reviewed,,,,"*Observed Behavior:*

When selecting a non-relative time range in the Studio time picker input (eg. between two dates and times), there is inconsistency in the displayed timezone:

* The resulting token value is a timestamp in the UTC timezone
* The timezone displayed in the picker dialog itself and in visualizations is in the Splunk user’s timezone
* The timezone in the time range input label is in the browser/system timezone, which in most cases would likely be the same as the user’s timezone, so this one is less of a concern

!Screenshot 2023-08-23 at 4.40.24 PM.png|width=433,height=523!



!image-20230823-231535 (45ecb938-08ed-4107-87f9-1931b3f4acff).png|width=823,height=457!

*Expected Behavior:* 

The displayed timezone should be consistent across the entire dashboard

*Steps To Reproduce:*
See example dashboard: [http://udfplayground:7000/en-US/app/search/timerange_timezone_bug|http://udfplayground:7000/en-US/app/search/timerange_timezone_bug]

or create a new Studio dashboard with the following definition

{noformat}{
    ""visualizations"": {
        ""viz_2OxnlEhs"": {
            ""type"": ""splunk.markdown"",
            ""options"": {
                ""markdown"": ""user timezone: $env:user_timezone$ \n\nearliest token value: $global_time.earliest$\n\nlatest token value: $global_time.latest$\n"",
                ""fontSize"": ""large""
            }
        },
        ""viz_xkrbcZrE"": {
            ""type"": ""splunk.table"",
            ""options"": {
                ""columnFormat"": {
                    ""_time"": {
                        ""width"": 238
                    }
                },
                ""showInternalFields"": false
            },
            ""dataSources"": {
                ""primary"": ""ds_zdQIMpaP""
            }
        }
    },
    ""dataSources"": {
        ""ds_zdQIMpaP"": {
            ""type"": ""ds.search"",
            ""options"": {
                ""query"": ""index=_internal\n| timechart count""
            },
            ""name"": ""Search_1""
        }
    },
    ""defaults"": {
        ""dataSources"": {
            ""ds.search"": {
                ""options"": {
                    ""queryParameters"": {
                        ""latest"": ""$global_time.latest$"",
                        ""earliest"": ""$global_time.earliest$""
                    }
                }
            }
        }
    },
    ""inputs"": {
        ""input_global_trp"": {
            ""type"": ""input.timerange"",
            ""options"": {
                ""token"": ""global_time"",
                ""defaultValue"": ""2023-08-08T19:00:00.000Z,2023-08-08T20:00:00.000Z""
            },
            ""title"": ""Global Time Range""
        }
    },
    ""layout"": {
        ""type"": ""absolute"",
        ""options"": {
            ""width"": 1440,
            ""height"": 960,
            ""display"": ""auto""
        },
        ""structure"": [
            {
                ""item"": ""viz_2OxnlEhs"",
                ""type"": ""block"",
                ""position"": {
                    ""x"": 0,
                    ""y"": 80,
                    ""w"": 420,
                    ""h"": 300
                }
            },
            {
                ""item"": ""input_global_trp"",
                ""type"": ""input"",
                ""position"": {
                    ""x"": 0,
                    ""y"": 0,
                    ""w"": 390,
                    ""h"": 82
                }
            },
            {
                ""item"": ""viz_xkrbcZrE"",
                ""type"": ""block"",
                ""position"": {
                    ""x"": 10,
                    ""y"": 160,
                    ""w"": 390,
                    ""h"": 300
                }
            }
        ],
        ""globalInputs"": []
    },
    ""description"": """",
    ""title"": ""timerange_timezone_bug""
}{noformat}",9.0.2303.101 Noah,Azra Lalji,Caleb Dyck,Scot Corrie,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,61d3f6edce3652006ab4641b,6053a74606cbba006a0dc602,6036b7a14623c60069c04029,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Jun/23 10:40 AM;ghu;Screenshot 2023-06-07 at 4.36.48 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5933734,08/Jun/23 10:40 AM;ghu;Screenshot 2023-06-07 at 4.37.00 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5933733,15/Jun/23 9:44 AM;ghu;Screenshot 2023-06-15 at 9.40.50 AM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5966292,15/Jun/23 9:44 AM;ghu;Screenshot 2023-06-15 at 9.41.09 AM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5966294,30/Jun/23 1:25 PM;ghu;Screenshot 2023-06-30 at 1.19.59 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6037750,30/Jun/23 1:25 PM;ghu;Screenshot 2023-06-30 at 1.20.55 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6037751,23/Aug/23 4:50 PM;cdyck;Screenshot 2023-08-23 at 4.40.24 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6272572,15/Jun/23 9:44 AM;ghu;dashboard definition.txt;https://splunk.atlassian.net/rest/api/3/attachment/content/5966293,24/Aug/23 9:59 AM;cdyck;image-20230823-231535 (45ecb938-08ed-4107-87f9-1931b3f4acff).png;https://splunk.atlassian.net/rest/api/3/attachment/content/6275517,15/Jun/23 9:44 AM;ghu;lululemon.splunkcloud.com.har;https://splunk.atlassian.net/rest/api/3/attachment/content/5966291,12/Jul/23 3:42 PM;ghu;lululemon.splunkcloud.com.har.zip;https://splunk.atlassian.net/rest/api/3/attachment/content/6099510,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5294d2fb,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,Unknown,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Splunk Cloud,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,65059200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"h1. Status Page Link(s)

* https://

h1. Technical IR Representatives
List of individuals to include in a review of this incident

* 

h1. Dependent Service Incident ID
(if there are dependent services involved e.g. 3rd party tool or SCS, please put in incident ID tracking on their side as well as links to the IR)

h1. Timeline

* START OF OUTAGE
* END OF OUTAGE

h1. Impact to customers


h1. What Happened?



h1. What Did We Do Well?



h1. What Do We Need to Improve?



h1. Dogfood Our Own Observability Products

||Product||Use||
|Infra Monitoring| |
|APM| |
|RUM| |
|Log Observer| |
|Synthetics| |

h3. Please share the links (in mon) the team used to handle this incident.


h3. Please share feedback about our products during this incident.



h3. Which observability tools/features could have helped you better handle this incident?

h1. Action Items
JIRAs should be linked to this ticket

* ",Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Fri Sep 01 18:15:18 UTC 2023,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,User Experience YVR - Enterprise Dashboards,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,31/Aug/23 12:00 AM,,,,,,,,11.0,24.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2f8518fd-525d-4e69-b34a-5297a7119ea8(2f8518fd-525d-4e69-b34a-5297a7119ea8),cdyck(cdyck),ghu(JIRAUSER51441),scorrie(scorrie),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,End User Experience,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i0mj13:,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Caleb Dyck,6053a74606cbba006a0dc602,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Lululemon Athletica Inc.,3236295,012400000005WzMAAU,P3,No,5005a00002Vu3kkAAB,,Closed,Resolved,Premium,Tier2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ent-Dash-FY24Q3-S3 (8/23-9/5),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,Cloud Premium,,,,,,,,,,,,,,,,2023-06-09 19:52:51.441,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_1_*:*_0_*|*_3_*:*_1_*:*_2957560043_*|*_10001_*:*_1_*:*_4310479504_*|*_10039_*:*_1_*:*_5324253,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Run,,,,,,,,,,,,2023-06-09 19:52:51.441,2023-06-09 19:52:51.441,,,,,,Caleb Dyck,6053a74606cbba006a0dc602,,,,,,"09/Jun/23 12:52 PM;61d3f6edce3652006ab4641b;Hello [~accountid:61121472627b5600686bfb49] , could you please attach the dashboard definition as well as a HAR file for when the dashboard with the error loads.",14/Jun/23 4:48 PM;61121472627b5600686bfb49;[~accountid:61d3f6edce3652006ab4641b] Could you please clarify what you meant by dashboard definition? ,"15/Jun/23 8:50 AM;61d3f6edce3652006ab4641b;Hi [~accountid:61121472627b5600686bfb49] I’m referring to the source code for the dashboard where this issue is observed. You can find it by clicking on Edit on the dashboard page, then clicking the source code icon in the toolbar.","15/Jun/23 9:44 AM;61121472627b5600686bfb49;[~accountid:61d3f6edce3652006ab4641b]  
*Dashboard definition and HAR:*


[^dashboard definition.txt]
[^lululemon.splunkcloud.com.har]



*Working:*

!Screenshot 2023-06-15 at 9.40.50 AM.png|width=2286,height=733!

*Not Working:*

!Screenshot 2023-06-15 at 9.41.09 AM.png|width=2295,height=816!",15/Jun/23 9:55 AM;6053a74606cbba006a0dc602;Thanks [~accountid:61121472627b5600686bfb49]. I’m the current on-call engineer so I’ll be taking over this ticket. Currently dealing with a couple higher priority issues but will let you know when I’ve had a chance to investigate this.,26/Jun/23 1:54 PM;61121472627b5600686bfb49;[~accountid:6053a74606cbba006a0dc602] Customer has been checking if there is any further update on this. can you please share any info if possible? Thank you.,"26/Jun/23 3:37 PM;6053a74606cbba006a0dc602;[~accountid:61121472627b5600686bfb49] I took a look and it seems this is due to the difference in how Dashboard Studio sets time tokens. SimpleXML sets time tokens to the UNIX timestamp format (eg. 1686528000), whereas Studio sets it to a human readable format (eg. “2023-06-26T00:00:00.000Z”), and it looks like the {{search}} command does not accept this format by default. However, you can make it work by specifying the {{timeformat}} in the {{search}} command:

For the search in the example dashboard provided, you could change it to 

{noformat}index=""ecomm_ev_ngc_prod"" source=""/aws/lambda/*atg*""  ""message sent to Kafka successfully"" 
| join orderNumber 
    [ search index=""ecomm_ev_ngc_prod"" namespace=digital-ngc action=""CREATE"" outcome=""SUCCESS""  earliest=$time.earliest$ latest=$time.latest$ timeformat=""%Y-%m-%dT%H:%M:%S.%QZ""]
| dedup orderNumber
| stats count{noformat}

Note the addition of {{timeformat=""%Y-%m-%dT%H:%M:%S.%QZ""}} to the {{search}} command","28/Jun/23 2:29 PM;61121472627b5600686bfb49;[~accountid:6053a74606cbba006a0dc602]  Thank you for the update. So is this behavior a workaround to a potential bug, or is this by design?",28/Jun/23 4:38 PM;6053a74606cbba006a0dc602;[~accountid:61121472627b5600686bfb49] the time tokens being formatted as human readable strings is expected behaviour. I will follow up with the Search team to check why the {{search}} command does not accept this format by default.,"29/Jun/23 11:21 AM;6053a74606cbba006a0dc602;Hi [~accountid:61121472627b5600686bfb49], I reached out to the search team in Slack ([see thread|https://splunk.slack.com/archives/CJWLPH9CJ/p1688055361916669]) and it looks like this is expected behaviour in the {{search}} command as well.","29/Jun/23 12:02 PM;61121472627b5600686bfb49;Thank you [~accountid:6053a74606cbba006a0dc602] . So it sounds like it’s just a change in behavior in terms of how time format is displayed between Classic versus Dashboard studio, correct?",29/Jun/23 12:22 PM;6053a74606cbba006a0dc602;[~accountid:61121472627b5600686bfb49] that’s almost correct. The change in behaviour is that the actual timerange token values are now human readable timestamps instead of UNIX timestamps. Just want to make it clear that the change is specifically about the timerange token value being different rather than a change in how times are displayed.,"30/Jun/23 1:25 PM;61121472627b5600686bfb49;[~accountid:6053a74606cbba006a0dc602]  Customer had reported back that the workaround does not work. 


Example:
Selected time in dashboard 06/29 19:00:00.000 to 06/30 20:00:00.000

!Screenshot 2023-06-30 at 1.20.55 PM.png|width=1293,height=379!

But the values passed to inner query as  06/29 19:00:00.000 to 06/29 20:19:16.000

!Screenshot 2023-06-30 at 1.19.59 PM.png|width=2111,height=559!","30/Jun/23 1:54 PM;6053a74606cbba006a0dc602;[~accountid:61121472627b5600686bfb49] the time range highlighted in your screenshot of the search page is the time range of the overall search, not the inner search. I would expect them to see the same time range and search results if they opened the original search from the simpleXML dashboard instead.","03/Jul/23 12:18 PM;6036b7a14623c60069c04029;[~accountid:6053a74606cbba006a0dc602]  - Covering this case while Grace is out. The customer is wanting a call, I said the following:

_There are differences between regular searches and Dashboard Studio which uses SimpleXML._

_The time range highlighted in your screenshot of the search page is the time range of the overall search, not the inner search._

 _We would expect you to see the same time range and search results if you opened the original search from the simpleXML dashboard instead._

 _Remember SimpleXML is using Javascript, results will not match exactly with regular Splunk._","12/Jul/23 3:42 PM;61121472627b5600686bfb49;[~accountid:6053a74606cbba006a0dc602] Thank you for joining the call. It looks like in dashboard studio, the inner query time stamp is not parsed correctly in the search. I have attached the HAR dump here.


[^lululemon.splunkcloud.com.har.zip]

","13/Jul/23 9:48 AM;6053a74606cbba006a0dc602;Hi [~accountid:61121472627b5600686bfb49], it looks like what’s happening is that the Studio timestamp token is in the GMT timezone but the search is interpreting it as being the PST timezone. To resolve this, the customer can modify the search in their Studio dashboard to be the following.

{noformat}index=""ecomm_ev_ngc_prod"" source=""/aws/lambda/*atg*""  ""message sent to Kafka successfully"" 
| join orderNumber 
    [ search index=""ecomm_ev_ngc_prod"" namespace=digital-ngc action=""CREATE"" outcome=""SUCCESS""  earliest=""$time.earliest$ GMT"" latest=""$time.latest$ GMT"" timeformat=""%Y-%m-%dT%H:%M:%S.%QZ %Z""]
| dedup orderNumber
| stats count{noformat}

I just added the {{ GMT}} timezone specifier to the {{earliest}} and {{latest}} parameters, and then added {{ %Z}} to the {{timeformat}} parameter to parse this timezone.",13/Jul/23 3:24 PM;61121472627b5600686bfb49;Thank you [~accountid:6053a74606cbba006a0dc602] . Is there any documentation about these syntax we can share with customer? It sounds like it is not a bug but more of syntax issue?,"13/Jul/23 3:39 PM;6053a74606cbba006a0dc602;The {{earliest}}, {{latest}}, and {{timeformat}} parameters are documented here: [https://docs.splunk.com/Documentation/SplunkCloud/latest/SearchReference/SearchTimeModifiers|https://docs.splunk.com/Documentation/SplunkCloud/latest/SearchReference/SearchTimeModifiers|smart-link] 

The time variables used in the {{timeformat}} parameter can be found here: [https://docs.splunk.com/Documentation/SplunkCloud/latest/SearchReference/Commontimeformatvariables#Time_variables|https://docs.splunk.com/Documentation/SplunkCloud/latest/SearchReference/Commontimeformatvariables#Time_variables|smart-link] ",14/Jul/23 11:09 AM;61121472627b5600686bfb49;Thank you [~accountid:6053a74606cbba006a0dc602] . However it seems the time zone specifier such as {{GMT}} is not documented anywhere. Should Splunk interprete time zone always as GMT?,"14/Jul/23 11:41 AM;6053a74606cbba006a0dc602;[~accountid:61121472627b5600686bfb49] the Studio timestamp token being in the GMT timezone is most likely a bug, so adding the {{GMT}} specifier to the {{earliest}} and {{latest}} in the search string is just a workaround while we work on fixing the bug.",14/Jul/23 4:10 PM;61121472627b5600686bfb49;Thank you [~accountid:6053a74606cbba006a0dc602] . Customer confirmed the workaround is working and the support case can be closed. I will let you decide how this SPL should continue to track this bug fix.,25/Jul/23 2:39 PM;6053a74606cbba006a0dc602;I’ve updated the ticket summary and description to more accurately reflect what the root cause is.,"01/Sep/23 11:15 AM;6053a74606cbba006a0dc602;Plan is to document the current behavior and then figure out a more general solution for time formatting in UDF/Studio

Documented some options for addressing this issue [here|https://docs.google.com/document/d/1_1NtYP2YZxfFfwCzXzdKDVDV7TeiSWN-EgO0tqmjhx4/edit]. Docs ticket is https://splunk.atlassian.net/browse/SPL-243996 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,01/Sep/23 11:15 AM
"[PUBLIC]  In standard mode federated search, outputlookup existence check on RSH causes search to terminate early although it is not run on RSH",SPL-239436,2829230,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Yuan Shen,611be82846c6b500717e747c,Yuan Shen,611be82846c6b500717e747c,Yuan Shen,611be82846c6b500717e747c,02/May/23 10:29 AM,30/Jul/25 4:25 PM,,18/Mar/25 8:52 PM,10.0.2503.x,10.0.x,9.0.0(Aurum),9.0.1,9.0.2,9.0.2205(Ferrero Rocher),9.0.2209(Hersheys),9.0.2305.100(JuicyFruitTick),9.0.3,9.0.4,9.0.5,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Ice Breaker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0.2503.x,Q release,,,,,,,Search Federation,,,,25/Apr/25 12:00 AM,0,,,,,,,,"When a command like {{search index=index_df_1 | outputlookup iplocation.csv}} executed, the {{outputlookup }}command gets sent to the remote search head although it is executed in phase 1 locally.  If the remote search head does not have this {{lookup}} defined, the RSH will error out first before executing the search, returning 0 result to FSH.",,Giri Basava,James Barnett,Suketu Shah,User known,User known,User known,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,62a6ed3b9cd13c0068aea7f7,557058:3aff60a9-3dcf-42eb-ac21-4b66ada38a27,6053a66586b0dd007188092d,unknown,unknown,unknown,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@34417a38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,5356800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,30/Apr/25 10:00 AM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wed Jul 23 18:59:00 UTC 2025,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Federation,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9657acd1-4db3-482a-a217-b59b5d5c075f(9657acd1-4db3-482a-a217-b59b5d5c075f),jabarnett(jabarnett),keaton(JIRAUSER47587),kgorzynski(JIRAUSER52213),chungmingc(JIRAUSER50229),suketus(suketus),yuans(JIRAUSER50024),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i01z6v:,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuan Shen,611be82846c6b500717e747c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,datafed-FY24Q1-S7,datafed-FY24Q2-S1,datafed-FY24Q2-S2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,16.0,,,,,,,,,,,,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2023-05-03 15:55:50.587,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3_*:*_2_*:*_618458630_*|*_10001_*:*_1_*:*_1133354796_*|*_10039_*:*_1_*:*_288407,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Define the lookup on both federated search head and remote search head.,,,,,,,,,,2023-05-03 15:55:50.587,2023-05-03 15:55:50.587,,,,,,Yuan Shen,611be82846c6b500717e747c,,,,,,"03/May/23 8:55 AM;6053a66586b0dd007188092d;[~accountid:611be82846c6b500717e747c] Juicyfruit-Tick Branch cut coming on May 10? If this is needed for Juicyfruit-Tick, please provide ETA or please file [https://splunk.atlassian.net/wiki/spaces/PROD/pages/1078256768028/Bug+Deferral+Exception+Handling+for+Get-The-Bugs-Out+JuicyFruit|https://splunk.atlassian.net/wiki/spaces/PROD/pages/1078256768028/Bug+Deferral+Exception+Handling+for+Get-The-Bugs-Out+JuicyFruit|smart-link]  at the earliest and provide FixVersion?",03/May/23 11:14 AM;611be82846c6b500717e747c;[~accountid:6053a66586b0dd007188092d] This is for K release.  Thanks.,"09/May/23 2:50 PM;611be82846c6b500717e747c;|*Summary of Issue:*|outputlookup requires users to define the lookup on both federated search head and remote search head.  Without the definition on the remote search head, the remote search head in standard mode will return zero result.
This issue does not happen when outputlookup is writing to an output csv file.|
|*Workaround:*|Define the lookup on both federated search head and remote search head.|
|*Frequency of Issue:*|With workaround, none.  
Without workaround, federated search commands with outputlookup will get zero result from the remote search head.|
|*Customer Impact:*|With the workaround above, no customer impact.  The workaround has been documented in release note too.
 Without the workaround, the remote search peer returns zero result.|
|*Regression:*|No|
|*Reason for deferral:*|We are planning a major change to how to handle lookup related commands in federated search, so that it can apply a solution to the {{outputlookup}} command, {{lookup}} command with/without {{local=true}} and {{inputlookup}} command with the same approach.|
|*Target Fix timeframe/release:*|Next release|",11/May/23 11:27 AM;6053a66586b0dd007188092d;Thanks [~accountid:611be82846c6b500717e747c]  for the update. removing from Juicyfruit,"23/Oct/23 10:39 AM;611be82846c6b500717e747c;|*Summary of Issue:*|outputlookup requires users to define the lookup on both federated search head and remote search head. Without the definition on the remote search head, the remote search head in standard mode will return zero result.
This issue does not happen when outputlookup is writing to an output csv file.|
|*Workaround:*|Define the lookup on both federated search head and remote search head.|
|*Frequency of Issue:*|With workaround, none.
Without workaround, federated search commands with outputlookup will get zero result from the remote search head.|
|*Customer Impact:*|With the workaround above, no customer impact. The workaround has been documented in release note too.
 Without the workaround, the remote search peer returns zero result.|
|*Regression:*|No|
|*Reason for deferral:*|There is a higher priority issue: [+SPL-244927+|https://splunk.atlassian.net/browse/SPL-244927]
The higher priority issue does not show events in verbose mode with specific commands, while this issue has a workaround, so the decision is to fix the above issue first.|
|*Target Fix timeframe/release:*|Next release|","20/Nov/24 11:05 AM;6116e89eb5e18c00705f4c9c;[~accountid:611bddfd28ae75006ac44d1c] and [~accountid:62a6ed3b9cd13c0068aea7f7] [~accountid:611be82846c6b500717e747c] 

We would need the M5+ to provide {color:#ff5630}*an ETA for fixing*{color} the issue {color:#ff5630}*in the ETA Date field* {color}before approving the deferral.

CC: [~accountid:557058:3aff60a9-3dcf-42eb-ac21-4b66ada38a27] ","20/Nov/24 11:23 AM;611be82846c6b500717e747c;This one requires a configuration change in the {{federated.conf}}, no code change required.  

Thanks,
Yuan","21/Nov/24 4:03 PM;62a6ed3b9cd13c0068aea7f7;With SPL-265792 the following configuration option is introduced to federated.conf in *P release*. 

{{s2s_standard_mode_local_only_commands = command1, command2}}

In *P release,* customer can add “outputlookup” command to restrict the execution of the “outputlookup” to local environment only ( both streaming and from style searches )

In *Q release,* we will add the “outputlookup” as a default to the above configuration option and update the documentation.  [~accountid:712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0] please comment if we need to provide a diffrent customer experience.

Q Release expected to go GA in April 2025, updating the ETA as such cc: [~accountid:611bddfd28ae75006ac44d1c] ","02/Dec/24 9:17 AM;6116e89eb5e18c00705f4c9c;[~accountid:62a6ed3b9cd13c0068aea7f7] 

Please update the ETA in the ETA field so that the release team can do the audit.",02/Dec/24 9:29 AM;62a6ed3b9cd13c0068aea7f7;ETA field updated. TY!,18/Mar/25 11:16 AM;557058:3aff60a9-3dcf-42eb-ac21-4b66ada38a27;[~accountid:611be82846c6b500717e747c] is this still waiting for a deferral approval for Q&F releases?,18/Mar/25 12:05 PM;6053b8ca686bf50070468998;I would like to see more details on why this has been deferred 7 times.  Please provide justification for the deferral if seeking one for Q/F.,"18/Mar/25 8:52 PM;611be82846c6b500717e747c;This is a duplicate of ticket [https://splunk.atlassian.net/browse/SPL-230045|https://splunk.atlassian.net/browse/SPL-230045|smart-link] 
The fix is already merged to Q-release.
Closing this ticket.","23/Jul/25 11:59 AM;61436d20e7c3280070a1979f;[~accountid:557058:89ace195-adc2-4f25-9924-ded7c8f10bb6] Since this issue has been fixed, I moved it from Known issues release notes to Fixed issues release notes for 10.0.2503. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,18/Mar/25 8:52 PM
"[PUBLIC] Federated Search for Splunk does not support the ""Show Source"" Field Action",SPL-238738,2811501,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,Unknown,Fixed,Cuong Dong,6053a57786b0dd007187fe97,Matthew Ness,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,Matthew Ness,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,14/Apr/23 5:27 PM,30/Jul/25 4:45 PM,,01/Apr/25 12:54 PM,10.0.2503.x,10.0.x,9.0.0(Aurum),9.0.2209(Hersheys),9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Ice Breaker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.1.2507.x,10.2.x,,,,,,,Search,Search Federation,,,,0,beryllium-defer-accept,,,,,,,"The “Show Source” *Field Action*, which is designed to run a search that show the indexed events that precede and follow an event in your search results, does not work in standard or transparent mode federated search. 

This issue was reported by a customer and subsequently discussed in [this Slack thread|https://splunk.slack.com/archives/C01VA6P7TD4/p1681232435292239]. [~accountid:6053a293009fee00694bc4e6] did some follow-up testing and verified that there does appear to be a problem with Show Source and federated searches. 

The other *Field Action* options - “Build Event Type” and “Extract Fields” appear to work as expected. 

I am filing this ticket so we can get it added as a known issue to the release notes. 

The “Show Source” *Field Action* is documented [here|https://docs.splunk.com/Documentation/Splunk/9.0.4/Knowledge/Controlworkflowactionappearanceinfieldandeventmenus]. *Field Actions* - which were added to the Splunk platform around 2010 - are  actually a demonstration of a feature called [workflow actions|https://docs.splunk.com/Documentation/Splunk/9.0.4/Knowledge/CreateworkflowactionsinSplunkWeb]. [This Answers post from 2013|https://community.splunk.com/t5/Splunk-Search/no-of-events-in-show-source-view/td-p/94355] provides a bit more context about what the “Show Source” *Field Action* actually does & what endpoint it uses.",,Andrew Brown,Ben Zeigler,Bharath Aleti,Cuong Dong,Jeremy Nenadal,Jessica Kulak,Matthew Ness,Nithin Krishna Reghunathan,Shalabh Goyal,Shreedeep Mitra,Suketu Shah,User known,User known,User known,User known,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,630489ffbcb35371090f2048,557058:b1e398c5-b004-4a33-8152-18daa36cda4b,6053a57786b0dd007187fe97,620a2057bba9ca0070c96c03,6036b87618537600702416e1,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0,6053a60cf180c300675e7e8b,557058:d3314561-fd53-4fb1-9081-0838424801b6,6053a66586b0dd007188092d,unknown,unknown,unknown,unknown,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,RDMP-2664,,,,SPL-273098,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Jan/25 12:08 PM;cdong;image-20250113-200349.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7871589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@38ab13d6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,System,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,5270400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thu Jul 24 17:49:41 UTC 2025,true,mness(mness),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Federation,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,1.0,40.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),bb27173e-b711-4f6c-ad12-71bd061f2aa9(bb27173e-b711-4f6c-ad12-71bd061f2aa9),baleti(baleti),chrish(JIRAUSER51966),cdong(cdong),9c513012-f2c9-4627-b9b1-882eaacd8bc3(9c513012-f2c9-4627-b9b1-882eaacd8bc3),jkulak(jkulak),mness(mness),mkulathumani(JIRAUSER42415),nfrenkel(JIRAUSER43952),dc729936-08dc-4a23-8bff-df98279dc85b(dc729936-08dc-4a23-8bff-df98279dc85b),sgoyal(sgoyal),smitra(smitra),ssundaresan(JIRAUSER50153),suketus(suketus),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Cuong Dong,6053a57786b0dd007187fe97,,,,,,,,,,,,,,,,,,0|ibma4v:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cuong Dong,6053a57786b0dd007187fe97,Boris Chen,Christina Noren,Matt Green,Rachel Perkins,61324249ac61dc00697cf056,613be989f6070d006b917dca,5af8dc294b719848347ec02c,5af5d6c28a1abd1427953558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Apple Inc.,3371147,012400000005WzMAAU,P4,No,5005a00002kZpusAAC,,Closed,Resolved,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Datafed-FY25Q4-S4,Datafed-FY25Q4-S6,Datafed-FY25Q4-S7,Datafed-FY25Q3-S1,Datafed-FY25Q3-S2,Datafed-FY25Q3-S3,Datafed-FY25Q3-S4,Datafed-FY25Q3-S5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2023-05-08 22:52:30.408,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_1_*:*_0_*|*_3_*:*_1_*:*_5701823394_*|*_10001_*:*_1_*:*_48117_*|*_10039_*:*_1_*:*_54677025264,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Updated the severity level from Sev-3 to Sev-2. Please refer to the P&T Customer Issues SLO: https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k/edit?usp=sharing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cuong Dong,,,,,,,,,,,,,,,2023-05-08 22:52:30.408,2023-05-08 22:52:30.408,,,,,,Cuong Dong,6053a57786b0dd007187fe97,,,,,,"08/May/23 3:52 PM;6053a47966c8790068386b56;This issue has been identified as a potential Beryllium blocker - Please refer to https://splunk.atlassian.net/wiki/spaces/PROD/pages/1078187033286/Berylium+Bugs+and+Blockers+-+Beryllium+9.1

We need to follow the same approach as we did for JuicyFruit -
* Validate if we can fix these in time for Beryllium branch cut - May 17
* If not by then, apply the label ""beryllium-defer-request"" and ask for a deferral to a future release. Please provide the following information to help in processing the deferral:

Summary of Issue: (Briefly summarize the issue encountered)

Workaround: (Note any workaround available to avoid the issue)

Frequency of Issue: (When and how does this occur. Note if the issue is a corner case.)

Customer Impact: (Explain the customer impact if the issue is encountered.  List the harm that might result, How will the customer recover from the issue?)

Regression: (Is this a regression from a previous release?)

Reason for deferral: (Detail the reason for deferral)


Target Fix timeframe/release: (Specify a target time-frame for fixing the issue)","23/May/23 3:45 PM;6053a66586b0dd007188092d;[~accountid:62a6ed3b9cd13c0068aea7f7] can we please confirm by May 25 if this is needed for Beryllium release since we have RC candidate planned for May 30? If not, please fill out [https://splunk.atlassian.net/wiki/spaces/PROD/pages/1078201743404/Bug+Deferral+Exception+Handling+for+Get-The-Bugs-Out|https://splunk.atlassian.net/wiki/spaces/PROD/pages/1078201743404/Bug+Deferral+Exception+Handling+for+Get-The-Bugs-Out|smart-link] and add label - *beryllium-defer-request* ?",23/May/23 4:17 PM;6053a293009fee00694bc4e6;[~accountid:6053a66586b0dd007188092d]  This ticket is not for Beryllium release.  I will fill the ticket.,"23/May/23 10:13 PM;6053a293009fee00694bc4e6;|*Summary of Issue:*|a missing parity with legacy searches where a section of a file from which the event was extracted can be seen along the search results.|
|*Workaround:*|Not using for federated search results.|
|*Frequency of Issue:*|_when the_ {{show results}} _option pressed for a federated search result_|
|*Customer Impact:*|The customer can’t see the whole source file from where the search result came from|
|*Regression:*|No. This is a day one behavior of federated search.|
|*Reason for deferral:*|This SPL is more an epic than a bug. It is categorized as a bug so it will be in the known issues and customers can see it if they encounter this missing feature ( or in this case a button that does not show the source code of the event they are observing). It is a big change and needs design, and implementation.|
|*Target Fix timeframe/release:*|L or later release. As of now we have no bandwidth to implement this feature because of more urgent priorities.|",25/May/23 12:12 PM;6053a66586b0dd007188092d;Thanks [~accountid:6053a293009fee00694bc4e6] for details moving out of beryllium,"07/Dec/23 1:58 PM;630489ffbcb35371090f2048;[~accountid:6053a293009fee00694bc4e6] I am linking SFDC case *3371147* to this JIRA, as *Apple* is requesting an update regarding when this fix will be made available due to their users requesting this ability.",02/Jul/24 11:30 AM;620a2057bba9ca0070c96c03;I have a customer in case [+3444227+|https://splunk.lightning.force.com/lightning/r/5005a00002pndx0AAA/view] asking when this will be fixed. Are there plans to resolve this issue in the near future?,23/Jul/24 9:32 PM;6149765d7a3c8800672ffe3a;[~accountid:6053a293009fee00694bc4e6]  Can we get an update on where we are with this ticket? [~accountid:61325b3a6e5e1e0071f0a55e] This is a blocker for Dell. ,"23/Jul/24 10:22 PM;6053a293009fee00694bc4e6;Hi [~accountid:6149765d7a3c8800672ffe3a] It is not yet funded. Hence I can’t give a date regarding when this will work for federated search.

Thanks ",29/Jul/24 3:37 PM;6149765d7a3c8800672ffe3a;[~accountid:61325b3a6e5e1e0071f0a55e] This is a blocker for Dell Digital,"30/Jul/24 5:33 PM;61325b3a6e5e1e0071f0a55e;[~accountid:6149765d7a3c8800672ffe3a] Need more context on this one. Overall, we need to define how I engage with the Dell so I have a holistic view of their asks.","31/Jul/24 12:08 PM;6149765d7a3c8800672ffe3a;[~accountid:61325b3a6e5e1e0071f0a55e] Sure. Id be happy to give you any context you need. As these issue comes up, I’ve been adding you to Jiras so you can see the volume of open Jiras they have, the progress on those jiras, and understand where we are with the issue. If there is a better way to provide this information I am open to any suggestion you have/prefer on how to provide you up to date information. ",26/Aug/24 3:08 PM;6149765d7a3c8800672ffe3a;[~accountid:557058:d3314561-fd53-4fb1-9081-0838424801b6] Can you provide [~accountid:61325b3a6e5e1e0071f0a55e] some context on this issue for Dell Inc?,26/Aug/24 3:20 PM;557058:d3314561-fd53-4fb1-9081-0838424801b6;[~accountid:61325b3a6e5e1e0071f0a55e] [~accountid:6149765d7a3c8800672ffe3a] - Dell wants to increase the adoption of Federated Search functionality in between its two large Splunk instances/divisions - Dell Digital (ITOps) and Dell SRO (Cybersecurity) - for cross querying datasources.  This ask maps to a basic search feature that is available outside of Federated Search. Users should be able to browse/peruse events near to prior and near to after the interested/indicative events found have been found.,26/Aug/24 4:03 PM;61325b3a6e5e1e0071f0a55e;Adding[~accountid:557058:b1e398c5-b004-4a33-8152-18daa36cda4b] [~accountid:6053a2e3686bf50070459604] to comment on whether this is part of our roadmap.,26/Aug/24 11:29 PM;557058:b1e398c5-b004-4a33-8152-18daa36cda4b;Thanks [~accountid:61325b3a6e5e1e0071f0a55e] for sharing this customer request.  I’ll sync with the engg team and get back in terms of roadmap plans.,03/Sep/24 12:45 PM;6149765d7a3c8800672ffe3a;Any updates on this issue? It's still sitting open marked as a P2 for the customer. [~accountid:61325b3a6e5e1e0071f0a55e] [~accountid:6053a2e3686bf50070459604] [~accountid:557058:b1e398c5-b004-4a33-8152-18daa36cda4b] ,09/Sep/24 9:55 AM;620a2057bba9ca0070c96c03;Hi team. Any updates on this? [~accountid:61325b3a6e5e1e0071f0a55e] [~accountid:6053a2e3686bf50070459604] [~accountid:557058:b1e398c5-b004-4a33-8152-18daa36cda4b] Is there any way we can start to work on moving this issue closer to resolution? ,"09/Sep/24 11:50 AM;557058:b1e398c5-b004-4a33-8152-18daa36cda4b;[~accountid:620a2057bba9ca0070c96c03] Engg team is engaged to look into this issue.  


As they look into the issue, can i get some additional clarity on the ask - 
Is the ask to report “before/after “ events related to matching events ? If so, is there a specific usecase that requires “before/after “ events related to matching events ?
Or 
Is the ask specific to reporting matching events only (not before/after events) corresponding to the federated search, as part of “show source” ?","10/Sep/24 1:16 PM;620a2057bba9ca0070c96c03;Hello [~accountid:557058:b1e398c5-b004-4a33-8152-18daa36cda4b] , 

The customer cannot view any events when using show source on a federated search head. The customer is Clicking on “Show Source” from Event actions and wants this to work all searches (federated or not) so they can see what events surround an event in a log. This error is experienced whether or not the search results come from a federated provider. The customer has both federated providers and indexers that they search in the same environment. They can use show source if they disable federated search, but this is not ideal/practical. They want to be able to see surrounding logs from an event in their search results.","10/Sep/24 10:18 PM;557058:b1e398c5-b004-4a33-8152-18daa36cda4b;Thanks [~accountid:620a2057bba9ca0070c96c03] 


[~accountid:62a6ed3b9cd13c0068aea7f7] and team identified that the root cause for missing events with “show source”  is that the corresponding events (& before/after events)  are located on the remote side and not fetched as part of federated search to  optimize for network traffic . 

To support this functionality, we need to ensure that the event metadata is sent from the Remote Search Head (RSH) to the FSH along with the search results. This may require an additional round-trip call to the RSH to retrieve the surrounding events 

Product team is looking to see how we can optimize the network traffic for usecase that require and do not require “show source” events info. It will then be prioritized accordingly. Meanwhile, we plan to update the docs to highlight this as a known issue

Unfortunately no easy fix here. The customer angst is understandable. Let us know how you would like to proceed. Glad to jump on a customer call to explain the rationale and share roadmap updates.","13/Sep/24 7:01 AM;620a2057bba9ca0070c96c03;Hello [~accountid:557058:b1e398c5-b004-4a33-8152-18daa36cda4b] ,

Thank you for the update. I will relay this info to the customer and find out if they want to schedule a meeting. I will keep you posted. ",17/Sep/24 10:30 AM;620a2057bba9ca0070c96c03;I haven’t seen a response from the customer yet on a Zoom session. I will let you know if that changes. ,"26/Sep/24 9:23 AM;620a2057bba9ca0070c96c03;Hello [~accountid:557058:b1e398c5-b004-4a33-8152-18daa36cda4b] and [~accountid:62a6ed3b9cd13c0068aea7f7] , Are there any updates on the prioritization of tasks to solve this issue? I see it isn’t a very easy task to accomplish, so I appreciate your time on this. ",26/Sep/24 9:41 AM;6053a60cf180c300675e7e8b;I don’t think that we have the capacity to prioritize this in the next 1-2 qtrs. We need to first discuss this with the customer because there is a cost involved.,"26/Sep/24 9:42 AM;557058:b1e398c5-b004-4a33-8152-18daa36cda4b;[~accountid:620a2057bba9ca0070c96c03] Can we get some customer feedback, on which is more critical for their operations - ancillary event data from remote search or the search latency.  This will shape our plans to address the issue.  Given this is ancillary search data, we haven’t heard this request from other federated search customers. ","27/Sep/24 5:52 AM;620a2057bba9ca0070c96c03;I reviewed the whole case before getting ready to respond, and it looks like the customer opened this case because they cannot retrieve ancillary events from remote searches. It seems like having a federated provider configured makes “show source” fail whether the federated provider returned results in the search or not. ","28/Oct/24 10:54 AM;620a2057bba9ca0070c96c03;Hello [~accountid:557058:b1e398c5-b004-4a33-8152-18daa36cda4b][~accountid:6053a293009fee00694bc4e6] and team,

I wanted to check-in and see if there are any updates on the roadmap for this issue? I do understand that there is a lot of work involved to solve this Jira, so I do want to be respectful of your time. Please let me know if there is anything I need to get from the customer. ",30/Oct/24 12:22 AM;6149765d7a3c8800672ffe3a;[~accountid:61325b3a6e5e1e0071f0a55e] Can you help get this item prioritized or added to the roadmap? Please feel free to reach out to [~accountid:557058:d3314561-fd53-4fb1-9081-0838424801b6] and me to give you the background and business impact of this issue for Dell. ,"12/Nov/24 6:14 AM;620a2057bba9ca0070c96c03;Hello [~accountid:557058:b1e398c5-b004-4a33-8152-18daa36cda4b] and [~accountid:6053a293009fee00694bc4e6] , 

I am checking in to see if there are any updates for this Jira I can share with the customer. If you have any information I can share with them, I would greatly appreciate it. ",12/Nov/24 9:44 AM;6053a60cf180c300675e7e8b;We are currently in planning cycle for next qtr and let me see if we can prioritize this.,"25/Nov/24 6:34 AM;620a2057bba9ca0070c96c03;Hello [~accountid:6053a60cf180c300675e7e8b], 

I wanted to follow up and see if you were able to get this Jira prioritized. If there is anything you need from me or the customer, please feel free to let me know. ","25/Nov/24 8:48 AM;712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0;[~accountid:620a2057bba9ca0070c96c03] Yes, we already have this in the priority list. Engg discussions are also in progress. We will let you know if we need any additional info from customer. cc [~accountid:62a6ed3b9cd13c0068aea7f7] [~accountid:6053a293009fee00694bc4e6] [~accountid:63055da649a5c6754d90fc7b] ","06/Jan/25 6:09 AM;620a2057bba9ca0070c96c03;Hello [~accountid:712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0] [~accountid:62a6ed3b9cd13c0068aea7f7] [~accountid:6053a293009fee00694bc4e6] [~accountid:63055da649a5c6754d90fc7b]  I am following up with the customer this week, and I wanted to know if there were any updates available that I can share with them. Any info would be helpful. Thank you for your time on this issue. ","06/Jan/25 9:01 AM;712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0;[~accountid:620a2057bba9ca0070c96c03] - This feature is on our roadmap for the upcoming Q-release. We are actively working on this feature enhancement request. However, the GA date (ETA) for the Q-release is yet to be determined by our releases team.","13/Jan/25 12:08 PM;6053a57786b0dd007187fe97;Update:

* For Transparent mode, Show Source works in my testing. The search used by Show Source is something like this: {{surrounding id=2:147845 index=_internal searchkeys="""" timeBefore=86400 timeAfter=86400 maxresults=50 timestamp=1736545027.580 bucket=""_internal~2~2175FF94-C4A6-432A-AC0C-4D17CBACDDB2"" filter=""}}  . This is sent to RSH in transparent mode and returns result. Thus, the effort for Transparent mode is likely testing to make sure it works for all cases.
* For Standard mode, Show Source doesn’t work because the query is similar to the above, using {{index=_internal}} . This doesn’t send to RSH to get result; in standard mode, the query should be {{index=federated:internal}} instead. Looking into the source code, I’ve found it relies on the event metadata “_si” and currently it shows the original index on RSH (see screenshot below). When FSH tries to search with this RSH index, it doesn’t send to RSH or worse, the index is not valid in FSH. Thus, we should update this {{_si}} field to provide more data to allow Show Source to build a {{surrounding}} query to use {{index=federated:internal}} instead. I’ll work on a proposal how to update this metadata field for Federated Search.



!image-20250113-200349.png|width=1407,height=394,alt=""image-20250113-200349.png""!",24/Jan/25 10:24 AM;6053a57786b0dd007187fe97;Proposed solution: [https://docs.google.com/document/d/1GYrq0faybJ1YzmmWntQApXylmwWAb7bh15g8FEj89zs/edit?tab=t.0#heading=h.e85igghg8dzf|https://docs.google.com/document/d/1GYrq0faybJ1YzmmWntQApXylmwWAb7bh15g8FEj89zs/edit?tab=t.0#heading=h.e85igghg8dzf|smart-link] ,"19/Feb/25 4:41 AM;6036b87618537600702416e1;Hi [~accountid:6053a57786b0dd007187fe97] can you confirm if an approach to the solution has been decided, and any potential timeline for merge? Thanks.",19/Feb/25 10:44 AM;6053a57786b0dd007187fe97;We target this for Q release and hope to merge by the end of Q release.,24/Jul/25 10:49 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;The clone bug is fixed in 10.1.2507 (the release after Q) so I’m guessing this will be fixed in Splunk Enterprise version 10.2. Please update the fix version appropriately for accuracy in release notes. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,01/Apr/25 12:54 PM
"[PUBLIC] Federated search ""outputlookup"" command cannot add data to local lookup table",SPL-238501,2806067,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,Yuan Shen,611be82846c6b500717e747c,Yuan Shen,611be82846c6b500717e747c,Yuan Shen,611be82846c6b500717e747c,10/Apr/23 3:58 PM,30/Jul/25 4:25 PM,,,10.0.x,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Ice Breaker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Federation,,,,,0,beryllium-defer-accept,Icebreaker-Reviewed,,,,,,"When a federated search query contains {{outputlookup}} command, this command is always sent to remote peers in both standard and transparent mode searches.  Currently users cannot add the remote search data to a local lookup table by using {{outputlookup}} .

For example: {{index=federated:index_main | outputlookup test_lookup}}
This entire command is sent to RSH, and if RSH does not have {{test_lookup}} defined, it errors out and returns 0 result.",,Giri Basava,Jack Herod,Matthew Ness,Shaista Chaudhry,Suketu Shah,User known,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,62a6ed3b9cd13c0068aea7f7,557058:1bed0fc4-5f71-4f24-a270-8af437b03698,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,5c310f3c16ac1e4f7cbbf534,6053a66586b0dd007188092d,unknown,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4bbe1ba3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,73612800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thu May 25 19:11:35 UTC 2023,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Federation,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9657acd1-4db3-482a-a217-b59b5d5c075f(9657acd1-4db3-482a-a217-b59b5d5c075f),mkulathumani(JIRAUSER42415),suketus(suketus),yuans(JIRAUSER50024),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|iblelb:,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yuan Shen,611be82846c6b500717e747c,Boris Chen,Christina Noren,Matt Green,Rachel Perkins,61324249ac61dc00697cf056,613be989f6070d006b917dca,5af8dc294b719848347ec02c,5af5d6c28a1abd1427953558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,8.0,,,,,,,,,,,,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2023-05-08 22:52:31.086,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Define the same lookup on the remote search head too, so the remote search head will not error out early and return 0 results.",,,,,,,,,,2023-05-08 22:52:31.086,2023-05-08 22:52:31.086,,,,,,Yuan Shen,611be82846c6b500717e747c,,,,,,"08/May/23 3:52 PM;6053a47966c8790068386b56;This issue has been identified as a potential Beryllium blocker - Please refer to https://splunk.atlassian.net/wiki/spaces/PROD/pages/1078187033286/Berylium+Bugs+and+Blockers+-+Beryllium+9.1

We need to follow the same approach as we did for JuicyFruit -
* Validate if we can fix these in time for Beryllium branch cut - May 17
* If not by then, apply the label ""beryllium-defer-request"" and ask for a deferral to a future release. Please provide the following information to help in processing the deferral:

Summary of Issue: (Briefly summarize the issue encountered)

Workaround: (Note any workaround available to avoid the issue)

Frequency of Issue: (When and how does this occur. Note if the issue is a corner case.)

Customer Impact: (Explain the customer impact if the issue is encountered.  List the harm that might result, How will the customer recover from the issue?)

Regression: (Is this a regression from a previous release?)

Reason for deferral: (Detail the reason for deferral)


Target Fix timeframe/release: (Specify a target time-frame for fixing the issue)",09/May/23 11:49 AM;62a6ed3b9cd13c0068aea7f7;[~accountid:611be82846c6b500717e747c] can you please provide the below information and request a deferral? Let’s plan it for the K release,"23/May/23 4:06 PM;6053a66586b0dd007188092d;[~accountid:611be82846c6b500717e747c] can we please confirm by May 25 if this is needed for Beryllium release since we have RC candidate planned for May 30? If not, please fill out [https://splunk.atlassian.net/wiki/spaces/PROD/pages/1078201743404/Bug+Deferral+Exception+Handling+for+Get-The-Bugs-Out|https://splunk.atlassian.net/wiki/spaces/PROD/pages/1078201743404/Bug+Deferral+Exception+Handling+for+Get-The-Bugs-Out|smart-link] and add label - *beryllium-defer-request* ?

cc [~accountid:62a6ed3b9cd13c0068aea7f7] ","23/May/23 4:41 PM;611be82846c6b500717e747c;

|*Summary of Issue:*|* When an {{outputlookup}} command is in a federated search query, the {{outputlookup}} command is always sent to the RSH although it is executed in phase 1 locally.
If the remote SH does not have the lookup definition, it errors out before doing any search, returns 0 result|
|*Workaround:*|Define the lookup used in the {{outputlookup}} command on both FSH and RSH.|
|*Frequency of Issue:*|N/A|
|*Customer Impact:*|The customer needs to define lookup on the remote search head too.  Once defined, no functional impact.|
|*Regression:*|No|
|*Reason for deferral:*|Federated search team is doing a re-design on handling of {{lookup}} related commands|
|*Target Fix timeframe/release:*|We will determine which release we can get this fix into once the design is done|","23/May/23 4:42 PM;611be82846c6b500717e747c;[~accountid:6053a66586b0dd007188092d] Just added it.  For some reason I thought I already did it for Berrylium.  It might have been for a different release.  Sorry about this.
If there is anything else needed from me, please feel free to ping me any time.  Thank you!",25/May/23 12:11 PM;6053a66586b0dd007188092d;thanks [~accountid:611be82846c6b500717e747c] for the details. moved out of beryllium,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,10/Apr/23 3:58 PM
[Schwab/CapOne/Itau Bank]scheduler performance issues with high skipped search (user-prefs.conf contention for timezone),SPL-238421,2803164,Bug,Resolved,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Ajeet Kumar,5bc64d495b8ba5286cb9abf4,Robert Phillips,5a1c7af2b87a44359db0e7dc,Robert Phillips,5a1c7af2b87a44359db0e7dc,06/Apr/23 2:18 PM,30/Jul/25 4:26 PM,,13/May/24 9:22 AM,10.0.x,8.1.5,8.2.2,8.2.8,9.0.x,9.1.2308.200(KitKat_Tock),9.1.x,9.2.x,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.3.x,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Ice Breaker,Lindt_Tock(9.1.2312.200),Milkyway(9.2.2403.100),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Nutella,,,,,,,,Search - Scheduler,,,,,0,COMP_SEARCH_SCHEDULER,sinfra:cos:slow_conf_cache,sinfra:sx:skipped_searches,SPLUNK_CONF,,,,"Schwab has been struggling with scheduler performance issues for some time now. We have taken several steps to try and improve the skip/deferred searches however we have reached a wall.

*Summary:* 

* SHC members resource usage looks normal (mem,cpu,disk latency)
* IDXC members resource usage looks normal (mem,cpu,disk latency)
* 10,112 savedsearches enabled
* scheduled search distribution high every 15m peaking at ~7200 at top of hour
* captain computed concurrency peaks for ~10m every 15m where we see high amount of skipped/deferred search
* clusterwide scheduler concurrency limit:1785
* member delegatejob endpoint mostly below 10s

{noformat}source=*/splunkd_access.log /shcluster/member/delegatejob/   | timechart max(spent) as spent_ms by host{noformat}
* also see high skip ratio of 50% during RR of the SHC


*Data Collected:*

started captain pstacks on a 15m ""hot minute"" start time:

[https://downloadsvc.splunk.com/download/splunk/04-05-2023/uploadsvc-46case3156796-04-05-2023-USER-0034000000p9F9OAAU-stacks-108269-s3147cdc-2023-04-05T12h30m22s951182096ns-0400.tar.xz|https://downloadsvc.splunk.com/download/splunk/04-05-2023/uploadsvc-46case3156796-04-05-2023-USER-0034000000p9F9OAAU-stacks-108269-s3147cdc-2023-04-05T12h30m22s951182096ns-0400.tar.xz]



!schwab_flamegraph.svg|width=1200,height=550!





diag of captain: 
[https://downloadsvc.splunk.com/download/splunk/04-05-2023/uploadsvc-44case3156796-04-05-2023-USER-0034000000p9F9OAAU-diag-s3147cdc-2023-04-05_13-08-46.tar.gz|https://downloadsvc.splunk.com/download/splunk/04-05-2023/uploadsvc-44case3156796-04-05-2023-USER-0034000000p9F9OAAU-diag-s3147cdc-2023-04-05_13-08-46.tar.gz]


We see a perf bottle neck of scheduler going into user-prefs.conf conf system to get the timezone to be user for a search

!image-20230406-235443.png|width=1191,height=373!

this same code path is hit by auth, and many other places, 

!image-20230406-235515.png|width=1189,height=537!

and thus creates major amounts skips , and greatly slows down scheduler.

they have over 12,000 users, and nearly all of them have timezone preferences 

{noformat}96/s3147cdc-sh_-20230406-181638-2QlpSfR3/etc/users$ find . -name user-prefs.conf | grep local | wc -l
12023{noformat}","SH splunk version 8.1.5

IDXR splunk version: 8.1.5

29 SHC members (16 of them doing scheduled search)

peers: 532",Adrian Edwards,Ajeet Kumar,Andrea Hong,David Paper,Eric Woo,Gaurav Gupta,Hongxun Liu,Jagmohan Singh,Jo Hornsby,Joshua Weinstein,Karthik Sabhanatarajan,Kieran Cairney,Kriti Ashok,Liang Han,Robert Phillips,Shalabh Goyal,Sirish Mohan,srv -jira-gitlabci,Subba Gontla,Sung Lim,User known,,,,,,,,,,,,,,,5fb818392730d80076782242,5bc64d495b8ba5286cb9abf4,6053a969695c3900707a9628,557058:7ab99593-8dd6-4bca-b17f-42fcc38afee7,5f55997610d187006f15c025,6053ac5ae394c30069cb0d7f,6053a3fb81b82500685ced67,6053ab68695c3900707aad51,557058:f87e2651-5132-4aed-8b96-f608ee969435,6053b70081b82500685dc4cb,611be44c883cef0077c90366,5ec586a199d9ab0c40159897,6053a53945a3bb006816a1a7,6053ae19695c3900707acb91,5a1c7af2b87a44359db0e7dc,6053a60cf180c300675e7e8b,5c6e33daa5f342215023f83e,62ec512f825fbfbfcff13ef5,6053a57390f288007008d5f2,6036b832f8c057007083c0e3,unknown,,,,,,,,,,,,,,,864000,288000,,0%,1152000,288000,,,SPL-241972,SPL-240927,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,24/May/23 8:28 PM;ajkumar;CapitalOneApr142023.svg;https://splunk.atlassian.net/rest/api/3/attachment/content/5871516,13/Apr/23 4:45 PM;rphillips;CapitalOne_3185241_flamegraph.svg;https://splunk.atlassian.net/rest/api/3/attachment/content/5650044,29/Jun/23 4:41 PM;rphillips;Itau_iteration2_stacks_flamegraph.svg;https://splunk.atlassian.net/rest/api/3/attachment/content/6033711,24/May/23 8:28 PM;ajkumar;May19CapitalOneFlamegraph.svg;https://splunk.atlassian.net/rest/api/3/attachment/content/5871511,05/Feb/24 12:23 PM;lhan;Screen Shot 2024-02-05 at 12.10.48 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6821007,09/Apr/23 12:08 PM;rphillips;Screenshot 2023-04-09 at 12.06.50 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5629661,23/Jun/23 2:58 PM;rphillips;Screenshot 2023-06-23 at 2.55.42 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6009691,23/Jun/23 2:58 PM;rphillips;Screenshot 2023-06-23 at 2.57.02 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6009692,29/Jun/23 4:41 PM;rphillips;Screenshot 2023-06-29 at 4.13.43 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/6033712,24/May/23 8:28 PM;ajkumar;captialone_may16_flamegraph.svg;https://splunk.atlassian.net/rest/api/3/attachment/content/5871517,05/Feb/24 12:23 PM;lhan;elosusbaws_captain (9).svg;https://splunk.atlassian.net/rest/api/3/attachment/content/6821008,06/Apr/23 4:56 PM;jweinstein;image-20230406-235443.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5619928,06/Apr/23 4:56 PM;jweinstein;image-20230406-235515.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5619929,24/May/23 8:28 PM;ajkumar;image-20230524-191813.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5871515,24/May/23 8:28 PM;ajkumar;image-20230524-192117.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5871507,24/May/23 8:28 PM;ajkumar;image-20230524-193846.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5871510,24/May/23 8:28 PM;ajkumar;image-20230524-200649.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5871519,24/May/23 8:28 PM;ajkumar;image-20230524-200751.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5871513,24/May/23 8:28 PM;ajkumar;image-20230524-201244.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5871509,24/May/23 8:28 PM;ajkumar;image-20230524-202715.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5871508,24/May/23 8:28 PM;ajkumar;image-20230524-203103.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5871514,24/May/23 8:28 PM;ajkumar;image-20230524-205300.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5871512,24/May/23 8:28 PM;ajkumar;image-20230524-213413.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5871518,26/May/23 5:43 PM;ajkumar;image-20230527-000443.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5881110,26/May/23 5:43 PM;ajkumar;image-20230527-001823.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5881111,26/May/23 5:43 PM;ajkumar;image-20230527-002438.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5881112,09/Jun/23 7:08 PM;ajkumar;image-20230610-005509.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5940194,09/Jun/23 7:08 PM;ajkumar;image-20230610-005717.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5940197,09/Jun/23 7:08 PM;ajkumar;image-20230610-005728.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5940195,09/Jun/23 7:08 PM;ajkumar;image-20230610-005749.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5940198,09/Jun/23 7:08 PM;ajkumar;image-20230610-020535.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5940193,21/May/24 1:54 PM;ajkumar;image-20240521-205406.png;https://splunk.atlassian.net/rest/api/3/attachment/content/7194898,11/May/23 4:51 PM;ajkumar;schwab_cron_use.csv;https://splunk.atlassian.net/rest/api/3/attachment/content/5766345,06/Apr/23 2:27 PM;rphillips;schwab_flamegraph.svg;https://splunk.atlassian.net/rest/api/3/attachment/content/5619366,12/Apr/23 5:11 PM;rphillips;user-prefs.conf.csv;https://splunk.atlassian.net/rest/api/3/attachment/content/5645117,24/May/23 8:27 PM;ajkumar;users_cron_capitalone.log;https://splunk.atlassian.net/rest/api/3/attachment/content/5871506,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@66b3824b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Performance,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,"Schwab, CapOne, Westpac, USbank, Progressiv",,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,43027200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mon May 13 16:17:51 UTC 2024,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Infrastructure - Scheduler,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,36.0,60.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ajkumar(ajkumar),ahong(ahong),dpaper(dpaper),hliu(hliu),jweinstein(jweinstein),ksabhanatarajan(ksabhanatarajan),kmanching(JIRAUSER49345),rk:7f6336e9-f1ca-4859-ba1b-edbfec48eba0(rk:7f6336e9-f1ca-4859-ba1b-edbfec48eba0),lhan(lhan),rphillips(rphillips),06ac125d-c228-4c3f-b77f-a6149c059c6f(06ac125d-c228-4c3f-b77f-a6149c059c6f),sylim(sylim),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Codefix,Feature/Usability Improvement,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Ajeet Kumar,5bc64d495b8ba5286cb9abf4,,,,,,,,,,,,,,,,,,0|ibkz0n:,,,,,,,"Feature/Usability Improvement:
With this fix, customers get choice to use cached timezone of user to reduce possibility of contention on scheduler hot path and improved scheduler cycle time.
Code fix:
Added a buffer/cache between scheduler and user conf system to sustain the contention in conf system (when/if they arise).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"An improvement has been made to scheduler performance by caching time zone information. As a side effect of this improvement, any changes to user time zone settings will take up to 150 minutes to reflect. During this time certain searches scheduled to run at a fixed cron schedule by that user will continue to use the previous time zone. An example of a fixed cron schedule is 0 6 * * * which runs the associated search at a fixed time of 6:00 AM every day.  Searches scheduled to run at a certain frequency, such as */5 * * * * (run the search every 5-minute frequency), do not have this side effect.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ajeet Kumar,5bc64d495b8ba5286cb9abf4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The scheduler hot path where scheduler schedules the saved searches can slow down when reading timezone for the users related to the saved searches. The slow down generally may occur when there are heavy read/edit on the user preferences (such as timezone of users).
",,,,Performance issue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Capital One Services, LLC",3185241,012400000005WzMAAU,P2,Yes,5005a00002T4VcVAAV,,Closed,Closed By Customer,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3156796,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SearchInfraScheduler-FY24Q2-S1,SearchInfraScheduler-FY24Q2-S2,SearchInfraScheduler-FY24Q2-S3,SearchInfraScheduler-FY24Q2-S4,SearchInfraScheduler-FY24Q2-S5,SearchInfraScheduler-FY24Q2-S6,SearchInfraScheduler-FY24Q3-S1,SearchInfraScheduler-FY24Q3-S2,SearchInfraScheduler-FY24Q3-S3,SearchInfraScheduler-FY24Q3-S4,SearchInfraScheduler-FY24Q3-S5,SearchInfraScheduler-FY24Q3-S7,SearchInfraScheduler-FY24Q3-S6,SearchInfraScheduler-FY24Q4-S1,SearchInfraScheduler-FY24Q4-S2,SearchInfraScheduler-FY24Q4-S3,SearchInfraScheduler-FY24Q4-S4,SearchInfraScheduler-FY24Q4-S5,SearchInfraScheduler-FY24Q4-S6,SearchInfraScheduler-FY25Q1-S1,SearchInfraScheduler-FY25Q1-S2,SearchInfraScheduler-FY25Q1-S3,SearchInfraScheduler-FY25Q1-S4,SearchInfraScheduler-FY25Q1-S5,SearchInfraScheduler-FY25Q1-S6,SearchInfraScheduler-FY25Q1-S7,SearchInfraScheduler-FY25Q2-S1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,,,,,,,,,,,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,On Prem Premium,,,,,,,,,,,,,,,,2023-04-06 23:58:51.545,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_3_*:*_20400498609_*|*_3_*:*_3_*:*_11989807984_*|*_5_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_1970288703_*|*_10039_*:*_1_*:*_440845641,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Run,,,,,,,,,,,,2023-04-06 23:58:51.545,2023-04-06 23:58:51.545,,,,,,Ajeet Kumar,5bc64d495b8ba5286cb9abf4,,,,,,"06/Apr/23 4:21 PM;5a1c7af2b87a44359db0e7dc;This customer has 12,023 users with local user-prefs set in user-prefs.conf
*96/s3147cdc-sh_-20230406-181638-2QlpSfR3/etc/users*$ find . -name user-prefs.conf | grep local | wc -l

12023","06/Apr/23 4:58 PM;6053b70081b82500685dc4cb;Note: due to the main problem seen being the conf system, this should be owned by the SH management team (discussed in slack).

Also since this has no fix in place in cloud, adding cloud version to possibly affected versions for awareness.",06/Apr/23 5:00 PM;6053b70081b82500685dc4cb;CCing [~accountid:6053a3fb81b82500685ced67] [~accountid:611be44c883cef0077c90366] [~accountid:6053ae19695c3900707acb91] ,06/Apr/23 5:14 PM;611be44c883cef0077c90366;[~accountid:6053b70081b82500685dc4cb] - Would upgrade to Icebreaker help here?,"06/Apr/23 5:19 PM;6053b70081b82500685dc4cb;[~accountid:611be44c883cef0077c90366]  in this case no, because the user-prefs.conf work Sanish did is not rolled out in cloud or to any customer now. ","06/Apr/23 10:09 PM;611be44c883cef0077c90366;I need some more context on this {{user-prefs.conf}} work. 

Would [~accountid:5ec586a199d9ab0c40159897] 's UI enhancements address any of this?","06/Apr/23 10:56 PM;6053b70081b82500685dc4cb;It’s possible it would. 

the use case or the main one for user-prefs is to store the time zone user prefers to use. But the problem is when scheduler hits this conf cache , it goes through updateIfDirty path and spends time updating caches. But scheduler uses  this time zone to also calculate the next run time of the search .

But it’s possible UI teams work might have reduced the updates or what’s stored in the conf system . This should be looked into. ",06/Apr/23 10:57 PM;6053b70081b82500685dc4cb;Certain customers like more prone to this than others . Schwab is head quartered in USA but people around the globe use same environment. Almost all the users have their own user-prefs ,"07/Apr/23 7:09 AM;5ec586a199d9ab0c40159897;Hi [~accountid:611be44c883cef0077c90366], my optimizations were w.r.t ui-prefs which is different than user-prefs",07/Apr/23 9:34 AM;611be44c883cef0077c90366;cc: [~accountid:6053a3fb81b82500685ced67]  to triage and assign appropriately.,"07/Apr/23 10:05 AM;6053a3fb81b82500685ced67;This seems a typical issue where loading conf causes the delay.

This issue can be mitigated by loading the conf asynchronously. ",07/Apr/23 10:39 AM;611be44c883cef0077c90366;[~accountid:6053a3fb81b82500685ced67]  - Are there mechanisms in place to load conf asynchronously? I know [~accountid:6053b70081b82500685dc4cb] implemented something for getSavedSearches. Is that relevant here?,"07/Apr/23 10:44 AM;6053b70081b82500685dc4cb;[~accountid:611be44c883cef0077c90366] , the approach taken for {{SavedSearchFetcher}} could definitely be applied here. In this case, the open question is the size of the cache for this conf type, and the frequency. It is likely changing of timezone for a user is very infrequent. But this is just a guess on my part. (maybe they travel a lot 😄 )","07/Apr/23 12:17 PM;611be44c883cef0077c90366; [~accountid:6053b70081b82500685dc4cb] -  Good point and I agree. The user timezone shouldn't have such frequent updates. 

I’m also trying to get some clarity on [~accountid:6053a3fb81b82500685ced67]'s proposed fix on async update and the recent changes w.r.t SavedSearchFetcher.  It looks like the current proposal could affect the {{UserPreferences::getTimezone(u)}}which has a pretty broad footprint. 

We should definitely socialize the proposal with the broader eng. folks before attempting it.","09/Apr/23 12:08 PM;5a1c7af2b87a44359db0e7dc;[~accountid:6053b70081b82500685dc4cb] From our last call you had recommended possibly having the customer re-assign a large amount of those scheduled searches to the “nobody” user. We would be risking throwing out the RBAC already in place for the users owning those searches (ie: which indexes can be searched , search filters, etc). This is no small amount of searches since they have nearly 7500 scheduled searches configured at the top of the hour.



!Screenshot 2023-04-09 at 12.06.50 PM.png|width=1584,height=248!","09/Apr/23 12:46 PM;6053b70081b82500685dc4cb;[~accountid:5a1c7af2b87a44359db0e7dc] , i think first the best would be to only do a small portion. Maybe a few hundered of those searches , and see if the problem lessens in severity. 

CC [~accountid:6053a3fb81b82500685ced67]  [~accountid:611be44c883cef0077c90366]  , this is active customer case with user-prefs.conf issue","09/Apr/23 3:12 PM;5a1c7af2b87a44359db0e7dc;Ok I will work with the customer to get some of those savedsearches reassigned to a new owner “nobody”. 



{noformat}curl -k -u admin:pass https://localhost:8089/servicesNS/{user}/{app}/saved/searches/{search%20name}/acl -d owner={user} -d sharing=app{noformat}","09/Apr/23 4:11 PM;5a1c7af2b87a44359db0e7dc;[~accountid:6053b70081b82500685dc4cb] I am also noticing another customer Capital One where a similar issue is present and they too have a large amount of local user-prefs. 
case: 3185241. I will collect pstacks from the captain and link that case to this Jira as well.


*$SPLUNK_HOME/etc/users*$  find . -name user-prefs.conf | grep local | wc -l
11597",11/Apr/23 6:16 PM;5a1c7af2b87a44359db0e7dc;[~accountid:6053b70081b82500685dc4cb] I’ve gotten pushback from Schwab over re-assigning searches to the nobody user due to security concerns around the lack of role based access controls so that is not a feasible workaround at this time.,"12/Apr/23 10:13 AM;611be44c883cef0077c90366;[~accountid:5a1c7af2b87a44359db0e7dc]  - Do we know why we have such frequent updates to user-prefs.conf ?  In other words, it’d be good to know : 

# Who is updating user-prefs.conf ?
# How frequently it is getting updated ?
# What field is getting updated?

One might be able to use some file watcher ( / strace tool ) to get answers to 2) and 3) and  then based on that figure out 1).","12/Apr/23 5:11 PM;5a1c7af2b87a44359db0e7dc;[~accountid:611be44c883cef0077c90366] 
The SHC captain diag was generated right after the pstack collection:

*conf.log earlist event:*
{""datetime"":""{color:#ff991f}03-13-2023 03:00:11.913 -0400{color}"",""log_level"":""INFO"",""component"":""ConfOp"",""data"":{""task"":""pullFrom"",""from_repo"":""[https://s3137bdc.cdc.schwab.com:8089"",""to_repo"":""https://s3147cdc.cdc.schwab.com:8089"",""to_repo_change_count"":20022,""asset_uri"":[""svc.emost_rrbus_alt"",""SCH_METSLoggingTierSearchApp_app"",""lookups"",""SRAlertThresholdRaw.csv""],""status"":""applied"",""op_id"":""559c0ea130847344c88380a193d4d9d43cade8d5"",""applied_at"":1678690811,""asset_id"":""19b58b8084b1b690a2daab8e7a42e207111a6b65"",""optype"":7,""optype_desc"":""NOTIFY_UPDATE_LOOKUP"",""payload"":{""value"":""$SPLUNK_HOME/var/run/splunk/lookup_tmp/559c0ea130847344c88380a193d4d9d43cade8d5""}}}|https://s3137bdc.cdc.schwab.com:8089%22,%22to_repo%22:%22https://s3147cdc.cdc.schwab.com:8089%22,%22to_repo_change_count%22:20022,%22asset_uri%22:%5B%22svc.emost_rrbus_alt%22,%22SCH_METSLoggingTierSearchApp_app%22,%22lookups%22,%22SRAlertThresholdRaw.csv%22%5D,%22status%22:%22applied%22,%22op_id%22:%22559c0ea130847344c88380a193d4d9d43cade8d5%22,%22applied_at%22:1678690811,%22asset_id%22:%2219b58b8084b1b690a2daab8e7a42e207111a6b65%22,%22optype%22:7,%22optype_desc%22:%22NOTIFY_UPDATE_LOOKUP%22,%22payload%22:%7B%22value%22:%22$SPLUNK_HOME/var/run/splunk/lookup_tmp/559c0ea130847344c88380a193d4d9d43cade8d5%22%7D%7D%7D]

*conf.log latest event:*
{""datetime"":""{color:#ff991f}04-05-2023 13:11:23.565 -0400{color}"",""log_level"":""INFO"",""component"":""ConfOp"",""data"":{""task"":""acceptPush"",""from_repo"":""[https://s3137bdc.cdc.schwab.com:8089"",""to_repo"":""https://s3147cdc.cdc.schwab.com:8089"",""to_repo_change_count"":20029,""asset_uri"":[""nobody"",""SCH_DDOS_app"",""lookups"",""cred_compromise_after_successful_logon.csv""],""status"":""applied"",""op_id"":""511a73010219261b75af5db07ef66a5017a6011c"",""applied_at"":1680714683,""asset_id"":""51ad04d61260e6b3f60004b6ec36f0768b5ef4f0"",""optype"":7,""optype_desc"":""NOTIFY_UPDATE_LOOKUP"",""payload"":{""value"":""$SPLUNK_HOME/var/run/splunk/lookup_tmp/511a73010219261b75af5db07ef66a5017a6011c""}}}|https://s3137bdc.cdc.schwab.com:8089%22,%22to_repo%22:%22https://s3147cdc.cdc.schwab.com:8089%22,%22to_repo_change_count%22:20029,%22asset_uri%22:%5B%22nobody%22,%22SCH_DDOS_app%22,%22lookups%22,%22cred_compromise_after_successful_logon.csv%22%5D,%22status%22:%22applied%22,%22op_id%22:%22511a73010219261b75af5db07ef66a5017a6011c%22,%22applied_at%22:1680714683,%22asset_id%22:%2251ad04d61260e6b3f60004b6ec36f0768b5ef4f0%22,%22optype%22:7,%22optype_desc%22:%22NOTIFY_UPDATE_LOOKUP%22,%22payload%22:%7B%22value%22:%22$SPLUNK_HOME/var/run/splunk/lookup_tmp/511a73010219261b75af5db07ef66a5017a6011c%22%7D%7D%7D]

No writes to user-prefs were recorded in conf.log over that time period (03-13-2023 03:00:11.913 -0400 - 04-05-2023 13:11:23.565 -0400) so it does not appear that frequent updates are made

*splunkbot search:*

{noformat}index=0014000000KByh4AAD case_number=3156796 host IN (*) case_file IN (*) source=*conf.log* component=ConfOp ""data.optype_desc""=WRITE_STANZA user-prefs{noformat}

*splunk search:*

{noformat}index=_internal host IN(*sh*) sourcetype=splunkd_conf component=ConfOp user-prefs  ""data.optype_desc""=WRITE_STANZA  | timechart count by host{noformat}

however we see several deployer pushes per day which is standard for this customer:
conf.log
{""datetime"":""04-04-2023 14:36:46.837 -0400"",""log_level"":""INFO"",""component"":""ConfDeployment"",""data"":{""task"":""downloadDeployableApps"",""source"":""[https://splunkdeployer.schwab.com:8089"",""apps"":[{""name"":""00-CORE-SH"",""action"":""matched"",""checksum"":""5a2df3406e4ca94309f6396734d3920dbab63a04""},{""name"":""ELI_Compliance"",""action"":""matched"",""checksum"":""a2a38dd14a546c1de3dc8d84cd233e16ce3e8ee5""},{""name"":""SCH_AIMS_app"",""action"":""matched"",""checksum"":""e3c94612631b1d5701176be050a726b17be4c005""},{""name"":""SCH_APMSearch_app"",""action"":""matched"",""checksum"":""e5d39d0edfbeeddb64267f86e4ef8e8b11f38bdd""},{""name"":""SCH_ARCHIBUS_app"",""action"":""matched"",""checksum"":""e89fd92ad9ec8902b23c05836fce3b586ae0cec9""},{""name"":""SCH_ASD_app"",""action"":""matched"",""checksum"":""361e4817c99fd6e898ba6e2d27f38c2788ded9ca""},{""name"":""SCH_AS_app"",""action"":""matched"",""checksum"":""4d793f7a805b5f99a969e8cbdb607f55b0fea605""},{""name"":""SCH_ATF_app"",""action"":""matched"",""checksum"":""a86401b701c6d98c3e0f7919aca1c96dda414c24""},{""name"":""SCH_ActionCenter_app"",""action"":""matched"",""checksum"":""d52f35a4f69961207f45196d2430ab0bc1fb7222""},{""name"":""SCH_Aerospike_app"",""action"":""matched"",""checksum"":""f4c79c46f68d0671ede0422cfbc6f466e259d4f4""},{""name"":""SCH_AppdynamicsSearchApp_app"",""action"":""matched"",""checksum"":""796c1a4e4f7d536ee0c140627c9bf25b9f561708""},{""name"":""SCH_Architecture_app"",""action"":""matched"",""checksum"":""ee5600bb81b08786244c640979a073833fbe465b""},{""name"":""SCH_Ariba_app"",""action"":""matched"",""checksum"":""29f9af764818133416fd3343d6b597e0d780c742""},{""name"":""SCH_AssetTransfer_app"",""action"":""matched"",""checksum"":""e881139cc8437d1d2a62d3f715162b4fb0b8371f""},{""name"":""SCH_|https://splunkdeployer.schwab.com:8089%22,%22apps%22:%5B%7B%22name%22:%2200-CORE-SH%22,%22action%22:%22matched%22,%22checksum%22:%225a2df3406e4ca94309f6396734d3920dbab63a04%22%7D,%7B%22name%22:%22ELI_Compliance%22,%22action%22:%22matched%22,%22checksum%22:%22a2a38dd14a546c1de3dc8d84cd233e16ce3e8ee5%22%7D,%7B%22name%22:%22SCH_AIMS_app%22,%22action%22:%22matched%22,%22checksum%22:%22e3c94612631b1d5701176be050a726b17be4c005%22%7D,%7B%22name%22:%22SCH_APMSearch_app%22,%22action%22:%22matched%22,%22checksum%22:%22e5d39d0edfbeeddb64267f86e4ef8e8b11f38bdd%22%7D,%7B%22name%22:%22SCH_ARCHIBUS_app%22,%22action%22:%22matched%22,%22checksum%22:%22e89fd92ad9ec8902b23c05836fce3b586ae0cec9%22%7D,%7B%22name%22:%22SCH_ASD_app%22,%22action%22:%22matched%22,%22checksum%22:%22361e4817c99fd6e898ba6e2d27f38c2788ded9ca%22%7D,%7B%22name%22:%22SCH_AS_app%22,%22action%22:%22matched%22,%22checksum%22:%224d793f7a805b5f99a969e8cbdb607f55b0fea605%22%7D,%7B%22name%22:%22SCH_ATF_app%22,%22action%22:%22matched%22,%22checksum%22:%22a86401b701c6d98c3e0f7919aca1c96dda414c24%22%7D,%7B%22name%22:%22SCH_ActionCenter_app%22,%22action%22:%22matched%22,%22checksum%22:%22d52f35a4f69961207f45196d2430ab0bc1fb7222%22%7D,%7B%22name%22:%22SCH_Aerospike_app%22,%22action%22:%22matched%22,%22checksum%22:%22f4c79c46f68d0671ede0422cfbc6f466e259d4f4%22%7D,%7B%22name%22:%22SCH_AppdynamicsSearchApp_app%22,%22action%22:%22matched%22,%22checksum%22:%22796c1a4e4f7d536ee0c140627c9bf25b9f561708%22%7D,%7B%22name%22:%22SCH_Architecture_app%22,%22action%22:%22matched%22,%22checksum%22:%22ee5600bb81b08786244c640979a073833fbe465b%22%7D,%7B%22name%22:%22SCH_Ariba_app%22,%22action%22:%22matched%22,%22checksum%22:%2229f9af764818133416fd3343d6b597e0d780c742%22%7D,%7B%22name%22:%22SCH_AssetTransfer_app%22,%22action%22:%22matched%22,%22checksum%22:%22e881139cc8437d1d2a62d3f715162b4fb0b8371f%22%7D,%7B%22name%22:%22SCH_] ...{skipping 24886 bytes}... ,""action"":""preserved""},{""name"":""learned"",""action"":""preserved""},{""name"":""search"",""action"":""preserved""},{""name"":""splunk_archiver"",""action"":""preserved""},{""name"":""splunk_gdi"",""action"":""preserved""},{""name"":""splunk_httpinput"",""action"":""preserved""},{""name"":""splunk_instrumentation"",""action"":""preserved""},{""name"":""splunk_internal_metrics"",""action"":""preserved""},{""name"":""splunk_metrics_workspace"",""action"":""preserved""},{""name"":""splunk_rapid_diag"",""action"":""preserved""},{{color:#ff5630}*""name"":""user-prefs"",""action"":""preserved""*{color}}]}}


From the captain diag the user-prefs.conf are privately owned so do not show up in btool output. 
Ive Splunk’d the etc/users/.../user-prefs/local/user-prefs.conf files so you can see the modtime & contents of the file by user:

most seem to be the [general] stanza with value: {{render_version_messages = 0}}

11945 total users with local user-prefs.conf

of those
only 6 entries do not have ""render_version_messages = 0""

and of the 11945; 7971 only have contents of :
[general]
render_version_messages = 0


[^user-prefs.conf.csv]


You can access the data here:

[http://10.202.2.222:8000/en-US/app/splunk_monitoring_console/schwab_userprefs_jira_spl238421|http://10.202.2.222:8000/en-US/app/splunk_monitoring_console/schwab_userprefs_jira_spl238421]

admin:5up3rn0va",13/Apr/23 10:43 AM;611be44c883cef0077c90366;cc: [~accountid:6053ac5ae394c30069cb0d7f]  - Can you kindly triage and assign an owner?,"13/Apr/23 4:45 PM;5a1c7af2b87a44359db0e7dc;linking Support case (*3185241*: CapitalOne: 8.2.8 : 40 node SHC: 11k users with local user-prefs.conf)

captain pstacks:
[https://downloadsvc.splunk.com/download/splunk/04-13-2023/uploadsvc-79case3185241-04-13-2023-USER-0035a00002vaKOtAAM-psplunkshcjzooec04_pstack_04132023.tar.xz|https://downloadsvc.splunk.com/download/splunk/04-13-2023/uploadsvc-79case3185241-04-13-2023-USER-0035a00002vaKOtAAM-psplunkshcjzooec04_pstack_04132023.tar.xz]



!CapitalOne_3185241_flamegraph.svg|thumbnail!",18/Apr/23 10:39 AM;6053a969695c3900707a9628;Updated the severity level from Sev-2 to Sev-3. Please refer to [P&T Customer Issue SLO|https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k/edit?usp=sharing]. ,24/Apr/23 5:50 PM;5a1c7af2b87a44359db0e7dc;[~accountid:6053b70081b82500685dc4cb] I’m trying to set some expectations with the customer. Per [P&T Customer Issue SLO|https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k/edit?usp=sharing] we should expect to have an update every 15 days for a Sev-3 which would be April 27 2023 and a Workaround in 8 weeks (June 7 2023). Are those reasonable asks given your understanding of this issue?,04/May/23 9:59 AM;60b9c12fa547eb0068683c27;[~accountid:6053b70081b82500685dc4cb] Capital One’s SFDC case associated to this bug has been escalated. Can we get some traction on a workaround fix? We have missed SLO’s on the SFDC support ticket and the JIRA-SPL ticket. ,"04/May/23 11:14 AM;5bc64d495b8ba5286cb9abf4;[~accountid:60b9c12fa547eb0068683c27] 

(CC: [~accountid:5a1c7af2b87a44359db0e7dc] ) 

Thanks for the updates. Looking into the Jira with Capital one first as priority. It seems there were some suggestions provided by [~accountid:6053b70081b82500685dc4cb]  regarding relieving the pressure by _moving the user scoped searches to ‘nobody’ so the scheduler does not have to find timezone for users._ This is keeping in the background that timezone does not matter for the searches that run at regular time interval like every minute.

As best practice, in a given system, the work should be derived from the perspective of the {{usecases}} being addressed. One of the good ways to logically group usecases is to scope them in different Apps (and not leave them with users scope). If you are concern about searches permissions, e.g., if you want to have the app to be available to certain ROLE only, by choosing right permissions/roles for the apps.

Other than above recommendations, I am looking into configurations settings to see if anything can be updated in this scenario. ","04/May/23 12:03 PM;5bc64d495b8ba5286cb9abf4;h2. Status Updates



Opened this slack channel:[https://splunk.slack.com/archives/C055VU3NMJT|https://splunk.slack.com/archives/C055VU3NMJT|smart-link] 

Per the discussion on slack:

Capital one upgraded their environment and were hoping to see the improvements and this is why the escalation.

Upgrade was done last week evening of *4/27 ET*

*Old version :* 8.2.8

*New version:* 9.0.4.1

While I am looking into the 8.2.8 diags/config for possible config fine tunings, it will be good to have latest diags and pstacks (on 9.x) if customer can provide, this way I can assess/validate if same code paths (timezone) are seen. Additionally it will help to see the latest configuration.

*Next Steps:*

Ajeet/Dev - Analysis of configuration and diags on 8.2.8 for any config improvements

Rob / Support - Have new diags and pstack from upgraded environment (9.0.4.1). Please post these to Jira and splunkbot.

                            Discuss with customer for assignment of user scoped saved searches to apps with permission.","11/May/23 11:36 AM;5a1c7af2b87a44359db0e7dc;[~accountid:5bc64d495b8ba5286cb9abf4] Nowhere in support case *3185241* was it mentioned to the customer that upgrading to 9.0.4.1 would improve the perf bottleneck seen in the captain pstack collection already done and reported in this Jira here (SPL-238421) so that should not have been the expectation of the customer here.

Per your request I have requested the following from the customer which they will gather once their site failvoer testing is completed (eta, next week)

1.) pstacks from the current shc captain for 30m period
[https://github.com/rephillips/debugging_splunk|https://github.com/rephillips/debugging_splunk|smart-link] 

2.) diag from current shc captain once pstack collection is done.

3.) shc log export:
you should have this view already on the MC and you can export the *.log panels to csv and attach to the case. May sure you set the time picker to the period over which the pstacks were running for that 30m period.
the view is called: support_shc_log_export

if you cannot find it you can re-create from the xml here:
[https://github.com/rephillips/dashboards/blob/master/support_shc_log_export.xml|https://github.com/rephillips/dashboards/blob/master/support_shc_log_export.xml|smart-link] ","11/May/23 12:42 PM;5bc64d495b8ba5286cb9abf4;Thanks [~accountid:5a1c7af2b87a44359db0e7dc] for your detailed updates and appreciate the background.

On my last comment for workaround or best practice:

{quote}
Discuss with customer for assignment of user scoped saved searches to apps with permission.{quote}

Just checking if there was any work from customer (CaptialOne side) to look into this topic.

For your reference, we had an ING Bank customer from SA who had also skip searches and similar symptoms seen on user pref side (timezone calls). The customer did bunch of cleanups and also moved user scoped searches to app scope. Might be a good read: [https://splunk.atlassian.net/browse/SPL-237637?focusedCommentId=11888321|https://splunk.atlassian.net/browse/SPL-237637?focusedCommentId=11888321|smart-link] ","11/May/23 4:51 PM;5bc64d495b8ba5286cb9abf4;Just noting down the analysis of how the cron settings are distributed keeping in mind the aggressive (more frequent cron schedule) should have more frequent calls to getTimeZone() for the associated users.

Schwab: 

$ pwd
schawb_SPL-238421/diag-s3147cdc-2023-04-05_13-08-46/etc/users

$find . -name ""saved*.conf"" -exec grep cron {} /dev/null \; > users_cron_schwab.log

I imported above log in a local splunk and downloaded the cron to count statistics as below.

[^schwab_cron_use.csv]

*Top ten lines:*

||cron setting||count of saved searches using this cron setting||
|0 * * * *|237|
|*/15 * * * *|201|
|*/5 * * * *|177|
|0 0 * * *|57|
|*/1 * * * *|56|
|*/10 * * * *|55|
|*/2 * * * *|54|
|0 6 * * 1|46|
|15 * * * *|44|

[~accountid:5a1c7af2b87a44359db0e7dc] Just wanted to check if the older diag from capital one (8.x) for captain is available somewhere. Let me know. The data is in splunkbot but I wanted to pull the cron info so untarring locally would be easier for me. I looked into SF case but could not find my way to the diag 🙂 

The intent is to check the table similar to Schwab I made above and see how the crons settings are spread.","15/May/23 4:14 PM;5a1c7af2b87a44359db0e7dc;[~accountid:5bc64d495b8ba5286cb9abf4] Here are the latest pstacks and diag from the Captial

captain diag: 

[https://downloadsvc.splunk.com/download/splunk/05-15-2023/uploadsvc-86case3185241-05-15-2023-USER-0035a00002vaKP1AAM-diag-psplunkshcjzoowa03-2023-05-15_17-26-56.tar.gz|https://downloadsvc.splunk.com/download/splunk/05-15-2023/uploadsvc-86case3185241-05-15-2023-USER-0035a00002vaKP1AAM-diag-psplunkshcjzoowa03-2023-05-15_17-26-56.tar.gz]

captain pstacks:

[https://downloadsvc.splunk.com/download/splunk/05-15-2023/uploadsvc-6case3185241-05-15-2023-USER-0035a00002vaKP1AAM-stacks-10926-psplunkshcjzoowa03-2023-05.tar|https://downloadsvc.splunk.com/download/splunk/05-15-2023/uploadsvc-6case3185241-05-15-2023-USER-0035a00002vaKP1AAM-stacks-10926-psplunkshcjzoowa03-2023-05.tar]

let me know if you still need older 8.x diag","15/May/23 4:20 PM;5a1c7af2b87a44359db0e7dc;[~accountid:5bc64d495b8ba5286cb9abf4]  thoughts on having the customer clean up:


the local user-prefs.conf setting : 
[general]
render_version_messages = 0



 of the 11945 local user-prefs.conf; 7971 only have render_version_messages = 0 as the content of that file. 

I will be out on PTO May17-Jun6 and [~accountid:6036b832f8c057007083c0e3] will look after the case for me","16/May/23 9:51 AM;6036b832f8c057007083c0e3;[~accountid:5bc64d495b8ba5286cb9abf4]   Further updates from cx, below are the matching logs with the diag & pstack from yesterday.

{noformat}I have been able to capture and upload the requested pstack/diag and all *.log csv 
from the support_shc_log_export dashboard.
Capture within 30mins period as requested from 12:50PM - 1:20Pm on 5/15/23, 
so as all the .log export fromt he dashboard.{noformat}

[https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-40case3185241-05-16-2023-USER-0035a00002vaKP1AAM-splunkd.log.csv|https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-40case3185241-05-16-2023-USER-0035a00002vaKP1AAM-splunkd.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-33case3185241-05-16-2023-USER-0035a00002vaKP1AAM-watchdog.log.csv|https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-33case3185241-05-16-2023-USER-0035a00002vaKP1AAM-watchdog.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-71case3185241-05-16-2023-USER-0035a00002vaKP1AAM-resource_usage.log.csv|https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-71case3185241-05-16-2023-USER-0035a00002vaKP1AAM-resource_usage.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-0case3185241-05-16-2023-USER-0035a00002vaKP1AAM-splunkd_access.log.csv|https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-0case3185241-05-16-2023-USER-0035a00002vaKP1AAM-splunkd_access.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-10case3185241-05-16-2023-USER-0035a00002vaKP1AAM-metrics.log.csv|https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-10case3185241-05-16-2023-USER-0035a00002vaKP1AAM-metrics.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-12case3185241-05-16-2023-USER-0035a00002vaKP1AAM-audit.log.csv|https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-12case3185241-05-16-2023-USER-0035a00002vaKP1AAM-audit.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-41case3185241-05-16-2023-USER-0035a00002vaKP1AAM-scheduler.log.csv|https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-41case3185241-05-16-2023-USER-0035a00002vaKP1AAM-scheduler.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-24case3185241-05-16-2023-USER-0035a00002vaKP1AAM-mongod.log.csv|https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-24case3185241-05-16-2023-USER-0035a00002vaKP1AAM-mongod.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-44case3185241-05-16-2023-USER-0035a00002vaKP1AAM-conf.log.csv|https://downloadsvc.splunk.com/download/splunk/05-16-2023/uploadsvc-44case3185241-05-16-2023-USER-0035a00002vaKP1AAM-conf.log.csv]","19/May/23 9:35 AM;6036b832f8c057007083c0e3;* Captain diag  :  [https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-29case3185241-05-19-2023-USER-0035a00002vaKP1AAM-diag-psplunkshcjzooeb01-2023-05-18_18-54-19.tar.gz|https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-29case3185241-05-19-2023-USER-0035a00002vaKP1AAM-diag-psplunkshcjzooeb01-2023-05-18_18-54-19.tar.gz] 
* Pstacks : [https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-75case3185241-05-19-2023-USER-0035a00002vaKP1AAM-stacks-9187-psplunkshcjzooeb01-2023-05-18.tar|https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-75case3185241-05-19-2023-USER-0035a00002vaKP1AAM-stacks-9187-psplunkshcjzooeb01-2023-05-18.tar]  
* Logs 

[https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-28case3185241-05-19-2023-USER-0035a00002vaKP1AAM-mongod.log.csv|https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-28case3185241-05-19-2023-USER-0035a00002vaKP1AAM-mongod.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-98case3185241-05-19-2023-USER-0035a00002vaKP1AAM-conf.log.csv|https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-98case3185241-05-19-2023-USER-0035a00002vaKP1AAM-conf.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-98case3185241-05-19-2023-USER-0035a00002vaKP1AAM-conf.log.csv|https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-98case3185241-05-19-2023-USER-0035a00002vaKP1AAM-conf.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-7case3185241-05-19-2023-USER-0035a00002vaKP1AAM-scheduler.log.csv|https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-7case3185241-05-19-2023-USER-0035a00002vaKP1AAM-scheduler.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-62case3185241-05-19-2023-USER-0035a00002vaKP1AAM-splunkd_access.log.csv|https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-62case3185241-05-19-2023-USER-0035a00002vaKP1AAM-splunkd_access.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-93case3185241-05-19-2023-USER-0035a00002vaKP1AAM-metrics.log.csv|https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-93case3185241-05-19-2023-USER-0035a00002vaKP1AAM-metrics.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-14case3185241-05-19-2023-USER-0035a00002vaKP1AAM-resources_usage.log.csv|https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-14case3185241-05-19-2023-USER-0035a00002vaKP1AAM-resources_usage.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-94case3185241-05-19-2023-USER-0035a00002vaKP1AAM-audit.log.csv|https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-94case3185241-05-19-2023-USER-0035a00002vaKP1AAM-audit.log.csv]

[https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-4case3185241-05-19-2023-USER-0035a00002vaKP1AAM-kvstore.log.csv|https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-4case3185241-05-19-2023-USER-0035a00002vaKP1AAM-kvstore.log.csv] 

[https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-15case3185241-05-19-2023-USER-0035a00002vaKP1AAM-watchdog.log.csv|https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-15case3185241-05-19-2023-USER-0035a00002vaKP1AAM-watchdog.log.csv] 

[https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-34case3185241-05-19-2023-USER-0035a00002vaKP1AAM-splunkd.log.csv|https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-34case3185241-05-19-2023-USER-0035a00002vaKP1AAM-splunkd.log.csv]

Here’s what Cx said about the info/diag collected: 

{noformat}I uploaded the new data capture due to the fact that, we were on a static captain when I captured the previously sent data and from what Rob said that might not be the ideal time for that. But the new data was captured while on dynamic captain.
there was no issue at that time, but I will forward a new screen shot of new findings from yesterday.{noformat}",24/May/23 9:26 AM;6053a969695c3900707a9628;Updated the severity level from Sev-3 to Sev-2. Please refer to the[ P&T Customer Issues SLO|https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k/edit?usp=sharing].,"24/May/23 8:28 PM;5bc64d495b8ba5286cb9abf4;[~accountid:6036b832f8c057007083c0e3] 

On Capital one diags, if I compare the skipping numbers between May 15 (diags from [Rob’s comment|https://splunk.atlassian.net/browse/SPL-238421?focusedCommentId=12247871]) and May 19 (latest diags from [this comment|https://splunk.atlassian.net/browse/SPL-238421?focusedCommentId=12291549]), the total skipping numbers are pretty low on May 19th (~3K) versus on May 15th (~13K) in regards to reason “[The maximum number of concurrent historical scheduled searches on this cluster has been reached|https://splunkbot.splunk.com/en-US/app/SplunkBOT/search?earliest=1684047600&latest=1684566000&q=search%20index%3D0014000000bs6ymaas%20case_number%3D3185241%20%0Asourcetype%3Dscheduler%20status%3D%22skipped%22%20search_type!%3D%22*_acceleration%22%0A%7C%20eval%20alert_actions%20%3D%20if(isnull(alert_actions)%20OR%20alert_actions%20%3D%3D%20%22%22%2C%20%22none%22%2C%20alert_actions)%0A%7C%20eval%20reason%20%3D%20if(isnull(reason)%20OR%20reason%20%3D%3D%20%22%22%2C%20%22none%22%2C%20reason)%0A%7C%20timechart%20count%20by%20reason&display.page.search.mode=fast&dispatch.sample_ratio=1&display.general.type=statistics&display.page.search.tab=statistics&sid=1684955833.468983_CF2F3A9D-AF66-47D0-8A01-00AF24F54675&display.statistics.sortColumn=The%20maximum%20number%20of%20concurrent%20historical%20scheduled%20searches%20on%20this%20cluster%20has%20been%20reached&display.statistics.sortDirection=desc#]”

Splunkbot search: [here|https://splunkbot.splunk.com/en-US/app/SplunkBOT/search?earliest=1684047600&latest=1684566000&q=search%20index%3D0014000000bs6ymaas%20case_number%3D3185241%20%0Asourcetype%3Dscheduler%20status%3D%22skipped%22%20search_type!%3D%22*_acceleration%22%0A%7C%20eval%20alert_actions%20%3D%20if(isnull(alert_actions)%20OR%20alert_actions%20%3D%3D%20%22%22%2C%20%22none%22%2C%20alert_actions)%0A%7C%20eval%20reason%20%3D%20if(isnull(reason)%20OR%20reason%20%3D%3D%20%22%22%2C%20%22none%22%2C%20reason)%0A%7C%20timechart%20count%20by%20reason&display.page.search.mode=fast&dispatch.sample_ratio=1&display.general.type=statistics&display.page.search.tab=statistics&sid=1684955833.468983_CF2F3A9D-AF66-47D0-8A01-00AF24F54675&display.statistics.sortColumn=The%20maximum%20number%20of%20concurrent%20historical%20scheduled%20searches%20on%20this%20cluster%20has%20been%20reached&display.statistics.sortDirection=desc]

{noformat}index=0014000000bs6ymaas case_number=3185241
sourcetype=scheduler status=""skipped"" search_type!=""*_acceleration""
| eval alert_actions = if(isnull(alert_actions) OR alert_actions == """", ""none"", alert_actions)
| eval reason = if(isnull(reason) OR reason == """", ""none"", reason)
| timechart count by reason{noformat}

!image-20230524-191813.png|width=83.33333333333334%!

!image-20230524-192117.png|width=25%!

On May 18th date, looks like skipping is occurring more around 8AM time (seems morning traffic)

!image-20230524-193846.png|width=83.33333333333334%!



{quote}Also customer noted:
{{I uploaded the new data capture due to the fact that, we were on a static captain when I captured the previously sent data and from what Rob said that might not be the ideal time for that. But the new data was captured while on dynamic captain. there was no issue at that time, but I will forward a new screen shot of new findings from yesterday.}}{quote}

On this I wanted to check if customer is no longer seeing the issue (might be related to the graph I put above)?



Revisiting the pstacks 

Pstacks analysis May 15th (File [https://downloadsvc.splunk.com/download/splunk/05-15-2023/uploadsvc-6case3185241-05-15-2023-USER-0035a00002vaKP1AAM-stacks-10926-psplunkshcjzoowa03-2023-05.tar|https://downloadsvc.splunk.com/download/splunk/05-15-2023/uploadsvc-6case3185241-05-15-2023-USER-0035a00002vaKP1AAM-stacks-10926-psplunkshcjzoowa03-2023-05.tar])

!captialone_may16_flamegraph.svg|width=50%!

Prominent activities are:



# Conf contention while getting a *users timezone* for a saved search

!image-20230524-200649.png|width=50%!

2. Conf contention while validating if user has permissions to run the given scheduled search 

!image-20230524-200751.png|width=1173,height=511!

3. Conf contention on saving the edit on a saved search.  (more prominent than above 2)

!image-20230524-201244.png|width=1174,height=550!



Pstacks analysis May 19th (File: [+https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-75case3185241-05-19-2023-USER-0035a00002vaKP1AAM-stacks-9187-psplunkshcjzooeb01-2023-05-18.tar+|https://downloadsvc.splunk.com/download/splunk/05-19-2023/uploadsvc-75case3185241-05-19-2023-USER-0035a00002vaKP1AAM-stacks-9187-psplunkshcjzooeb01-2023-05-18.tar])

!May19CapitalOneFlamegraph.svg|width=1200,height=594!

# Conf contention while validating if user has permissions to run the given scheduled search (seeing in May 15h also)
 

!image-20230524-202715.png|width=1173,height=562!

2. It seems the UserConfCacheSharing conf object has lots of foot print in the pstacks.

!image-20230524-203103.png|width=810,height=457!

On Capital One update:

Thanks for the .csv exported logs. 

The pstacks from do show that timezone related calls are seen after upgrade, too.

I looked into the diags [https://downloadsvc.splunk.com/download/splunk/05-15-2023/uploadsvc-86case3185241-05-15-2023-USER-0035a00002vaKP1AAM-diag-psplunkshcjzoowa03-2023-05-15_17-26-56.tar.gz|https://downloadsvc.splunk.com/download/splunk/05-15-2023/uploadsvc-86case3185241-05-15-2023-USER-0035a00002vaKP1AAM-diag-psplunkshcjzoowa03-2023-05-15_17-26-56.tar.gz] that [~accountid:5a1c7af2b87a44359db0e7dc] had it his [comment|https://splunk.atlassian.net/browse/SPL-238421?focusedCommentId=12247871].



Total Savedsearches within users scope

$ pwd
…/May16_2023/diag_captain/diag-psplunkshcjzoowa03-2023-05-15_17-26-56/etc/users


$ find . -name ""savedsearches.conf"" |wc -l
    4347

Total users:

$ ls |wc -l
   17330

$ find . -name ""user-prefs.conf""|wc -l
   11597



*Distribution of scheduled searches cron with respect of total users* 

$ find . -name ""savedsearches.conf"" -exec grep cron {} /dev/null \; > users_cron_capitalone.log

Total saved searches with cron setting = 2136

[^users_cron_capitalone.log]

 $wc -l users_cron_capitalone.log
    *2136* users_cron_capitalone.log

How these 2136 saved searches are having their cron settings.

E.g. 199 users have saved saved search with */5 * * * * setting (that is run search every 5 minute)

!image-20230524-205300.png|width=83.33333333333333%!

!image-20230524-213413.png|width=75%!

At present I am analyzing the code change so the timezone fetch for users can be cached , this way the scheduler does not always have to go to Conf layer (which is generally very busy) . The possible paths covered in this change:

# *Calculating the priority if saved searches* 

{{ SearchDispatcher::getPrioritizedSavedSearches}}

→ {{calcPriority}}



{noformat}TID 10882:
#0  0x00007ff5fcba154d __lll_lock_wait
#1  0x00007ff5fcb9ce9b _L_lock_883
#2  0x00007ff5fcb9cd68 pthread_mutex_lock
#3  0x000055668003f319 PthreadMutexImpl::lock()
#4  0x00005566810c910f ScopedIConfCacheLock::ScopedIConfCacheLock(BaseConfCache&)
#5  0x00005566810d8255 PropertyPages::getStanza(Str const&, PropertiesMap&)
#6  0x000055667fcb09e7 UserPreferences::getForRole(Role const&, PropertiesMap&)
#7  0x000055667fcb21fe UserPreferences::getForUser(User const&, PropertiesMap&)
#8  0x000055667fb24f5d LoggedInUserPreferences::set(User const&)
#9  0x000055667fb2aebf LoggedInUser::drop_lock_and_fill_preferences(LoggedInUserPreferences*, User const&)
#10 0x000055667fb2b010 LoggedInUser::Locked_getFor(User const&)
#11 0x000055667fb2bb3e LoggedInUser::getTimezoneFor(User const&)
#12 0x000055667fabcbdd SavedSplunk::tz() const
#13 0x000055667f8485b5 SearchDispatcher::calcPriority(SavedSplunk const&, long, SavedSearchHistory const*, UserCapabilitiesCache&, double, double, SavedSplunker::priority*)
#14 0x000055667f855a14 SearchDispatcher::getPrioritizedSavedSearches(long, long, std::set<SavedSearchSelector, std::less<SavedSearchSelector>, std::allocator<SavedSearchSelector> >, unsigned long)
#15 0x000055667f85ab70 SearchDispatcher::execute()
#16 0x000055667f8347ca SchedulerTimeout::when_expired(Interval*)
#17 0x000055668004800c TimeoutHeap::runExpiredTimeouts(MonotonicTime&)
#18 0x000055667ff84dd1 EventLoop::run()
#19 0x000055667f1907be SavedSplunker::main()
#20 0x000055668003feb7 Thread::callMain(void*)
#21 0x00007ff5fcb9aea5 start_thread
#22 0x00007ff5fc8c3b0d __clone{noformat}



*2. Getting next run time of a saved search*

{{SearchDispatcher::processScheduledSearch_locked}}

→ {{SearchDispatcher::advanceNextRunTime}}

{noformat}
TID 10882:
#0  0x00007ff5fcba154d __lll_lock_wait
#1  0x00007ff5fcb9ce9b _L_lock_883
#2  0x00007ff5fcb9cd68 pthread_mutex_lock
#3  0x000055668003f319 PthreadMutexImpl::lock()
#4  0x00005566810c910f ScopedIConfCacheLock::ScopedIConfCacheLock(BaseConfCache&)
#5  0x00005566810d8255 PropertyPages::getStanza(Str const&, PropertiesMap&)
#6  0x000055667fcb09e7 UserPreferences::getForRole(Role const&, PropertiesMap&)
#7  0x000055667fcb21fe UserPreferences::getForUser(User const&, PropertiesMap&)
#8  0x000055667fb24f5d LoggedInUserPreferences::set(User const&)
#9  0x000055667fb2aebf LoggedInUser::drop_lock_and_fill_preferences(LoggedInUserPreferences*, User const&)
#10 0x000055667fb2b010 LoggedInUser::Locked_getFor(User const&)
#11 0x000055667fb2bb3e LoggedInUser::getTimezoneFor(User const&)
#12 0x000055667fabcbdd SavedSplunk::tz() const
#13 0x000055667f848e0f SearchDispatcher::advanceNextRunTime(SavedSplunk const&, bool)
#14 0x000055667f8525a0 SearchDispatcher::processScheduledSearch_locked(SavedSplunk&, long, long, long, unsigned long&, unsigned long&, unsigned long, unsigned long, unsigned long, unsigned long, unsigned long, bool)
#15 0x000055667f853421 SearchDispatcher::processAllSearches_locked(SavedSplunker*, unsigned long*, unsigned long*, unsigned long*, long const&, long const&, SavedSplunker::limits*)
#16 0x000055667f85ad24 SearchDispatcher::execute()
#17 0x000055667f8347ca SchedulerTimeout::when_expired(Interval*)
#18 0x000055668004800c TimeoutHeap::runExpiredTimeouts(MonotonicTime&)
#19 0x000055667ff84dd1 EventLoop::run()
#20 0x000055667f1907be SavedSplunker::main()
#21 0x000055668003feb7 Thread::callMain(void*)
#22 0x00007ff5fcb9aea5 start_thread
#23 0x00007ff5fc8c3b0d __clone{noformat}



*3. Validate Arguments of an alert* 

{noformat}#14 0x000055667fabcbdd SavedSplunk::tz() const
#15 0x000055667f8105a2 SavedSearchAdminHandler::validateAlertArgs(SavedSplunk&) const
#16 0x000055667f825a76 SavedSearchAdminHandler::validateSavedSearch(SavedSplunk&) const
#17 0x000055667f827bea SavedSearchAdminHandler::handleEditCreate(ConfigInfo&, bool)
#18 0x000055667f829f6c SavedSearchAdminHandler::handleEdit(ConfigInfo&){noformat}



{noformat}TID 29728:
#0  0x00007ff5fcba154d __lll_lock_wait
#1  0x00007ff5fcb9ce9b _L_lock_883
#2  0x00007ff5fcb9cd68 pthread_mutex_lock
#3  0x000055668003f319 PthreadMutexImpl::lock()
#4  0x00005566810c9ca9 UserConfCacheSharing::searchSharedState(PropertyPagesContext const&, ConfPathTrie&, ConfPathTriePruner*)
#5  0x00005566810ca272 UserConfCache::updateConfCache()
#6  0x00005566810c8fa4 BaseConfCache::updateIfDirty()
#7  0x00005566810c9117 ScopedIConfCacheLock::ScopedIConfCacheLock(BaseConfCache&)
#8  0x00005566810d8255 PropertyPages::getStanza(Str const&, PropertiesMap&)
#9  0x000055667fcb1fe2 UserPreferences::getForUser(User const&, PropertiesMap&)
#10 0x000055667fb24f5d LoggedInUserPreferences::set(User const&)
#11 0x000055667fb2aebf LoggedInUser::drop_lock_and_fill_preferences(LoggedInUserPreferences*, User const&)
#12 0x000055667fb2b010 LoggedInUser::Locked_getFor(User const&)
#13 0x000055667fb2bb3e LoggedInUser::getTimezoneFor(User const&)
#14 0x000055667fabcbdd SavedSplunk::tz() const
#15 0x000055667f8105a2 SavedSearchAdminHandler::validateAlertArgs(SavedSplunk&) const
#16 0x000055667f825a76 SavedSearchAdminHandler::validateSavedSearch(SavedSplunk&) const
#17 0x000055667f827bea SavedSearchAdminHandler::handleEditCreate(ConfigInfo&, bool)
#18 0x000055667f829f6c SavedSearchAdminHandler::handleEdit(ConfigInfo&)
#19 0x000055667f8c4c95 MConfigHandler::executeHandler(ConfigInfo&)
#20 0x000055667f8d6d92 MConfigHandler::go(ConfigInfo&)
#21 0x000055667f8d7aae AdminManagerReplyDataProvider::go()
#22 0x000055667f9dd5a8 ServicesEndpointReplyDataProvider::rawHandle()
#23 0x000055667f9d046a RawRestHttpHandler::getPreBody(HttpServerTransaction*)
#24 0x000055667ffb3ef0 HttpThreadedCommunicationHandler::communicate(TcpSyncDataBuffer&)
#25 0x000055667f1ea717 TcpChannelThread::main()
#26 0x000055668003feb7 Thread::callMain(void*)
#27 0x00007ff5fcb9aea5 start_thread
#28 0x00007ff5fc8c3b0d __clone{noformat}

(Related stacks call from Apr 13 2023: 

!CapitalOneApr142023.svg|width=1200,height=578!

Schwab has most of the patterns in the {{getPrioritizedSavedSearches}} 

*Next steps:*

Dev/Ajeet: continue the code change analysis (summary provided above).

[~accountid:6036b832f8c057007083c0e3] After the dynamic captain switch, what is the overall customer’s feedback on the search skip/performance?","26/May/23 5:43 PM;5bc64d495b8ba5286cb9abf4;*Updates on research in progress:*

This update is keeping following question in mind:
*Question:* What are the possible areas that are making the conf cache dirty?  (and hence possibly keeping user pref not able to provide timezone)

Reference to May 15th Stacks from capitalone: 

!captialone_may16_flamegraph.svg|thumbnail!

+Possible Area1:+ Most likely one or more savedsearches updates are marking the cache dirty in the user conf area as customer has lot of savedsearches in users scope also

!image-20230527-000443.png|width=91.66666666666666%!

Related pstack:

{noformat}TID 15074:
#0  0x00007fc8687a754d __lll_lock_wait
#1  0x00007fc8687a2e9b _L_lock_883
#2  0x00007fc8687a2d68 pthread_mutex_lock
#3  0x000055ae5518a0e9 PthreadMutexImpl::lock()
#4  0x000055ae56256989 UserConfCacheSharing::markDirtyForAll()
#5  0x000055ae562567e4 take_lock_and_dirty_cache(BaseConfCache&)
#6  0x000055ae56267d26 PropertyPages::recomputeImpl(BundlesSetup const&, Str const*, StrSet const*)
#7  0x000055ae56268217 ScopedCacheUpdater::~ScopedCacheUpdater()
#8  0x000055ae562738f1 PropertyPages::saveStanzaToApp(Str const&, Str const&, PropertiesMap const&, bool, AtomTyper const&)
#9  0x000055ae5627399e PropertyPages::saveStanza(Str const&, PropertiesMap const&, bool, AtomTyper const&)
#10 0x000055ae55ada63f NewSavedSearchManager::Locked_save(SavedSplunk const&, bool, bool)
#11 0x000055ae55adacd0 NewSavedSearchManager::save(SavedSplunk const&, bool, bool)
#12 0x000055ae5488fd79 SavedSearchAdminHandler::saveOnEditCreate(SavedSplunk&, PropertyPagesContext const&, bool)
#13 0x000055ae548a570e SavedSearchAdminHandler::handleEditCreate(ConfigInfo&, bool)
#14 0x000055ae548a79dc SavedSearchAdminHandler::handleEdit(ConfigInfo&)
#15 0x000055ae54949575 MConfigHandler::executeHandler(ConfigInfo&)
#16 0x000055ae5495b73b MConfigHandler::go(ConfigInfo&)
#17 0x000055ae5495c5e4 AdminManagerReplyDataProvider::go()
#18 0x000055ae54a7a998 ServicesEndpointReplyDataProvider::rawHandle()
#19 0x000055ae54a6c78a RawRestHttpHandler::getPreBody(HttpServerTransaction*)
#20 0x000055ae550f5620 HttpThreadedCommunicationHandler::communicate(TcpSyncDataBuffer&)
#21 0x000055ae541777d7 TcpChannelThread::main()
#22 0x000055ae55189dbd Thread::_callMainAndDiscardTerminateException()
#23 0x000055ae5518acb2 Thread::callMain(void*)
#24 0x00007fc8687a0ea5 start_thread
#25 0x00007fc867e6bb0d __clone
{noformat}



Possible Area 2: Conf replication among SHs can possibly make User conf cache dirty. 

!image-20230527-001823.png|width=91.66666666666667%!

{noformat}TID 34511:
#0  0x00007fc8687a754d __lll_lock_wait
#1  0x00007fc8687a2e9b _L_lock_883
#2  0x00007fc8687a2d68 pthread_mutex_lock
#3  0x000055ae5518a0e9 PthreadMutexImpl::lock()
#4  0x000055ae56256989 UserConfCacheSharing::markDirtyForAll()
#5  0x000055ae562567e4 take_lock_and_dirty_cache(BaseConfCache&)
#6  0x000055ae56267d26 PropertyPages::recomputeImpl(BundlesSetup const&, Str const*, StrSet const*)
#7  0x000055ae562270c9 ScopedPropertyPagesBouncer::~ScopedPropertyPagesBouncer()
#8  0x000055ae5624862c ConfRepo::acceptPush(ConfRepoId const&, ConfOpId const&, ConfOpStorage const&)
#9  0x000055ae549cd505 ConfReplicationDataProvider::handleRepositories(ConfRepo&, ConfRepoId const&, ConfOpId const&)
#10 0x000055ae549d2209 ConfReplicationDataProvider::go()
#11 0x000055ae54a7a998 ServicesEndpointReplyDataProvider::rawHandle()
#12 0x000055ae54a6c78a RawRestHttpHandler::getPreBody(HttpServerTransaction*)
#13 0x000055ae550f5620 HttpThreadedCommunicationHandler::communicate(TcpSyncDataBuffer&)
#14 0x000055ae541777d7 TcpChannelThread::main()
#15 0x000055ae55189dbd Thread::_callMainAndDiscardTerminateException()
#16 0x000055ae5518acb2 Thread::callMain(void*)
#17 0x00007fc8687a0ea5 start_thread
#18 0x00007fc867e6bb0d __clone{noformat}

Another accetPush example - navigating to modular inputs 

(TBD - impact)

{noformat}TID 28516:
#0  0x00007fc8687a754d __lll_lock_wait
#1  0x00007fc8687a2e9b _L_lock_883
#2  0x00007fc8687a2d68 pthread_mutex_lock
#3  0x000055ae5518a0e9 PthreadMutexImpl::lock()
#4  0x000055ae561ee523 ApplicationManager::getDisabledAppNames(StrSet&)
#5  0x000055ae5627feaa SpecFiles::loadInputSchemeDefinitions(BundlesSetup const&)
#6  0x000055ae56280b58 SpecFiles::SpecFiles(BundlesSetup const&, SpecModifier*, bool, bool, bool)
#7  0x000055ae54ac81b4 ModularInputs::addMissingSchemes(StrVector&)
#8  0x000055ae54ac8c4a ModularInputs::reloadInputsForApps(StrSet const&, StrVector*, Str const&)
#9  0x000055ae54ff67f2 ApplicationUpdater::reloadConfsImpl(StrVector const&, std::map<Str, StrSet, std::less<Str>, std::allocator<std::pair<Str const, StrSet> > > const&, bool, StrVector*, StrSet*, StrSet*, Str const*) const
#10 0x000055ae54ff6c7e ApplicationUpdater::reloadConfsGlobalContext(StrVector const&, StrVector*)
#11 0x000055ae561eded8 AppUpdaterCallback::callback_reloadConfs(StrSet const&)
#12 0x000055ae562270d1 ScopedPropertyPagesBouncer::~ScopedPropertyPagesBouncer()
#13 0x000055ae5624862c ConfRepo::acceptPush(ConfRepoId const&, ConfOpId const&, ConfOpStorage const&)
#14 0x000055ae549cd505 ConfReplicationDataProvider::handleRepositories(ConfRepo&, ConfRepoId const&, ConfOpId const&)
#15 0x000055ae549d2209 ConfReplicationDataProvider::go()
#16 0x000055ae54a7a998 ServicesEndpointReplyDataProvider::rawHandle()
#17 0x000055ae54a6c78a RawRestHttpHandler::getPreBody(HttpServerTransaction*)
#18 0x000055ae550f5620 HttpThreadedCommunicationHandler::communicate(TcpSyncDataBuffer&)
#19 0x000055ae541777d7 TcpChannelThread::main()
#20 0x000055ae55189dbd Thread::_callMainAndDiscardTerminateException()
#21 0x000055ae5518acb2 Thread::callMain(void*)
#22 0x00007fc8687a0ea5 start_thread
#23 0x00007fc867e6bb0d __clone{noformat}

unlocking - just an example to see we are also unlocking (good part)

{noformat}TID 47522:
#0  0x00007fc8687a75ea __lll_unlock_wake
#1  0x00007fc8687a3f9e _L_unlock_738
#2  0x00007fc8687a3f10 __pthread_mutex_unlock
#3  0x000055ae5518a119 PthreadMutexImpl::unlock()
#4  0x000055ae5620c5cc ConfObjectManagerDB::lookupPathRecursive(ConfPath const&, ConfPathTrie&, unsigned long)
#5  0x000055ae5621223a ConfPath::cgetRecursiveImpl(ConfPathTrie&, unsigned long) const
#6  0x000055ae56212edd ConfPath::cgetRecursive(ConfPathTrie&, unsigned long) const
#7  0x000055ae56214273 ConfPath::cexistsImpl(bool) const
#8  0x000055ae56246230 ConfOp::applyChange(ConfRepo&)
#9  0x000055ae56247d6c ConfRepo::acceptPush_Locked(ConfRepoId const&, ConfOpId const&, ConfOpStorage const&)
#10 0x000055ae5624860e ConfRepo::acceptPush(ConfRepoId const&, ConfOpId const&, ConfOpStorage const&)
#11 0x000055ae549cd505 ConfReplicationDataProvider::handleRepositories(ConfRepo&, ConfRepoId const&, ConfOpId const&)
#12 0x000055ae549d2209 ConfReplicationDataProvider::go()
#13 0x000055ae54a7a998 ServicesEndpointReplyDataProvider::rawHandle()
#14 0x000055ae54a6c78a RawRestHttpHandler::getPreBody(HttpServerTransaction*)
#15 0x000055ae550f5620 HttpThreadedCommunicationHandler::communicate(TcpSyncDataBuffer&)
#16 0x000055ae541777d7 TcpChannelThread::main()
#17 0x000055ae55189dbd Thread::_callMainAndDiscardTerminateException()
#18 0x000055ae5518acb2 Thread::callMain(void*)
#19 0x00007fc8687a0ea5 start_thread
#20 0x00007fc867e6bb0d __clone{noformat}

!image-20230527-002438.png|width=100%!



{noformat}
TID 28523:
#0  0x00007fc8687a754d __lll_lock_wait
#1  0x00007fc8687a2e9b _L_lock_883
#2  0x00007fc8687a2d68 pthread_mutex_lock
#3  0x000055ae5518a0e9 PthreadMutexImpl::lock()
#4  0x000055ae56267f51 PropertyPages::getStanza(Str const&, PropertiesMap&)
#5  0x000055ae56261ec5 PathFromPropertyPages::init(Str const&, Pathname const&, PropertyPagesContext const&, bool)
#6  0x000055ae56262088 PathFromPropertyPages::PathFromPropertyPages(Str const&, Pathname const&, PropertyPagesContext const&, bool)
#7  0x000055ae549d0053 ConfReplicationDataProvider::prepareLookupDownload(BundlesSetup const&)
#8  0x000055ae549d21e9 ConfReplicationDataProvider::go()
#9  0x000055ae54a7a998 ServicesEndpointReplyDataProvider::rawHandle()
#10 0x000055ae54a6c78a RawRestHttpHandler::getPreBody(HttpServerTransaction*)
#11 0x000055ae550f5620 HttpThreadedCommunicationHandler::communicate(TcpSyncDataBuffer&)
#12 0x000055ae541777d7 TcpChannelThread::main()
#13 0x000055ae55189dbd Thread::_callMainAndDiscardTerminateException()
#14 0x000055ae5518acb2 Thread::callMain(void*)
#15 0x00007fc8687a0ea5 start_thread
#16 0x00007fc867e6bb0d __clone
{noformat}



The user conf cache is apparently a super set of user pref data also. So any changes to user conf cache, e.g. changes to saved searches that happen to reside in both app and user context can have high usage of user conf cache and possibly more locks, which is most likely leading to blocking  smaller more specific activities like getting timezone of a user. Further design research needs to be done to see following items:

#  If there are customer configurations that can be fine tuned – such as any aggressive saved searches update, or lookup updates – in nutshell reducing conf replications if possible. 
#  Design improvements - 
##  how to separate out the small unit of data in different caches so one cache contention does not impact other items 
##   the time zone for users should relatively be unchanged through out course of an users lifecycle so think about having  a separate store of users--> timezone mapping or other items in user pref conf.

*Next Steps:* 

Dev/Ajeet: Investigate further such as mapping the pstacks activities with diags – try to scope down whether specific activities are causing user pref constraints.  continue on code research as summarized above.

Support / [~accountid:6036b832f8c057007083c0e3]  - update on the current state of capital one environment. same question I asked in next steps of my [previous comment|https://splunk.atlassian.net/browse/SPL-238421?focusedCommentId=12344614]",26/May/23 6:08 PM;6036b832f8c057007083c0e3;Haven’t received any feedback for that yet. Will check on Tue and update here.Thanks [~accountid:5bc64d495b8ba5286cb9abf4] ,"07/Jun/23 4:15 PM;5a1c7af2b87a44359db0e7dc;[~accountid:5bc64d495b8ba5286cb9abf4] I have asked the customer if they have seen any difference in skipped search in static vs dynamic captaincy and waiting for a response, although I do not see how that would make a difference.

Following up on my previous comment on your thoughts on having the customer clean up:


the local user-prefs.conf setting : 
[general]
render_version_messages = 0

of the 11945 local user-prefs.conf; 7971 only have render_version_messages = 0 as the content of that file.","09/Jun/23 7:08 PM;5bc64d495b8ba5286cb9abf4;(This is for capital one and my research is from May 15th diags - so not mix with Schwab 🙂 )

[~accountid:5a1c7af2b87a44359db0e7dc] thanks for the updates and question around {{render_version_messages}}

This property should impact if there are frequent changes made to the user-prefs.conf file (e.g user turning off the version message from UI). But looking into the conf updates for various artifacts, I am not seeing that there are frequent conf updates for user-prefs.conf.

The graph for all conf types is below from May 15th diag 

Also you can refer to this dashboard in splunkbot : [Capital one triage dashboard|https://splunkbot.splunk.com/en-US/app/workspace/capitalone_spl238421?form.field1.earliest=1684090800&form.field1.latest=1684208901.042&form.spaninput=30s&form.customerhostinfo=index%3D0014000000bs6ymaas%20case_number%3D3185241%20%20host%3Dpsplunkshcjzoowa03]

Here I see mostly savedsearches updates increasing around 10, 12 and 2PM . I don’t find user-pref.conf.

!image-20230610-005509.png|width=100%!

user-prefs updates from access log is given below:

There are no events during May 15th or later. It surprise me but if it is true than user-prefs is not coming into picture so the render_version_message would not have impact.

{noformat}index=0014000000bs6ymaas case_number=3185241
sourcetype=splunkd_access OR sourcetype=splunkd_ui_access user ""*user-prefs*""{noformat}

!image-20230610-020535.png|width=1753,height=441!



Now if we go back to the search skipping issue. What I find is that the the delegated_scheduled numbers are higher (reaching to more 1500 per 30 second time  (refer to splunkbot dashboard [Capital one triage dashboard|https://splunkbot.splunk.com/en-US/app/workspace/capitalone_spl238421?form.field1.earliest=1684090800&form.field1.latest=1684208901.042&form.spaninput=30s&form.customerhostinfo=index%3D0014000000bs6ymaas%20case_number%3D3185241%20%20host%3Dpsplunkshcjzoowa03]) 

See the skips spiking

!image-20230610-005717.png|width=83.33333333333334%!

We see around around 9AM (when spikes are high), the scheduler delegate metrics is higher. That indicates that the captain is having slow response time from SH members to accept the dispatched searches (and scheduled searches are backing up)

!image-20230610-005728.png|width=83.33333333333334%!

Another similar thing we we on how the searches scheduled time is lagging behind current time (_time)



!image-20230610-005749.png|width=83.33333333333334%!

These data statistics look similar to the Dell issue [https://splunk.atlassian.net/browse/SPL-238165|https://splunk.atlassian.net/browse/SPL-238165|smart-link] whose fix is address in this issue [https://splunk.atlassian.net/jira/software/c/projects/SPL/issues/SPL-238239|https://splunk.atlassian.net/jira/software/c/projects/SPL/issues/SPL-238239] (its release still needs to be finalizes as per my knowledge)

Another recommendation is from this comment from Dell’s ticket [https://splunk.atlassian.net/browse/SPL-238165?focusedCommentId=11683758|https://splunk.atlassian.net/browse/SPL-238165?focusedCommentId=11683758|smart-link] :

Set {{precalculate_required_fields_for_alerts = false}} for all savedsearches.

If you are able to collect pstack from a SH where the Search performances are generally low, we can cross check where the delays are coming from {{precalculate_required_fields_for_alerts}} related logic or not. 

We can recheck the above analysis if capital one can give use new set of data with their current feedback on the search scheduling and skipping.

# Diag from last three captains (current captain and previous two captains) – assuming captaincy is switching if not last captain data is fine.
# Diag from one of the SH members (a SH that has not been captain in the last 3-5 weeks)
# Diag from one of the indexers (this is to check if search latency are coming from indexer side at all, just in case)
# pstack from captain and a SH when customer generally sees search skip issue (if there is a time of day consistently the skipping come ups, choose that time).","21/Jun/23 2:27 PM;5bc64d495b8ba5286cb9abf4;Status Update

Based on my sync -up with [~accountid:5a1c7af2b87a44359db0e7dc] here are the updates:

# The Schwab SF case has been already closed -  customer has gone through configuration updates/fine-tuning (such as allow_skew).
# The captial one issue is still open - the open item on this area is to address the code enhancement for reducing timezone reading for user’s scoped saved searches - this Jira [https://splunk.atlassian.net/browse/SPL-240927|https://splunk.atlassian.net/browse/SPL-240927|smart-link]  is open for research and further actions.



Next Steps:

Dev - Continue work on [https://splunk.atlassian.net/browse/SPL-240927|https://splunk.atlassian.net/browse/SPL-240927|smart-link] 

Support - Rob will update Jira after code change is provided.","23/Jun/23 2:58 PM;5a1c7af2b87a44359db0e7dc;[~accountid:5bc64d495b8ba5286cb9abf4] 
I had a meeting with the customer yesterday. Some changes have occured since this Jira was filed:


1.) The Zoo SHC which was previously serving adhoc users and scheduled search has been split into 2 SHCs. 
1 for only ad-hoc search (18 members)

1 for only scheduled search (46 members) (these nodes are not exposed to user base through load balancer pool, so no users logging into these nodes)

The SHC for scheduled search is the cluster we are dealing with and has 46 members. 

Scheduled search distribution has been evened out a bit better than before although we still see high scheduled concurrency at top /bottom of the hour.


captain diag (generated after pstack collection finished):

[https://downloadsvc.splunk.com/download/splunk/06-22-2023/uploadsvc-65case3185241-06-22-2023-USER-0035a00002vaKP1AAM-diag-psplunkshcjzooeb03-2023-06-22_19-24-31.tar.gz|https://downloadsvc.splunk.com/download/splunk/06-22-2023/uploadsvc-65case3185241-06-22-2023-USER-0035a00002vaKP1AAM-diag-psplunkshcjzooeb03-2023-06-22_19-24-31.tar.gz]

captain pstacks (ran from 2:55pm - 3:20pm EST) :
[https://downloadsvc.splunk.com/download/splunk/06-22-2023/uploadsvc-16case3185241-06-22-[…]2-psplunkshcjzooeb03-2023-06-22T18h54m54s063883230.tar.xz|https://downloadsvc.splunk.com/download/splunk/06-22-2023/uploadsvc-16case3185241-06-22-2023-USER-0035a00002vaKP1AAM-stacks-6552-psplunkshcjzooeb03-2023-06-22T18h54m54s063883230.tar.xz]

scheduler dashboard pdf (last 60m view generated after pstack collection finished so 3pm would be the hot minute period) :
[https://downloadsvc.splunk.com/download/splunk/06-22-2023/uploadsvc-4case3185241-06-22-2[…]035a00002vaKP1AAM-support_template_scheduler-20230622.pdf|https://downloadsvc.splunk.com/download/splunk/06-22-2023/uploadsvc-4case3185241-06-22-2023-USER-0035a00002vaKP1AAM-support_template_scheduler-20230622.pdf]


dashboard pdf is relevant as it was generated after pstack collection. pstack ran 6/22/2023 from 2:55pm EDT - 3:20pm PDT


We do see a few minutes where the captain has no delegations:

!Screenshot 2023-06-23 at 2.55.42 PM.png|width=1133,height=690!



Scheduled Search Distribution:

!Screenshot 2023-06-23 at 2.57.02 PM.png|width=1096,height=565!","29/Jun/23 4:41 PM;5a1c7af2b87a44359db0e7dc;[~accountid:5bc64d495b8ba5286cb9abf4] I believe we are seeing same issue on another customer (Itau Bank, case: *3249767*) on 9.0.4 and we have pstacks collected during scheduler lock:

captain diag: [https://downloadsvc.splunk.com/download/splunk/06-29-2023/uploadsvc-84case3249767-06-29-2023-USER-0035a00002pHZ73AAG-diag-ip-172-19-226-75.sa-east-1.compute.internal-2023-06-29_15-41-29.tar|https://downloadsvc.splunk.com/download/splunk/06-29-2023/uploadsvc-84case3249767-06-29-2023-USER-0035a00002pHZ73AAG-diag-ip-172-19-226-75.sa-east-1.compute.internal-2023-06-29_15-41-29.tar]

pstacks: [https://downloadsvc.splunk.com/download/splunk/06-29-2023/uploadsvc-84case3249767-06-29-2023-USER-0034000001ZWPE6AAP-itau_pstacks_iteration2.zip|https://downloadsvc.splunk.com/download/splunk/06-29-2023/uploadsvc-84case3249767-06-29-2023-USER-0034000001ZWPE6AAP-itau_pstacks_iteration2.zip]



!Itau_iteration2_stacks_flamegraph.svg|thumbnail!

!Screenshot 2023-06-29 at 4.13.43 PM.png|width=1209,height=858!

Perhaps you can use this since we are having a difficult time catching the issue in CapitalOne’s case (*3185241*)","06/Jul/23 10:56 AM;5a1c7af2b87a44359db0e7dc;[~accountid:5bc64d495b8ba5286cb9abf4] [~accountid:6053ac5ae394c30069cb0d7f] Can we please get an update on where dev is with this JIRA? We have 3 large customers hitting this issue with no workaround and likely more which have yet to be discovered. 

cc [~accountid:6053b70081b82500685dc4cb] ","10/Jul/23 11:56 AM;5bc64d495b8ba5286cb9abf4;[~accountid:5a1c7af2b87a44359db0e7dc] 

Thanks for the follow-up. I was in PTO for last two weeks, so started to reconvene on this Jira today. 

As you mentioned, I also see that the Itau bank pstacks have getTimezone related contention so it might be a good candidate for the assessing the design improvements. I will keep pstacks as part of research.

Does Itau Bank has scheduler issue? ",10/Jul/23 5:37 PM;5a1c7af2b87a44359db0e7dc;[~accountid:5bc64d495b8ba5286cb9abf4] Yes the scheduler for Itau is impacted and stops delegating searches for some period of time ~5-10m. The pstacks were collected during this time of scheduler lock.,"12/Jul/23 3:15 PM;5bc64d495b8ba5286cb9abf4;[~accountid:5a1c7af2b87a44359db0e7dc] 

I am getting statistics of number of users who may have saved searches from the three customers attached to this Jira. I see that etc/users folders in itau_bank_3249767 diag is empty. Details in the info below. What do you say? As part of code enhancement I am pushing things in user → timezone cache so wanted to get some estimate of how big the cache should be. 

+Schwab -+ 

Total users: 12290

(datf_3810_be) C02X60GPJGH5:users ajkumar$ ls |wc -l
   12290

Total saved searches within users folder:1818

(datf_3810_be) C02X60GPJGH5:users ajkumar$ find . -name ""savedsearches.conf"" | wc -l
    1818

Total user that have saved searches

(datf_3810_be) C02X60GPJGH5:users ajkumar$ find . -name ""savedsearches.conf"" | cut -d  / -f 2| sort -u| wc -l
    1298

+Captial One -+

Total users:    17396

(datf_3810_be) C02X60GPJGH5:users ajkumar$ ls |wc -l
   17396

Total saved searches within users folder:4360

(datf_3810_be) C02X60GPJGH5:users ajkumar$  find . -name ""savedsearches.conf"" | wc -l
    4360

Total users that have saved searches: 3247

(datf_3810_be) C02X60GPJGH5:users ajkumar$ find . -name ""savedsearches.conf"" | cut -d  / -f 2| sort -u| wc -l
    3247

Itau Bank - Found no users - so wanted to check if this is right diag.

/Users/ajkumar/sustaining/schawb_SPL-238421/itau_bank_3249767/diag/diag-ip-172-19-226-75.sa-east-1.compute.internal-2023-06-29_15-41-29/etc
(datf_3810_be) C02X60GPJGH5:etc ajkumar$ cd users
(datf_3810_be) C02X60GPJGH5:users ajkumar$ ls",13/Jul/23 5:16 PM;5a1c7af2b87a44359db0e7dc;[~accountid:5bc64d495b8ba5286cb9abf4] itau likely filters out etc/users from the diag which is why you see it empty. The 13K mark sounds about right for Schwab but I cannot say they are the customer with the largest user base. ,26/Jul/23 10:25 AM;5bc64d495b8ba5286cb9abf4;[~accountid:5a1c7af2b87a44359db0e7dc] Just wanted to update that the spike/research work on the solution approach is coming to some viable solutions. I will share the details and next step to get the chosen solution implemented.,03/Aug/23 10:03 AM;5bc64d495b8ba5286cb9abf4;Code changes has started in this MR: [https://cd.splunkdev.com/splcore/main/-/merge_requests/62189|https://cd.splunkdev.com/splcore/main/-/merge_requests/62189],"22/Aug/23 9:05 AM;5bc64d495b8ba5286cb9abf4;Raised the MR for this Jira here:
[https://cd.splunkdev.com/splcore/main/-/merge_requests/62189|https://cd.splunkdev.com/splcore/main/-/merge_requests/62189]

Currently under review. 

Once review is done, I will mark it done and will know about the release.","30/Nov/23 9:23 AM;62ec512f825fbfbfcff13ef5;This comment is auto-generated to inform you that the Jira severity level has been updated to align with the SFDC priority.
 If there is more than one support case linked, it will be set to align with the highest priority of open cases. If all cases are closed, then it will align to the highest priority of all closed cases. Please refer to the [P&T Customer Issues SLO|https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k] or contact [#help-customer-slo|https://slack.com/app_redirect?channel=C02KZJ51P4H]","19/Jan/24 10:27 AM;5bc64d495b8ba5286cb9abf4;Current updates:
We have two MRs for this Jira in progress:

1. Provide configurable Auto setting to increase Logged In Used Cache size:

[https://cd.splunkdev.com/splcore/main/-/merge_requests/65763|https://cd.splunkdev.com/splcore/main/-/merge_requests/65763] : providing a configurable setting to have larger Logged in user cache size. This need to be perf tested to see the outcomes (reduce scheduler contentions, reduce skips and no impact on other activities like User logins, and hitting the user prefs page). [~accountid:61c97b6e68926d0068e5e298] (CC [~accountid:611bdb2341ff95006a838c89] ) from perf team has started looking into the scenarios. 

2. Provide configurable setting to increase Logged In User Cache expiry:

[https://cd.splunkdev.com/splcore/main/-/merge_requests/65763|https://cd.splunkdev.com/splcore/main/-/merge_requests/65763]
The user-prefs data stored in the LoggedInUserCache entries expire every 5 seconds so these entries need to be refreshed from Conf layer often. As we know, _Scheduler runs every minute_ and it may process thousands of saved searches during every run where it needs Timezone to get the next runtimes of the searches. Inside conf system, these timezone settings reside in  user-prefs associated to the users owning the saved searches (plus NOBODY user scenario). 

The proposed fix is to allow larger {{LoggedInUserCache entries expiry time}} for the Scheduler related sessions. We will leave other session origins (like sessions from UI) to keep default 5 seconds so there cache entries follow default 5 sec expiry. If we configure  300 seconds user pref cache expiry for Scheduler related sessions instead of 5 seconds, then the cache entry is refreshed ~60 times lesser than it was originally (300/5=60) - that, in turn should reduce possibility of Conf Contentions by similar ~60 times. The drawback is here that the User’s timezone will be not reflected as most current, but this drawback is only limited to schedules that are absolute time ( e.g. schedule a search to run at 1PM PST everyday) and it is not applicable to schedules that are periodic such as as every 5 min ( */5 * * * * ). ","19/Jan/24 11:07 AM;557058:7ab99593-8dd6-4bca-b17f-42fcc38afee7;Give yourself credit where it’s due, [~accountid:5bc64d495b8ba5286cb9abf4] 300/5 = 60 times reduction, not 6, unless I completely read the comment wrong.","19/Jan/24 4:22 PM;5bc64d495b8ba5286cb9abf4;Good observation, [~accountid:557058:7ab99593-8dd6-4bca-b17f-42fcc38afee7] , Thanks for commenting on it.👍 
I have edited my comment and also added more colors on the solution approach. ","26/Jan/24 9:44 AM;5bc64d495b8ba5286cb9abf4;Update the status to ""In Progress"" to look into some scenario in a local SHC environment.","05/Feb/24 12:23 PM;6053ae19695c3900707acb91;[~accountid:5bc64d495b8ba5286cb9abf4]  FYI, from the flamegraph of another customer,  one of the scheduler bottlenecks is in the SavedSplunk::tz() function. 

!Screen Shot 2024-02-05 at 12.10.48 PM.png|width=1228,height=626!



!elosusbaws_captain (9).svg|width=1280,height=668!



[https://splunk.atlassian.net/jira/software/c/projects/SPL/issues/SPL-246068|https://splunk.atlassian.net/jira/software/c/projects/SPL/issues/SPL-246068]  



After a discussion with [~accountid:611be44c883cef0077c90366], we think your MR [https://cd.splunkdev.com/splcore/main/-/merge_requests/66520|https://cd.splunkdev.com/splcore/main/-/merge_requests/66520] should help this case. ","15/Apr/24 9:40 AM;5bc64d495b8ba5286cb9abf4;The [MR|https://cd.splunkdev.com/splcore/main/-/merge_requests/66520] has most of the approvals. Following items are pending:
1. Docs approval ([~accountid:557058:0cc20ab8-5f1d-4593-afd8-ec7e8404a9ce] )
2. KPI with local cache expiry values. Basically assessment from a local/perf environment for KPIs (scheduler cycle) with scheduler local cache off/on (and with different refresh settings)","13/May/24 9:17 AM;5bc64d495b8ba5286cb9abf4;The [MR|https://cd.splunkdev.com/splcore/main/-/merge_requests/66520] has been merged to develop. The target release for this is cloud (Nutella). 
At present this fix is backed by an improvement on scheduler cycle (hotpath) using a scheduler local cache to reduce round trips to user pref configurations. The configuration for the cache is currently set to cache OFF (see Default: 0 below). However, i have opened another [https://splunk.atlassian.net/browse/SPL-255534|https://splunk.atlassian.net/browse/SPL-255534|smart-link]  ticket to enable this setting. So the plan is that this will be default enabled (Nutella onwards) once the setting is turned ON as part of [https://splunk.atlassian.net/browse/SPL-255534|https://splunk.atlassian.net/browse/SPL-255534|smart-link]. 

{noformat}[scheduler]

scheduler_user_timezone_cache_expiry = <integer>[s|m|h|d]
* The amount of time that the scheduler caches the timezones
  that are associated with the scheduled searches.
* Use this setting to decrease scheduler cycle time by letting the scheduler
  use cached timezones instead of fetching them from the
  configuration cache.
* A value of 0 turns off time zone caching.
* Specify the interval as a string with minutes, seconds, hours, or days.
  For example: 60s, 1m, 1h, 1d, etc.
* Default: 0{noformat}",,,,,,,,,,,,,,,,,,,,,2771272,SPL-237049,Active support & ops blocker issues - Scheduler,Done,13/May/24 9:22 AM
[PUBLIC] Ad hoc searches that specify earliest relative time offset assuming from 'now' should explicitly include 'latest=now' to avoid a potential time range inaccuracy,SPL-237902,2792642,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Won't Fix,Ashish Mathew,5cd341229435c90fd6efce8f,Yasutaka Hirasawa,6053b7f0695c3900707b3943,Yasutaka Hirasawa,6053b7f0695c3900707b3943,28/Mar/23 6:24 PM,31/Jul/25 4:01 AM,,24/Apr/24 12:08 PM,10.0.x,8.1.0 GA (Rarity),8.2.0,8.2.0 GA (Scootaloo),8.2.2203(Emerald)-Duplicate,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.x,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.2208(Gummy Bear),9.0.2305.100(JuicyFruitTick),9.0.2305.200 (JuicyFruitTock),9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.2308.100(KitKat_Tick),9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Ice Breaker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search,,,,,0,Icebreaker-Reviewed,,,,,,,"Please refer to [https://splunk.atlassian.net/browse/SPL-235797|https://splunk.atlassian.net/browse/SPL-235797|smart-link]   test run on 3/20 with IceBreaker TC1 with 50TB day ingest - stack *noah-300tb-4*.

While ramp up ingestion, *after third scale-out event*, we observed that ad-hoc search workload (which is set up to send constant rate ~ 1000 searches per min) *suddenly dropped to nearly zero throughput for about 2 hours*.  We found that during the period of time, max events returned per ad-hoc search linearly *increased to 1 billion.* note that this is max - average was 26 million.  those ad-hoc searches are expected return fairly stable events counts as they query fix time duration from now against the same index.  

{{search index=akamai  earliest=-10s ``` psr_search_id=adhoc_10s_akamai_6 ```| table geo.region  geo.city | sort  geo.city}}

!image-20230329-010823.png|width=1571,height=657!

It was noted also that after 4th scale-out event ad-hoc search throughput came back as expected level and events count per search became stable.  This might to suggest that scale-out event may trigger/untrigger this behavior but it is not conclusive.

We sample the same stats from stack  *noah-50tb-7 on 3/28* where we did not encounter the ad-hoc search throughput issue, they are stable as expected.

!image-20230329-011431.png|width=1521,height=466!

It is noted by [~accountid:6053a29d686bf500704592b6] that  stack *noah-300tb-4*. (IB-TC1 with the problem behavior)

* SH search dispatch directory size was 5X higher (500GB vs 100GB per SH)
* SH search dispatch directory count was higher (10K vs 5K per SH).  *compared to Hersheys counterparts.*



Actual search query time periods (in seconds) for {{search index=akamai earliest=-10s ``` psr_search_id=adhoc_10s_akamai_6 ```| table geo.region geo.city | sort geo.city}}

This value should be 10 if search is doing correct search span. It shows that searches are actually querying for a way larger time span that {{earliest=-10s}} which is 10 seconds span.

!image-20230329-180131.png|width=1450,height=603!



The above observation may explain what was reported by Jason.

[https://splunk.atlassian.net/browse/SPL-235797?focusedCommentId=11382754|https://splunk.atlassian.net/browse/SPL-235797?focusedCommentId=11382754|smart-link] ","This was observed in repro test done 3/20-22 for [https://splunk.atlassian.net/browse/SPL-235797|https://splunk.atlassian.net/browse/SPL-235797|smart-link] 
Please refer to test run of stck_name = noah-300tb-4",Ashish Mathew,Brian Mignosa,Eric Woo,Hongxun Liu,James Ervin,Jason Beyers,Jo Hornsby,Justin Conrad,srv -jira-gitlabci,User known,User known,Wenjin Ruan,,,,,,,,,,,,,,,,,,,,,,,,5cd341229435c90fd6efce8f,557058:d7ddd712-79a8-40f4-b794-6eb432a1119e,5f55997610d187006f15c025,6053a3fb81b82500685ced67,557058:ab939132-f05e-4f5f-8bec-6939a8133dcc,6053a29d686bf500704592b6,557058:f87e2651-5132-4aed-8b96-f608ee969435,6036b8e24623c60069c04d7c,62ec512f825fbfbfcff13ef5,unknown,unknown,5d826b3e36a52a0da790e367,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,SPL-238030,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Mar/23 6:25 PM;yhirasawa;image-20230329-010823.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5573426,28/Mar/23 6:25 PM;yhirasawa;image-20230329-011431.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5573425,29/Mar/23 11:03 AM;yhirasawa;image-20230329-180131.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5577156,29/Mar/23 12:47 PM;yhirasawa;image-20230329-194106.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5577599,29/Mar/23 12:47 PM;yhirasawa;image-20230329-194401.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5577598,29/Mar/23 12:47 PM;yhirasawa;image-20230329-194624.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5577600,29/Mar/23 1:37 PM;amathew;image-20230329-202100.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5577779,29/Mar/23 1:37 PM;amathew;image-20230329-203639.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5577780,29/Mar/23 1:37 PM;amathew;image-20230329-203704.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5577781,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3908e564,,,,,,,,,Diag file,Search artifacts,Search SPL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Performance,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,45273600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wed Apr 17 16:48:34 UTC 2024,true,jhornsby(jhornsby),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Lifecycle,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,9.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,amathew(amathew),jbeyers(jbeyers),jconrad(jconrad),kgorzynski(JIRAUSER52213),06ac125d-c228-4c3f-b77f-a6149c059c6f(06ac125d-c228-4c3f-b77f-a6149c059c6f),yhirasawa(JIRAUSER46259),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|ibjc6n:,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ashish Mathew,5cd341229435c90fd6efce8f,Boris Chen,Christina Noren,Matt Green,Rachel Perkins,61324249ac61dc00697cf056,613be989f6070d006b917dca,5af8dc294b719848347ec02c,5af5d6c28a1abd1427953558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Takeda Pharmaceuticals U.S.A., Inc.",3506603,012400000005WzMAAU,P3,No,5005a00003137QIAAY,,Closed,Resolved,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SearchLifecycle-FY24Q1-S5,SearchLifecycle-FY24Q1-S6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This was observed in repro test done 3/20-22 for https://splunk.atlassian.net/browse/SPL-235797
Please refer to test run of stck_name = noah-300tb-4
 ",,,,,,,,,,,,,,,,,,,,,,,,,"*  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313529564443/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://splunk.atlassian.net/wiki/spaces/PROD/pages/313533760418/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2023-03-29 20:37:15.799,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13628_*:*_2_*:*_17296637105_*|*_10008_*:*_1_*:*_2941669412_*|*_3_*:*_1_*:*_1021611860_*|*_6_*:*_1_*:*_0_*|*_10001_*:*_4_*:*_12528100223_*|*_10039_*:*_2_*:*_144606215,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Updated the severity level from Sev-3 to Sev-2. Please refer to the P&T Customer Issues SLO: https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k/edit?usp=sharing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Ad hoc searches searches that use the earliest time modifier with a relative time offset should also include 'latest=now' in order to avoid time range inaccuracies. For example, if you want to get all events from the last 10 seconds starting at 01:00:10, the following search returns all events that occur between the time of 01:00:00 and 01:00:10, as expected.

index=main earliest=-10s latest=now

Running the same search without including 'latest=now' might produce unpredictable results or impact performance in certain scenarios when the search head is overloaded with ad hoc searches. See [https://docs.splunk.com/Documentation/Splunk/latest/Search/Specifytimemodifiersinyoursearch#Specify_earliest_relative_time_offset_and_latest_time_in_ad_hoc_searches Specify earliest relative time offset and latest time in ad hoc searches] in the Splunk platform ''Search Manual''. ",,,,,,,,,,2023-03-29 20:37:15.799,2023-03-29 20:37:15.799,,,,,,Ashish Mathew,5cd341229435c90fd6efce8f,,,,,,"28/Mar/23 9:43 PM;6053b7f0695c3900707b3943;[~accountid:5cd341229435c90fd6efce8f] analysis.
[https://splunk.slack.com/archives/C04SANM9FBM/p1680053307308669|https://splunk.slack.com/archives/C04SANM9FBM/p1680053307308669|smart-link] 

this explain why the event count returned grew.

Now question is why this happen (only ?) with Ice-breaker or not isolated to IceBreaker?

Is this unrealistic workload design issue of this particular test or could be general risk to customer env for some use cases? ","29/Mar/23 10:50 AM;6053b7f0695c3900707b3943;test data from IB-TC-1 50TB stack [noah-50tb-7|https://skynet-staging.stg.splunkcloud.com/en-US/app/PSR/search?earliest=1679961600&latest=1680134400&q=search%20index%3Dcustomer_audit%20%60stack(sh%2C%20noah-50tb-7)%60%20%20info%3D%22completed%22%20action%3Dsearch%20%7C%20table%20*%20%7C%20search%20savedsearch_name%3D%22%22%20%7C%20table%20*%20%7C%20timechart%20fixedrange%3Df%20span%3D1m%20limit%3D100%20avg(event_count)%20as%20%22Avg%22%2C%20p90(event_count)%20as%20%22P90%22%2C%20max(event_count)%20as%20Max&display.page.search.mode=verbose&dispatch.sample_ratio=1&display.page.search.tab=visualizations&display.general.type=visualizations&sid=1680112079.2553_A4363A8D-903F-4D9D-8BCD-5D2A0ECDF42B] retuned event counts from ad-hoc searches:
{{search index=akamai earliest=-10s ``` psr_search_id=adhoc_10s_akamai_6 ```| table geo.region geo.city | sort geo.city}}

[https://skynet-staging.stg.splunkcloud.com/en-US/app/PSR/search?earliest=1679961600&latest=1680134400&q=search index%3Dcustomer_audit `stack(sh%2C noah-50tb-7)`  info%3D""completed"" action%3Dsearch | table * | search savedsearch_name%3D"""" | table * | timechart fixedrange%3Df span%3D1m limit%3D100 avg(event_count) as ""Avg""%2C p90(event_count) as ""P90""%2C max(event_count) as Max&display.page.search.mode=verbose&dispatch.sample_ratio=1&display.page.search.tab=visualizations&display.general.type=visualizations&sid=1680112079.2553_A4363A8D-903F-4D9D-8BCD-5D2A0ECDF42B|https://skynet-staging.stg.splunkcloud.com/en-US/app/PSR/search?earliest=1679961600&latest=1680134400&q=search%20index%3Dcustomer_audit%20%60stack(sh%2C%20noah-50tb-7)%60%20%20info%3D%22completed%22%20action%3Dsearch%20%7C%20table%20*%20%7C%20search%20savedsearch_name%3D%22%22%20%7C%20table%20*%20%7C%20timechart%20fixedrange%3Df%20span%3D1m%20limit%3D100%20avg(event_count)%20as%20%22Avg%22%2C%20p90(event_count)%20as%20%22P90%22%2C%20max(event_count)%20as%20Max&display.page.search.mode=verbose&dispatch.sample_ratio=1&display.page.search.tab=visualizations&display.general.type=visualizations&sid=1680112079.2553_A4363A8D-903F-4D9D-8BCD-5D2A0ECDF42B]","29/Mar/23 11:03 AM;6053b7f0695c3900707b3943;actual search query time periods (in seconds) for {{search index=akamai earliest=-10s ``` psr_search_id=adhoc_10s_akamai_6 ```| table geo.region geo.city | sort geo.city}}



This value should be 10 if search is doing correct search span.

!image-20230329-180131.png|width=1450,height=603!","29/Mar/23 12:47 PM;6053b7f0695c3900707b3943;Looked at top 3 scheduled searches 

#  {color:#ff5630}sched_30s_1m_akamai_quick*{color}
# {color:#ff5630}sched_10s_1m_null* {color}
# {color:#ff5630}sched_20m_1m_akamai*{color}

for the same stats (search time span)



psr_search_id = {color:#ff5630}sched_30s_1m_akamai_quick*{color}

!image-20230329-194106.png|width=1134,height=498!

psr_search_id ={color:#ff5630} sched_10s_1m_null* {color}

!image-20230329-194401.png|width=1120,height=549!



psr_search_id = {color:#ff5630}sched_20m_1m_akamai* {color}

!image-20230329-194624.png|width=1138,height=508!","29/Mar/23 1:37 PM;5cd341229435c90fd6efce8f;The search time range is expanding because the search spends time being queued at the SH.

The sequence of events is:
1. Search arrives at SH REST handler. We store the time of arrival as {{now}} Relative time specifiers like {{earliest=-10s}} are resolved against this {{now}} timestamp.
2. However when latest is not specified; the end time of the search gets set to the time at which the search is parsed during execution in the search process.
3. In between 1. and 2. if search spends time in the queue at the SH the timerange for the search will expand unintentionally

Here is the timerange of the search from audit logs. Notice that even before the problem occurs the timerange is not exactly 10s. It’s always slightly more than 10s
[https://skynet-staging.stg.splunkcloud.com/en-US/app/search/search?sid=1680120581.5550_A4363A8D-903F-4D9D-8BCD-5D2A0ECDF42B|https://skynet-staging.stg.splunkcloud.com/en-US/app/search/search?sid=1680120581.5550_A4363A8D-903F-4D9D-8BCD-5D2A0ECDF42B]

Then around the time (7:35PM) the problem happens metrics log shows ~130 searches being queued. The more time a search spends in the queue the longer it’s timerange becomes 
[https://skynet-staging.stg.splunkcloud.com/en-US/app/search/search?sid=1680121649.5833_A4363A8D-903F-4D9D-8BCD-5D2A0ECDF42B|https://skynet-staging.stg.splunkcloud.com/en-US/app/search/search?sid=1680121649.5833_A4363A8D-903F-4D9D-8BCD-5D2A0ECDF42B]

There is also a bunch of SH restarts around (7:35PM) which is probably why searches are getting queued. Less SH available to do the same work so
[https://skynet-staging.stg.splunkcloud.com/en-US/app/search/search?sid=1680121843.5887_A4363A8D-903F-4D9D-8BCD-5D2A0ECDF42B|https://skynet-staging.stg.splunkcloud.com/en-US/app/search/search?sid=1680121843.5887_A4363A8D-903F-4D9D-8BCD-5D2A0ECDF42B]


As to fixing it such that latest time also uses {{now}}
Given that this is >10 year old code I'm hesitant to change this behavior this late in the release. 
This is not a regression. Verified on 8.2.2201.3


!image-20230329-203639.png|width=1862,height=487!



!image-20230329-203704.png|width=1864,height=577!




But if we still want to the fix would be 1 line.

!image-20230329-202100.png|width=939,height=291!",31/Mar/23 12:54 PM;6053a29d686bf500704592b6;Investigation summary: [https://splunk.atlassian.net/wiki/spaces/PSR/pages/1078247035464/SPL-235797+SPL-237902+Retrospective|https://splunk.atlassian.net/wiki/spaces/PSR/pages/1078247035464/SPL-235797+SPL-237902+Retrospective],03/Apr/23 2:41 PM;5cd341229435c90fd6efce8f;Short 1 pager on root cause: [https://docs.google.com/document/d/10833xqUjJ_-CK6wmfNCs5TT9664MYyz9RB1kwUrNpyE/edit#heading=h.knt9wsfn51cw|https://docs.google.com/document/d/10833xqUjJ_-CK6wmfNCs5TT9664MYyz9RB1kwUrNpyE/edit#heading=h.knt9wsfn51cw|smart-link] ,"19/Apr/23 1:18 PM;61436d20e7c3280070a1979f;[~accountid:5cd341229435c90fd6efce8f] I've added the workaround this ticket and set *Document as=include in release notes*. The workaround text is based on the text we came up with for [SPL-238030|https://splunk.atlassian.net/browse/SPL-238030]. 

Would you please update *Affects versions* for all applicable cloud and on prem releases. 

*For on prem release notes:* Once you’ve updated the *Affects versions*, the release note will appear automatically (via automation) in [*Known issues*|https://docs.splunk.com/Documentation/Splunk/9.0.4/ReleaseNotes/Knownissues] for the affected versions. 

*For cloud release notes:* I will manually add the issue to the [*Known issues*|https://docs.splunk.com/Documentation/SplunkCloud/9.0.2303/ReleaseNotes/Issues] for the affected versions (this will be the last step--after we’re sure there won’t be any more changes to the text). 

Let me know if you have any questions or concerns. Thanks!","19/Apr/23 1:21 PM;61436d20e7c3280070a1979f;I changed the title of this ticket from *“Implicit adhoc search end time is calculated after a search is removed from the queue, leading to longer-than-expected time ranges”* to *“Ad hoc searches that specify earliest relative time offset assuming from ‘now’ should explicitly include ‘latest=now’ to avoid a potential time range inaccuracy”*. Feel free to change it back if you want to instead preserve the original title and clone this ticket to another public ticket with the new title. ","24/Apr/23 11:50 AM;61436d20e7c3280070a1979f;I added the following to the Cloud Known issues for [*Version 9.0.2209*|https://docs.splunk.com/Documentation/SplunkCloud/9.0.2209/ReleaseNotes/Issues#Version_9.0.2209] and [*Version 9.0.2303*|https://docs.splunk.com/Documentation/SplunkCloud/9.0.2303/ReleaseNotes/Issues#Version_9.0.2303]. 

||*Date filed or added*||*Issue number*||*Description*||
|2023-04-24|SPL-237902|Ad hoc searches that specify earliest relative time offset assuming from 'now' should explicitly include 'latest=now' to avoid a potential time range inaccuracy.
Workaround: Ad hoc searches searches that use the earliest time modifier with a relative time offset should also include {{latest=now}} in order to avoid time range inaccuracies. For example, if you want to get all events from the last 10 seconds starting at 01:00:10, the following search returns all events that occur between the time of 01:00:00 and 01:00:10, as expected: {{index=main earliest=-10s latest=now}}.
Running the same search without including {{latest=now}} might produce unpredictable results or impact performance in certain scenarios when the search head is overloaded with ad hoc searches.|",01/May/23 4:31 PM;5cd341229435c90fd6efce8f;We had a syncup on how best to address this issue. Next steps is to gather some data from skynet and write up a proposal. Not targetted at Juicyfruit since the solution will need alot of thought.,25/Jul/23 5:43 PM;61436d20e7c3280070a1979f;[~accountid:5cd341229435c90fd6efce8f] I’m assigning this back to you since there doesn’t seem to be anything for me to do at this point. ,02/Aug/23 9:01 AM;6036b8e24623c60069c04d7c;Do we have any timelines for a fix for this one?,"04/Dec/23 9:22 AM;62ec512f825fbfbfcff13ef5;This comment is auto-generated to inform you that the Jira severity level has been updated to align with the SFDC priority.
 If there is more than one support case linked, it will be set to align with the highest priority of open cases. If all cases are closed, then it will align to the highest priority of all closed cases. Please refer to the [P&T Customer Issues SLO|https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k] or contact [#help-customer-slo|https://slack.com/app_redirect?channel=C02KZJ51P4H]","17/Apr/24 9:48 AM;5cd341229435c90fd6efce8f;After an internal discussion within the team it was concluded that while fixing this unexpected behavior is good for performance of the system as a whole it is also a significant breaking change in terms of what results are ultimately returned to the end user. 

We need more data into how much time ranges might be getting expanded, which can’t be measured easily using existing skynet data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,24/Apr/24 12:08 PM
"[PUBLIC] Create/Edit Role - In the UI, the ""Wildcards"" tool cannot be used to specify allowed federated indexes for standard mode federated search",SPL-231712,2627516,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Won't Do,,,Dariusz Sikora,6036b7ac1853760070240e47,Dariusz Sikora,6036b7ac1853760070240e47,19/Oct/22 1:21 AM,30/Jul/25 4:25 PM,,05/Nov/24 12:35 AM,10.0.x,8.2.0 GA (Scootaloo),8.2.2203(Emerald)-Duplicate,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.2205(Ferrero Rocher),9.0.2208(Gummy Bear),9.0.2209(Hersheys),9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Federation,,,,,0,2022-warroom-reviewed,axp_auth_migrated,AxpReviewed,,,,,"The customer is trying to specify searchable FEDERATED indexes during role creation using wildcard. This seems to be not working. However, the same thing works fine when customer specifies normal/standard indexes

I believe this must be something with the name federated:<index_name> and how the punctuation characters are interpreted. I asked the customer to try _:<string>_ . However, you can't add the: to filter as it complains.

I am aware that there are limitations of specifying wildcard for search 

[https://docs.splunk.com/Documentation/Splunk/9.0.1/Search/Aboutfederatedsearch#About_standard_and_transparent_mode|https://docs.splunk.com/Documentation/Splunk/9.0.1/Search/Aboutfederatedsearch#About_standard_and_transparent_mode|smart-link]

However, this document doesn’t mention if the same limitation applies during role cration",,Balaji Rao,Matthew Ness,Niclas Andersson,Srinivas Bobba,User known,Vitalii Varfolomieiev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053b5f52f452d006f833293,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,6036b7996bc3f300699ce83a,6053b9162f452d006f83555b,unknown,712020:7df0af86-9ad2-4d1a-a59f-1dcd61f8ca04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Oct/22 1:23 AM;dsikora;image (132).png;https://splunk.atlassian.net/rest/api/3/attachment/content/4910713,19/Oct/22 1:23 AM;dsikora;image (133).png;https://splunk.atlassian.net/rest/api/3/attachment/content/4910714,19/Oct/22 1:23 AM;dsikora;image (134).png;https://splunk.atlassian.net/rest/api/3/attachment/content/4910712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@446372dc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"o Have they made any changes to their environment just before the issue started?
no",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Splunk Cloud,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,27907200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 
cloud
https://skynet-search.splunkcloud.com/en-US/app/cloudops/stack_overview?form.stack=navify


o If available, provide Splunk topology diagram, host name with IP mapping.
https://skynet-search.splunkcloud.com/en-US/app/cloudops/stack_overview?form.stack=navify",,,,,,,,,,"o What errors are being reported?
n/a",,,,,,,,,,,,,No,,,,,,,,,,,,,"o What is the expectation when this problem is not there?
specify searchable feared indexes during role creation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• How long has the customer been tracking this issue and how frequent this issue is being hit?
ongoing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Tue Nov 05 08:33:49 UTC 2024,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Core Admin UI,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,3.0,7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,claral(claral),dsikora(dsikora),mness(mness),rjohny(JIRAUSER50149),344113a6-4ac5-4f94-aab3-429560f2eaf1(344113a6-4ac5-4f94-aab3-429560f2eaf1),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Admin Experience,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i02at3:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Boris Chen,Christina Noren,Matt Green,Rachel Perkins,61324249ac61dc00697cf056,613be989f6070d006b917dca,5af8dc294b719848347ec02c,5af5d6c28a1abd1427953558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,F. Hoffmann-La Roche AG,3074576,012400000005WzMAAU,P3,No,5005a00002GyCfFAAV,,Closed,Resolved,Standard,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

create a role, and specify the the searchable federated index with wildcard 

• If available, will customer upgrade to fixed version?
this cloud/yes

• If support is able to reproduce, share the setup.
create a role, and specify the the searchable federated index with wildcard ",,,,,,,,,,3.0,,,,,,,,,,,,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?
yes
 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection
can provide diag if necessary?

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

functionality?
 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 
n/a

 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.

confirm if this is a known limitation or bug?",,,Standard,,,,,,,,,,,,,,,,2022-10-19 17:38:38.273,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_61995524618_*|*_10039_*:*_1_*:*_2632518664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2022-10-19 17:38:38.273,2022-10-19 17:38:38.273,,,,,,,,,,,,,"19/Oct/22 10:38 AM;6053ac373adeca0067edc6ab;I think this is something missing from the documentation? IE, this is expected behavior, and the UI is functioning correctly, we just need to update that doc page.

CC: [~accountid:557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc] & [~accountid:6053b5f52f452d006f833293] & [~accountid:6053b9162f452d006f83555b] ","19/Oct/22 1:53 PM;6053ac373adeca0067edc6ab;[~accountid:557058:89ace195-adc2-4f25-9924-ded7c8f10bb6] sorry, should have pinged you instead of Steven. can you advise on functionality vs docs here?","19/Oct/22 3:23 PM;557058:89ace195-adc2-4f25-9924-ded7c8f10bb6;Hi [~accountid:6053ac373adeca0067edc6ab]. I went over this this issue with the *Wildcards* tool throwing an error for {{federated:*}} and similar strings with the Federated Search team and they have confirmed that it is indeed a UI bug. The *Wildcards* tool must be allowed to accept colon characters. 

I can set this bug ticket up as a known issue that will appear in the release notes for each Splunk version to which it applies. ","19/Oct/22 3:41 PM;6053ac373adeca0067edc6ab;Known issue would be great! Thanks [~accountid:557058:89ace195-adc2-4f25-9924-ded7c8f10bb6] !

I will now redirect this to the AXP Auth & Commerce team, cc: [~accountid:6053aa1e06cbba006a0de6f8] ","18/Nov/22 11:36 AM;6132579ff0bf52006968a7f1;This looks like UI issue for our team. However, we also believe that the customer can hit the API URL as a workaround till we get our fix in ",18/Nov/22 11:40 AM;557058:89ace195-adc2-4f25-9924-ded7c8f10bb6;[~accountid:6132579ff0bf52006968a7f1] Can you update the Workaround section of this ticket with instructions for doing that? I’m not sure offhand which API is involved. ,"05/Nov/24 12:33 AM;712020:7df0af86-9ad2-4d1a-a59f-1dcd61f8ca04;This issue is *closed* due to no updates for the *past 400 days*.
If you believe that this issue still exists:

* please reopen and assign it to [~accountid:712020:35e6e1f3-3bd2-4276-bb4b-27c98f7d797b]
* make sure that ProductBacklogArea field is set to *Admin Experience*
* make sure that Mission Team field is set to *Core Admin UI*
* provide recent reproduction steps
* provide least Splunk supported version on which you were able to reproduce the issue
* provide HAR and diag files collected at the same time during reproduction of the issue

*In case of any of the above steps missing, this ticket may not be taken under investigation or may have lower priority for investigation than tickets with all informations provided.*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2349666,SCP-50253,"FY23Q4 Bug, Performance, and Release Optimization",Done,05/Nov/24 12:35 AM
[PUBLIC] [SPIN-OFF] - Crash in search process in PrecacheUsersThread when max_searches_per_process is set lower than default,SPL-231587,2624672,Crash,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,Jo Hornsby,557058:f87e2651-5132-4aed-8b96-f608ee969435,Jo Hornsby,557058:f87e2651-5132-4aed-8b96-f608ee969435,Jo Hornsby,557058:f87e2651-5132-4aed-8b96-f608ee969435,17/Oct/22 11:29 AM,30/Jul/25 4:24 PM,,,10.0.x,8.1.0 GA (Rarity),8.1.1,8.1.2,8.1.3,8.1.4,8.1.5,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Identity - Authentication (AuthN),Search - Core,,,,0,cmpkanban,mr_regression,,,,,,"I found a crash during I was testing 08187535c166

{code}
[build 08187535c166] 2020-12-03 19:52:36
Received fatal signal 6 (Aborted).
 Cause:
   Signal sent by PID 23620 running under UID 2023.
 Crashing thread: PrecacheUsersThread
 Registers:
    RIP:  [0x00007F2F9AFF2387] gsignal + 55 (libc.so.6 + 0x36387)
    RDI:  [0x0000000000005C44]
    RSI:  [0x0000000000005C4D]
    RBP:  [0x00007F2F957FE570]
    RSP:  [0x00007F2F957FE428]
    RAX:  [0x0000000000000000]
    RBX:  [0x000055FE946A1750]
    RCX:  [0xFFFFFFFFFFFFFFFF]
    RDX:  [0x0000000000000006]
    R8:  [0x0000000000000100]
    R9:  [0x00007F2F9AC00080]
    R10:  [0x0000000000000008]
    R11:  [0x0000000000000206]
    R12:  [0x000055FE9398EA10]
    R13:  [0x00007F2F9A433908]
    R14:  [0x00007F2F957FE6F8]
    R15:  [0x00007F2F957FE718]
    EFL:  [0x0000000000000206]
    TRAPNO:  [0x0000000000000000]
    ERR:  [0x0000000000000000]
    CSGSFS:  [0x0000000000000033]
    OLDMASK:  [0x0000000000000000]

 OS: Linux
 Arch: x86-64

 Backtrace (PIC build):
{code}",,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-198284,,,,,,,,,,,,,,,,,,,,17/Oct/22 11:29 AM;jhornsby;crash-2020-12-03-19-52-36.log;https://splunk.atlassian.net/rest/api/3/attachment/content/4901388,17/Oct/22 11:29 AM;jhornsby;diag-ip-10-202-9-177.ec2.splunkit.io-2020-12-04_01-13-03.tar.gz;https://splunk.atlassian.net/rest/api/3/attachment/content/4901389,17/Oct/22 11:29 AM;jhornsby;image001.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4901390,17/Oct/22 11:29 AM;jhornsby;image001.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4901391,17/Oct/22 11:29 AM;jhornsby;image001.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4901392,17/Oct/22 11:29 AM;jhornsby;image001.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4901393,17/Oct/22 11:29 AM;jhornsby;uploadsvc-69case2943109-04-06-2022-USER-0030b00001yMJSfAAO-20220406_pudc-sps-006_splunkd-6.core.78407.gdb.log;https://splunk.atlassian.net/rest/api/3/attachment/content/4901394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Domino's Pizza LLC,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@2ec6a8ea,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,92620800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,false,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2022-10-17 18:29:35.311,true,adallas(adallas),,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CMPEntUsability,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,jhornsby(jhornsby),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cust Managed Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|iasplr:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Telstra Corporation Limited,2943109,012400000005WzMAAU,P3,No,5005a000023CD0pAAG,,Closed,Resolved - Work Around,Standard,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,0.0,,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Set limits.conf back to default, by removing any override of max_searches_per_process.

For example:
<pre>
[search]
max_searches_per_process=1
</pre>
to
<pre>
[search]
</pre>",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,17/Oct/22 11:29 AM
"Searches might not return all lookup values during search result enrichment. Affected searches include lookups with more duplicate entries than specified in the max_matches setting, and lookups larger than the value in the max_memtable_bytes setting. ",SPL-230673,2600032,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Workaround,Mateusz Juda,614d2968f1c17400684bde7f,Becky Simmons,6036b837d3fc7c006809a2d7,Becky Simmons,6036b837d3fc7c006809a2d7,26/Sep/22 11:35 AM,30/Jul/25 4:25 PM,,09/Aug/24 1:36 PM,10.0.x,9.1.0,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search - Lookups,,,,,0,,,,,,,,"Problem description: Customer is expecting 0 results for his search on field_out on both searches regardless of what is set in max_memtable_bytes under limits.conf. From below, once he modifies the max_memtable_bytes setting (workaround), they are seeing different results:

# Run the following searches: With \[lookup]:max_memtable_bytes = 26214400 (default) in limits.conf, we get the following
splunk search 'index=* splunk_server=""[spls056.gcld1.ngext.com|https://spls056.gcld1.ngext.com/]"" sourcetype=test123456 | search field_out=""NOTFOUND"" | stats count'
count
-----
763
We also see this in search.log for that particular job on spls056:
/opt/splunk/var/run/splunk/dispatch/remote_spls062.gcld1.ngext.com_1662728318.48/search.log:09-09-2022 12:58:36.949 INFO IndexedCsvDataProvider \[366617 BatchSearch] - Indexing lookup table.reason=""Lookup table file size exceeds limits.conf \[lookup] max_memtable_bytes threshold."" lookup_table=""/opt/splunk/var/run/searchpeers/spls06
With \[lookup]:max_memtable_bytes = (value greater than kvstore size, perhaps 40000000) in limits.conf, we get the following:
splunk search 'index=* splunk_server=""[spls056.gcld1.ngext.com|https://spls056.gcld1.ngext.com/]"" sourcetype=test123456 | search field_out=""NOTFOUND"" | stats count'
count
-----
0

h3. Steps to reproduce:

# Create lookup table
# Create lookup definition with a default value
# Create automatic lookup
# Run a few searches: With \[lookup]:max_memtable_bytes = 26214400 (default) in limits.conf, we get the following
splunk search 'index=* splunk_server=""[spls056.gcld1.ngext.com|https://spls056.gcld1.ngext.com/]"" sourcetype=test123456 | search field_out=""NOTFOUND"" | stats count'
count
-----
763
We also see this in search.log for that particular job on spls056:
/opt/splunk/var/run/splunk/dispatch/remote_spls062.gcld1.ngext.com_1662728318.48/search.log:09-09-2022 12:58:36.949 INFO IndexedCsvDataProvider \[366617 BatchSearch] - Indexing lookup table.reason=""Lookup table file size exceeds limits.conf \[lookup] max_memtable_bytes threshold."" lookup_table=""/opt/splunk/var/run/searchpeers/spls06
With \[lookup]:max_memtable_bytes = (value greater than kvstore size, perhaps 40000000) in limits.conf, we get the following:
splunk search 'index=* splunk_server=""[spls056.gcld1.ngext.com|https://spls056.gcld1.ngext.com/]"" sourcetype=test123456 | search field_out=""NOTFOUND"" | stats count'
count
-----
0

h3. Expected Results:

We believe that 0 results should be returned at both searches. It shouldn’t find any results and yet it found 763, which the customer feels that it’s finding results on field_out and it shouldn’t be.

h4. Additional information, if necessary",Splunk 8.2.7,Aditya Anil Dhoke,Ajay Patel,Alex Kislov,Andre Williams,Becky Simmons,Doug Ewald,Nate Murillo,Samuel Curtis,Sirish Mohan,Steph Mills,User known,User known,User known,,,,,,,,,,,,,,,,,,,,,,,557058:849fbcca-205e-4649-ac02-bf68448d529d,712020:7e34a4c0-1b40-4444-97da-310a77ee1b07,712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee,6036b8348ff09800716a078d,6036b837d3fc7c006809a2d7,622f94911f014e0069cc209b,6036b79d6bc3f300699ce87b,61d406430586a20069486775,5c6e33daa5f342215023f83e,557058:611ecd3d-c00a-44a3-b460-31db5af8ed89,unknown,unknown,unknown,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,06/Oct/22 4:48 AM;rsimmons;Screen Shot 2022-10-06 at 7.43.11 AM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4855113,06/Oct/22 4:48 AM;rsimmons;Screen Shot 2022-10-06 at 7.44.54 AM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4855114,06/Oct/22 4:48 AM;rsimmons;Screen Shot 2022-10-06 at 7.45.36 AM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4855115,07/Oct/22 7:42 AM;rsimmons;uploadsvc-56case3053575-10-07-2022-USER-0035a000031fXf4AAE-mylookup.csv.gz;https://splunk.atlassian.net/rest/api/3/attachment/content/4861751,04/Oct/22 6:04 AM;rsimmons;uploadsvc-72case3053575-09-22-2022-USER-0035a000031fXf4AAE-CASE_3053575_Northrop_Grumman_Supporting_Docs.pdf;https://splunk.atlassian.net/rest/api/3/attachment/content/4847819,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5ed4d098,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Performance,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started? They stated that they hadn't made any.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,5356800,,,,,,,,,,,,,,,,,,,,,,,,,Installed Version,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"o What is the customer environment for the issue? Splunk 8.2.7 (standalone) Dev environment to repo what they are encountering in their PROD envir.  Dev envir: 8 cores for Indexer and 8 cores for SH
SH: spls06
Indexer: spls056

o If available, provide Splunk topology diagram, host name with IP mapping.",,,,,,,,,,"o What errors are being reported? 
Lookup table file size exceeds limit.conf re:[lookup]:max_memtable_bytes",,,,,,,,,,,,,No,,,,,,,,,,,,,"o What is the expectation when this problem is not there?
When running [lookup]:max_memtable_bytes = 26214400 (default) in limits.conf, we get the following
splunk search 'index=* splunk_server=""spls056.gcld1.ngext.com"" sourcetype=test123456 | search field_out=""NOTFOUND"" | stats count'

it should return count = 0, instead they are getting a number. The only way to get it to return 0 is by using workaround,  [lookup]:max_memtable_bytes = (value greater than kvstore size, perhaps 40000000) in limits.conf, we get the following:

splunk search 'index=* splunk_server=""spls056.gcld1.ngext.com"" sourcetype=test123456 | search field_out=""NOTFOUND"" | stats count'
count
-----
0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wed Jul 23 18:44:05 UTC 2025,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,WLM,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,5.0,80.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,adhoke(adhoke),472b5588-5ae4-4f7e-9221-15cf811ebad9(472b5588-5ae4-4f7e-9221-15cf811ebad9),32dcaec3-e280-4264-901a-30cf7583c457(32dcaec3-e280-4264-901a-30cf7583c457),awilliams(awilliams),rsimmons(rsimmons),2517d7b5-dd6f-46b7-a1c3-74b705d1cf52(2517d7b5-dd6f-46b7-a1c3-74b705d1cf52),ericc(JIRAUSER53243),kshridhar(kshridhar),kgorzynski(JIRAUSER52213),mjuda(JIRAUSER52333),nmurillo(nmurillo),e5ef8fd6-eb0e-4c63-ad75-de6ba4cd3898(e5ef8fd6-eb0e-4c63-ad75-de6ba4cd3898),stephaniem(stephaniem),zahinak(JIRAUSER51812),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cust Managed Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Mateusz Juda,614d2968f1c17400684bde7f,,,,,,,,,,,,,,,,,,0|iaoxcv:,,,,,,,"As the workaround is accepted by customer we also have future enhance request

https://splunk.atlassian.net/browse/SPL-260572",,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mateusz Juda,614d2968f1c17400684bde7f,Boris Chen,Christina Noren,Matt Green,Rachel Perkins,61324249ac61dc00697cf056,613be989f6070d006b917dca,5af8dc294b719848347ec02c,5af5d6c28a1abd1427953558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Northrop Grumman - CSOC,3053575,012400000005WzCAAU,P4,Yes,5005a00002ETs19AAD,,Open,Waiting on Splunk,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3053575,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment
This appears to be a regression. It was first mentioned in SPL-181155 - Searching for lookup default_match value includes default_match value in lispy

• If available, will customer upgrade to fixed version?
Yes

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2022-09-28 12:51:52.316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"cp /root/files/props.conf /opt/splunk/etc/system/local/
cp /root/files/transforms.conf /opt/splunk/etc/system/local/
cp /root/files/collections.conf /opt/splunk/etc/system/local/
echo ""\\[kvstore\]"" >> /opt/splunk/etc/system/local/server.conf
echo ""storageEngine = wiredTiger"" >> /opt/splunk/etc/system/local/server.conf
cp /root/files/limits.conf /opt/splunk/etc/system/local/
splunk start --accept-license
splunk restart

spls062 -- SH
spls056 -- Indexer

splunk add search-server 10.109.248.56:8089 -remoteUsername admin
splunk search '|inputlookup mylookup.csv.gz | head 20000 | collect index=main sourcetype=test123456'
splunk search '|inputlookup mylookup.csv.gz | outputlookup mylookup'
splunk search 'index=* splunk_server=""spls056.gcld1.ngext.com"" sourcetype=test123456 | search field_out=""NOTFOUND"" | stats count'

The issue is that field_out shouldn't have been returned.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13628_*:*_1_*:*_420691752_*|*_10008_*:*_1_*:*_9683376247_*|*_3_*:*_3_*:*_47497677680_*|*_5_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_485953090_*|*_10039_*:*_1_*:*_930754923,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Change the max_matches and max_memtable_bytes settings to values greater than the KV store size or the CSV lookup file size.,,,,,,,,,,2022-09-28 12:51:52.316,2022-09-28 12:51:52.316,,,,,,Mateusz Juda,614d2968f1c17400684bde7f,,,,,,"28/Sep/22 5:51 AM;6036b79d6bc3f300699ce87b;Please update the Summary/Title for the Jira to reflect what is required in the JIRA Creation SOP.
[https://splunk.atlassian.net/wiki/spaces/HSSR/pages/188770819883/Creating+Splunk+Core+Product+Jiras#Submitting-an-SPL-Jira|https://splunk.atlassian.net/wiki/spaces/HSSR/pages/188770819883/Creating+Splunk+Core+Product+Jiras#Submitting-an-SPL-Jira|smart-link]

{noformat}(Software Component) [Version Number] (The deviation of the expected / documented behavior){noformat}

EX:  {{Indexer Clustering [9.0.0.1] Many ""preforked"" errors in logs and crash after the upgrade}}



Also can you please complete the “Engineering/Support Template” as well.

Also where are the search logs and/or the diags for this issue?",05/Oct/22 8:04 AM;6036b79d6bc3f300699ce87b;[~accountid:6036b837d3fc7c006809a2d7] From what I am getting from your Jira description is that the customer is see an INFO stating that the search if going from using RAM for a LOOKup to on disk. Please take a look at the setting max_memtable_bytes via the limits.conf. This parameter use an in-memory for lookups. Now if the lookup size is above max_memtable_bytes it will be indexed on disk. With that I am not seeing an issues other than the results are different based on how many times the search is ran? Now if there is more info in the zoom can you please create screenshot with timestamps for dev to easily reference. Also I think the description of this Jira is lacking a start to finish of what the issues is or a picture of what the issues is other than strange search behavior.,07/Oct/22 6:38 AM;614d2968f1c17400684bde7f;[~accountid:6036b837d3fc7c006809a2d7] do you have the file {{mylookup.csv.gz}} mentioned in the repro steps?,"07/Oct/22 7:42 AM;6036b837d3fc7c006809a2d7;[~accountid:614d2968f1c17400684bde7f] {{mylookup.csv.gz}} is attached now

[^uploadsvc-56case3053575-10-07-2022-USER-0035a000031fXf4AAE-mylookup.csv.gz]

",11/Oct/22 11:06 AM;6036b837d3fc7c006809a2d7;@mateusz Cust would to get an update?,11/Oct/22 12:59 PM;611b58d546c6b50071778634;Hello [~accountid:614d2968f1c17400684bde7f] do we have an update on this JIRA? We need to provide an update to the cx. ,"12/Oct/22 7:24 AM;614d2968f1c17400684bde7f;Hi [~accountid:6036b837d3fc7c006809a2d7] and [~accountid:611b58d546c6b50071778634] 

I’m working on the issue (however, since it is P3 I cannot concentrate on it full time now). On my env I’m able to observe NOTFOUND results, but I’m not sure if this is exactly what was tested by you. Sorry, I was a little bit lost when trying to reproduce what you did. Let me ask the following:

# I do not see the *.conf files in the attachments, so I took them from the SH diags. Is it correct? If no, then please give me your files.
# {{mylookup}} definition uses {{kvstore}} and you also transfer the csv file content to mylookup ({{|inputlookup mylookup.csv.gz | outputlookup mylookup}}). It means that in the search we shouldn’t use the file anymore because the automatic lookup is from the kvstore. Correct?
# In my repro I do not see log entry {{INFO IndexedCsvDataProvider}} . If the answer for above (2.) is yes, then it is expected, because we shouldn’t touch the csv file anymore. Otherwise 2. is false and I need more detailed reproduction steps.","12/Oct/22 10:49 PM;614d2968f1c17400684bde7f;*Update*:

In my scenario changing case sensitivity in transforms.conf:

{noformat}case_sensitive_match = true{noformat}

change the behaviour, no more NOTFOUND entries.{{true}} is the default value, in the diags I’ve found {{false}}

From the [docs|https://docs.splunk.com/Documentation/Splunk/8.2.7/admin/transformsconf]:

{noformat}* NOTE: For KV Store lookups, a setting of 'case_sensitive_match=false' is
  honored only when the data in the KV Store lookup table is entirely in lower
  case. The input data can be in any case.{noformat}

We insert the data from the CSV all in upper case, which means it won’t work for {{case_sensitive_match = false}} .

[~accountid:6036b837d3fc7c006809a2d7] I need a confirmation if the cx uses kvstore lookup or cvs lookup. If kvstore, then *Working as designed.*","18/Oct/22 4:33 AM;6036b837d3fc7c006809a2d7;@mjuda The customer responded: 

We have made the changes to the transforms.conf that you suggested and tested this. We do not see a change in the results and the issue persists. We can repro this on our end. We would like to set up a call to do some screensharing or maybe screenshare on your side as we watch how it is repro'd on your end.

Are you available for a quick zoom so that you can see firsthand what they are encountering? If so, please provide your availability and I setup the zoom.",18/Oct/22 4:47 AM;614d2968f1c17400684bde7f;[~accountid:6036b837d3fc7c006809a2d7]  10PM CEST time zone works for me on Wed&Thu. Next week Mon-Thu.,18/Oct/22 7:51 AM;6036b837d3fc7c006809a2d7;@mjuda the invite has been sent for 10PM CEST tomorrow. Thank you,"19/Oct/22 1:06 PM;6036b837d3fc7c006809a2d7;movie: 

[https://splunk.zoom.us/rec/share/YeUusvjVxwGISFnP0HBpws0P4y881bDlvFZNA0pYbQ-L_7nyuWEwhf2SOlJxrwgx.a05p65YIoJuHOX9O|https://splunk.zoom.us/rec/share/YeUusvjVxwGISFnP0HBpws0P4y881bDlvFZNA0pYbQ-L_7nyuWEwhf2SOlJxrwgx.a05p65YIoJuHOX9O]
Passcode: &f6Kd6BN",19/Oct/22 1:11 PM;6036b837d3fc7c006809a2d7;vanilla instance - [https://downloadsvc.splunk.com/download/splunk/09-16-2022/uploadsvc-71case3053575-09-16-2022-USER-0035a000031fXf4AAE-cpu-cores-specs.txt|https://downloadsvc.splunk.com/download/splunk/09-16-2022/uploadsvc-71case3053575-09-16-2022-USER-0035a000031fXf4AAE-cpu-cores-specs.txt],19/Oct/22 1:12 PM;6036b837d3fc7c006809a2d7;[https://downloadsvc.splunk.com/download/splunk/09-22-2022/uploadsvc-72case3053575-09-22-2022-USER-0035a000031fXf4AAE-CASE_3053575_Northrop_Grumman_Supporting_Docs.pdf|https://downloadsvc.splunk.com/download/splunk/09-22-2022/uploadsvc-72case3053575-09-22-2022-USER-0035a000031fXf4AAE-CASE_3053575_Northrop_Grumman_Supporting_Docs.pdf],"20/Oct/22 2:12 AM;614d2968f1c17400684bde7f;*Summary after our call with the customer:*

It turned out that there is a recording from the previous call (cx + support, linked bellow by Becky). The cx proposed to meet again once I know what was showed there. One member mentioned that my testing scenario may not cover their env, because they reproduced the issue with SH+IDX, while I have only standalone instance. In the SFDC case they attached full dump of their SH & IDX (vanilla installations). Also they have added a step by step instruction.

*Next steps*

With the new details unknown for me from the Jira description I’m going to recreate the env and reproduce the issue.",20/Oct/22 4:12 AM;614d2968f1c17400684bde7f;I’ve reproduced the issue as it is shown on the recording.,"20/Oct/22 4:18 AM;6036b837d3fc7c006809a2d7;[https://downloadsvc.splunk.com/download/splunk/10-19-2022/uploadsvc-24case3053575-10-19-2022-USER-0035a000031fXf4AAE-SupportCase3053575-StepbyStepInstructions.zip|https://downloadsvc.splunk.com/download/splunk/10-19-2022/uploadsvc-24case3053575-10-19-2022-USER-0035a000031fXf4AAE-SupportCase3053575-StepbyStepInstructions.zip]

This ZIP files contains instructions for the search head and indexer side by side in excel format with easy-to-follow step by step to repro this issue.

This ZIP file also includes the four .conf files and the mylookup.csv.gz file mentioned in the instructions for your convenience.","20/Oct/22 7:19 AM;614d2968f1c17400684bde7f;Re the spreed sheet with instruction: for the indexer they mention copy of: props, transforms, collection. However, the files are not in the archives provided (see bellow). I also didn’t copy them  for the successful reproduction.

{noformat}➜  SPL-230673 tar tzf uploadsvc-71case3053575-09-22-2022-USER-0035a000031fXf4AAE-diag-spls056_indexer_2022-09-22_18-28-58.tar.gz | grep etc/system/local
diag-spls056.gcld1.ngext.com-2022-09-22_18-28-58/etc/system/local/
diag-spls056.gcld1.ngext.com-2022-09-22_18-28-58/etc/system/local/README
diag-spls056.gcld1.ngext.com-2022-09-22_18-28-58/etc/system/local/server.conf
diag-spls056.gcld1.ngext.com-2022-09-22_18-28-58/etc/system/local/migration.conf
diag-spls056.gcld1.ngext.com-2022-09-22_18-28-58/etc/system/local/limits.conf

➜  SPL-230673 tar tzf uploadsvc-56case3053575-09-16-2022-USER-0035a000031fXf4AAE-indexer_1_of_1--2022-09-16.tar.gz | grep etc/system/local
diag-spls056.gcld1.ngext.com-2022-09-16_11-24-26/etc/system/local/
diag-spls056.gcld1.ngext.com-2022-09-16_11-24-26/etc/system/local/README
diag-spls056.gcld1.ngext.com-2022-09-16_11-24-26/etc/system/local/server.conf
diag-spls056.gcld1.ngext.com-2022-09-16_11-24-26/etc/system/local/migration.conf
diag-spls056.gcld1.ngext.com-2022-09-16_11-24-26/etc/system/local/distsearch.conf
diag-spls056.gcld1.ngext.com-2022-09-16_11-24-26/etc/system/local/limits.conf

➜  SPL-230673 tar tzf uploadsvc-4case3053575-09-14-2022-USER-0035a000031fXf4AAE-splunk_idx_spls56.gz | grep etc/system/local
opt/splunk/etc/system/local/
opt/splunk/etc/system/local/README
opt/splunk/etc/system/local/server.conf
opt/splunk/etc/system/local/migration.conf
opt/splunk/etc/system/local/distsearch.conf
opt/splunk/etc/system/local/limits.conf{noformat}



The indexer gets the content of the mylookup as csv files located in:

{noformat}opt/splunk/var/run/searchpeers/spls062.gcld1.ngext.com-1662726968/kvstore_s_systemMX8edh8vqo2ngaR2K53MLFyt_mycollvhj9v0UM11RiPmfMc8YsbKGS/00000000.csv
opt/splunk/var/run/searchpeers/spls062.gcld1.ngext.com-1662726968/kvstore_s_systemMX8edh8vqo2ngaR2K53MLFyt_mycollvhj9v0UM11RiPmfMc8YsbKGS/00000001.csv
opt/splunk/var/run/searchpeers/spls062.gcld1.ngext.com-1662726968/kvstore_s_systemMX8edh8vqo2ngaR2K53MLFyt_mycollvhj9v0UM11RiPmfMc8YsbKGS/00000002.csv
opt/splunk/var/run/searchpeers/spls062.gcld1.ngext.com-1662726968/kvstore_s_systemMX8edh8vqo2ngaR2K53MLFyt_mycollvhj9v0UM11RiPmfMc8YsbKGS/00000003.csv{noformat}

Thats explains why the logic for csv lookups is visible in the logs.","21/Oct/22 8:15 AM;614d2968f1c17400684bde7f;The issue may be ralated to duplicate values in {{field1}} of the lookup. Simple modification of the scenario, which populates the lookup without duplicates

{{bin/splunk search '|inputlookup mylookup.csv.gz | dedup field1 | outputlookup mylookup'}}

makes the issue disappear. At least for the data we have.

I need to check how the lookup index {{tsidx}} files are created and used. While playing with the data in lookup I’ve noticed the {{NOTFOUND}} results are not deterministic.","27/Oct/22 3:56 AM;614d2968f1c17400684bde7f;*Simplified* *_Steps to reproduce_*

(fresh build, {{develop}}, {{ac44988e891f63881e5723b6c0bb9c24b0fae8cb}})

# Deploy Standalone Splunk instance.
# Create {{etc/system/lookups/test_lookup.csv}}:
{noformat}field1,field2
1,v1
1,v2
2,v3
2,v4{noformat}
# Create {{etc/system/local/transforms.conf}}:
{noformat}[test_lookup]
default_match = NOTFOUND
filename = test_lookup.csv
fields_list = field1,field2
max_matches = 1
min_matches = 1{noformat}
# Restart.
# Lookup search for {{field1=""1""}}
{{splunk search '|makeresults | eval field1=""1"" | lookup  _force_indexed=t  test_lookup field1 OUTPUTNEW field2 as field_out | table field1,field_out'}}
{noformat}field1 field_out
------ ---------
     1 v2{noformat}
(/) 
# Lookup search for {{field1=""2""}}
{{splunk search '|makeresults | eval field1=""2"" | lookup  _force_indexed=t  test_lookup field1 OUTPUTNEW field2 as field_out | table field1,field_out'}}
{noformat}field1 field_out
------ ---------
     2 v4{noformat}
(/) 
# Lookup search for {{field1 IN (""1"", ""2"")}} *without* {{max_memtable_bytes}} violation:
{{splunk search '|makeresults count=2 | streamstats count as field1 | lookup test_lookup field1 OUTPUTNEW field2 as field_out | table field1,field_out'}}
{noformat}field1 field_out
------ ---------
     1 v1
     2 v3{noformat}
(/) 
# Lookup search for {{field1 IN (""1"", ""2"")}} *with* {{max_memtable_bytes}} violation, so Splunk creates an index for the lookup csv data (actually we do not have to change {{max_memtable_bytes}} because there is a hidden flag {{_force_indexed}}):
{{splunk search '|makeresults count=2 | streamstats count as field1 | lookup  _force_indexed=t  test_lookup field1 OUTPUTNEW field2 as field_out | table field1,field_out'}}
{noformat}field1 field_out
------ ---------
     1 NOTFOUND
     2 v3{noformat}
(x) 


The customer scenario needs: kvstore lookup, SH + IDX, automatic lookup, modification of {{max_memtable_bytes}} . All needed is to enable IndexedCsv/IndexedCsvDataProvider functionality in Splunk ({{tsidx}} files for the indexed csv data are stored in {{etc/system/lookups/test_lookup.csv_1666861414.577101.cs.index}}). 

*Key points in the reproduction scenario.*

# We need IndexedCsv/IndexedCsvDataProvider functionality.
# We need duplicated keys in the lookup (more entries than {{max_matches}} for a value, in our case 2 are enough).

*Root cause*
The problem is in the way we query the tsidx files. We use batch queries, so in one shot we check multiple {{field1}} values, i.e. in the logs we see:

{noformat}DEBUG IndexedCSV [3731230 phase_1] - IndexedCSV::search called with lispy = [ OR field1::1 field1::2 ]{noformat}

However, we pass maximum number of results allowed to be returned from the tsidx files, see [IndexedCsvDataProvider.cpp|https://cd.splunkdev.com/splcore/main/-/blob/ac44988e891f63881e5723b6c0bb9c24b0fae8cb/src/lookup/IndexedCsvDataProvider.cpp#L204]:

{code:c++}size_t capacity = _context.maxMatches * outputRequestSet.size();{code}

In our scenario it means {{capacity == 2}}, but our index contains 4 values which match the lispy query. So something has to be skipped.

Interestingly, there was a very similar issue for temporal lookups. Root cause was the same, but the fix was provided only for temporal case ([https://splunk.atlassian.net/browse/SPL-160039|https://splunk.atlassian.net/browse/SPL-160039|smart-link]) ([comment in the code|https://cd.splunkdev.com/splcore/main/-/blob/ac44988e891f63881e5723b6c0bb9c24b0fae8cb/src/lookup/IndexedCsvDataProvider.cpp#L176])

*Next step*

Check possible solutions. Possibilities for now:

# Modify the code as it was done for temporal lookups: query tsidx key by key.
# Check if it is possible to use {{st_query_count}} mode for [st_tsidx_query (tsidx.c|https://cd.splunkdev.com/splcore/main/-/blob/ac44988e891f63881e5723b6c0bb9c24b0fae8cb/src/searchthing/tsidx.c#L2268]) in the following way. For each batch:
## First run a query to count the matching values (without retrieving them).
## Reserve memory for the values, run the query again and retrieve the values.
## It might happen that the memory consumption is too high (according to a limit) (e.g. max_mathces=1 but we have 1M keys with the same key), then it is better to fallback to key by key mode with `capacity=max_matches`.
## If works, then the same approach may be used for temporal lookups. 


[~accountid:5cd3413f27454f0fe4579c11] as I see you have been involved in the indexed lookups a lot some time ago, any thoughts about possible solution?","02/Nov/22 11:41 AM;6036b837d3fc7c006809a2d7;@mjuda Any new update that can be provided to the customer? They are inquiring….

","04/Nov/22 3:55 AM;614d2968f1c17400684bde7f;Nothing special [~accountid:6036b837d3fc7c006809a2d7] , I’m investigating the possible fixes. Since it is P3 issue we shouldn’t hurry too much and do it properly.



fyi: next week I’m on PTO, will continue after 14th Nov.","08/Dec/22 3:57 AM;614d2968f1c17400684bde7f;*Update*: as you may noticed the new status in Jira, the  code is under review",08/Dec/22 5:17 AM;6036b837d3fc7c006809a2d7;[~accountid:614d2968f1c17400684bde7f] Thank you for the update,06/Feb/23 10:35 AM;6036b8348ff09800716a078d;[~accountid:614d2968f1c17400684bde7f] Any updates on this release? Thanks,14/Feb/23 10:14 AM;6036b8348ff09800716a078d;[~accountid:614d2968f1c17400684bde7f] Checking in again to get any kind of update on a release for this issue.,"14/Feb/23 10:34 AM;614d2968f1c17400684bde7f;Hi [~accountid:6036b8348ff09800716a078d] , trying to push on the code reviewers. Cannot say anything more now. ",14/Feb/23 2:01 PM;6036b8348ff09800716a078d;Thank you [~accountid:614d2968f1c17400684bde7f] ,08/Mar/23 7:34 AM;6036b837d3fc7c006809a2d7;[~accountid:614d2968f1c17400684bde7f] Do we have any timeframe on when the fix will be released? ,"13/Mar/23 8:28 AM;614d2968f1c17400684bde7f;[~accountid:6036b837d3fc7c006809a2d7] - I cannot promise anything now. The issue is there from the initial implementation of the indexed lookups and the code change is not a simple bug fix. We need a deep changes in the way we handle indexed lookups. We know that the change I propose has some corner cases which can be problematic (e.g. a large lookup table with same keys may require all records in the memory, even if we need only one of them at the end). However a better solution requires much bigger redesign of the code used by other functionalities.

The current state of the review is that I need to propose also some performance tests which represents “a common use case”.","21/Mar/23 11:39 AM;622f94911f014e0069cc209b;[~accountid:614d2968f1c17400684bde7f] [~accountid:6036b837d3fc7c006809a2d7] I’m the SE assigned to this customer and we need to get them some sort of answer on how/when we might have a fix. Going back to them and saying “we don’t know” is not really an option for us as they have had support issues in the past, we recently lost our CSM as well as have no TSAM assigned. There is a $2M+ renewal coming up with a $500K upgrade (pending) and we can not afford to jeopardize this opportunity. Also, [~accountid:614d2968f1c17400684bde7f] , do you have any update regarding your comment on proposing performance tests?","21/Mar/23 12:55 PM;614d2968f1c17400684bde7f;[~accountid:622f94911f014e0069cc209b] I’m going to spend next days on this ticket, can I update you on Monday 27 Mar, is it ok?

Regarding the the possible release (assuming the fix and tests will pass smoothly soon) : I just want to let you know, that according to [go/backport|http://go/backport] policy, with the current priority of this Jira we may not be able to backport it to any maintenance release. If we know already that this ticket is crucial for the renewal, we should plan how to handle it. Or maybe the customer is ok to wait for a major release. ","21/Mar/23 3:10 PM;622f94911f014e0069cc209b;[~accountid:614d2968f1c17400684bde7f] Appreciate the quick response and yes, if you can provide an update Monday that would be great! If you need anything from me, please let me know. ","27/Mar/23 8:06 AM;614d2968f1c17400684bde7f;*Status update:*

working on the performance verification. Computations in progress, I hope to have a report soon. For now it looks promising as long as we assume not so many duplicates for a key. A bottle neck is for sure when we have a degenerate case like keys with dozen thousands duplicates. However, a good coincidence is that we have an upper limit for a csv file size: *4GB* (a signle tsidx file cannot index bigger files and we support only one tsidx file per csv). In practice it means that the lookup files cannot have more than a few hundred of thousands entries. In my report I will present detailed time/memory consumption.","07/Apr/23 12:36 PM;614d2968f1c17400684bde7f;[~accountid:6036b837d3fc7c006809a2d7] / [~accountid:622f94911f014e0069cc209b] We know that the solution we have now has a corner case when there is a lot of entries for a key ( “k1,v1”, “k1,v2” etc). A synthetic data can show that easily: imagine a large csv file with same key in millions of rows. However, it doesn’t tell us if this is a real problem for customers. Since we do not have other tickets then either most of the customers do not have duplicates, or there is not so many duplicates and they do not see the issue.

I wonder if you can just ask the customer statistics for their lookups? For the provided mylookup.csv.gz file we have at most 4 entries, which is ok. 

{noformat}|inputlookup <LOOKUP_NAME> | stats count by field1 | where count > 1 | sort -count | head 10{noformat}

I just want to find a balance between a reasonable solution and time we need for validation & testing. The main problem is that this functionality is broken almost “by design”.

cc:[~accountid:6164c889c7bea400699a7e2f] 

PS

As a reference: I think same issue was observed recently here: [https://community.splunk.com/t5/Splunk-Search/Why-does-lookup-return-null-when-there-are-multiple-matches/m-p/631946|https://community.splunk.com/t5/Splunk-Search/Why-does-lookup-return-null-when-there-are-multiple-matches/m-p/631946|smart-link] . ",10/Apr/23 4:25 AM;6036b837d3fc7c006809a2d7;[~accountid:614d2968f1c17400684bde7f] I need further clarification. Are we asking the customer to run this search - |inputlookup mylookup.csv.gz | stats count by field1 | where count > 1 | sort 0 -countand provide the results?,11/Apr/23 6:52 AM;614d2968f1c17400684bde7f;[~accountid:6036b837d3fc7c006809a2d7] As I remember {{mylookup.csv.gz}} is only an example provided by the cutomer. It should be replaced by the lookup(s) in which they noticed the problem. Is it possible to get such information?,28/Apr/23 8:09 AM;6036b837d3fc7c006809a2d7;[~accountid:614d2968f1c17400684bde7f] I don’t believe we can ask for anything adddditional according to the recent exchange from this customer. Please let me know if you can proceed with what has already been provided for by the customer.,15/May/23 12:36 PM;6164c889c7bea400699a7e2f;[~accountid:614d2968f1c17400684bde7f] please see last note and provide update,"16/May/23 7:59 AM;614d2968f1c17400684bde7f;Hi [~accountid:6036b837d3fc7c006809a2d7] and [~accountid:6164c889c7bea400699a7e2f] , I was unavailable last 2 weeks. It is as it is. I will validate our current solution with codeowners.","05/Jun/23 9:33 AM;614d2968f1c17400684bde7f;Fyi [~accountid:6036b837d3fc7c006809a2d7]: since we do not know what is the cx data distribution, I’m working on an additional safety mechanisms - just in case. ","15/Jun/23 8:49 AM;614d2968f1c17400684bde7f;We (with [~accountid:6036b7a49b3ef700693a0fa3] ) just had a very pleasant call with the customer. They described the environment, which is: 2 sites, 9TB/d, linux, 100 users. They observe the problem with a 300MB kvstore lookup, on the indexer side.

Now the lookup is not indexed. They have noticed performance issues, validated logs, and are claiming that the lookup load takes ~10+ sec. That is possible.

I explained where is the problem, explicitly mentioned duplicated keys. They were surprised and didn’t expect duplicated keys there! The lookup data is generated, so now they need to understand the process and are trying to eliminate duplicates.

In the meantime they will provide us search logs where the delay related to lookups is visible.

We are going to meet with them next week. ",28/Jul/23 3:19 AM;6053a73f4a91ad0068c8310e;updated priority inline with SFDC priority,10/Nov/23 10:25 AM;622f94911f014e0069cc209b;Are there any updates on this? Would like to provide customer any new updates possible if there are any. ,12/Nov/23 9:36 AM;614d2968f1c17400684bde7f;We had a meeting with them on Friday. They promised to check for duplicates in the prod lookups and eliminate them if possible.,"12/Dec/23 10:48 AM;6036b837d3fc7c006809a2d7;[~accountid:614d2968f1c17400684bde7f] Customers update - In this case they do in fact only want to return ONE result. Other duplicate entries exist to show historical data and the lookup may be searched for that older data from time to time.

Here is a very simple lookup file example to illustrate. Imagine the lookup file with historical and current construction jobs for construction work being done at a university. In this example only one construction job is allowed to be active by a contractor at any given time. The lookup contains historical data of all jobs with the status of completed and active. In this example several fields will contain duplicate entries such as 'job_status' and 'contractor_name'. In our case the lookup file is over 100 mb.

job_number construction_start_date job_title job_status contractor_name
1000 02-20-2023 Annex building construction active John Doe Contactors
989 11-15-2022 Repaint parking garage completed John Doe Contactors
987 10-20-2021 Replace office windows completed John Doe Contactors
971 01-12-2023 Replace tile in bathrooms active Hernandez Construction
960 12-10-2022 Install new office lighting completed Hernandez Construction
955 06-06-2022 Replace exit signs completed Hernandez Construction","15/Feb/24 12:47 PM;6036b837d3fc7c006809a2d7;[~accountid:614d2968f1c17400684bde7f] The customer has decided to stick with the workaround of increasing the maxmentable size loading the lookup files as a short term solution. They will wait for you to come up with a permanent solution. Also, they had a question - when the lookup is loaded to memory, doe that happen every time the search is ran or is it cached in memory long term?","22/Feb/24 12:34 PM;6036b837d3fc7c006809a2d7;[~accountid:614d2968f1c17400684bde7f] The customer has decided to stick with the workaround of increasing the maxmentable size loading the lookup files as a short term solution. They will wait for you to come up with a permanent solution. Also, they had a question - when the lookup is loaded to memory, doe that happen every time the search is ran or is it cached in memory long term?",18/Mar/24 4:45 AM;6036b837d3fc7c006809a2d7;[~accountid:614d2968f1c17400684bde7f] Customer wanted to get any update. Do we have any additional information that we can provide since they implemented the second workaround suggested?,"18/Mar/24 7:55 AM;614d2968f1c17400684bde7f;{quote}when the lookup is loaded to memory, doe that happen every time the search is ran or is it cached in memory long term?{quote}

[~accountid:6036b837d3fc7c006809a2d7] when the lookup size is bellow {{max_memtable_bytes}} threshold (current workaround), then the lookup data is loaded into memory by each search in which the lookup is used.","22/Mar/24 10:21 AM;61d406430586a20069486775;[~accountid:614d2968f1c17400684bde7f] [~accountid:712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee] This customer would like to find out when this will be incorporated into a Sprint. Is there currently a pipeline for this or do we know when these scoping discussions will begin?

CC: [~accountid:6036b837d3fc7c006809a2d7] [~accountid:6053a73f4a91ad0068c8310e] ","26/Mar/24 6:09 AM;61d406430586a20069486775;[~accountid:614d2968f1c17400684bde7f] [~accountid:712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee] This customer would like to find out when this will be incorporated into a Sprint as they are seeking a permanent solution to this behavior. We would like to provide them with an estimated timeline, can you please detail the pipeline for this or know when these scoping discussions will begin?

CC: [~accountid:6036b837d3fc7c006809a2d7] [~accountid:6053a73f4a91ad0068c8310e][~accountid:5c6e33daa5f342215023f83e] ",26/Mar/24 7:01 AM;712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee;Hi [~accountid:61d406430586a20069486775] unfortunately the team is currently busy with the urgent feature development so I realistically it looks like we aren’t able to return back to this case before May '24.,"27/Mar/24 5:42 AM;61d406430586a20069486775;Thanks you [~accountid:712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee] , I appreciate the follow up. I definitely understand there are higher priorities on the list, but if your team can fit it in any sooner it would be very much appreciated! 🙂 ","19/Apr/24 9:31 AM;61d406430586a20069486775;[~accountid:712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee] Thanks again, I wanted to reach out to ensure we were still on a May timeframe for getting this implemented into a sprint? Thanks!","23/Apr/24 2:19 AM;712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee;Hi Samuel, we’re still targeting end of May for this ticket however there’s a probability that team will be able to prioritise earlier.",03/May/24 10:13 AM;61d406430586a20069486775;[~accountid:712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee] Wanted to check in on the status and see if the team was able to get this in a bit sooner? Thanks!,07/May/24 1:40 AM;712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee;Hi [~accountid:61d406430586a20069486775] unfortunately still needs to be postponed as there are more P3 customer issues in the backlog.,07/May/24 6:32 AM;61d406430586a20069486775;Thanks [~accountid:712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee] do we have an estimated timeline for the integration into the sprint?,07/May/24 7:53 AM;712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee;Unfortunately we have 8 P1-P3 issues in the backlog in ToDo status at the moment. We need to resolve these first.,"31/May/24 6:48 AM;61d406430586a20069486775;[~accountid:712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee] Can you please provide a timeline for this issue?

CC: [~accountid:5c6e33daa5f342215023f83e] ","24/Jun/24 7:24 AM;61d406430586a20069486775;[~accountid:614d2968f1c17400684bde7f] Any update on this issue?

CC: [~accountid:712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee] [~accountid:5c6e33daa5f342215023f83e] ",24/Jun/24 9:12 AM;614d2968f1c17400684bde7f;[~accountid:61d406430586a20069486775] we have an ongoing discussion with the Lookups owners. [~accountid:557058:849fbcca-205e-4649-ac02-bf68448d529d] and [~accountid:5ace957301a2012a6c32308d] will give an update.,"28/Jun/24 7:39 AM;61d406430586a20069486775;[~accountid:614d2968f1c17400684bde7f] [~accountid:557058:849fbcca-205e-4649-ac02-bf68448d529d] [~accountid:5ace957301a2012a6c32308d]
Any update on this issue?

CC: [~accountid:712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee] [~accountid:5c6e33daa5f342215023f83e]","12/Jul/24 10:55 AM;6036b837d3fc7c006809a2d7;[~accountid:614d2968f1c17400684bde7f] [~accountid:557058:849fbcca-205e-4649-ac02-bf68448d529d] [~accountid:5ace957301a2012a6c32308d]
Any update on this issue?

CC: [~accountid:712020:d175ef42-449b-4f59-9eb8-9f83eaefb7ee] [~accountid:5c6e33daa5f342215023f83e]","12/Jul/24 3:20 PM;557058:849fbcca-205e-4649-ac02-bf68448d529d;[~accountid:614d2968f1c17400684bde7f] is it reasonable to put limit on the number of duplicate? based on your [comment|https://splunk.atlassian.net/browse/SPL-230673?focusedCommentId=11427191]

for looking up {{K}} keys, the result from tsidx cannot exceed {{duplicate_max_count*K}} results. If the number of matches is within limit then we can load results in memory and perform appropriate lookup. If it exceeds this limit we can fail the search. Putting a cap max duplicates will put guardrail on how much memory the search can use.

cc: [~accountid:712020:7e34a4c0-1b40-4444-97da-310a77ee1b07] ","15/Jul/24 10:45 PM;557058:849fbcca-205e-4649-ac02-bf68448d529d;Based on [update|https://splunk.atlassian.net/browse/SPL-230673?focusedCommentId=14423907] from Samuel, they have increased max_memtable_bytes as workaround. This means lookups are loaded in memory on every search and adds performance overhead.

Proposed [solutions|https://splunk.atlassian.net/browse/SPL-230673?focusedCommentId=10247566] by [~accountid:614d2968f1c17400684bde7f]  :

# One key per batch: This solution  is similar to temporal lookup.
# Solution using {{st_query_count}} to first check count and then load the lookup data to perform processing within lookup processor.

These solutions can be implemented configuration so that it is applied to specific lookups. Do we know how these approaches compare with the current workaround of max_memtable_bytes? if perf is not great and implementation efforts is significant then it might be better to invest time in long term solution of replacing tsidx layer.

[~accountid:712020:7e34a4c0-1b40-4444-97da-310a77ee1b07] we would need your inputs to decide on this.","16/Jul/24 5:12 AM;614d2968f1c17400684bde7f;Hi [~accountid:557058:849fbcca-205e-4649-ac02-bf68448d529d] bellow are my comments about the ideas. My answers may sound pessimistic, so please read it like a worst case scenario analysis with justification from the calls with the customer.

# “These solutions can be implemented configuration so that it is applied to specific lookups.” - as I remember from meetings with this particular customer, they may need to modify multiple lookups and the sentiment was: we need a permanent fix for all lookups. They were a bit sceptical to apply workarounds which requires additional work from them (e.g. we proposed to add {{|dedup}} in their pipeline, which was not accepted). We do not even know how many lookups had duplicates in their env (at least a few). 
However, I it is worth to emphasise *the impact of this issue*. It is strange that this is reported only by one customer. The only explanation I have is that the issue is hard to notice, so all other customers are just not aware of it. The repro scenario is very simple, but nothing is failing until manually someone verifies the data.
# “One key per batch: This solution is similar to temporal lookup.” - indeed temporal lookups were changed, however it looks like a big difference in performance. Currently we have batching strategy. For temporal lookups it might not be important because they are not used as often as regular, right? Again, can be easily implemented per lookup, but it is only a kind of workaround if applied manually per lookup without informing the users about hidden errors in lookups where batching is still used. 
#  “Solution using {{st_query_count""}} - easier to implement than my current solution (the MR with callback from tsidx), however worst case scenario is the same: we need to load everything (plus we query the index twice).
# “is it reasonable to put limit on the number of duplicate” - it means that we will have two parameters which (in some sense, from the user perspective) control max entries. Existing max_matches  and the new one, {{duplicate_max_count}}. From the code point of view it might be a right thing to do as a safety mechanism. However, the customer rely heavily on our [doc for max_matches|https://docs.splunk.com/Documentation/Splunk/9.2.2/Admin/Transformsconf#:~:text=stanza%20is%20used).-,max_matches,-%3D%20%3Cinteger%3E%0A*%20The%20maximum] which says: {{Splunk software uses the first <integer> entries, in file order}}. They have a whole process implemented on this assumption. It is worth to ask if they have any reasonable upper bound. I’m guessing not, because (as I remember) the lookup entries represents a kind of user activity (signed up/off). Again, we didn’t get an answer about number of duplicates, for them it just has to work as documented.

Other ideas:

# Maybe we should modify the {{max_matches}} definition: we expect at most {{max_matches}} events and when we detect there is more we at least signal an error. It is not a solution for the current customer. However, when we think deeper about their use case, then it looks like lookups are not the right tool for the job. Of course it is not an easy decision, because we change the current behaviour. However, we should get rid off something which is not working silently.
# Modify the code deeper so that we use the callback for tsidx data in batches, but additionally we process returned data immediately up to max_matches entries per key. The processing is mostly  about applying filters which is crucial to determine amount of data needed. It is yet another refactoring in the existing code. ",16/Jul/24 10:49 AM;557058:849fbcca-205e-4649-ac02-bf68448d529d;Adding guardrail using {{max_matches}} makes sense. What is the effort in making changes to tsidx code? Another idea that we were discussing was replacing tsidx with sqlite where we can have more flexibility retrieve results using sql.,"17/Jul/24 8:09 AM;614d2968f1c17400684bde7f;Signalling error when {{tsidx}} has more data than expected can be done without tsidx code modification:

* IndexedCSV calls tsidx (via a few layers) from [here|https://cd.splunkdev.com/splcore/main/-/blob/develop/src/searchthingmgr/IndexedCSV.cpp#L1895].
* We pass there the buffere and its capacity.
* We expect that returned size of data is at most the {{capacity}} (batch size * max_matches).
* We do not know if there is more, but
* We can call {{sth.query}} with {{capacity + 1}}
* When we see {{capacity + 1}} values returned, then we know that there is more data than expected. Then we report an error.

However it doesn’t solve the customer pain, only can give us the scale of the issue.","22/Jul/24 12:01 PM;712020:7e34a4c0-1b40-4444-97da-310a77ee1b07;[~accountid:6036b837d3fc7c006809a2d7] could you please setup a call with a customer and the Product team?
I want to learn about their use case and how the workaround impacts their app.",22/Jul/24 12:13 PM;6036b837d3fc7c006809a2d7;[~accountid:712020:7e34a4c0-1b40-4444-97da-310a77ee1b07] I can but need to wait until Mateusz Juda returns from vacation after 8/6. He is off from 26 Jul - 6 Aug. Unless your taking his spot for engineering.,"22/Jul/24 12:28 PM;712020:7e34a4c0-1b40-4444-97da-310a77ee1b07;Thanks [~accountid:6036b837d3fc7c006809a2d7] If the customer could wait for another few days till the time Maeusz is back, I am good with that.",22/Jul/24 12:39 PM;6036b837d3fc7c006809a2d7;[~accountid:712020:7e34a4c0-1b40-4444-97da-310a77ee1b07] I believe that the TAM is relaying this to the customer. Thanks,"09/Aug/24 1:33 PM;712020:7e34a4c0-1b40-4444-97da-310a77ee1b07;Modified-- we are working on this as a bug 
Created the -enhancement- bug request for future - [https://splunk.atlassian.net/browse/SPL-260572|https://splunk.atlassian.net/browse/SPL-260572|smart-link] ","05/May/25 2:47 PM;557058:611ecd3d-c00a-44a3-b460-31db5af8ed89;I’m preparing this ticket for release notes automatic generation. Please do not change the title or the workaround field; they will be public facing. The previous contents of the workaround field are here:

Workaround:

With \[lookup]:max_memtable_bytes = (value greater than kvstore size, perhaps 40000000) in limits.conf, we get the following:

h2. splunk search 'index=* splunk_server=""[spls056.gcld1.ngext.com|http://spls056.gcld1.ngext.com]"" sourcetype=test123456 | search field_out=""NOTFOUND"" | stats count'
count

0

We don't see the ""Lookup table file size exceeds limit.conf...."" error in this scenario.","08/May/25 6:10 AM;61d406430586a20069486775;[~accountid:614d2968f1c17400684bde7f] [~accountid:557058:611ecd3d-c00a-44a3-b460-31db5af8ed89] It was noted by the customer that the workaround is incorrect. Please see their note below:

_Thanks for the update. Regarding the workaround language in the known issue: “Change the max_matches and max_memtable_bytes settings to values greater than the KV store size.”. I don’t believe we’ve encountered this issue with a KV store collection given how the collection is chunked into .csv files (not indexed) and replicated to the indexers (assuming replication for the collection is enabled). The language should read “_*_Change the max_matches and max_memtable_bytes settings to values greater than the CSV lookup file size_*_”._

CC: [~accountid:712020:7e34a4c0-1b40-4444-97da-310a77ee1b07] [~accountid:622f94911f014e0069cc209b] ","08/May/25 6:35 AM;614d2968f1c17400684bde7f;[~accountid:61d406430586a20069486775] Indeed, the customer is right. It is even more complicated because it is not clear what do we mean by “KV store size.”. I do not have the context of the recent changes related to public facing information. Please add me if there is a place with an active discussion, maybe I can add my 2 cents.","08/May/25 6:36 AM;61d406430586a20069486775;[~accountid:614d2968f1c17400684bde7f] Thanks Mateusz, [~accountid:557058:611ecd3d-c00a-44a3-b460-31db5af8ed89] / [~accountid:712020:7e34a4c0-1b40-4444-97da-310a77ee1b07] would you be able to assist with the public facing information?","23/Jul/25 11:44 AM;61436d20e7c3280070a1979f;[~accountid:557058:611ecd3d-c00a-44a3-b460-31db5af8ed89] Note that this ticket was included in Splunk Cloud 10.0.2503 Known Issues release notes by mistake (since the issue is Splunk Enterprise only). I have removed it from Splunk Cloud 10.0.2503 release notes. 

If this issue does affect Splunk Cloud after all, update the Affects versions field on this ticket accordingly and add the release note to Known issues.",,,,Done,09/Aug/24 1:36 PM
[PUBLIC] Restart is needed when AWS access key pairs rotate (w/o grace period) or other S3 config settings for Ingest Actions become invalid ,SPL-228646,2555772,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,,,,Samat Jain,6053b24d45a3bb006817344b,Samat Jain,6053b24d45a3bb006817344b,17/Aug/22 1:59 PM,30/Jul/25 4:25 PM,,,10.0.x,9.0.10,9.0.2,9.0.2209(Hersheys),9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Ice Breaker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ingest Actions,Network Output,,,,0,fy23q4-cmp-operability,Hersheys-Reviewed,intern_candidate_core_framework,mr_55580,Q3-2022,qa_da_issue,,"If I, as a Splunk Administrator, enter invalid configuration for Ingest Actions, I should be able to recover any data that was ingest/routed when the configuration was invalid.

For example, if I entered an invalid access key/secret key, ingest data that is routed with ingestion, but then realize my mistake, I should be able to enter a fixed access key/secret key, and all data I previously ingested should use the new access key/secret key.",,Howard (Shih-Chen) Chin,Richard Wang,srv- ssc-gitlab,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:30e41e2b-e9d1-427a-ae34-cf6659ef443a,6053a68b009fee00694bf279,613254346fa73c006a9e37be,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-228645,,,,,,,,,,SPL-228766,SPL-229300,,,,,,,SPL-228946,,17/Aug/22 1:59 PM;samatj;image-20220817-205714.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4626574,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3bb76ca5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,71280000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wed Jun 21 21:40:40 UTC 2023,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CMPOperability,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,1.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,fjiang(JIRAUSER47701),jwang(jwang),samatj(samatj),srv-ssc-gitlab(srv-ssc-gitlab),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cust Managed Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|iahx4f:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Samat Jain,6053b24d45a3bb006817344b,Boris Chen,Christina Noren,Matt Green,Rachel Perkins,61324249ac61dc00697cf056,613be989f6070d006b917dca,5af8dc294b719848347ec02c,5af5d6c28a1abd1427953558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,1.0,,,,,,,,,,,,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2022-08-17 21:41:56.716,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2022-08-17 21:41:56.716,2022-08-17 21:41:56.716,,,,,,Samat Jain,6053b24d45a3bb006817344b,,,,,,17/Aug/22 2:41 PM;6053a68b009fee00694bf279;[~accountid:6053bea406cbba006a0eca0c] FYI,23/Aug/22 8:26 AM;613254346fa73c006a9e37be;[Howard (Shih-Chen) Chin|https://cd.splunkdev.com/hchin] mentioned this issue in [a merge request|https://cd.splunkdev.com/splcore/qa/-/merge_requests/28618] of [Splunk Core / qa|https://cd.splunkdev.com/splcore/qa] on branch [bugfix/SPL-220099|https://cd.splunkdev.com/splcore/qa/-/tree/bugfix/SPL-220099]:{quote}SPL-220099/SPL-219483 Test Reloadable RfsOutputProcessor and S3 settings{quote},09/Nov/22 10:02 AM;6053a68b009fee00694bf279;Not prioritized in Q4 due to bandwidth constraint + edge case/unclear use-case. Let’s continue the discussion on this one - [~accountid:6053b24d45a3bb006817344b] to verify the possibility that the key expire scenario can happen from AWS side. cc [~accountid:5a1ef9dc40207c40ef9126d7] for customer use case. ,21/Jun/23 2:40 PM;6053bea406cbba006a0eca0c;Let’s check with Swetha on how they managed this for SmartStore,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2755137,SPL-236343,Remaining Backlog for S3 Output config and restartability post-9.0.2,To Do,17/Aug/22 1:59 PM
"""file"" is incorrectly listed as a supported scheme for ingest actions in outputs.conf.spec",SPL-228117,2546658,Bug,Resolved,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Fixed,Marcel Widjaja,6053a57445a3bb006816a45e,Felix Jiang,6053bea406cbba006a0eca0c,Felix Jiang,6053bea406cbba006a0eca0c,09/Aug/22 1:35 PM,30/Jul/25 4:26 PM,,10/Jun/24 4:24 PM,10.0.x,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.0.x,9.1.0,9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.1.x,9.2.0(Cobalt),9.2.1,9.2.2,9.2.x,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.2.3,,,,,,,,Ingest Actions,,,,,0,Backport_Approved,,,,,,,"{{    * Currently ""s3"" and ""file"" are the only supported schemes.}} 

is currently in [outputs.conf.spec.in|http://outputs.conf.spec.in/] , see [here|https://cd.splunkdev.com/splcore/main/-/merge_requests/55032/diffs#c6357a323e2b79127bb9bb12fc22750613cd1e74]

We should remove any mention of official support for “file” or “NFS” until we launch file system / NFS support in 9.0.X",,Andrew Brown,Automation for Jira,Izzy Park,Marcel Widjaja,srv- jira-backport,srv- ssc-gitlab,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,557058:f58131cb-b67d-43c7-b30d-6b58d40bd077,5a1ef9dc40207c40ef9126d7,6053a57445a3bb006816a45e,712020:c2fe0caf-bbfa-4cd1-b2e0-357df4e56984,613254346fa73c006a9e37be,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-257140,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@19d9681b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,40003200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mon Jun 17 23:18:37 UTC 2024,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CMPOperability,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,11.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),addon_com.codebarrel.addons.automation(addon_com.codebarrel.addons.automation),fjiang(JIRAUSER47701),ipark(ipark),mwidjaja(mwidjaja),15a91377-3491-4528-a796-d0763ea70df0(15a91377-3491-4528-a796-d0763ea70df0),srv-ssc-gitlab(srv-ssc-gitlab),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cust Managed Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Marcel Widjaja,6053a57445a3bb006816a45e,,,,,,,,,,,,,,,,,,0|iagh2f:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Marcel Widjaja,6053a57445a3bb006816a45e,Boris Chen,Christina Noren,Matt Green,Rachel Perkins,61324249ac61dc00697cf056,613be989f6070d006b917dca,5af8dc294b719848347ec02c,5af5d6c28a1abd1427953558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IngestActions-FY25Q2-S3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2024-02-07 23:05:53.838,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3_*:*_1_*:*_19280714_*|*_5_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_10385_*|*_10039_*:*_1_*:*_57965261228,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2024-02-07 23:05:53.838,2024-02-07 23:05:53.838,,,,,,Marcel Widjaja,6053a57445a3bb006816a45e,,,,,,07/Feb/24 3:05 PM;5a1ef9dc40207c40ef9126d7;Is there an update on this? cc [~accountid:6053bea406cbba006a0eca0c] ,"05/Jun/24 2:00 PM;6053bea406cbba006a0eca0c;[~accountid:6053a57445a3bb006816a45e] even though we’re launching in 9.3.0, we need to make sure this is not in any of the 9.0.X, 9.1.X, 9.2.X spec files - it will confuse customers w.r.t. what is supported","10/Jun/24 10:41 AM;6053bea406cbba006a0eca0c;Hey [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] - what should we do about older spec file versions in docs? I think you’ve mentioned to me previously that we shouldn’t adjust them, since a customer on that older version can look into their spec file in the software and see the error there as well. But shouldn’t we try to capture somewhere that the older spec files have an error?","10/Jun/24 11:16 AM;6053a57445a3bb006816a45e;* Code Quality Risk Levels (High/Medium/Low): Low
* Urgency (High/Medium/Low): Medium
** Prevent customer from using feature that is not yet ready/unsupported.  If customer is using the unsupported feature, it may lead to issues impacting their operations unnecessarily
* Does the fix have any breaking changes that changes default user behavior? No
* Unit Test coverage metrics: Not needed. This is a doc change
* Functional Tests (details, QA Champion, link to tests and/or test plan ):  Not needed
* How contained is the change? contained within outputs.conf spec file.  There’s zero risk in terms of change in functionality. 
Let us know if additional testing is required in these areas: 
** System testing: N
** Performance: N
** App Testing: N
** Upgrade tests: N
** Cloud Testing: N (1 week for 1 run)
* Is field training (Support/Sustaining) required? N
* Do documentation need to be updated? N","10/Jun/24 11:39 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:6053bea406cbba006a0eca0c] we can write a known issue to say that some text in the spec file is incorrect. If we want to use this SPL Jira for that purpose, you or someone can rewrite the title to something that describes the error in terms a customer would understand and then load it up with all of the specific {{Affects versions}} where we care to have it listed as a known issue. ","10/Jun/24 1:07 PM;712020:c2fe0caf-bbfa-4cd1-b2e0-357df4e56984;This request is Backport_Approved.

Does this issue need to be included in the release notes? No - Do_Not_Document_In_Release_Notes",10/Jun/24 4:21 PM;6053bea406cbba006a0eca0c;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] - loaded up the affect versions and changed the title. Open to your suggestions on title,"11/Jun/24 7:40 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;I’ve updated the title and am setting Document as = Include in release notes. Our automated publishing has been having some issues so I have to manually push this into each affected version. I’ll start with the more recent releases of 9.0.x, 9.1.x, and 9.2.x and try to get all of them eventually","11/Jun/24 7:40 AM;557058:f58131cb-b67d-43c7-b30d-6b58d40bd077;[~accountid:6053a57445a3bb006816a45e] since you have selected to document this in the release notes, please ensure that the Jira's title is clearly written and hide any internal terms or code names in the title by surrounding them with square brackets.","11/Jun/24 8:38 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;I took care of that ^^^ we’re good. Anyone else, please feel free to modify the title further, if desired","17/Jun/24 4:18 PM;613254346fa73c006a9e37be;[Marcel Widjaja|https://cd.splunkdev.com/mwidjaja] mentioned this issue in [a commit|https://cd.splunkdev.com/splcore/ufexotic/-/commit/4c8098f4af56f6ea3ff12c3d89027ba9f34fc6a3] of [Splunk Core / ufexotic|https://cd.splunkdev.com/splcore/ufexotic]:{quote}SPL-228117 Remove official text for supporting ""file"" / ""NFS"" until we launch...{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,10/Jun/24 4:24 PM
[Public] Error : Script execution failed for external search command 'runshellscript',SPL-227633,2535910,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Workaround,,,Joey Stewart,6036b80f4623c60069c04495,Joey Stewart,6036b80f4623c60069c04495,29/Jul/22 12:02 PM,30/Jul/25 4:24 PM,,18/Mar/25 11:21 PM,10.0.2503.x,10.0.x,8.2.2105(Monarch),8.2.2106(Northstar),8.2.2107(Olympus),8.2.2109(Pilatus),8.2.2111(Aero),8.2.2112(Butterfinger),8.2.2201.1(Crunch-Patch1),8.2.2201.2(Crunch-Patch2),8.2.2201(Crunch),8.2.2202(Dove),8.2.2203(Emerald)-Duplicate,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.2205(Ferrero Rocher),9.0.2208(Gummy Bear),9.0.2209(Hersheys),9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,Ice Breaker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search,,,,,0,AxpTriaged,cfit_reviewed,SEARCH_ALERTING,slo_violation,,,,"Cloud Customer {{progressive-pci}} is seeing messages since updating to v8.2.2201.1 on 6.25.22

{noformat} ERROR script [59709 TcpChannelThread] - Script execution failed for external search command 'runshellscript'{noformat}

It does match up with the date of the stack upgrade per [SkynetSearch - Error|https://skynet-search.splunkcloud.com/en-US/app/search/search?earliest=1656046800&latest=1656306000&q=search%20index%3Dcustomer_splunkd%20%60stack(sh%2Cprogressive-pci)%60%20%22*Script%20execution%20failed%20for%20external%20search%20command%20%27runshellscript%27*%22&sid=1658944612.44637_E8E82E3B-4665-4ADA-84CD-8E70A2DAF142&display.page.search.mode=fast&dispatch.sample_ratio=1]. Per the external docs, this is an internal, experimental command: [SplunkDoc|https://docs.splunk.com/Documentation/SplunkCloud/8.2.2201/SearchReference/Runshellscript] ([SplunkDoc - Internal Commands|https://docs.splunk.com/Documentation/Splunk/latest/SearchReference/Aboutinternalcommands]). I can see that nothing looks to have changed for customer search(s): [SkynetBtool - 24hrs|https://skynet-search.splunkcloud.com/en-US/app/search/search?earliest=-24h%40h&latest=now&q=search%20index%3Dcustomer_btool%20%60stack(sh%2Cprogressive-pci)%60%20%22*runshellscript*%22%20file%3D*%20stanza%3D*&display.page.search.mode=fast&dispatch.sample_ratio=1&sid=1658948089.60291_E8E82E3B-4665-4ADA-84CD-8E70A2DAF142] / [SkynetBtool - Prior to Upgrade|https://skynet-search.splunkcloud.com/en-US/app/search/search?earliest=1656046800&latest=1656306000&q=search%20index%3Dcustomer_btool%20%60stack(sh%2Cprogressive-pci)%60%20file%3D*%20stanza%3D%22Delete%20Action%20Triggered%22&display.page.search.mode=fast&dispatch.sample_ratio=1&display.events.timelineEarliestTime=1656079200&display.events.timelineLatestTime=1656082800&sid=1658948213.61087_E8E82E3B-4665-4ADA-84CD-8E70A2DAF142]. Does this have to do with [SPL-215850|https://splunk.atlassian.net/browse/SPL-215850] ([Splunk Cloud Known Doc|https://docs.splunk.com/Documentation/SplunkCloud/8.2.2203/ReleaseNotes/Issues#Version_8.2.2201])? Is there a solution? I don’t see anything calling this script except for internal processes: [SkynetBtool|https://skynet-search.splunkcloud.com/en-US/app/search/search?earliest=-24h%40h&latest=now&q=search%20index%3Dcustomer_btool%20%60stack(sh%2Cprogressive-pci)%60%20%22*runshellscript*%22%20file%3D*%20stanza!%3D%22Delete%20Action%20Triggered%22&display.page.search.mode=fast&dispatch.sample_ratio=1&display.prefs.events.offset=50&sid=1659022731.127127_ACA4E255-5D88-400C-8281-F91A1AB3372C].",,James Ervin,Joey Stewart,Jo Hornsby,Joshua Weinstein,Ketan Pawar,Mitika Pandya (C),PC Carlow,Shalabh Goyal,Sirish Mohan,srv -jira-gitlabci,Steven Roback,Thomas Evans,User known,User known,Victor Ebken,,,,,,,,,,,,,,,,,,,,,557058:ab939132-f05e-4f5f-8bec-6939a8133dcc,6036b80f4623c60069c04495,557058:f87e2651-5132-4aed-8b96-f608ee969435,6053b70081b82500685dc4cb,557058:84a600ad-55da-4237-b002-71f4acb2ec9d,6053a2d7695c3900707a4c68,6036b88a8ff09800716a0bdd,6053a60cf180c300675e7e8b,5c6e33daa5f342215023f83e,62ec512f825fbfbfcff13ef5,557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc,5af8dc4e79a85f5592620efc,unknown,unknown,557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30/Sep/22 10:06 AM;pcarlow;SPL-227633_SID.txt;https://splunk.atlassian.net/rest/api/3/attachment/content/4835058,29/Jul/22 12:18 PM;pcarlow;image-20220729-191851.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4552091,22/Aug/22 5:12 AM;pcarlow;image-20220822-121214.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4647506,28/Sep/22 9:26 AM;pcarlow;image-20220928-162545.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4823943,29/Sep/22 6:42 AM;pcarlow;image-20220929-133827.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4829470,29/Sep/22 6:42 AM;pcarlow;image-20220929-134205.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4829471,30/Sep/22 10:06 AM;pcarlow;image-20220930-170634.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4835059,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@27651c96,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Documentation,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started? Stack upgraded on 2022-06-25 @ 01:01:06,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Splunk Cloud,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,62985600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"o What is the customer environment for the issue? Cloud - Classic architecture - v8.2.2201.1

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,"o What errors are being reported?
ERROR script \[59709 TcpChannelThread] - Script execution failed for external search command 'runshellscript'",,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there? no error,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit? seen since upgrade,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mon Sep 25 21:40:31 UTC 2023,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Infrastructure - Scheduler,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,7.0,49.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,jervin(jervin),josephs(josephs),jweinstein(jweinstein),pcarlow(pcarlow),saileej(JIRAUSER48752),sgoyal(sgoyal),smohan(smohan),06ac125d-c228-4c3f-b77f-a6149c059c6f(06ac125d-c228-4c3f-b77f-a6149c059c6f),sroback(sroback),tevans(tevans),tracyr(JIRAUSER51787),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|iayk4v:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Boris Chen,Christina Noren,Matt Green,Rachel Perkins,61324249ac61dc00697cf056,613be989f6070d006b917dca,5af8dc294b719848347ec02c,5af5d6c28a1abd1427953558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SCSK Corporation,3310329,012400000005WzMAAU,P3,No,5005a00002ffPhKAAU,,Closed,Resolved,Standard,Tier4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3020315,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.
This looks to be an issue with an internal script

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 
no steps taken to tune


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
is this related to SPL-215850? is there a solution?",,,Cloud Premium,,,,,,,,,,,,,,,,2022-07-29 19:18:55.637,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10001_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_5278647203,,,,,,,,,,,,,,,,,,,,,,,,,,"Sustaining/Support template Accuracy  (data collected at same time, Support analysis filled in etc.)",Sustaining/Support template complete,Valid JIRA mandatory fields,Valid JIRA problem statement in JIRA Summary,,,,,,,,,,,,,,,,,,,,,,,,,,,,Updated the severity level from Sev-3 to Sev-2. Please refer to the P&T Customer Issues SLO: https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k/edit?usp=sharing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The setting precalculate_required_fields_for_alerts=0 can be set on saved searches that have no other alert actions attached aside from the ""Run A Script"" action, to quash the error. For saved searches that have multiple alert action attached, this may not be safe as it will disable back propagation of required fields for all alert actions, which might result in the parent search extracting more fields than required, which could negatively impact performance for that search.",,,,,,,,,,2022-07-29 19:18:55.637,2022-07-29 19:18:55.637,,,,,,,,,,,,,"29/Jul/22 12:18 PM;6036b88a8ff09800716a0bdd;This issue affecting multiple stacks.
[https://skynet-search.splunkcloud.com/en-US/app/cloudops/search?sid=1659121695.265297_102321C9-FC72-434C-BFE2-50722F6AC313|https://skynet-search.splunkcloud.com/en-US/app/cloudops/search?sid=1659121695.265297_102321C9-FC72-434C-BFE2-50722F6AC313]

!image-20220729-191851.png|width=948,height=401!","02/Aug/22 1:16 PM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;Running a {{""Show Source” }} action on any of the log events returned by the search in the ticket description shows that these events are coming from custom alert actions being executed by code in {{ExternProcessor.cpp}} (see the {{AlertActionsRequredFields}} string in the SID for the search):

{noformat}06-27-2022 04:59:23.149 +0000 ERROR script [49171 TcpChannelThread] - Script execution failed for external search command 'runshellscript'.
06-27-2022 04:59:23.153 +0000 ERROR TimeParser [49171 TcpChannelThread] - Invalid value ""+time+"" for time term 'latest'
06-27-2022 04:59:24.166 +0000 ERROR script [52106 TcpChannelThread] - sid:AlertActionsRequredFields_1656305964.244908_F02C6B89-D8B8-481E-8BC7-3EAF8D2DF3AA Invalid search ID ''.
06-27-2022 04:59:24.167 +0000 ERROR script [52106 TcpChannelThread] - Script execution failed for external search command 'runshellscript'.
06-27-2022 04:59:24.174 +0000 ERROR TimeParser [52106 TcpChannelThread] - Invalid value ""+time+"" for time term 'latest'
06-27-2022 04:59:25.202 +0000 ERROR TimeParser [49171 TcpChannelThread] - Invalid value ""+time+"" for time term 'latest'
06-27-2022 04:59:25.218 +0000 ERROR TimeParser [34683 TcpChannelThread] - Invalid value ""+time+"" for time term 'latest'
06-27-2022 04:59:26.196 +0000 ERROR script [65539 TcpChannelThread] - sid:AlertActionsRequredFields_1656305966.244921_F02C6B89-D8B8-481E-8BC7-3EAF8D2DF3AA Invalid search ID ''.
06-27-2022 04:59:26.196 +0000 ERROR script [65539 TcpChannelThread] - Script execution failed for external search command 'runshellscript'.{noformat}

It’s not clear whether the {{TimeParser}} errors are related. I think {{ExternProcessor}} is used to manage any old-style scripted alerts, so this error implies that one or more scripted alerts are passing an incorrect command line to the {{ExternProcessor}} code. This would fall into the purview of whichever team manages modular alerting now.

CC [~accountid:5c6e33daa5f342215023f83e] - I think this might be in your area these days?","04/Aug/22 7:27 AM;6036b80f4623c60069c04495;[~accountid:5c6e33daa5f342215023f83e] Can you confirm [~accountid:557058:ab939132-f05e-4f5f-8bec-6939a8133dcc] details? Are you the correct team member to review? If not, do you know who would own this piece?

Thanks,
Joey","04/Aug/22 12:07 PM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;[~accountid:5cd3411ee826800fcda5716f] , this is a SearchInfrastructure team issue - so far as I know Alerting resides in that area now. I’m not sure what team corresponds to that in Jira - possibly EnterpriseServices?","04/Aug/22 1:39 PM;5c6e33daa5f342215023f83e;Updated the Mission team to Enterprise services for us to take a look. 

cc: [~accountid:6053a60cf180c300675e7e8b] ","11/Aug/22 6:15 AM;6036b80f4623c60069c04495;[~accountid:5c6e33daa5f342215023f83e] [~accountid:6053a60cf180c300675e7e8b] 

Can you provide an update? The team was moved a week ago but no comments since.

Thanks,
Joey","11/Aug/22 9:26 AM;6053a60cf180c300675e7e8b;Assigning a Jira to a team does not mean that it will be picked up for execution right away. The team has a backlog of >30 customer issues and they are prioritized based on the severity of impact and number of customers impacted.

This ticket is in the queue but I don't think will be picked up soon. Can you please elaborate on the customer impact?","11/Aug/22 9:49 AM;6036b80f4623c60069c04495;[~accountid:6053a60cf180c300675e7e8b] I understand but it was being looked at and then moved into a team that didn’t respond. So, the thought then is that if it starts to be looked at by one team, when the next team is assigned, that team will also look at it. Seems reasonable. 

As for impact, I don’t believe there is performance issues with this message but impact is relative. From a customer perspective, they are seeing these messages and are looking for confirmation that they are non-impacting overall, whether that be front-end or back-end.

Please review as quickly as possible.

Thanks,
Joey","19/Aug/22 6:32 AM;6036b80f4623c60069c04495;Hi, 

Reaching back out on weekly request for an update. Please provide one as soon as possible.

Thanks,
Joey","22/Aug/22 5:12 AM;6036b88a8ff09800716a0bdd;[~accountid:6053a60cf180c300675e7e8b] This issue is producing 1000's of errors daily across multiple customer Cloud stacks:
[https://skynet-search.splunkcloud.com/en-US/app/cloudops/search?sid=1661170038.2031_ACA4E255-5D88-400C-8281-F91A1AB3372C|https://skynet-search.splunkcloud.com/en-US/app/cloudops/search?sid=1661170038.2031_ACA4E255-5D88-400C-8281-F91A1AB3372C]

!image-20220822-121214.png|width=1888,height=784!

{{index=customer_splunkd component=script log_level=ERROR ""Script execution failed for external search command"" 'runshellscript' | stats count by stack | sort - count}}","22/Aug/22 2:59 PM;6053a60cf180c300675e7e8b;We will look into this based on priority of other issues in backlog. Just FYI this is not a supported command:

[https://docs.splunk.com/Documentation/Splunk/9.0.0/SearchReference/Runshellscript|https://docs.splunk.com/Documentation/Splunk/9.0.0/SearchReference/Runshellscript]","12/Sep/22 9:35 AM;6036b80f4623c60069c04495;[~accountid:6053a60cf180c300675e7e8b] 

This is a script being run by internal. Please see the description as the details you provided were already there. I understand backlog but this case has been open since July 29th.

-Joey","23/Sep/22 6:30 AM;6036b80f4623c60069c04495;[~accountid:6053b70081b82500685dc4cb] 

When you get a chance, can you take a look at this? The review has been opened for a while and just assigned last week.

Thanks,
Joey","28/Sep/22 9:26 AM;6036b88a8ff09800716a0bdd;[~accountid:6053b70081b82500685dc4cb] This issue is producing 1000's of errors daily across multiple customer Cloud stacks:

[https://skynet-search.splunkcloud.com/en-US/app/cloudops/search?sid=1664382283.435155_D190358C-4307-49F2-B949-F59E6FCC5730|https://skynet-search.splunkcloud.com/en-US/app/cloudops/search?sid=1664382283.435155_D190358C-4307-49F2-B949-F59E6FCC5730]

!image-20220928-162545.png|width=1890,height=814!

{{index=customer_splunkd component=script log_level=ERROR ""Script execution failed for external search command"" 'runshellscript' | stats count by stack | sort - count}}","28/Sep/22 9:27 AM;6053b70081b82500685dc4cb;taking a look now,","28/Sep/22 10:21 AM;6053b70081b82500685dc4cb;looks like before each of these script failure messages , theres a message regarding invalid search ID

{noformat}09-28-2022 16:39:11.659 +0000 ERROR script [66660 TcpChannelThread] - sid:AlertActionsRequredFields_1664383151.1462072_FB21FCB0-9957-465E-B443-1037D720A917 Invalid search ID ''.
09-28-2022 16:39:11.659 +0000 ERROR script [66660 TcpChannelThread] - Script execution failed for external search command 'runshellscript'.{noformat}

this is referring to the custom search command packaged with splunk:

{noformat}	
/opt/splunk/etc/apps/search/default/commands.conf                       [runshellscript]
/opt/splunk/etc/system/default/commands.conf                            changes_colorder = true
/opt/splunk/etc/system/default/commands.conf                            enableheader = true
/opt/splunk/etc/apps/search/default/commands.conf                       filename = runshellscript.py
/opt/splunk/etc/system/default/commands.conf                            generates_timeorder = false
/opt/splunk/etc/system/default/commands.conf                            generating = false
/opt/splunk/etc/apps/search/default/commands.conf                       is_risky = true
/opt/splunk/etc/system/default/commands.conf                            maxinputs = 50000
/opt/splunk/etc/system/default/commands.conf                            outputheader = false
/opt/splunk/etc/system/default/commands.conf                            pass_timezone = false
/opt/splunk/etc/apps/search/default/commands.conf                       passauth = true
/opt/splunk/etc/system/default/commands.conf                            perf_warn_limit = 0
/opt/splunk/etc/apps/search/default/commands.conf                       python.version = python3
/opt/splunk/etc/apps/search/default/commands.conf                       required_fields = 
/opt/splunk/etc/system/default/commands.conf                            requires_preop = false
/opt/splunk/etc/system/default/commands.conf                            retainsevents = false
/opt/splunk/etc/system/default/commands.conf                            streaming = false
/opt/splunk/etc/system/default/commands.conf                            supports_getinfo = false
/opt/splunk/etc/system/default/commands.conf                            type = python{noformat}

this is configured in alert_actions.conf

{noformat}/opt/splunk/etc/system/default/alert_actions.conf                      [script]
/opt/splunk/etc/system/default/alert_actions.conf                      command = runshellscript ""$action.script.filename$"" ""$results.count$"" ""$search$"" ""$search$"" ""$name$"" ""Saved Search [$name$] $counttype$($results.count$)"" ""$results.url$"" ""$deprecated_arg$"" ""$search_id$"" ""$results.file$"" maxtime=""$action.script.maxtime{default=5m}$""
/opt/splunk/etc/system/default/alert_actions.conf                      description = Invoke a custom script
/opt/splunk/etc/system/default/alert_actions.conf                      filename = 
/opt/splunk/etc/system/default/alert_actions.conf                      forceCsvResults = auto
/opt/splunk/etc/system/local/alert_actions.conf                        hostname = https://progressive-pci.splunkcloud.com:443
/opt/splunk/etc/system/default/alert_actions.conf                      icon_path = mod_alert_icon_script.png
/opt/splunk/etc/system/default/alert_actions.conf                      label = Run a script
/opt/splunk/etc/system/default/alert_actions.conf                      maxresults = 10000
/opt/splunk/etc/system/default/alert_actions.conf                      maxtime = 5m
/opt/splunk/etc/system/default/alert_actions.conf                      track_alert = 1
/opt/splunk/etc/system/default/alert_actions.conf                      ttl = 600{noformat}

[https://skynet-search.splunkcloud.com/en-US/app/search/search?earliest=-24h%40h&latest=now&q=search index%3Dcustomer_btool stack%3Dprogressive-pci host%3Dsh* runshellscript&display.page.search.mode=verbose&dispatch.sample_ratio=1&sid=1664383549.352035_93CF241F-EBF2-4A25-83C1-D7CF90EBEE5D|https://skynet-search.splunkcloud.com/en-US/app/search/search?earliest=-24h%40h&latest=now&q=search%20index%3Dcustomer_btool%20stack%3Dprogressive-pci%20host%3Dsh*%20runshellscript&display.page.search.mode=verbose&dispatch.sample_ratio=1&sid=1664383549.352035_93CF241F-EBF2-4A25-83C1-D7CF90EBEE5D]

runshellscript is actually a python file we package, which is at [https://cd.splunkdev.com/splcore/main/-/blob/develop/cfg/bundles/search/bin/runshellscript.py|https://cd.splunkdev.com/splcore/main/-/blob/develop/cfg/bundles/search/bin/runshellscript.py]

looks like it is throwing this error defined in messages.conf here

{noformat}[EXTERN:RUNSHELLSCRIPT_INVALID_SID__S]
message      = Invalid search ID '%s'.
severity     = error
{noformat}

and that message is thrown from here in the ExternProcessor code

[https://cd.splunkdev.com/splcore/main/-/blob/develop/src/search/processors/ExternProcessor.cpp#L224|https://cd.splunkdev.com/splcore/main/-/blob/develop/src/search/processors/ExternProcessor.cpp#L224]

looks like the C++ code is using {{Str sid = Str::from(scriptArgs[8]);}} , or the 8th script argument 

but if we look at the command string in alert actions .conf, 

{noformat}command = runshellscript ""$action.script.filename$"" ""$results.count$"" ""$search$"" ""$search$"" ""$name$"" ""Saved Search [$name$] $counttype$($results.count$)"" ""$results.url$"" ""$deprecated_arg$"" ""$search_id$"" ""$results.file$"" maxtime=""$action.script.maxtime{default=5m}$""{noformat}

and then enumerate the arguments

{noformat}0 ""$action.script.filename$"" 
1 ""$results.count$"" 
2 ""$search$"" 
3 ""$search$"" 
4 ""$name$"" 
5 ""Saved Search [$name$] $counttype$($results.count$)"" 
6 ""$results.url$"" 
7 ""$deprecated_arg$"" 
8 ""$search_id$"" 
9 ""$results.file$"" 
10 maxtime=""$action.script.maxtime{default=5m}$""{noformat}

given this line , [https://cd.splunkdev.com/splcore/main/-/blame/develop/src/search/processors/ExternProcessor.cpp#L213|https://cd.splunkdev.com/splcore/main/-/blame/develop/src/search/processors/ExternProcessor.cpp#L213]

we know “script arguments” means the arguments to the the python module running the shell script, so the correct argument is picked for the sid. 

however, we can see from {{Invalid search ID ''.}} , that the sid string is actually empty , so we need to understand what this alert action is actually passing in as arguments to the python script.

[~accountid:6036b88a8ff09800716a0bdd]  can u check the audit logs in skynet for {{AlertActionsRequredFields_1664383151.1462072_FB21FCB0-9957-465E-B443-1037D720A917}} this sid ?","29/Sep/22 6:42 AM;6036b88a8ff09800716a0bdd;!image-20220929-133827.png|width=1644,height=380!

[~accountid:6053b70081b82500685dc4cb] , let me know if you need more than this or if we can get on a call to investigate further.

This maybe relevant, so I wanted to make you aware:
[https://splunk.slack.com/archives/CJWLPH9CJ/p1635278311073800|https://splunk.slack.com/archives/CJWLPH9CJ/p1635278311073800|smart-link]

!image-20220929-134205.png|width=1834,height=166!","29/Sep/22 8:13 AM;6053b70081b82500685dc4cb;[~accountid:6036b88a8ff09800716a0bdd] thats just just splunkd logs, i want the audit action entry for that search under customer_audit index for that sid.

thanks for the slack thread, going through that now","29/Sep/22 12:46 PM;6053b70081b82500685dc4cb;[~accountid:6036b88a8ff09800716a0bdd]  regarding Justin’s comment, one possibility here is progressive or other stacks may have custom settings for alert.expires, which is how long we keep an artifact or dispatch dir for after search completed and hit an alert trigger. Normally this is 24 hours. If this is less than that or some abnormally low, number, that could be issue. 

But still want the _audit entry to know what arguments are being passed into the script","30/Sep/22 10:06 AM;6036b88a8ff09800716a0bdd;[~accountid:6053b70081b82500685dc4cb] The only events being returned for the past 7 days are the searches we’ve performed for this text string. Am I doing something wrong?

{{index=customer_audit AlertActionsRequredFields_1664383151.1462072_FB21FCB0-9957-465E-B443-1037D720A917}}

!image-20220930-170634.png|width=1902,height=833!

[^SPL-227633_SID.txt]

","30/Sep/22 10:13 AM;6053b70081b82500685dc4cb;[~accountid:6036b88a8ff09800716a0bdd]  nope thats current, its just unfortunate we do not log what is being passed to this script. I will try to reproduce this on test stack next. ","05/Oct/22 9:44 AM;5af8dc4e79a85f5592620efc;Hi [~accountid:6053b70081b82500685dc4cb] and [~accountid:6036b88a8ff09800716a0bdd] ,

Just checking in to see if we can string together a current update for Progressive. 

They had brought this particular case to my attention by requesting a current update.

Best,

Thomas  ","05/Oct/22 10:48 AM;6053b70081b82500685dc4cb;[~accountid:5af8dc4e79a85f5592620efc]  we identified the issue in the sense that the search_id isn’t being properly passed into the script, leading it to throw, but we don’t have a root cause yet. Due to limited logging on the shell script args, needing to repro this in a test stack currently.","18/Oct/22 7:24 AM;6125db9a1827d1006846cee2;Team, I have an on-prem customer experiencing the same issue since upgrading to 9.0.0, relating to deprecated script alert action functionality ('Run as a script' trigger action). A colleague was able to reproduce the error in 9.0.0 test instance using a script that merely touches a file. So the issue isn’t with the customer’s script… let me know if this is the same thing\! SFDC case 3075810.","19/Oct/22 2:35 PM;613255fe6e5e1e0071f06e5a;[~accountid:616539acc669a60069cf7d1e] : Assigning this bug to you as you are the current on-call person. 

CC: [~accountid:6053a3fb81b82500685ced67] ","19/Oct/22 3:03 PM;6053b70081b82500685dc4cb;[~accountid:6125db9a1827d1006846cee2]  you are right, I think the issue is this is a regression, the code is not considering the 9 or 10 possible arguments","20/Oct/22 9:56 AM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;FYI, I read the last few comments on this ticket and looked at the SFDC case since I was just curious. I was actually able to confirm that the ""run a script"" action *does* work on 9.0.0 on heavy forwarders, although it is deprecated so shows a little warning in the UI. 

I was able to reproduce this error using the following configuration: 

{noformat}[test]
action.script = 1
action.script.filename = test.sh
action.webhook.enable_allowlist = 0
alert.suppress = 0
alert.track = 0
counttype = number of events
cron_schedule = * * * * *
dispatch.earliest_time = -24h@h
dispatch.latest_time = now
display.general.type = statistics
display.page.search.tab = statistics
enableSched = 1
quantity = 1
relation = rises by
request.ui_dispatch_app = search
request.ui_dispatch_view = search
search = | makeresults count=100 | eval c=random()% 100 | head c>5 {noformat}

It does *_not_* require being run on a Splunk forwarder.  The {{test.sh}} script is located in {{/opt/splunk/bin/scripts/test.sh}} and contains these contents: 

{noformat}#!/bin/sh
date >> /tmp/tmp.txt{noformat}

Adding the following debugging yields more information:

{noformat}splunk set log-level -level debug -name SearchParser -auth admin:changeme
splunk set log-level -level debug -name script -auth admin:changeme
splunk set log-level -level debug -name SearchScheduler -auth admin:changeme{noformat}

Once the alert runs we get this log output: 

{noformat}10-20-2022 07:45:00.150 -0700 INFO  SearchParser [23415 SchedulerThread] - PARSING: stats count AS query| where query >= [bj artifact_offset=1 job_delegate=scheduler ignore_running=f savedsearch=""nobody;search;test"" | stats count AS query] + 1
10-20-2022 07:45:00.181 -0700 INFO  SearchParser [23415 SchedulerThread] - PARSING: runshellscript ""test.sh"" ""0"" ""| makeresults count=100 | eval c=random()% 100 | head c>5"" ""| makeresults count=100 | eval c=random()% 100 | head c>5"" ""test"" ""Saved Search [test] number of events(0)"" ""http://SC-JE-DEV-C7.sv.splunk.com:8000/app/search/@go?sid="" """" """" ""/opt/splunk/var/run/splunk/dispatch/results.srs.zst"" maxtime=""5m""
10-20-2022 07:45:00.181 -0700 DEBUG script [23415 SchedulerThread] - Using Python interpreter python3
10-20-2022 07:45:00.181 -0700 INFO  script [23415 SchedulerThread] - found script file=/opt/splunk/etc/apps/search/bin/runshellscript.py
10-20-2022 07:45:00.181 -0700 ERROR script [23415 SchedulerThread] - SearchMessage orig_component=script sid=AlertActionsRequredFields_1666277100.24 message_key=EXTERN:RUNSHELLSCRIPT_INVALID_SID__%s message=Invalid search ID ''.
10-20-2022 07:45:00.181 -0700 ERROR script [23415 SchedulerThread] - Script execution failed for external search command 'runshellscript'.
10-20-2022 07:45:00.181 -0700 WARN  SearchScheduler [23415 SchedulerThread] - addRequiredFields: SearchProcessorException is ignored, sid=AlertActionsRequredFields_1666277100.24, error=Error in 'script': Script execution failed for external search command 'runshellscript'.
10-20-2022 07:45:00.181 -0700 INFO  SearchParser [23415 SchedulerThread] - PARSING: | makeresults count=100 | eval c=random()% 100 | head c>5{noformat}

So we have the script arguments. Let’s break these down:

{noformat}0: script name:          ""test.sh"" 
1: count of events:      ""0""
2: search string SPL:    ""| makeresults count=100 | eval c=random()% 100 | head c>5""
3: fully qualified SPL:  ""| makeresults count=100 | eval c=random()% 100 | head c>5"" 
4: savedsearch name:     ""test"" 
5: trigger reason:       ""Saved Search [test] number of events(0)"" 
6: link to saved search: ""http://SC-JE-DEV-C7.sv.splunk.com:8000/app/search/@go?sid=""
7: deprecated arg (tags): """"
8: results file:          """" 
9: unknown:               ""/opt/splunk/var/run/splunk/dispatch/results.srs.zst"" 
10: unknown:              maxtime=""5m""{noformat}

This corresponds to the structure of the alert_actions.conf setting:

{noformat}command     = runshellscript ""$action.script.filename$"" ""$results.count$"" ""$search$"" ""$search$"" ""$name$"" ""Saved Search [$name$] $counttype$($results.count$)"" ""$results.url$"" ""$deprecated_arg$"" ""$search_id$"" ""$results.file$"" maxtime=""$action.script.maxtime{default=5m}$""{noformat}

However it seems to be out of sync with the expectations in the {{runshellscript.py}} script itself: 

{noformat}
# These values will be sent to the shell script:
# $0 = scriptname
# $1 = number of events returned
# $2 = search terms
# $3 = fully qualified query string
# $4 = name of saved splunk
# $5 = trigger reason (i.e. ""The number of events was greater than 1"")
# $6 = link to saved search
# $7 = DEPRECATED - empty string argument
# $8 = file where the results for this search are stored(contains raw results)
{noformat}

which makes no reference to a SID. However, I think the {{runshellscript.py}} comments are out of date.

Let’s look at the _complete_ set of log messages that emanate when this alert (really, any alert) is run using the “run a script” action:

{noformat}10-20-2022 08:55:00.211 -0700 INFO  SearchParser [23415 SchedulerThread] - PARSING: stats count AS query| search query > 0
10-20-2022 08:55:00.213 -0700 INFO  SearchParser [23415 SchedulerThread] - PARSING: runshellscript ""test.sh"" ""0"" ""| makeresults count=100 | eval c=random()% 100 | head c>5"" ""| makeresults count=100 | eval c=random()% 100 | head c>5"" ""test"" ""Saved Search [test] number of events(0)"" ""http://SC-JE-DEV-C7.sv.splunk.com:8000/app/search/@go?sid="" """" """" ""/opt/splunk/var/run/splunk/dispatch/results.srs.zst"" maxtime=""5m""
10-20-2022 08:55:00.213 -0700 DEBUG script [23415 SchedulerThread] - Using Python interpreter python3
10-20-2022 08:55:00.213 -0700 INFO  script [23415 SchedulerThread] - found script file=/opt/splunk/etc/apps/search/bin/runshellscript.py
10-20-2022 08:55:00.213 -0700 ERROR script [23415 SchedulerThread] - SearchMessage orig_component=script sid=AlertActionsRequredFields_1666281300.432 message_key=EXTERN:RUNSHELLSCRIPT_INVALID_SID__%s message=Invalid search ID ''.
10-20-2022 08:55:00.213 -0700 ERROR script [23415 SchedulerThread] - Script execution failed for external search command 'runshellscript'.
10-20-2022 08:55:00.213 -0700 WARN  SearchScheduler [23415 SchedulerThread] - addRequiredFields: SearchProcessorException is ignored, sid=AlertActionsRequredFields_1666281300.432, error=Error in 'script': Script execution failed for external search command 'runshellscript'.
10-20-2022 08:55:00.213 -0700 INFO  SearchParser [23415 SchedulerThread] - PARSING: | makeresults count=100 | eval c=random()% 100 | head c>5
10-20-2022 08:55:00.385 -0700 INFO  SearchParser [28076 AlertNotifierWorker-0] - PARSING: runshellscript ""test.sh"" ""2"" ""| makeresults count=100 | eval c=random()% 100 | head c>5"" ""| makeresults count=100 | eval c=random()% 100 | head c>5"" ""test"" ""Saved Search [test] number of events(2)"" ""http://SC-JE-DEV-C7.sv.splunk.com:8000/app/search/@go?sid=scheduler__nobody__search__test_at_1666281300_74"" """" ""scheduler__nobody__search__test_at_1666281300_74"" ""/opt/splunk/var/run/splunk/dispatch/scheduler__nobody__search__test_at_1666281300_74/results.csv.gz"" maxtime=""5m""
10-20-2022 08:55:00.386 -0700 DEBUG script [28076 AlertNotifierWorker-0] - Using Python interpreter python3
10-20-2022 08:55:00.386 -0700 INFO  script [28076 AlertNotifierWorker-0] - found script file=/opt/splunk/etc/apps/search/bin/runshellscript.py
10-20-2022 08:55:00.386 -0700 INFO  script [28076 AlertNotifierWorker-0] - SearchMessage orig_component=script sid=scheduler__nobody__search__test_at_1666281300_74 message_key=EXTERN:RUNSHELLSCRIPT_RESULTS_FILE_PATH_ARG_DEPRECATED message=The results file path argument is now deprecated. Ignoring the option.
10-20-2022 08:55:00.386 -0700 INFO  script [28076 AlertNotifierWorker-0] - stderr for script runshellscript will be added to search.log
10-20-2022 08:55:00.386 -0700 DEBUG script [28076 AlertNotifierWorker-0] - Running script in non-streaming mode: runshellscript
10-20-2022 08:55:00.386 -0700 DEBUG script [28076 AlertNotifierWorker-0] - elapsed input: 1
10-20-2022 08:55:00.550 -0700 INFO  script [28076 AlertNotifierWorker-0] - Invoked script runshellscript with 756 input bytes (2 events).  Returned 39 output bytes in 164 ms.
10-20-2022 08:55:00.550 -0700 DEBUG script [28076 AlertNotifierWorker-0] - elapsed output: 1{noformat}

Towards the end we can see that _the script actually does run_: 

{noformat}10-20-2022 08:55:00.550 -0700 INFO  script [28076 AlertNotifierWorker-0] - Invoked script runshellscript with 756 input bytes (2 events).  Returned 39 output bytes in 164 ms.{noformat}

So this error is coming not from the script when it actually *_runs the action_*, but in an earlier code path in {{BranchedSearches::setRequiredFields }}which is called from {{ActionedSearch::setRequiredFields}} . This attempts to parse the alerting condition string to determine if the alerting condition itself requires any specific fields from the parent saved search that the alert condition is attached to. This behavior is known as “backpropagation” and can allow the parent search to be more efficient about its field extractions.

This backpropagation process _always_ seems to fail now, so far as I can tell, because of this mismatch in the argument list - which I haven’t tracked down, so I don’t know whether it’s a regression yet. Since we catch the exception, the script continues to run successfully.

Interestingly, [~accountid:5cd3413f27454f0fe4579c11] added an argument to control this backpropagation behavior in [https://splunk.atlassian.net/browse/SPL-217017|https://splunk.atlassian.net/browse/SPL-217017|smart-link], {{precalculate_required_fields_for_alerts}}. Setting this to 0 prevents the backpropagation and will eliminate the error.

Since we already know that the backpropagation work is not happening - because of the errors we see - and because we also know that customers are not complaining about their results being incorrect, we could make the recommendation to customers who are seeing this error that they set this in a {{local/savedsearches.conf}} stanza:

{noformat}[default]
precalculate_required_fields_for_alerts=0{noformat}

This can also be set on a per-search basis if only certain searches are affected.

In the general case, that may cause additional work to be done in streaming searches with alert actions since they will potentially have to extract more fields. However since for these customer’s searches we know that the backpropagation is broken, the setting should be safe to apply. 

The issue of why this error is occurring in the first place - when the mismatch in arguments happened - is still something that needs to be researched, probably. A basic Skynet search shows no occurrences of this error before 8.2.2203, so an additional reproduction on 8.2.2202 or lower might be sufficient to determine whether this is a regression.

*TL;DR*

If the customer is not seeing any misbhehavior in processing alerts, the error is benign and it should be safe to apply {{precalculate_required_fields_for_alerts=0}} to their saved searches.

There is still an undiagnosed issue with the behavior of {{BranchedSearches::setRequiredFields}} which is preventing backpropagation of required fields. This issue appears to be a mismatch in the actual and expected arguments provided to the {{runshellscript}}command when evaluating arguments. This may be a regression, or may have never worked - unknown at this time.","20/Oct/22 3:04 PM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;As it turns out, the code that logs the error message here in {{addRequiredFields}} in DispatchSearchUtils.cpp was added in Aurum and Emerald in SHA 6391ee65841d8009ec996d704ec29d760dd7c544:

{noformat}        }catch(const SearchProcessorException& ignore){
            gLogger.warn() << __func__ << "": SearchProcessorException is ignored, sid="" << info._sid << "", error="" << ignore;
        }{noformat}

Previously we did this and just swallowed up the exception (WHY? FOR THE LOVE OF….):

{noformat}        }catch(const SearchProcessorException& ignore){ (void) ignore; }{noformat}

which had been in place since *2009**_._* 

So, worst case scenario - backpropagation of required fields from alert actions _never worked at all_, we just didn’t notice. Best case, this actually regressed at some point. I am trying to rebuild Rarity (our oldest supported version) to see if this worked on that release, but it’s a bit of a pain building the older version.

The backpropagation logic was added in this MR dd87b1bce4a5a8b595d34166bc3cb17b0be8cf58 which has this elusive comment:

{noformat}SPL-27792 - backpropagate the union of required field sets from alert actions/conditions to the scheduled search such that the correct set of fields is extracted.
Can always be overriden by append the 'fields <required fields>' to the scheduled search. Added required_fields to commands, this way running a search script
does not trigger full kv-extraction - however for backwards compatibility the defualt value is set to '*' all fields. The following scripts do not require any fields:
sendemail, runshellscript, createrss
{noformat}

So… “the following scripts do not require any fields.” Which raises the question… what does this mean? An alert action using “runshellscript” might require any number of fields. Specifically, this seems to refer to the fact that the value of {{required_fields}} in commands.conf is set to the empty string for the {{runshellscript}} stanza. Which is true, but still confusing I think.

So this lends a bit more more credence to the theory that setting {{precalculate_required_fields_for_alerts=0}} for searches using the {{runshellscript}} alert action is safe _if no other alert actions are attached to that search_. For this alert action specifically, the required fields logic was pointless. If other alert actions are attached to the search, there could be the aforementioned performance impact.","21/Oct/22 7:20 AM;6036b80f4623c60069c04495;Thanks [~accountid:557058:ab939132-f05e-4f5f-8bec-6939a8133dcc] for that detailed description. I had to read over it twice tbh. Should we expect this to be fixed in an upcoming release since [~accountid:6036b88a8ff09800716a0bdd] pointed out that this is not just with the customer noted in the title of this case? In looking at {{progressive-pci}} we can see they are not running this command in any saved searches : [Skynet Btool|https://skynet-search.splunkcloud.com/en-US/app/search/search?earliest=-24h%40h&latest=now&q=search%20index%3Dcustomer_btool%20%60stack(sh%2Cprogressive-pci)%60%20%22*runshellscript*%22%20file%3D*%20stanza!%3D%22Delete%20Action%20Triggered%22&display.page.search.mode=fast&dispatch.sample_ratio=1&sid=1666361566.215398_93CF241F-EBF2-4A25-83C1-D7CF90EBEE5D]. Is there any expectation that can be set, and/or added to release notes, to alert customers that “if you are running this command in your searches, please add {{precalculate_required_fields_for_alerts=0}} which should resolve any errors **note: this can also cause unwanted performance behavior”? 

I may be jumping in here to soon so please disregard if the review is still ongoing.

-Joey","21/Oct/22 7:38 AM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;[~accountid:6036b80f4623c60069c04495] , this bug is not assigned to me so will need to be followed up on by the team handling Alerting - I merely tried to move the analysis along a bit. 

However based on my analysis if a customer has only the {{run a script}} action attached to a saved search, then it should be safe to set {{precalculate_required_fields_for_alerts=0}} to that search which will eliminate the error. If however they have other custom alerts action attached to the search, it’s possible that those alert actions would be depending on the backpropagation in some way and that there could be a regression in performance for those searches. 

Keep in mind that each alert action uses a distinct sequence of commands and the only one known to be problematic at this time is the “runshellscript” option - which technically shouldn’t be allowed in Cloud at this stage, so I assume that this is a case where the customer is depending on legacy behavior.","21/Oct/22 7:56 AM;6036b80f4623c60069c04495;Thanks [~accountid:557058:ab939132-f05e-4f5f-8bec-6939a8133dcc] ! 

I gotcha. If a customer is _not_ depending on the legacy behavior though, what is the expectation since the message are still seen? Maybe I am looking in the wrong place but Btool shows, specific to {{progressive-pci}}, they are _not_ using this script/command which would indicate that they are not depending on legacy behavior. I was only able to locate this command used in internal processes, not in saved searches. Which is kind of where my confusion comes in as to the ‘why’. I can’t speak on other customer stacks but this is kind of the question the {{progressive-pci}} asked in their case.

{quote}But we don't know what jobs these associate to or is this just generic messaging of certain commands in Splunk that are subject to security vulnerabilities and this is actually non-impacting? but we can't associate it to jobs so we don't know what is actually happening.{quote}

I’m kind of in the same boat as they are when I’m not able to locate any searches that are created by the customer. Honestly though, thanks for the additional review. It was super helpful 🙂 

-joey","21/Oct/22 8:23 AM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;[~accountid:6036b80f4623c60069c04495] - According to [this Skynet search|https://skynet-search.splunkcloud.com/en-US/app/search/search?earliest=-4h%40m&latest=now&q=search%20index%3Dcustomer_btool%20sourcetype%3D%22splunk%3Aconfig%3Abtool%3Asavedsearches%22%20stack%3A%3Aprogressive-pci%20%0A%7C%20where%20action_script%3D1%20OR%20action_script%3D%22true%22%0A%7C%20fillnull%20value%3DNULL%20stanza%20action_script%20action_script_filename%20disabled%20cron_schedule%20enableSched%0A%7C%20stats%20count%20by%20stanza%20action_script%20action_script_filename%20disabled%20cron_schedule%20enableSched&display.page.search.mode=smart&dispatch.sample_ratio=1&display.page.search.tab=statistics&display.general.type=statistics&sid=1666365303.210048_E8E82E3B-4665-4ADA-84CD-8E70A2DAF142] they have  quite a few searches that are attached to the “script” alert action - 35 at least although not all may be enabled. Any search using the {{action.script}} and {{action.script.filename}} parameters will be impacted by this issue. You will not be able to assess this by looking at the {{action.script.filename}}value in {{savedsearches.conf}} though and checking so see if it matches “runshellscript”- that parameter only specifies the customer’s script which will be run via the shell. All such scripts get passed through the {{runshellscript}} command, per the settings in the {{script}} stanza in {{alert_actions.conf}}.

These alerts are using the legacy style of alert actions which has been prone to security vulnerabilities. Modular Alerts do not leverage the {{runshellscript}} command and are preferred now, but we are not in a position to force customers to migrate away from the older usage. The effort to eliminate the {{runshellscript}} command was begun on [https://splunk.atlassian.net/browse/SPL-186254|https://splunk.atlassian.net/browse/SPL-186254|smart-link]in 2020, but unfortunately that effort has outlasted two product managers who were involved with it and there is no foreseeable date when that will happen.","21/Oct/22 9:05 AM;6036b80f4623c60069c04495;Thanks again [~accountid:557058:ab939132-f05e-4f5f-8bec-6939a8133dcc] for the additional details. What would be the right step going forward since you pointed out that Splunk won’t be pushing for using modular alerts on a wide-scale anytime soon (or at least that’s the assumption)? Would it not be beneficial to have this in our release notes? That way customers' can start to work towards it? Not to say the notes will be read but if it is publicized, Support can point back to it when customers inquire about the behavior. Does this Jira need to be pushed forward through [~accountid:616539acc669a60069cf7d1e] 's team with all the details you have provided already?

-Joey","21/Oct/22 9:38 AM;6125db9a1827d1006846cee2;Thanks [~accountid:557058:ab939132-f05e-4f5f-8bec-6939a8133dcc] , that was very helpful. My customer is On-Prem running Splunk 9.0, but it sounds like the same applies. I will test this out and provide the workaround to my customer. Will follow up here regarding the results.
PS: Since mine is an On-Prem customer, should I submit a separate JIRA?","21/Oct/22 10:03 AM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;[~accountid:6125db9a1827d1006846cee2] , I think since this code is present for both on-prem and Cloud customers, one ticket will likely suffice for both.

I am pretty sure at this point that this bug is a consequence of the path sanitization added to {{runshellscript}} as a security measure in this ticket:

{noformat}commit 76c66412a623e980920a4b573554aae597842a18
Author: Justin Kim <justink@splunk.com>
Date:   Wed Apr 28 21:42:52 2021 +0000

    SPL-91347 runshellscripts lacks path sanitization{noformat}

Unfortunately, this fix failed to take into account - unsurprisingly since there is no testing for any of this code - that there are two different contexts in which {{runshellscript}} is manipulated: once to do the backpropagation, and one to actually execute the alert. In the former case, we are not constructing a string of arguments that will work with the path sanitization. We are not actually running the script, we’re just evaluating the arguments to the script.

If my supposition is correct then this regressed in the Monarch Cloud release. Confirming that now and I will update the affectedVersions to match. To your question [~accountid:6036b80f4623c60069c04495] , once we know what versions are affected we can assess what the best response is. ","21/Oct/22 11:17 AM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;Confirmed that this regressed in Monarch in the same way: 

{noformat}
10-21-2022 10:54:00.326 -0700 INFO  SearchParser [9064 SchedulerThread] - PARSING: stats count AS query| search query > 0
10-21-2022 10:54:00.327 -0700 INFO  SearchParser [9064 SchedulerThread] - PARSING: runshellscript ""test.sh"" ""0"" ""| makeresults count=100 | eval c=random()% 100 | head c>5"" ""| makeresults count=100 | eval c=random()% 100 | head c>5"" ""test"" ""Saved Search [test] number of events(0)"" ""http://SC-JE-DEV-C7.sv.splunk.com:8000/app/search/@go?sid="" """" """" ""/opt/splunk/var/run/splunk/dispatch/results.srs.zst"" maxtime=""5m""
10-21-2022 10:54:00.327 -0700 DEBUG script [9064 SchedulerThread] - Using Python interpreter python3
10-21-2022 10:54:00.327 -0700 INFO  script [9064 SchedulerThread] - found script file=/opt/splunk/etc/apps/search/bin/runshellscript.py
10-21-2022 10:54:00.327 -0700 ERROR script [9064 SchedulerThread] - sid:AlertActionsRequredFields_1666374840.9 Invalid search ID ''.{noformat}

Adjusting affectsVersion accordingly.

I’m unsure what the best remedy is at this point, so I’d like to reserve judgment about how best to release note this until next week to let this settle a bit, but a release note will likely be necessary.","21/Oct/22 1:13 PM;6125db9a1827d1006846cee2;Thanks a ton, [~accountid:557058:ab939132-f05e-4f5f-8bec-6939a8133dcc] . My customer bit the bullet and moved over to the new functionality, so my work here is done. One more customer weaned from a long-deprecated feature!","02/Nov/22 5:16 AM;6036b88a8ff09800716a0bdd;Hi [~accountid:557058:ab939132-f05e-4f5f-8bec-6939a8133dcc], have there been any updates on the release notes or what our response to the customer should be?","02/Nov/22 6:50 AM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;[~accountid:6036b88a8ff09800716a0bdd] , this ticket is not assigned to me. The Alerting scrum team can make that determination.","02/Nov/22 2:03 PM;6053a60cf180c300675e7e8b;yes, this will be release notes as an issue with a workaround. Please feel free to provide the workaround to your customer","10/Nov/22 6:24 AM;6036b80f4623c60069c04495;The customer scripts using the workaround. Determined by [~accountid:557058:ab939132-f05e-4f5f-8bec-6939a8133dcc] 's provided search. The workaround has been confirmed successful and they will continue to monitor. Please update the release notes as soon as possible with the details of the workaround. 

As for this JIRA, once confirmed that it’s been updated to the front-facing docs, I think it can be closed.

Thanks!
Joey","10/Nov/22 7:03 AM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;[~accountid:616539acc669a60069cf7d1e] and I have been talking about this ticket. This Jira should not be closed until we determine whether there has been a regression in functionality in the reverse propagation of required fields for alert actions. At a minimum, if there has been no regression we can avoid doing any work in the case of the {{script}} action, and clean up the code accordingly.",01/Mar/23 3:42 PM;6053a60cf180c300675e7e8b;[~accountid:557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc] - can you please help documenting this issue and the workaround. Please let me know if there is anyone else frm doc team that needs to be involved.,01/Mar/23 4:14 PM;557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc; [~accountid:6053a60cf180c300675e7e8b] Can you clarify what the documentation ask is for this? Is it just to add this JIRA with workaround to known issues for cloud and on prem? Or does it require additional documentation? ,01/Mar/23 4:18 PM;6053a60cf180c300675e7e8b;yes list of known issues both on prem and cloud,01/Mar/23 5:04 PM;557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc;Hi [~accountid:6053a60cf180c300675e7e8b]. I just checked the release notes and it appears we already added this JIRA with the workaround to known issues for both cloud (back to 8.2.2111) and on prem (back to 9.0.0). So look like this one is complete in terms of docs. Let me know if you want me to change the ticket status or reassign. Thanks.,02/Mar/23 9:44 AM;6053a60cf180c300675e7e8b;Thanks [~accountid:557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc] . I will unassign you from this. This is still something we will verify and then close.,"25/Sep/23 12:53 PM;62ec512f825fbfbfcff13ef5;This comment is auto-generated to inform you that the Jira severity level has been updated to align with the SFDC priority.
 If there is more than one support case linked, it will be set to align with the highest priority of open cases. If all cases are closed, then it will align to the highest priority of all closed cases. Please refer to the [P&T Customer Issues SLO|https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k] or contact [#help-customer-slo|https://slack.com/app_redirect?channel=C02KZJ51P4H]","25/Sep/23 2:40 PM;62ec512f825fbfbfcff13ef5;This comment is auto-generated to inform you that the Jira severity level has been updated to align with the SFDC priority.
 If there is more than one support case linked, it will be set to align with the highest priority of open cases. If all cases are closed, then it will align to the highest priority of all closed cases. Please refer to the [P&T Customer Issues SLO|https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k] or contact [#help-customer-slo|https://slack.com/app_redirect?channel=C02KZJ51P4H]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2771272,SPL-237049,Active support & ops blocker issues - Scheduler,Done,18/Mar/25 11:21 PM
[PUBLIC] Federated Search UI Error: Cannot create saved search dataset for federated index if dataset name contains space,SPL-226877,2520015,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,Unknown,,Srinivas Bobba,6053b9162f452d006f83555b,Kathyhan Nguyen,6053bb23e394c30069cbb342,Kathyhan Nguyen,6053bb23e394c30069cbb342,15/Jul/22 9:49 AM,03/Sep/25 6:30 AM,,,10.0.2503.x,10.0.x,8.2.2202(Dove-duplicate),8.2.2203(Emerald)-Duplicate,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.2205(Ferrero Rocher),9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Federation,,,,,0,FederatedSearch_UI,,,,,,,"

!image-20220715-164741.png|width=750,height=569!

Steps to repro:

# Set up standard mode federated search between FSH and RSH on latest current build
# On RSH, create a saved search called “ss with space”
# On FSH, go to Federated Search → Create new federated index (see attached screenshot)
# Creation fails due to spaces in saved search name ",,Andrew Brown,Duane Waddle,Joshua Cowling,Matthew Ness,Nithin Krishna Reghunathan,Srinivas Bobba,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,557058:cbe7648c-3ff3-4c6f-90e3-c4a597bdaab7,5a2544e6151a4f1e363d493a,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,712020:aa092f51-80ff-4faa-bcbe-94ff3f918bc0,6053b9162f452d006f83555b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/Jul/22 9:49 AM;kathyhann;image-20220715-164741.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4493122,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@24120fde,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,26524800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wed Nov 20 21:35:23 UTC 2024,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Federation,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,1.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),kathyhann(kathyhann),mness(mness),sbobba(sbobba),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|iaccs7:,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2024-11-20 01:30:56.719,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Use REST API to create the federated saved search instead:<br>
<code>curl -k -u <username>:<password> -X POST https://localhost:8089/servicesNS/nobody/search/data/federated/index -d name=federated:index_kathy -d federated.dataset='savedsearch:ss with space' -d federated.provider=remote_deployment_1</code><br><br>
See [https://docs.splunk.com/Documentation/Cloud/latest/RESTREF/RESTfederated Federated search endpoint descriptions] in the ''REST API Reference Manual''.",,,,,,,,,,2024-11-20 01:30:56.719,2024-11-20 01:30:56.719,,,,,,,,,,,,,19/Nov/24 5:30 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;Apologies for all the minor updates on this Jira today. I’m trying to get the link in the workaround field to function when it shows up in the known issues list. My own first update to it accidentally deleted a part of the syntax. ,"20/Nov/24 11:03 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:557058:89ace195-adc2-4f25-9924-ded7c8f10bb6] I can’t get the mediawiki link in the workaround to function when it shows up in the release notes. The pipe character converts in the release notes html to {{&#124;}} making the link syntax invalid. Not sure if it’s better to remove the hyperlink and let people search for the right topic, or to hard code a link in html syntax that points to {{latest}} or some other named version.

Nothing urgent. This has been live for a while and no one has complained. I just noticed it while doing other release notes work. ","20/Nov/24 1:17 PM;557058:89ace195-adc2-4f25-9924-ded7c8f10bb6;Hardcoded it in html, per your suggestion. We’ll see if that works.

I’m pretty sure I’ve provided doc links using MediaWiki syntax in workarounds before…but maybe I’m misremembering. ",20/Nov/24 1:35 PM;557058:89ace195-adc2-4f25-9924-ded7c8f10bb6;That didn’t work. Looked at other tickets with links in their workarounds - turns out you need to format it like a MediaWiki external link. Done. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,15/Jul/22 9:49 AM
"Warning appears in the universal forwarder whenever any spl command is run: Warning: Attempting to revert the SPLUNK_HOME ownership Warning: Executing ""chown -R splunk /opt/splunkforwarder"".  This warning is expected and will not affect functionality.",SPL-226019,2499682,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Withdrawn,Cheney Li,557058:bc6d5618-b98e-471c-b56b-23062840fcd0,Esteban Calderon (C),616f231658006300690065a9,Esteban Calderon (C),616f231658006300690065a9,23/Jun/22 5:57 AM,30/Jul/25 4:26 PM,,27/Jul/22 11:52 AM,10.0.x,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Universal Forwarder,,,,,0,,,,,,,,"Problem Description:

UF 9.0 shows warning messages when any splunk command is run 

/opt/splunkforwarder/bin/splunk status
Warning: Attempting to revert the SPLUNK_HOME ownership
Warning: Executing ""chown -R splunk /opt/splunkforwarder""

Problem Analysis:

-Proper doc was followed step by step to enable boot-start: [https://docs.splunk.com/Documentation/Splunk/9.0.0/Admin/ConfigureSplunktostartatboottime#Enable_boot-start_on_.2Anix_platforms|https://docs.splunk.com/Documentation/Splunk/9.0.0/Admin/ConfigureSplunktostartatboottime#Enable_boot-start_on_.2Anix_platforms|smart-link] 

--Attempted to replicate the issue
-After enabling boot start like this: sudo /opt/splunkforwarder/bin/splunk enable boot-start -user splunk
**I got this message:
Systemd unit file installed by user at /etc/systemd/system/SplunkForwarder.service.
Configured as systemd managed service.
-This doesnt seem correct, as the steps followed were supposed to enable it with init.d, create a script under /opt/forwarder/etc/init.d/splunk but it did not

-This probably happened due to incompatibility of the OS with init.d, no problem there 

-Lab environment also presented warnings, with and without boot-start enabled ",,Aseem Anand (C),Becky Simmons,Gregory Runyon,Jennifer Worthington,Jo Hornsby,Manish Pant (C),Pavan Chandrashekar (C),Shradha Balakrishna (C),Shreeraj Panchasara (C),srv- ssc-gitlab,Tahoor Hashmi (C),Uggen Dhanabalan,User known,User known,User known,User known,User known,Vinu Alazath,,,,,,,,,,,,,,,,,,61f42cd1e4a724006afe21cf,6036b837d3fc7c006809a2d7,6036b881185376007024178f,557058:236db286-0673-4667-9c0b-a9114f695d13,557058:f87e2651-5132-4aed-8b96-f608ee969435,61b6cf50ead1440069705e4d,61b6cf282c73380069d11313,61b6cf2f57d5c30071f81092,712020:c389ba6f-df10-4ed0-a394-c96e84ea349a,613254346fa73c006a9e37be,63c6d968417bef6fc906d679,62343730867a4e0070974a78,unknown,unknown,unknown,unknown,unknown,6053a62006cbba006a0db90f,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-227428,,,,,,,,,,,,,,,,23/Jun/22 5:43 AM;ecalderon;Installation_2993203.txt;https://splunk.atlassian.net/rest/api/3/attachment/content/4400356,30/Mar/23 8:28 AM;a52c0ac4-15d6-41b8-a2e6-4d19940cad84;image-20230330-152834.png;https://splunk.atlassian.net/rest/api/3/attachment/content/5582711,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@481d3e3,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"o Have they made any changes to their environment just before the issue started?
Not relevant as this happens with a fresh installed UF 9.0 or upgraded from previous version ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,31795200,,,,,,,,,,,,,,,,,,,,,,,,,Installed Version,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"o What is the customer environment for the issue? 
UF9.0 
o If available, provide Splunk topology diagram, host name with IP mapping.
Not available, subcontractor reviewing possible upgrade issues before implementing on his cx's environment
",,,,,,,,,,"o What errors are being reported?
No errors on functionality so far have been reported ",,,,,,,,,,,,,No,,,,,,,,,,,,,"o What is the expectation when this problem is not there?
Warnings should not show up ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• How long has the customer been tracking this issue and how frequent this issue is being hit?
The issue is hit every time a spl command is run ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Fri Sep 20 15:40:56 UTC 2024,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Edge,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,2.0,31.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,fd8b34e1-d754-4311-be39-20a7e7aaaccb(fd8b34e1-d754-4311-be39-20a7e7aaaccb),chli(lcheney),ecalderon(JIRAUSER53427),jworthington(jworthington),a52c0ac4-15d6-41b8-a2e6-4d19940cad84(a52c0ac4-15d6-41b8-a2e6-4d19940cad84),mhoustonludlam(mhouston-ludlam),48971d7d-7f5a-46af-ac1b-5c03efe95ad8(48971d7d-7f5a-46af-ac1b-5c03efe95ad8),d951e9a3-df10-4a58-9e5e-9335ac0e5ef2(d951e9a3-df10-4a58-9e5e-9335ac0e5ef2),srv-ssc-gitlab(srv-ssc-gitlab),bae2ed88-213e-4505-8808-b04d06481dcf(bae2ed88-213e-4505-8808-b04d06481dcf),f0ba2203-57c5-4e83-b78a-8db981902e73(f0ba2203-57c5-4e83-b78a-8db981902e73),valazath(valazath),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cust Managed Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|ia96hr:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cheney Li,557058:bc6d5618-b98e-471c-b56b-23062840fcd0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This is by design from the security perspective.
Documentation for fix https://splunk.atlassian.net/browse/SPL-",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ITConcepts Solutions GmbH - Partner,3023298,012400000005WzMAAU,P3,No,5005a00002BtGXXAA3,,Closed,Resolved,Standard,Tier3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,GDI Infra-FY23Q2-S3,GDI Infra-FY23Q2-S4,GDI Infra-FY23Q2-S5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment
-Either upgrade or install UF 9.0
-start splunk 
• If available, will customer upgrade to fixed version?
N/A
• If support is able to reproduce, share the setup.
Used NOVA machine to install UF 9.0
Start splunk, as soon as it is installed, the warnings show up 
-Even after enabling boot-start which is what the end user did 
\[sudo] $SPLUNK_HOME/bin/splunk enable boot-start -user bob
Attached file with output of terminal with steps done ",,,,,,,,,,,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,Standard,,,,,,,,,,,,,,,,2022-06-23 15:33:54.886,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_27869_*|*_10039_*:*_1_*:*_8403045,,,,,,,,,,,,,,,,,,,,,,,,,,Sustaining/Support template complete,Valid JIRA mandatory fields,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2022-06-23 15:33:54.886,2022-06-23 15:33:54.886,,,,,,Cheney Li,557058:bc6d5618-b98e-471c-b56b-23062840fcd0,,,,,,"23/Jun/22 6:03 AM;616f231658006300690065a9;They pushed their init.d scripts as they are working with RHEL 7. No issues with that. 

This is just an FYI

Using init.d or systemd doesn’t seem to be relevant to the warnings showing up at this point ","23/Jun/22 8:33 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;Hi [~accountid:616f231658006300690065a9] this is as design on 9.0 Linux.

There is a new feature called “Least privilege mode” on Linux, which is automatically enabled for both fresh install and upgrade. The basic idea is run UF as *non root* by default improve the security level.

We found a bug during testing [https://splunk.atlassian.net/browse/SPL-215840|https://splunk.atlassian.net/browse/SPL-215840|smart-link] . The root cause is that every time users run {{sudo ./bin/splunk start}}, splunk runs as root for a while, writing some conf/log files as root, then drop to non root. Therefore the non root cannot access these files anymore, and splunk cannot start. To fix this, we run {{chown}} automatically to revert the file ownership.

During security review, ProdSec team asked us to add this warning to let customers know this privileged command has been executed automatically. So we have this warning when starting UF. Please note if users run {{splunk start}} as non root, this chown will NOT change anything without errors. This is also as expected since no files will be modified by root at all.

Please let me know if customers still have concerns. Thanks.","23/Jun/22 1:28 PM;616f231658006300690065a9;[~accountid:557058:bc6d5618-b98e-471c-b56b-23062840fcd0] Thank you for the answer, cx is asking if there is a way to hide the warnings and if this is documented publicly to share with his customer.

Would the warnings be avoided if they start splunk as non root?","23/Jun/22 1:42 PM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;[~accountid:6036b79be2020c0070c2e265] sorry this cannot be hidden as security perspective. As non root it still shows up but wont change anything. This is the original ticket [https://splunk.atlassian.net/browse/SPL-224143|https://splunk.atlassian.net/browse/SPL-224143|smart-link] .

I think it’s not documented. [~accountid:557058:236db286-0673-4667-9c0b-a9114f695d13] could you please update the doc about this? Thank you.","24/Jun/22 9:57 AM;557058:236db286-0673-4667-9c0b-a9114f695d13;Hi Cheney, I set this up to appear as a known issue. Once you fix and close the ticket, it will move to “resolved issues” automatically. I edited the summary as it will appear in the release notes, feel free to modify it if i did not capture the issue accurately.","24/Jun/22 10:10 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;Thank you [~accountid:557058:236db286-0673-4667-9c0b-a9114f695d13] \!

[~accountid:616f231658006300690065a9] , please let me know if we can close this ticket then it will show up in the release note. Thanks.","24/Jun/22 11:04 AM;557058:236db286-0673-4667-9c0b-a9114f695d13;Right now it shows up as a “known issue” if you want it to show up as a known issue you need to keep it open. Once you close it, it will show as resolved. But If it is fixed in the same release where it is found it will disappear. ","24/Jun/22 11:19 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;Thank you [~accountid:557058:236db286-0673-4667-9c0b-a9114f695d13] . Maybe we can keep it open.

BTW, about another doc problem described by [~accountid:616f231658006300690065a9], pls update the [https://docs.splunk.com/Documentation/Splunk/9.0.0/Admin/ConfigureSplunktostartatboottime#Enable_boot-start_on_.2Anix_platforms|https://docs.splunk.com/Documentation/Splunk/9.0.0/Admin/ConfigureSplunktostartatboottime#Enable_boot-start_on_.2Anix_platforms|smart-link] as well. We made the {{-systemd-managed}} option default to 1 for 9.0, so I think we should mention 2 things in this doc:

# Boot start is enabled automatically during install/upgrade by systemd ({{-systemd-managed}} 1)
# If users want to enable boot-start by systemd manually, they can just enable it with or without {{-systemd-managed 1}} 
# If users want to use init.d only, they have to disable boot-start manually first (if it’s successfully enabled by default), then run “enable boot-start” with {{-systemd-managed 0}} explicitly",02/Jul/22 9:20 AM;613254346fa73c006a9e37be;[Bhavya Lalithya Tetali|https://cd.splunkdev.com/btetali] mentioned this issue in [a commit|https://cd.splunkdev.com/splcore/qa/-/commit/640a43454d14bf4e59c5d555d5de26ec4975a3d0] of [Splunk Core / qa|https://cd.splunkdev.com/splcore/qa] on branch [test/btetali/TNT-14950|https://cd.splunkdev.com/splcore/qa/-/tree/test/btetali/TNT-14950]:{quote}TNT-14950: Add the remaining test cases and fix guid issue due to SPL-226019{quote},"12/Jul/22 4:35 AM;61b6cf2f57d5c30071f81092;Hi [~accountid:557058:236db286-0673-4667-9c0b-a9114f695d13] We tried enabling boot-start manually, but that was not persistent. Could you please let me know if there is any other way we can try to resolve this.","12/Jul/22 4:03 PM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;[~accountid:61b6cf2f57d5c30071f81092] , this is as design. We will not fix it until we completely redesign the Splunk launcher(main.c). I guess it’s not gonna happen in the short term. Now we just document it as a known issue.

CC [~accountid:6053a6d62f452d006f82899b] [~accountid:6053ba4a94d7b90069f9193d] [~accountid:604a8d29bfef95006be55e4b] ",01/Aug/22 8:48 AM;613254346fa73c006a9e37be;[Oleksandr Shestopalov|https://cd.splunkdev.com/oshestopalov] mentioned this issue in [a commit|https://cd.splunkdev.com/core-ee/qa-tools/helmut/-/commit/341059dfaa93af9f0ae6045be66fcc22d531ce07] of [EE - Developer Platforms / QA Tools / helmut|https://cd.splunkdev.com/core-ee/qa-tools/helmut] on branch [INFRA-37553-Update-`helmut`-to-support-Splunk-v.9|https://cd.splunkdev.com/core-ee/qa-tools/helmut/-/tree/INFRA-37553-Update-%60helmut%60-to-support-Splunk-v.9]:{quote}The `re.match` was replaced by `re.search`.{quote},01/Aug/22 8:51 AM;613254346fa73c006a9e37be;[Oleksandr Shestopalov|https://cd.splunkdev.com/oshestopalov] mentioned this issue in [a merge request|https://cd.splunkdev.com/core-ee/qa-tools/helmut/-/merge_requests/404] of [EE - Developer Platforms / QA Tools / helmut|https://cd.splunkdev.com/core-ee/qa-tools/helmut] on branch [INFRA-37553-Update-`helmut`-to-support-Splunk-v.9|https://cd.splunkdev.com/core-ee/qa-tools/helmut/-/tree/INFRA-37553-Update-%60helmut%60-to-support-Splunk-v.9]:{quote}Draft: The `re.match` was replaced by `re.search`.{quote},01/Aug/22 10:11 AM;613254346fa73c006a9e37be;[Oleksandr Shestopalov|https://cd.splunkdev.com/oshestopalov] mentioned this issue in [a merge request|https://cd.splunkdev.com/core-ee/qa-tools/helmut/-/merge_requests/405] of [EE - Developer Platforms / QA Tools / helmut|https://cd.splunkdev.com/core-ee/qa-tools/helmut] on branch [INFRA-37553-Update-helmut-to-support-Splunk-v9|https://cd.splunkdev.com/core-ee/qa-tools/helmut/-/tree/INFRA-37553-Update-helmut-to-support-Splunk-v9]:{quote}INFRA-37553: Update helmut to support Splunk v9{quote},"10/Aug/22 5:53 AM;62343730867a4e0070974a78;[~accountid:557058:bc6d5618-b98e-471c-b56b-23062840fcd0] 

I have case where customer facing issues due this warning messages , Below is what customer facing :
Our problem is, that sourcing of setSplunkEnv in a BASH will not add Splunk bin in front of PATH.
Because it use ""splunk envvars"" command which showing first Warning lines. This is not a good idea for the used eval command:
/usr/app/splunkforwarder/bin/splunk envvars
Warning: Attempting to revert the SPLUNK_HOME ownership
Warning: Executing ""chown -R splunk /usr/app/splunkforwarder""
...

For Testing I remove sourcing of setSplunkEnv in .bashrc of our splunk user and create systemd configuration as written on [https://docs.splunk.com/Documentation/Forwarder/9.0.0/Forwarder/Installleastprivileged|https://docs.splunk.com/Documentation/Forwarder/9.0.0/Forwarder/Installleastprivileged|smart-link] .

You can test like this :

# switch to user splunk (using bash - do not not sourcing setSplunkEnv in .bashrc) and

# su - splunk
$ env | egrep -i 'path|splunk|shell' | sort
HOME=/home/splunk
LOGNAME=splunk
MAIL=/var/spool/mail/splunk
PATH=/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/splunk/.local/bin:/home/splunk/bin
PWD=/home/splunk
SHELL=/bin/bash
USER=splunk

# source setSplunkEnv (setSplunkEnv is trying to run command Warning in eval...)

$ . /usr/app/splunkforwarder/bin/setSplunkEnv
-bash: Warning:: command not found
Tab-completion of ""splunk


What would be the ETA for this SPL fix ","10/Aug/22 7:15 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;[~accountid:62343730867a4e0070974a78] as mentioned, this is by design and will not be fixed. It’s just a warning and will not affect any functionalities no matter the cmd is successful or failed. Please just ignore it.","26/Aug/22 9:49 AM;5d9519c44af8460dd557b6d0;Never going to be fixed?  The warning is misleading 
_Attempting to revert the SPLUNK_HOME owner_ship
”revert” implies it was wrong. Customer checked deployment and ownership was splunk, nothing required to revert. Listing two WARNINGS to IGONRE at every CLI command makes “real” warnings less noticeable. Makes Splunk look sloppy to our customers.","26/Aug/22 10:32 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;[~accountid:5d9519c44af8460dd557b6d0] , pls see my comments [https://splunk.atlassian.net/browse/SPL-226019?focusedCommentId=9446173|https://splunk.atlassian.net/browse/SPL-226019?focusedCommentId=9446173|smart-link] .

{{Customer checked deployment and ownership was splunk}} does not mean the file owner is always splunk - it might be changed after they run CLI or Splunk pre-checks.

I know this warning is annoy, but it does not affect any functionalities. Pls kindly explain this to our customers. Thanks.

CC [~accountid:604a8d29bfef95006be55e4b] ","26/Aug/22 11:48 AM;5d9519c44af8460dd557b6d0;The “Warning” lines cause problems for customer’s python scripts which expect a response from a cli call, not preamble warning lines.  Lots of extra work for cx to ignore the two warning lines. cx suggested sending warning to std err to std out.",08/Nov/22 11:43 AM;613254346fa73c006a9e37be;[Bhavya Lalithya Tetali|https://cd.splunkdev.com/btetali] mentioned this issue in [a commit|https://cd.splunkdev.com/splcore/qa/-/commit/f3a6659cdb9c994d37e97e1c0294c9c4ce1426c2] of [Splunk Core / qa|https://cd.splunkdev.com/splcore/qa] on branch [cherry-pick-22f84115|https://cd.splunkdev.com/splcore/qa/-/tree/cherry-pick-22f84115]:{quote}Merge branch 'test/btetali/TNT-14950' into 'develop'{quote},"13/Dec/22 8:35 AM;61f42cd1e4a724006afe21cf;Hi [~accountid:557058:bc6d5618-b98e-471c-b56b-23062840fcd0] , 

I have a customer who has re-open the case which is with me currently and has the below concerns:

{quote}So could you clarify if I understand you correctly that using the Splunk_TA_stream application for capturing traffic is impossible with a default starting configuration of Splunk (i.e. with the configuration that is set during the RPM package installation) and it is by design? If I am correct, could you please clarify what way to run Splunk if preferable for using Splunk_TA_stream for capturing traffic:

1. Changing systemd splunk unit by adding necessary capabilities

2. Running splunk with root user

3. Something else{quote}

Can you please look into this and help me with the answer to this?

Thanks!","13/Dec/22 10:04 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;[~accountid:61f42cd1e4a724006afe21cf] As discussed with Stream TA owner [~accountid:6053a62006cbba006a0db90f] , please add CAP_NET_ADMIN capability to the unit file. Yes this is by Splunk UF design. 

For further questions about Stream TA, pls open a Stream ticket if needed. Thanks.",13/Dec/22 12:45 PM;6053a62006cbba006a0db90f;See this documentation - [https://docs.splunk.com/Documentation/StreamApp/8.1.0/ReleaseNotes/NewFeatures|https://docs.splunk.com/Documentation/StreamApp/8.1.0/ReleaseNotes/NewFeatures|smart-link],"30/Mar/23 8:28 AM;61b6cf50ead1440069705e4d;Hi [~accountid:557058:bc6d5618-b98e-471c-b56b-23062840fcd0] , I could also see on the UF that “splunkd was not running”. even though it is active and accessible. Thanks,

Screenshot attached.

!image-20230330-152834.png|width=719,height=220!",30/Mar/23 8:35 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;[~accountid:61b6cf50ead1440069705e4d] I believe this is irrelevant. These warnings show up in *ALL* splunk CLIs. Please open another ticket with details. Thanks.,"04/Jul/23 6:55 AM;63c6d968417bef6fc906d679;[~accountid:557058:bc6d5618-b98e-471c-b56b-23062840fcd0] HI,
I have a customer who has the same issue and is aware of the situation that the functionality is not affected because of the issue, but he is more concerned about the security issue it will cause. If you could say is there any workaround with respect to the security issue it can cause or there wont be any problem with the security, please confirm.
Is there any information as to which version will have the fix?","04/Jul/23 7:01 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;[~accountid:63c6d968417bef6fc906d679] this is a security improvement by design other than a bug, and will not fix. Thanks.","17/Sep/24 9:49 PM;61b6cf282c73380069d11313;Hi [~accountid:557058:bc6d5618-b98e-471c-b56b-23062840fcd0] I have a customer who is facing similar issue, we have informed it as security improvement by design. Customer responded as below:

“Thanks for the response, and the reference to the documented behaviour.
The comment could go further to say that ownership is changed even if it already has the expected value…

I understand that there is a use case for this behaviour, there will also be a significant number of organisations -us included- where this behaviour causes quite the significant amount of alarm.

How can this behaviour be turned off?
I can tell the next response might be ‘no’, so to skip ahead a bit, lets see how we can have the behaviour wrapped in an ‘if’ statement.
Most programming languages include the ability to change their execution path based on tests. Or at least all the programming languages I’m familiar with, lets be honest they’re too numerous to list here.

Lets have this behaviour configurable. I look forward to your confirmation this is well understood. If you’d like to reference the programming language used to change ownership, I’d be happy to provide links to online documentation for if-statements.“

Please let me know how can we help customer for their requirement.

CC: [~accountid:6036b8d15ddf020069bffe0f] [~accountid:712020:c0217ac3-baf2-42c2-923b-07fae643b5b5] [~accountid:622657aaa12450006889ec73] 

Thank you!","18/Sep/24 8:20 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;[~accountid:61b6cf282c73380069d11313] I’m not sure I understand the question and concern, especially {{ownership is changed even if it already has the expected value}} - this should not happen. 

Pls open another ticket with detailed reproduce steps, actual results and expected results. Also upload the diag and the UF unit file(if any). Thanks. ","19/Sep/24 10:05 PM;61b6cf282c73380069d11313;[~accountid:557058:bc6d5618-b98e-471c-b56b-23062840fcd0] I have confirmed with customer that they are  experiencing same issue as described in this ticket with the same warnings on UF.  I understand it is a expected behavior in Splunk designed as security perspective. However, customer wants to customize to turn off this behavior. We appreciate if you could suggest any workaround/ custom build trial for this request?

Thank you! ","20/Sep/24 8:40 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;[~accountid:61b6cf282c73380069d11313] Do you mean cx enabled both init.d and systemd, as mentioned in this ticket, and expect splunk create a script in init.d other than splunkd? Or their file ownership is set to another user, but splunk actually reverts it to splunk user, which is not mentioned in this ticket?

Again, pls file a new ticket. I don’t want to abuse this one. There is no easy answer until we know the real concern from cx.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,27/Jul/22 11:52 AM
"[PUBLIC] [Federated search UI] Remote dataset dropdown menu resets to ""Index"" after selecting federated provider",SPL-225037,2477503,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,,,,Kathyhan Nguyen,6053bb23e394c30069cbb342,Kathyhan Nguyen,6053bb23e394c30069cbb342,31/May/22 2:14 PM,30/Jul/25 4:24 PM,,,10.0.x,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Federation,UI - Misc,,,,0,aurum-bug-bash,Aurum-Reviewed,axp_customization_migrated,,,,,"Please see attached video for repro 

[http://10.224.127.164:6461/en-US/manager/search/federated_search|http://10.224.127.164:6461/en-US/manager/search/federated_search] (admin/Chang3d\!)

# Log into search head on aurum build *d1ea205911f0*
# Go to Settings → Federated Search 
## Create federated provider mapping to RSH 
## Create federated index (Federated Indexes → Add federated index)
# Under “remote dataset”, select “saved search”. Then choose your provider in the “federated provider” dropdown
# “Remote dataset” has been reset to “index”",,Matthew Ness,Srinivas Bobba,Suketu Shah,User known,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,6053b9162f452d006f83555b,6053a66586b0dd007188092d,unknown,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,31/May/22 2:11 PM;kathyhann;Screen Recording 2022-05-31 at 2.09.05 PM.mov;https://splunk.atlassian.net/rest/api/3/attachment/content/4297887,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@69dcca57,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,104457600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Fri Jun 03 00:43:33 UTC 2022,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Core Admin UI,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,1.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,asednev(JIRAUSER44723),kathyhann(kathyhann),sangp(JIRAUSER44904),sbobba(sbobba),suketus(suketus),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Admin Experience,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i02ai7:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2022-06-01 02:33:08.763,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2022-06-01 02:33:08.763,2022-06-01 02:33:08.763,,,,,,,,,,,,,"31/May/22 7:33 PM;6053ac5e66c879006838c561;[~accountid:6053b5f52f452d006f833293]  and [~accountid:6053b9162f452d006f83555b] , can you confirm this is not a blocker?","01/Jun/22 10:18 PM;6053b9162f452d006f83555b;Thanks [~accountid:6053bb23e394c30069cbb342]  for the video attachment.

Is this a regression from earlier version?

I don’t believe this is a blocker issue. Looks like an UI related issue

cc [~accountid:6053b5f52f452d006f833293]  [~accountid:602463dcc4ec0f006e926cec] ","02/Jun/22 10:54 AM;6053a66586b0dd007188092d;as per [~accountid:6053b9162f452d006f83555b] its not a blocker for Aurum, hence marking out of Aurum

[~accountid:6053b5f52f452d006f833293] [~accountid:602463dcc4ec0f006e926cec] can you please confirm?","02/Jun/22 5:18 PM;602463dcc4ec0f006e926cec;[~accountid:6053a66586b0dd007188092d]  Confirmed, not a blocker.

I moved this issue to AXP Customization team since we own the UI for Federated Search.



cc: [~accountid:6053b5f52f452d006f833293] [~accountid:6053b9162f452d006f83555b] [~accountid:6053ac9bf180c300675ec9ce] ",02/Jun/22 5:43 PM;6053a66586b0dd007188092d;Thanks [~accountid:602463dcc4ec0f006e926cec] for the update. will move out of Aurum,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2340691,SPL-218341,"Phase 4 - MDL - Fed Search, Structured Data Sources, and Flex Index ",To Do,31/May/22 2:14 PM
"[PUBLIC] When all inherited roles are taken out from admin role, it will cause admin user failed to show other users even though all capabilities is set natively.",SPL-222105,2415764,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Workaround,Patrycja Ciepielewska,615ab34bbfa2c1006ba2c6a5,Daniel Mak,6036b8354623c60069c04637,Daniel Mak,6036b8354623c60069c04637,06/Apr/22 6:38 AM,30/Jul/25 4:25 PM,,10/May/22 12:48 AM,10.0.x,8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Identity - Authentication (AuthN),,,,,0,apac_support:triaged,,,,,,,"When all inherited roles are taken out from admin role, it will cause admin user failed to show other users even though all capabilities is set natively. (can_delete role is used in this repro).

h3. *Steps to repro:*

* Install fresh 8.2.1
* Add the following to etc/system/local/authorized.conf to make it same as customer environment 

(Note that they have “importRoles =” set as customer want to remove imherited role and each role have native capabilities set)



{noformat}[role_default_user]
accelerate_search = enabled
change_own_password = enabled
cumulativeRTSrchJobsQuota = 0
cumulativeSrchJobsQuota = 0
edit_search_schedule_window = enabled
export_results_is_visible = enabled
get_metadata = enabled
get_typeahead = enabled
input_file = enabled
list_inputs = disabled
output_file = enabled
pattern_detect = enabled
request_remote_tok = enabled
rest_apps_view = enabled
rest_properties_get = enabled
schedule_search = enabled
search = enabled
srchMaxTime = 0
upload_lookup_files = enabled

[role_3pp-usr]
accelerate_search = disabled
change_own_password = disabled
edit_search_schedule_window = disabled
export_results_is_visible = disabled
get_metadata = disabled
get_typeahead = disabled
importRoles = default_user
list_inputs = disabled
output_file = disabled
pattern_detect = disabled
request_remote_tok = disabled
rest_apps_view = disabled
rest_properties_get = disabled
rest_properties_set = disabled
schedule_rtsearch = disabled
search = disabled
srchMaxTime = 0

[role_admin]
accelerate_search = enabled
change_own_password = enabled
cumulativeSrchJobsQuota = 400
delete_by_keyword = disabled
edit_search_schedule_window = enabled
edit_sourcetypes = enabled
edit_statsd_transforms = enabled
embed_report = enabled
export_results_is_visible = enabled
get_metadata = enabled
get_typeahead = enabled
grantableRoles = admin
importRoles = can_delete
input_file = enabled
list_inputs = enabled
list_metrics_catalog = enabled
output_file = enabled
pattern_detect = enabled
request_pstacks = enabled
request_remote_tok = enabled
rest_apps_view = enabled
rest_properties_get = enabled
rest_properties_set = enabled
rtsearch = enabled
schedule_search = enabled
search = enabled
securegateway_read = disabled
securegateway_write = disabled
srchIndexesDefault = *;_*
srchJobsQuota = 100
srchMaxTime = 8640000
upload_lookup_files = enabled{noformat}



* Add user=user_3pp to use role=user_3pp

{noformat}./splunk add user user_3pp -role user_3pp -password 12345678{noformat}

* Restart splunk
* List user to make sure user_3pp is visable

{noformat}# /home/daniel/splunk/bin/splunk list user

username:		admin
full-name:		Administrator
role:			admin

username:		user_3pp
full-name:
role:			3pp-usr{noformat}



* From UI, remove can_delete role and list user again. Only admin shown now.

{noformat}# /home/daniel/splunk/bin/splunk list user

username:		admin
full-name:		Administrator
role:			admin{noformat}

Now, user_3pp can’t be seen.

As {{grantableRoles = admin}} is set, we need to check the capabilities of admin to see why.

{noformat}From the document, when ""grantableRoles = admin"" is set,
They can see only users who have been assigned roles that containcapabilities that are a union of these capabilities.{noformat}



* First, check what capabilities role_can_delete have.

{noformat}# ./splunk cmd btool authorize list  role_can_delete
[role_can_delete]
cumulativeRTSrchJobsQuota = 0
cumulativeSrchJobsQuota = 0
deleteIndexesAllowed = *
delete_by_keyword = enabled
rtSrchJobsQuota = 6
run_collect = enabled
run_mcollect = enabled
schedule_rtsearch = enabled
srchDiskQuota = 100
srchFilterSelecting = true
srchJobsQuota = 3
srchMaxTime = 100days{noformat}

* From above, role_can_delete got the following 3 roles. 

{noformat}run_collect = enabled
run_mcollect = enabled
schedule_rtsearch = enabled{noformat}

Does admin account got this role. Let’s check it.

{noformat}# /home/daniel/splunk/bin/splunk cmd btool --debug authorize list role_admin|egrep 'run_collect|run_mcollect|schedule_rtsearch'
/home/daniel/splunk/etc/system/default/authorize.conf              run_collect = enabled
/home/daniel/splunk/etc/system/default/authorize.conf              run_mcollect = enabled
/home/daniel/splunk/etc/system/default/authorize.conf              schedule_rtsearch = enabled{noformat}

How about RESTAPI.

{noformat}# curl -k -u admin:adminadmin https://localhost:8089//services/authentication/users/admin|egrep 'run_collect|run_mcollect|schedule_rtsearch'
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 10884  100 10884    0     0   124k      0 --:--:-- --:--:-- --:--:--  125k{noformat}

*To workaround:*

Remove ""grantableRoles = admin” from role_admin will workaround this. However, this is not permanent as any changes to role_admin, Splunk will add back ""grantableRoles = admin”

*Questions:*

Look like when role_can_delete removed, the capabilities is gone although btool can show it is there but RESTAPI prove that it didn’t exist.",,Daniel Mak,Leo Liou,User known,William Li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6036b8354623c60069c04637,5ad8089dbaaf152a765686e7,unknown,6036b8099b3ef700693a13bb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,SPL-211994,,,,,,,,,,,,,,,,,08/Apr/22 7:38 AM;pciepielewska;Screenshot 2022-04-08 at 14.16.09.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4055337,08/Apr/22 7:38 AM;pciepielewska;Screenshot 2022-04-08 at 14.17.23.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4055335,08/Apr/22 7:38 AM;pciepielewska;Screenshot 2022-04-08 at 14.21.11.png;https://splunk.atlassian.net/rest/api/3/attachment/content/4055336,06/Apr/22 6:56 AM;daniel;diag-dmaklin03.sv.splunk.com-2022-04-06_21-52-52.tar.gz;https://splunk.atlassian.net/rest/api/3/attachment/content/4045415,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3912ff16,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"o Have they made any changes to their environment just before the issue started?

No.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,99273600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"o What is the customer environment for the issue? 
On-Prem 8.2.1

o If available, provide Splunk topology diagram, host name with IP mapping.
Just a single host",,,,,,,,,,"o What errors are being reported?

No error. Admin account can't see other user",,,,,,,,,,,,,No,,,,,,,,,,,,,"o What is the expectation when this problem is not there?

Admin user can see other user.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• How long has the customer been tracking this issue and how frequent this issue is being hit?

Always",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Tue Aug 02 02:55:48 UTC 2022,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,4.0,11.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,daniel(daniel),cweiliou(cweiliou),pciepielewska(JIRAUSER53220),williaml(williaml),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cust Managed Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i9w1xr:,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Patrycja Ciepielewska,615ab34bbfa2c1006ba2c6a5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Länsförsäkringar AB (Publ),3698033,012400000005WzMAAU,P3,No,5005a00003JKAUUAA5,,Closed,Resolved - Work Around,Standard,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

No UI bug.
 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

Repro steps and diag attached.

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

Repro steps and diag attached.

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

As customer remove all inherited role, may be some capabilities is missing.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 

It can workaround by remove ""grantableRoles = admin"" but it will add back by Splunk if admin role is change again.

 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.

Need to find out whether it is a bug or not.",,,Standard,,,,,,,,,,,,,,,,2022-04-07 01:52:26.605,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13628_*:*_1_*:*_1728996550_*|*_3_*:*_2_*:*_1097662672_*|*_10001_*:*_1_*:*_45864620_*|*_10039_*:*_1_*:*_44060037,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Two possible approaches:
1. Remove the option grantableRoles = admin from authorize.conf -  this is not permanent workaround and will need to be done every time admin role is modified.

2. Add any capabilities that the other user roles have to the ""admin"" role.",,,,,,,,,,2022-04-07 01:52:26.605,2022-04-07 01:52:26.605,,,,,,Patrycja Ciepielewska,615ab34bbfa2c1006ba2c6a5,,,,,,"06/Apr/22 6:52 PM;5ad8089dbaaf152a765686e7;# Review the Jira Summary {color:#36B37E}*[ PASS ]*{color} 
# Review Accuracy and Completeness {color:#36B37E}*[ PASS ]*{color}  
# Review the Description Field {color:#36B37E}*[ PASS ]*{color} 
# Review the Support Template Tab {color:#36B37E}*[ PASS ]*{color} 

+Comments to engineering+

+Comments to TSE+

*_Triaged_*

","08/Apr/22 7:38 AM;615ab34bbfa2c1006ba2c6a5;Hello [~accountid:6036b8354623c60069c04637] , I would like to confirm the scenario and results that you see. 

Could you please confirm that this is the reproduction scenario and that you see the same results?

# Install splunk 8.2.1
# Create the {{etc/system/local/authorize.conf}} with the exact same content as you put in the description.
# Add the user with role 3pp_usr and restart splunk

*RESULT:* Both admin and 3pp_usr are visible when doing{{ ./splunk list user}}, both users are visible when doing {{./splunk _internal call /services/authentication/users}} (REST call)

4. Edit admin role so that it has no imported roles (uncheck can_delete from the UI)

*RESULT:* Both {{./splunk list user}} and  {{./splunk _internal call /services/authentication/users}} show only the admin user

5.Splunk restart
*RESULT:*  Both{{ ./splunk list user }}and REST call show both admin and user_3pp. 

Do you see the same behavior?

Regarding the crucial capabilities and how they appear in the UI:

{noformat}run_collect = enabled
run_mcollect = enabled
schedule_rtsearch = enabled{noformat}

They are first visible in the role admin as inherited (after points 1-3 from above):

!Screenshot 2022-04-08 at 14.16.09.png|width=1222,height=188!

After the can_delete role is removed from the admin inheritance (point 4), they appear disabled thus causing the 3pp_user not being visible:

!Screenshot 2022-04-08 at 14.17.23.png|width=1222,height=188!

After splunk restart (point 5 from above) they are again visible as native (and the 3pp_user can be listed):

!Screenshot 2022-04-08 at 14.21.11.png|width=1222,height=188!



Could you please confirm that the behaviour you experience is the same?

If so, I think the Jira is to investigate why the default enabled capabilities (run_collect, run_mcollect, schedule_rtsearch) are not enabled for admin just after the admin role has been edited, but only after the Splunk restart.","10/Apr/22 6:49 AM;6036b8354623c60069c04637;[~accountid:615ab34bbfa2c1006ba2c6a5] Yes, that is the exactly behaviour I’n seeing as well. ","12/Apr/22 7:41 AM;615ab34bbfa2c1006ba2c6a5;[~accountid:6036b8354623c60069c04637], I have investigated this issue and it happens because of how we currently handle the default roles' capabilities defined in authorize.conf \[default] stanza, namely:

{noformat}run_collect
run_mcollect
schedule_rtsearch{noformat}

In your scenario at first when the admin role is edited, the default capabilities are not turned ON and because of that (and because of {{grantableRoles = admin}}), the {{user_3pp}} cannot be listed by admin.

After restart the configuration with the default capabilities is loaded. This means that then admin has them enabled and the user user_3pp should be listed.

Please confirm with the customer that this workaround (restart Splunk) is working for them. If so I would like to suggest closing this case with this workaround.


In order to fix the current behaviour to have the default capabilities enabled just after the role was edited from the UI there are bigger changes needed. They were partially discussed as part of the following Jiras:
[https://splunk.atlassian.net/browse/SPL-195604|https://splunk.atlassian.net/browse/SPL-195604|smart-link] 
[SPL-145827] 
There is additionally one open UI Jira created for that:
[https://splunk.atlassian.net/browse/SPL-207782|https://splunk.atlassian.net/browse/SPL-207782|smart-link]  and I’ll create another internal task to investigate changes needed on the backend side.",03/May/22 6:09 PM;6036b8354623c60069c04637;[~accountid:615ab34bbfa2c1006ba2c6a5] Seem customer still looking for a permanent fix. Do we have any timeline to fix this? ,"05/May/22 7:53 AM;615ab34bbfa2c1006ba2c6a5;[~accountid:6036b8354623c60069c04637] As I recently pointed out this issue is broader than initially estimated and because of that I have created a separate epic to investigate it: [https://splunk.atlassian.net/browse/SPL-222547|https://splunk.atlassian.net/browse/SPL-222547|smart-link]. 
For now it is unfortunately not possible to provide the exact timeline for this fix.","10/May/22 10:47 AM;615ab34bbfa2c1006ba2c6a5;[~accountid:6036b8354623c60069c04637], please see some more detailed description of the behavior that the customer is seeing and the proposed workaround. I hope this helps you to clarify the way forward.

*Issue scenario:*

At first both admin and 3pp_usr are visible when doing{{ ./splunk list user}}, both users are visible when doing {{./splunk _internal call /services/authentication/users}} (REST call).

Then after the admin role  is edited so that it does not have any  imported roles (uncheck can_delete from the UI) both commands{{./splunk list user}} and  {{./splunk _internal call /services/authentication/users}} show only the admin user.

 
The reason why admin user is not able to list the 3pp_usr is the following:

When role admin is edited, Splunk automatically adds grantableRoles = admin to the configuration, as described in [https://docs.splunk.com/Documentation/Splunk/8.2.6/Admin/authorizeconf#.5Brole_.26lt.3BroleName.26gt.3B.5D|https://docs.splunk.com/Documentation/Splunk/8.2.6/Admin/authorizeconf#.5Brole_.26lt.3BroleName.26gt.3B.5D|smart-link] :

{noformat}Default (if 'admin' role is edited): admin{noformat}

This configuration change results in the following behaviour for admin (according to the above mentioned docs):

{noformat}They can see only users who have been assigned roles that contain capabilities that are a union of these capabilities.{noformat}

And now let’s look at the capabilities of admin and the user that the customer wants to list:

Admin:

{noformat}<s:key name=""capabilities"">
<s:list>
<s:item>accelerate_datamodel</s:item>
<s:item>accelerate_search</s:item>
<s:item>admin_all_objects</s:item>
<s:item>apps_backup</s:item>
<s:item>apps_restore</s:item>
<s:item>change_authentication</s:item>
<s:item>change_own_password</s:item>
<s:item>dispatch_rest_to_indexers</s:item>
<s:item>dmc_deploy_apps</s:item>
<s:item>dmc_deploy_token_http</s:item>
<s:item>dmc_manage_topology</s:item>
<s:item>edit_authentication_extensions</s:item>
<s:item>edit_bookmarks_mc</s:item>
<s:item>edit_cmd</s:item>
<s:item>edit_deployment_client</s:item>
<s:item>edit_deployment_server</s:item>
<s:item>edit_dist_peer</s:item>
<s:item>edit_encryption_key_provider</s:item>
<s:item>edit_forwarders</s:item>
<s:item>edit_global_banner</s:item>
<s:item>edit_health</s:item>
<s:item>edit_httpauths</s:item>
<s:item>edit_indexer_cluster</s:item>
<s:item>edit_indexerdiscovery</s:item>
<s:item>edit_input_defaults</s:item>
<s:item>edit_kvstore</s:item>
<s:item>edit_local_apps</s:item>
<s:item>edit_log_alert_event</s:item>
<s:item>edit_metric_schema</s:item>
<s:item>edit_metrics_rollup</s:item>
<s:item>edit_modinput_journald</s:item>
<s:item>edit_monitor</s:item>
<s:item>edit_restmap</s:item>
<s:item>edit_roles</s:item>
<s:item>edit_scripted</s:item>
<s:item>edit_search_concurrency_all</s:item>
<s:item>edit_search_head_clustering</s:item>
<s:item>edit_search_schedule_priority</s:item>
<s:item>edit_search_schedule_window</s:item>
<s:item>edit_search_scheduler</s:item>
<s:item>edit_search_server</s:item>
<s:item>edit_server</s:item>
<s:item>edit_server_crl</s:item>
<s:item>edit_sourcetypes</s:item>
<s:item>edit_splunktcp</s:item>
<s:item>edit_splunktcp_ssl</s:item>
<s:item>edit_splunktcp_token</s:item>
<s:item>edit_statsd_transforms</s:item>
<s:item>edit_tcp</s:item>
<s:item>edit_tcp_stream</s:item>
<s:item>edit_telemetry_settings</s:item>
<s:item>edit_token_http</s:item>
<s:item>edit_tokens_all</s:item>
<s:item>edit_tokens_own</s:item>
<s:item>edit_tokens_settings</s:item>
<s:item>edit_udp</s:item>
<s:item>edit_upload_and_index</s:item>
<s:item>edit_user</s:item>
<s:item>edit_view_html</s:item>
<s:item>edit_web_settings</s:item>
<s:item>edit_workload_policy</s:item>
<s:item>edit_workload_pools</s:item>
<s:item>edit_workload_rules</s:item>
<s:item>embed_report</s:item>
<s:item>export_results_is_visible</s:item>
<s:item>fsh_manage</s:item>
<s:item>fsh_search</s:item>
<s:item>get_diag</s:item>
<s:item>get_metadata</s:item>
<s:item>get_typeahead</s:item>
<s:item>indexes_edit</s:item>
<s:item>input_file</s:item>
<s:item>install_apps</s:item>
<s:item>license_edit</s:item>
<s:item>license_tab</s:item>
<s:item>license_view_warnings</s:item>
<s:item>list_cascading_plans</s:item>
<s:item>list_deployment_client</s:item>
<s:item>list_deployment_server</s:item>
<s:item>list_dist_peer</s:item>
<s:item>list_forwarders</s:item>
<s:item>list_health</s:item>
<s:item>list_httpauths</s:item>
<s:item>list_indexer_cluster</s:item>
<s:item>list_indexerdiscovery</s:item>
<s:item>list_inputs</s:item>
<s:item>list_introspection</s:item>
<s:item>list_metrics_catalog</s:item>
<s:item>list_pipeline_sets</s:item>
<s:item>list_remote_input_queue</s:item>
<s:item>list_remote_output_queue</s:item>
<s:item>list_search_head_clustering</s:item>
<s:item>list_search_scheduler</s:item>
<s:item>list_settings</s:item>
<s:item>list_storage_passwords</s:item>
<s:item>list_token_http</s:item>
<s:item>list_tokens_all</s:item>
<s:item>list_workload_policy</s:item>
<s:item>list_workload_pools</s:item>
<s:item>list_workload_rules</s:item>
<s:item>never_expire</s:item>
<s:item>never_lockout</s:item>
<s:item>output_file</s:item>
<s:item>pattern_detect</s:item>
<s:item>refresh_application_licenses</s:item>
<s:item>request_pstacks</s:item>
<s:item>request_remote_tok</s:item>
<s:item>rest_apps_management</s:item>
<s:item>rest_apps_view</s:item>
<s:item>rest_properties_get</s:item>
<s:item>rest_properties_set</s:item>
<s:item>restart_reason</s:item>
<s:item>restart_splunkd</s:item>
<s:item>rtsearch</s:item>
<s:item>run_debug_commands</s:item>
<s:item>schedule_search</s:item>
<s:item>search</s:item>
<s:item>search_process_config_refresh</s:item>
<s:item>select_workload_pools</s:item>
<s:item>upload_lookup_files</s:item>
<s:item>use_file_operator</s:item>
<s:item>web_debug</s:item>
</s:list>{noformat}

3pp_usr:

{noformat}[role_3pp-usr]
accelerate_search = disabled
change_own_password = disabled
cumulativeRTSrchJobsQuota = 100
cumulativeSrchJobsQuota = 50
edit_search_schedule_window = disabled
export_results_is_visible = disabled
get_metadata = disabled
get_typeahead = disabled
importRoles = default_user
list_inputs = disabled
output_file = disabled
pattern_detect = disabled
request_remote_tok = disabled
rest_apps_view = disabled
rest_properties_get = disabled
rest_properties_set = disabled
rtSrchJobsQuota = 6
run_collect = enabled
run_mcollect = enabled
schedule_rtsearch = disabled
search = disabled
srchDiskQuota = 100
srchFilterSelecting = true
srchJobsQuota = 3
srchMaxTime = 0{noformat}

Please note the capabilities run_collect and run_mcollect present in the 3pp_usr configuration. Because admin user does not have those capabilities, it cannot list the user 3pp_usr (because it can only list users that have roles with capabilities being a subset of the admin’s capabilities due to grantableRoles = admin). 
Please review the detailed example of the grantableRoles handling, that is described in the docs: [+https://docs.splunk.com/Documentation/Splunk/8.2.1/Admin/authorizeconf#.5Brole_.26lt.3BroleName.26gt.3B.5D+|https://docs.splunk.com/Documentation/Splunk/8.2.1/Admin/authorizeconf#.5Brole_.26lt.3BroleName.26gt.3B.5D] 


In order to workaround the issue you can take one of the two approaches:

# Remove the option grantableRoles = admin from authorize.conf -  this is not permanent workaround and will need to be done every time capabilities are modified in the admin role.
# Add any capabilities that the other user roles have to the ""admin"" role. In your case scenario it will be enough to restart Splunk after you remove the importRoles from the UI. Because the run_collect and run_mcollect belong to default capabilities they will be loaded for admin after Splunk restart.



Regarding the ETA for the permanent fix it is not possible to tell yet because the solution will need deeper analysis and review. For now the Epic was created to track this: [https://splunk.atlassian.net/browse/SPL-222547|https://splunk.atlassian.net/browse/SPL-222547|smart-link].
Please let me know if anything more is needed from my side.","12/May/22 5:54 AM;615ab34bbfa2c1006ba2c6a5;This issue is not only connected to the default capabilities mentioned previously - run_collect, run_mcollect, but in general to any difference in the capabilities that are present in the role set in grantableRoles (admin) and in the roles of a user that admin would like to list (in our case between admin and user_3pp). 

I have confirmed this today by first just opening the admin role in the UI and just clicking “Save” without modifying anything. This triggers populating the grantableRoles = admin. After that both users admin and user_3pp could be listed (because the imported roles were untouched and the capabilities sets match for both users' roles).

Then I edited 3pp-usr role by adding a new capability that is not present in the admin capabilities set (but is not a default capability): edit_health_subset. After that the user_3pp cannot be listed by admin.

If I enable this capability without admin role having grantableRoles = admin set, both users can be listed.


I have found another Jira today, that has been registered for the same issue: [+https://splunk.atlassian.net/browse/SPL-211994+|https://splunk.atlassian.net/browse/SPL-211994#icft=SPL-211994].","27/Jul/22 11:59 PM;6036b8099b3ef700693a13bb;[~accountid:615ab34bbfa2c1006ba2c6a5] May I know if it is able to make the ticket as known issue in release document(requested by customer), thank you.","01/Aug/22 5:08 AM;615ab34bbfa2c1006ba2c6a5;[~accountid:6036b8099b3ef700693a13bb] , yes I have just added this Jira to the known issues with the workaround.",01/Aug/22 7:55 PM;6036b8099b3ef700693a13bb;Thank you for update.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,10/May/22 12:48 AM
[PUBLIC] System Introspect App fails when universal forwarder is installed at non-admin user,SPL-221239,2399363,Bug,Reopened,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,Unknown,,,,Devan Samineedi,6053ba0960d39e006f8179ba,Devan Samineedi,6053ba0960d39e006f8179ba,23/Mar/22 9:56 AM,30/Jul/25 4:42 PM,,,10.0.x,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Universal Forwarder,,,,,0,Aurum-Reviewed,beryllium-defer-accept,dataedge_triaged,GDI_board,,,,"Splunk universal forwarder is installed with msa account and *SET_ADMIN_USER = 0*, enable the ""*System Introspect App*"" from default apps by setting ""*state = enabled*"". Monitor splunkd logs and you will the following error 

{code:java}
ERROR IntrospectionGenerator:resource_usage [8896 ExecProcessor] -  RU - Splunk was started with insufficient privileges to collect resource usage metrics. Please modify the service properties to run with Administrator privileges. Exiting.
{code}


However if the installation is done with SET_ADMIN_USER = 1, splund log does not have the error and contains this log instead

{code:java}
ModularInputs [6716 MainThread] - Introspection setup completed for scheme ""powershell2"".
{code}
",,Alexander Binkin,Andrew Brown,Jennifer Worthington,Jo Hornsby,Shubham Jain,Suketu Shah,User known,Wim Colgate,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053b41990f2880070097bd9,557058:37b4518c-e757-440f-87b7-181f5f425e80,557058:236db286-0673-4667-9c0b-a9114f695d13,557058:f87e2651-5132-4aed-8b96-f608ee969435,557058:b5b57914-e806-46d4-a078-0df47b39a8b6,6053a66586b0dd007188092d,unknown,557058:ef43a796-f89f-4463-801b-30ded73fb41e,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,,,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@2e04fba4,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,73785600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Tue May 23 23:41:48 UTC 2023,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,21.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,abinkin(abinkin),andrewb(andrewb),chli(lcheney),dsamineedi(dsamineedi),jworthington(jworthington),shujain(shujain),zzheng(zzheng),suketus(suketus),wcolgate(wcolgate),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,GDI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|ia9onj:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cheney Li,557058:bc6d5618-b98e-471c-b56b-23062840fcd0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,GDI Infra-FY23Q2-S4,GDI Infra-FY23Q2-S5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2022-03-25 01:34:45.131,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2022-03-25 01:34:45.131,2022-03-25 01:34:45.131,,,,,,Jennifer Worthington,557058:236db286-0673-4667-9c0b-a9114f695d13,,,,,,"24/Mar/22 6:34 PM;5d6ef2d4522f790d9c771b18;Although with the failure, I can still saw the log in the introspection index. Might need to let telemetry team know which part is generate this log","01/Jul/22 10:45 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;This introspection app explicitly requires {{SeDebugPrivilege}} to collect resource usage metrics. Personally I feel this is the wrong privilege because it's used to debug any processes rather than monitor the performance. I guess “Performance Monitor User” group is good enough, but need to be verified.

So for now, the impact is unknown. I just checked where this error comes, but have no idea which  introspection features are blocked by this error. There is no owner of this app now, so I think we can just update the log rather than fixing any app features.

Also this priv is dangerous, from Microsoft [https://docs.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/debug-programs|https://docs.microsoft.com/en-us/windows/security/threat-protection/security-policy-settings/debug-programs|smart-link] . 

====================================

The *Debug programs* user right can be exploited to capture sensitive device information from system memory or to access and modify kernel or application structures. Some attack tools exploit this user right to extract hashed passwords and other private security information or to insert malware. By default, the *Debug programs* user right is assigned only to administrators, which helps mitigate risk from this vulnerability.

====================================

So I think we should not even provide an option to grant this privilege during installation. The only change here is the log: ask customers manually grant the priv if they want to use (some  of) Introspection app features. May also need doc changes on Introspection app side.

[~accountid:5d6ef2d4522f790d9c771b18] [~accountid:6053a6d62f452d006f82899b] [~accountid:6053ba4a94d7b90069f9193d] , pls review and share your comments. Thanks.","12/Jul/22 11:59 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;[~accountid:6053a6d62f452d006f82899b] [~accountid:5d6ef2d4522f790d9c771b18] , trying to close this ticket in this sprint. 

This error is caused by lack of {{SeDebugPrivilege}} of UF. Ideally we have 3 options:

# Grant this privilege during installation
# Find alternative and verify introspection app
# Just update the logs and documents

For #1, it’s not possible since this is a pretty high privilege, which can inject any code to any applications, including system processes. If UF has this privilege, it equals to local admin rather than least privilege.

For #2, for now no one owns this app. I don’t think we can quickly learn its features and do a thoroughly tests.

So I only did #3. Let me know if you have any concerns on this. Thanks.","12/Jul/22 2:58 PM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;Old logs 

{noformat}""Splunk was started with insufficient privileges to collect resource usage metrics. Please modify the service properties to run with Administrator privileges. Exiting.""{noformat}

New logs

{noformat}Introspection app requires SeDebugPrivilege to collect resource usage metrics. Please manually add Splunk service user to the local Administrator group, or the 'Debug programs' user rights. Exiting.{noformat}","29/Jul/22 11:16 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;[~accountid:557058:236db286-0673-4667-9c0b-a9114f695d13] , as [~accountid:557058:ef43a796-f89f-4463-801b-30ded73fb41e] requested, please review the log changes. Thanks.",29/Jul/22 11:21 AM;557058:ef43a796-f89f-4463-801b-30ded73fb41e;[~accountid:557058:bc6d5618-b98e-471c-b56b-23062840fcd0] Which resources is the introspection app not able to retrieve?,"29/Jul/22 11:26 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;[~accountid:557058:ef43a796-f89f-4463-801b-30ded73fb41e] , from the code, the app requires {{SeDebugPrivilege}} to debug all the Splunk (sub)processes including data inputs, otherwise this error happens. We have no idea how this app works, and unfortunately no one owns this app for now. So after talking to [~accountid:6053a6d62f452d006f82899b] , we decided to only update the error log, since the dev  & testing effort is unknown. ","29/Jul/22 11:40 AM;557058:ef43a796-f89f-4463-801b-30ded73fb41e;[~accountid:557058:bc6d5618-b98e-471c-b56b-23062840fcd0] you didn’t actually answer my question; which resources or APIs, specifically is needed?","29/Jul/22 11:50 AM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;Sorry [~accountid:557058:ef43a796-f89f-4463-801b-30ded73fb41e] I don’t know. This piece of code is the pre check when the app is starting, no idea where are the codes of this app and how to check which APIs require this privilege, unless we do a thorough test. Usually we just file a ticket to the app team, but now no team owns it.",29/Jul/22 11:56 AM;557058:ef43a796-f89f-4463-801b-30ded73fb41e;Where are the sources for the introspection app? Or is the sub-process actually splunk running as {{introspection}} as the first arg? It would behoove to know what you are changing and why.,"29/Jul/22 12:13 PM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;The only thing i changed is the log: it said require {{local admin}} permission but actually it’s {{SeDebugPrivilege}}. I just made it clear to reduce the security overhead - don’t add the user to local admin, otherwise there is no point to implement least privilege mode.

The code to cause this error is here [https://cd.splunkdev.com/splcore/main/-/blob/develop/src/instrument/RUCollection-Windows.cpp#L178|https://cd.splunkdev.com/splcore/main/-/blob/develop/src/instrument/RUCollection-Windows.cpp#L178]

I know this property can enable introspection {{run_introspection = true}} but no idea if this is the only one and if there are other use cases.

As a team agreement, it’s app team’s responsibility to go through all the app specific stacks and try to fix them with least privilege mode. We filed multiple tickets to Linux and HyperV addon teams. 

Correct me if I’m wrong. Thanks.","29/Jul/22 4:12 PM;557058:ef43a796-f89f-4463-801b-30ded73fb41e;To be fair, it’s not an app, it’s actually splunkd running as a subprocess to splunkd with the {{introspection}} argument. But that is sorta splitting hairs.

Is introspection something that is optional on UFs? I don’t recall ever running splunk with introspection on UFs… but that said, if it is an optional thing, is it something we want to have them choose at installation time? Or is it an always thing we should be doing?",29/Jul/22 4:16 PM;557058:ef43a796-f89f-4463-801b-30ded73fb41e;I wonder which pieces actually need debug rights. Most of that is the instrumentation code use PDH (performance counters). I suspect its OpenProcess() on a handle….,"29/Jul/22 4:27 PM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;Yea this is exactly what I thought. Please see my previous comments [https://splunk.atlassian.net/browse/SPL-221239?focusedCommentId=9498468|https://splunk.atlassian.net/browse/SPL-221239?focusedCommentId=9498468|smart-link] . {{So I think we should not even provide an option to grant this privilege during installation}} because the SeDebugPrivilege equals to local admin - it can inject any codes to any processes including system services. Grant this privilege may not pass the prodsec review. 

I guess there are other approaches to introspect subprocesses, but the problem is testing if we changed some code. Now our team does not have enough knowledge about this app. If we got the full feature list of this app, we can run a test and solve the permission issues case by case. 

Please let [~accountid:6053ba4a94d7b90069f9193d] [~accountid:6053a6d62f452d006f82899b] know if we want to plan it in this release. Thank you.","29/Jul/22 4:32 PM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;Generally speaking, I think it’s OK to let a service to debug its own subprocesses for introspection purpose, but it should not have the permission to debug any other processes. It’s kinda design issue as security perspective. To solve this issue completely, we need to prioritize it and invest more time.","20/Jan/23 3:32 PM;557058:b5b57914-e806-46d4-a078-0df47b39a8b6;Hi [~accountid:557058:bc6d5618-b98e-471c-b56b-23062840fcd0], is this pending on a doc update? or requires further investigation? ",20/Jan/23 3:35 PM;557058:bc6d5618-b98e-471c-b56b-23062840fcd0;I think we can just doc it as known issue for 9.1,05/May/23 4:24 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;{{Beryllium}} is the official Jira version for the 9.1.0 release. I’ll replace 9.1.0 with Beryllium so this issue gets included in the known issues. Someone created Jira version 9.1.0 independently and it’s creating confusion. ,"23/May/23 12:29 PM;557058:236db286-0673-4667-9c0b-a9114f695d13;I checked the labels and this looks good for showing up in release notes. Thanks for looping me in, going forward there’s no need to assign me to this ticket in order for it to show up in RNs, Cheney you might want to assign this to the developer who will fix it in the future. As long as it is kept open and only closed in the future fix release this will show up as a known issue for 9.1 ad a fixed issue in the future fix release.","23/May/23 4:01 PM;6053a66586b0dd007188092d;since it will be documented, moving out for beryllium-defer-accept",23/May/23 4:41 PM;6053b41990f2880070097bd9;[~accountid:557058:bc6d5618-b98e-471c-b56b-23062840fcd0] is there anything to fix here?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,29/Jul/22 11:15 AM
[PUBLIC] Reporting command in verbose mode returns 0 events despite correct event_count,SPL-218841,2347647,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P1-Immediate,Duplicate,Vinay Manivel,613257abdae0a20071b1453c,Kathyhan Nguyen,6053bb23e394c30069cbb342,Kathyhan Nguyen,6053bb23e394c30069cbb342,08/Feb/22 4:52 PM,30/Jul/25 4:24 PM,,26/Apr/22 3:11 PM,10.0.x,8.2.2202(Dove),8.2.2203(Emerald),9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Federation,,,,,0,Aurum-Maintenance-Release,Aurum-Reviewed,df-bugs,df-qa,,,,"Reporting commands in verbose mode returns 0 events in left-hand ""events"" column (event count is correct, actual events are missing, please see attached screenshot). 

This issue affects federated indexes, federated saved searches, and federated datamodels 

*Example queries:* 
_Federated Index:_ 

* index=federated:index_df_1_1 | stats count by field1M
* | union \[search index=federated:index_df_1_1] \[search index=federated:index_df_2_1] | stats count by field1M
* index=federated:index_df_1_1 | stats count by field1M| join usetime=f left=L right=R where L.field1M=R.field1M\[search index=federated:index_df_2_1 | stats count by field1M]

_Federated Saved Search:_ 

*  | from federated:fsh_rsh_ss_1
*  | from federated:fsh_rsh_ss_1 | stats count by field1M

_Federated Datamodel:_ 

*  | from federated:universal_data_10k_1_dm_1 | stats count by clientip",,Matthew Ness,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,SPL-217288,SPL-223218,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,08/Feb/22 4:52 PM;kathyhann;image-20220209-005210.png;https://splunk.atlassian.net/rest/api/3/attachment/content/3753289,08/Feb/22 4:52 PM;kathyhann;image-20220209-005221.png;https://splunk.atlassian.net/rest/api/3/attachment/content/3753290,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@ed94c39,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,107654400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,Tue Apr 26 22:10:57 UTC 2022,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Federation,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,2.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,kathyhann(kathyhann),vmanivel(JIRAUSER51788),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i9l9k7:,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vinay Manivel,613257abdae0a20071b1453c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DataFed-FY22Q4-S5,DataFed-FY22Q1-S1,DataFed-FY22Q1-S2,DataFed-FY22Q1-S3,DataFed-FY22Q1-S4,DataFed-FY22Q1-S5,DataFed-FY22Q1-S6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,64.0,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2022-03-14 20:53:37.501,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3_*:*_2_*:*_2908606696_*|*_10001_*:*_2_*:*_2577426799_*|*_10039_*:*_1_*:*_14690338,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2022-03-14 20:53:37.501,2022-03-14 20:53:37.501,,,,,,Vinay Manivel,613257abdae0a20071b1453c,,,,,,"14/Mar/22 1:53 PM;613257abdae0a20071b1453c;Root cause is that events are stored in a separate directory on the RSH and are not transferred to the FSH.

Fixing this one will require some infrastructure changes to get the events to the FSH (and perhaps combine them with other event streams?) Unlikely to happen within this sprint…",26/Apr/22 3:10 PM;613257abdae0a20071b1453c;We have multiple bugs for the same issue. Closing as dupe of [https://splunk.atlassian.net/browse/SPL-223218|https://splunk.atlassian.net/browse/SPL-223218|smart-link].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,26/Apr/22 3:11 PM
[PUBLIC] Searches are cancelled or time out when the user leaves the browser window or switches tabs.,SPL-216787,2296916,Bug,Resolved,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,Unknown,Workaround,Andrew Brown,557058:37b4518c-e757-440f-87b7-181f5f425e80,Andrew Brown,557058:37b4518c-e757-440f-87b7-181f5f425e80,Andrew Brown,557058:37b4518c-e757-440f-87b7-181f5f425e80,21/Dec/21 1:27 PM,30/Jul/25 4:26 PM,,21/Dec/21 1:35 PM,10.0.x,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.7,8.1.8,8.1.9,8.2.10,8.2.11,8.2.12,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search - Splunkweb,,,,,0,,,,,,,,"This is a proxy bug that we can tag to include in known issues for Splunk Enterprise. 

There is a long chain of bugs and stories related to the issue. For the discussion that led to the creation of this proxy bug, see SPL-214064. ",,Andrew Brown,Hugh Tan,John McCurdy,Sung Lim,Yann Kherian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,5eb1eb17767b660b7723e8ee,6053b09af180c300675ef6a5,6036b832f8c057007083c0e3,6036b8996bc3f300699cf30e,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-214064,,,,,,,,,,,08/Feb/22 9:49 AM;addon_com.servicerocket.jira.salesforce;image001.jpg;https://splunk.atlassian.net/rest/api/3/attachment/content/3752195,08/Feb/22 9:49 AM;addon_com.servicerocket.jira.salesforce;~WRD0000.jpg;https://splunk.atlassian.net/rest/api/3/attachment/content/3752193,08/Feb/22 9:49 AM;addon_com.servicerocket.jira.salesforce;~WRD0000.jpg;https://splunk.atlassian.net/rest/api/3/attachment/content/3752194,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@2239d439,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Black/White,,7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,112924800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Fri Feb 25 07:20:27 UTC 2022,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,3.0,7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),htan(htan),ykherian(ykherian),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,End User Experience,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Andrew Brown,557058:37b4518c-e757-440f-87b7-181f5f425e80,,,,,,,,,,,,,,,,,,0|i9di7z:,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MDS Tech Inc.,3082433,012400000005WzMAAU,P3,No,5005a00002GziQwAAJ,,Closed,Resolved,Standard,Tier4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,See SPL-203971,,,,,,,,,,,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2022-02-07 23:32:44.721,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_483960,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Change the <code>job_default_auto_cancel</code> setting in $SPLUNK_HOME/etc/system/local/web.conf from the default value of 30 to 62.<br /><br />

Details<br />
This issue is caused by power saving settings in recent browser versions, where Javascript timers may be throttled. The user typically sees the following message in the search window on foreground searches:

<code>DAG Execution Exception: Search has been cancelled<br />
Search auto-canceled<br ?>
The search job has failed due to an error. You may be able to view the job in the Job Inspector</code>",,,,,,,,,,2022-02-07 23:32:44.721,2022-02-07 23:32:44.721,,,,,,Andrew Brown,557058:37b4518c-e757-440f-87b7-181f5f425e80,,,,,,"07/Feb/22 3:32 PM;6036b8996bc3f300699cf30e;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80]  
A customer on Splunk 8.1.6 is looking for the the setting job_default_auto_cancel

 but it is not in the spec for that version (it only appears on 8.1.7)
can they use it on older versions ?",07/Feb/22 4:47 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:6036b8996bc3f300699cf30e] this setting is not available in 8.1.x versions prior to 8.1.7.,"09/Feb/22 11:05 AM;6036b8996bc3f300699cf30e;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80]  a customer is trying to use the workaround for 8.1.6, with editing job.js
is there a way to validate the value of the variable (in splunk or in the browser debugger ) ?",09/Feb/22 11:23 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:6036b8996bc3f300699cf30e] I don’t know more about this issue than the published workaround using the setting in web.conf for 8.1.7 and higher. The developer who added the setting in [https://splunk.atlassian.net/browse/SPL-214064|https://splunk.atlassian.net/browse/SPL-214064|smart-link] might have more information about how to address the situation in earlier versions like 8.1.6.,09/Feb/22 5:02 PM;6036b8996bc3f300699cf30e;there is a better workaround in [https://splunk.atlassian.net/browse/SPL-206862|https://splunk.atlassian.net/browse/SPL-206862|smart-link] ,09/Feb/22 5:30 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;Not sure how to respond to that observation. This Jira was created purely as a mechanism to publish the workaround given in this Jira in the public known issues. The team that worked on [https://splunk.atlassian.net/browse/SPL-206862|https://splunk.atlassian.net/browse/SPL-206862|smart-link] provided the workaround to share. If they want to change the workaround they’re welcome to do so.,"24/Feb/22 11:20 PM;5eb1eb17767b660b7723e8ee;Hello, I have a cloud stack on 8.2.2107.2 encounter this bug, they have implemented the workaround in [https://splunk.atlassian.net/browse/SPL-206862|https://splunk.atlassian.net/browse/SPL-206862|smart-link] but it doesn’t fix it. I saw some case notes mentioned this is fixed in Areo - 8.2.1111, can Dev please confirm? (as the “fix versions“ field is blank in this SPL)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,21/Dec/21 1:35 PM
[PUBLIC] Standard mode federated search: Unable to set federated index as default index,SPL-213745,2212091,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,Marcus Rodrigues Galdino,6053add4e394c30069cb1e0e,Kathyhan Nguyen,6053bb23e394c30069cbb342,Kathyhan Nguyen,6053bb23e394c30069cbb342,14/Oct/21 4:59 PM,30/Jul/25 4:24 PM,,,10.0.x,8.2.2201(Crunch),8.2.2202(Dove),8.2.2203(Emerald),9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.2205(Ferrero Rocher),9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Federation,,,,,0,Aurum-Reviewed,df-bugs,df-qa,release_notes_candidate,,,,"We can set federated indexes as default indexes, but this change does not take effect. Please see repro steps below: 
 # Set up federated search FSH and RSH using splunk-jaws tool
 # On FSH, go to Settings --> Roles --> admin --> indexes --> check ""default"" next to federated:index_df_1_1 and index_df_1
 ## If index_df_1 does not appear in the index list, you may need to add it by entering ""index_df_1*"" in the Wildcards section
 # Restart FSH to refresh authorize.conf. (Sometimes you also need to run *./splunk reload auth* for role changes to fully take effect
 # On FSH, run command *| tstats count by index* - this should give you a list of all default indexes and their event counts  
 # We get results from local index_df_1, but not federated:index_df_1_1",manual-build 3e8530e812b1,Balaji Rao,Kathyhan Nguyen,Matthew Ness,User known,User known,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053b5f52f452d006f833293,6053bb23e394c30069cbb342,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,unknown,unknown,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-251131,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Oct/21 4:58 PM;kathyhann;image-2021-10-14-16-58-11-334.png;https://splunk.atlassian.net/rest/api/3/attachment/content/3179379,14/Oct/21 4:58 PM;kathyhann;image-2021-10-14-16-58-40-118.png;https://splunk.atlassian.net/rest/api/3/attachment/content/3179378,14/Oct/21 4:58 PM;kathyhann;image-2021-10-14-16-58-50-787.png;https://splunk.atlassian.net/rest/api/3/attachment/content/3179377,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@72fad3c8,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,105840000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Tue May 17 20:07:02 UTC 2022,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Federation,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,3.0,7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,brao(brao),cogle(JIRAUSER49869),jjblack(jenniferb),kathyhann(kathyhann),mgaldino(mgaldino),mness(mness),sangp(JIRAUSER44904),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i01z7z:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2022-02-15 20:19:01.388,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2022-02-15 20:19:01.388,2022-02-15 20:19:01.388,,,,,,,,,,,,,"15/Feb/22 12:19 PM;611be664883cef0077c91a16;An update on this issue, I looked into this issue and it appears to be an issue with our implementation of {{tstats}}. I am wondering if there are other commands that use default indexes that we can cross validate this issue with. Otherwise, I will look into how to fix this issue for {{tstats}} specifically. ",11/Mar/22 10:36 AM;6053b5f52f452d006f833293;Could this be because we don’t support wildcards?,"16/May/22 6:13 PM;6053ac5e66c879006838c561;Hello [~accountid:6053b5f52f452d006f833293] , this just came up on the Aurum bugs dashboard. Appears to be an existing customer issue for Cloud. But if this is a “new” issue for OnPrem, would this be a blocker for Aurum? ","16/May/22 6:17 PM;557058:89ace195-adc2-4f25-9924-ded7c8f10bb6;It’s not a new issue [~accountid:6053ac5e66c879006838c561] and probably not a blocker. It’s been around for a long time. I updated the Affects Versions field with Aurum so this bug shows up as a Known Issue in the Release Notes for 9.0.0. 

Now that Federated Search has been out for awhile I am trying to identify bugs that really need to be called out as known issues…and this is an obvious one. ","16/May/22 6:21 PM;557058:89ace195-adc2-4f25-9924-ded7c8f10bb6;[~accountid:6053bb23e394c30069cbb342] Do you only see this behavior with {{tstats}} searches, or does it apply to all federated searches? ","17/May/22 12:17 PM;6053bb23e394c30069cbb342;[~accountid:557058:89ace195-adc2-4f25-9924-ded7c8f10bb6] This behavior applies to all federated searches. After setting a federated index as default index, if you search “host=*”, the federated index will not show up in the results. (cc [~accountid:611be664883cef0077c91a16] )","17/May/22 1:07 PM;5f3c088a91e67a003fa8c478; [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80]: Matt recently tagged this as a known issue for SCP and CMP. Currently, there is no workaround. Should I add it to the Emerald Known issues--and also Crunch and Dove?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3451712,SPL-255660,Federated search Standard Mode - Eventcount and Default Indexes support,To Do,14/Oct/21 4:59 PM
"[PUBLIC] Excessive logging 'WARN SearchResultsFiles Unable to parse site_label, label=invalid due to err=""Invalid site id: invalid""' for SearchResultsFiles",SPL-212495,2173702,Bug,WAITING FOR REPORTER,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,Ifty Hasan,6343d50e754fb6b373b8f017,Daniel Madeira,6036b7b12de69b006a170ca0,Daniel Madeira,6036b7b12de69b006a170ca0,22/Sep/21 3:10 AM,30/Jul/25 4:25 PM,,,10.0.x,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search,Search Head Clustering,,,,1,,,,,,,,"*Problem Statement:* 
 Version 8.2.2 seems to have the same issue as Jira SPL-196040

09-11-2021 16:42:41.042 +0200 WARN SearchResultsFiles [36391 TcpChannelThread] - Unable to parse site_label, label=invalid due to err=""Invalid site id: invalid

 

The workaround described in that JIRA, seems that it isn't working:

======
Increase the minimum logging level for SearchResultsFiles:
<pre>
#log-local.cfg

[splunkd]
category.SearchResultsFiles=ERROR
</pre>

[https://docs.splunk.com/Documentation/Splunk/latest/Troubleshooting/Enabledebuglogging#log-local.cfg]","Multisite SH Cluster, v 8.2.2, on_prem

six search heads are in Munich and six in Verona (12 totally)

 ",Andrew Brown,Andrew Foote,Andrew Nguyen,Arnaud Fieffe,Daniel Madeira,Daniel West,Da Xu,Dhruv Bhagi,Divya Sandur,Edwin Steenvoorden,Hongxun Liu,Jaclyn Delle Site,James Ervin,Jasleen Koshti,Joe Gedeon,Jo Hornsby,Jonah Reuter,Kelly Snyder,Matteo Garaventa,Michael Wegener,Nic Henke,Patrick Bang,Paulo Santos,Rajpal Bal,Richard Huber,Richard Morgan,Samat Jain,Srinivas Bobba,Sung Lim,Svitlana Perelshtein,Troy Wollenslegel,User known,User known,Wiriadi Wangsa,Zofnat Shy,557058:37b4518c-e757-440f-87b7-181f5f425e80,5c19d687fbbe6428a7f2d424,6036b79b21d9bc006858d502,557058:7ad23c04-5122-43fa-9d3a-1bd74f1624a9,6036b7b12de69b006a170ca0,6036b7eb783a4600687c4be2,5c77260b8a38a065324ba55b,6053a437695c3900707a5bc4,5f996bce58f26200723ff93b,5de4f74dbe6c1f0d07206861,6053a3fb81b82500685ced67,6036b812783a4600687c4d9e,557058:ab939132-f05e-4f5f-8bec-6939a8133dcc,6036b7d1d321ef0068d439e3,6053a4f490f288007008d021,557058:f87e2651-5132-4aed-8b96-f608ee969435,6053a5692f452d006f82793b,6036b88f18537600702417ec,5a848585a08cc5310a6cae0a,557058:772ff077-c75f-4dbd-9798-298838c06821,5d2f676be161600bfbc9e750,6036b7b68ff09800716a026a,6036b7ed8ff09800716a04a8,557058:f9466031-49a3-4131-8cb7-7d482a9eb937,6036b80118537600702411d0,557058:8863c778-22a2-4ac5-8ebb-25e7a9430a4d,6053b24d45a3bb006817344b,6053b9162f452d006f83555b,6036b832f8c057007083c0e3,6053bba8f180c300675f7113,557058:cef60de8-1390-451b-a8a2-aa7771e5dee3,unknown,unknown,6036b7ad4623c60069c040b7,6053add6695c3900707ac8b7,0,0,,,0,0,,,,,,,,,,,,,,,SPL-219811,,,,,,,SPL-196040,,,,,,,,,,,,,,,,,,,,22/Sep/21 3:31 AM;dmadeira;Screenshot 2021-09-22 at 11.31.21.png;https://splunk.atlassian.net/rest/api/3/attachment/content/3081739,11/Jan/22 12:50 AM;rboyce;frequent_off_error.png;https://splunk.atlassian.net/rest/api/3/attachment/content/3603537,07/Jul/22 4:20 AM;rboyce;image001 (8a65a2d4-f45b-4702-9ed1-ada87aa3789c).png;https://splunk.atlassian.net/rest/api/3/attachment/content/4462202,14/Jan/22 7:06 AM;addon_com.servicerocket.jira.salesforce;image001.gif;https://splunk.atlassian.net/rest/api/3/attachment/content/3619130,14/Jan/22 7:06 AM;addon_com.servicerocket.jira.salesforce;image001.gif;https://splunk.atlassian.net/rest/api/3/attachment/content/3619134,11/Jan/22 12:24 AM;addon_com.servicerocket.jira.salesforce;image001.png;https://splunk.atlassian.net/rest/api/3/attachment/content/3603425,11/Jan/22 12:24 AM;addon_com.servicerocket.jira.salesforce;image001.png;https://splunk.atlassian.net/rest/api/3/attachment/content/3603426,14/Jan/22 7:06 AM;addon_com.servicerocket.jira.salesforce;image002.jpg;https://splunk.atlassian.net/rest/api/3/attachment/content/3619131,14/Jan/22 7:06 AM;addon_com.servicerocket.jira.salesforce;image002.jpg;https://splunk.atlassian.net/rest/api/3/attachment/content/3619133,14/Jan/22 7:06 AM;addon_com.servicerocket.jira.salesforce;image002.jpg;https://splunk.atlassian.net/rest/api/3/attachment/content/3619132,14/Jan/22 4:53 AM;lstoppa;splunkd_8_1_5_instrumented.gz;https://splunk.atlassian.net/rest/api/3/attachment/content/3618893,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IHS Markit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@30e801a5,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"o Have they made any changes to their environment just before the issue started?

They upgraded to 8.2.2 recently and since then this issue started",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,50.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,30844800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"o What is the customer environment for the issue? 
Running 8.2.2
multisite SH (12 in total)

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,"o What errors are being reported?

09-11-2021 16:42:41.042 +0200 WARN SearchResultsFiles [36391 TcpChannelThread] - Unable to parse site_label, label=invalid due to err=""Invalid site id: invalid",false,,,,,,,,,,,,No,,,,,,,,,,,,,"o What is the expectation when this problem is not there?

Thousands of messages not showing up",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• How long has the customer been tracking this issue and how frequent this issue is being hit?

Since upgrade to 8.2.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,Tue Oct 01 21:00:32 UTC 2024,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,11.0,67.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bfrederisy(JIRAUSER45514),dmadeira(dmadeira),dwest(dwest),dbhagi(dbhagi),dsandur(dsandur),hliu(hliu),04448c70-c709-447c-8f7c-ab7b2df57b1e(04448c70-c709-447c-8f7c-ab7b2df57b1e),jreuter(jreuter),lstoppa(JIRAUSER45981),mgaraventa(mgaraventa),hdbang(hdbang),rboyce(rboyce),e5ef8fd6-eb0e-4c63-ad75-de6ba4cd3898(e5ef8fd6-eb0e-4c63-ad75-de6ba4cd3898),twollenslegel(twollenslegel),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i8u9br:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Luca Stoppa,6053c2ec06cbba006a0ef899,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"TD Bank, Canada",3560863,012400000005WzMAAU,P3,No,5005a0000314MpKAAU,,Closed,Resolved - Work Around,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2681162,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,I didn't attempt reprodution as having a multisite is quite labour intensive and the code debugging and analysis have been done in SPL-196040 already.,,,,,,,,,,,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?
This does not appear to be a UI bug.

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?
Was not sure what debug would be helpful in this situation, so I have just done a regular diag. If more details are needed, I am fine with pulling additional details/turning on debug for certain features.

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection
I have gave information for where a diag is located as it would not let me upload the file. Stated it was too large.

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.
This does not appear to be affecting performance, but we are concerned if it is causing issues or potentially a bigger problem due to the high number of errors this is showing.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 
we tried the workaround in 1 SH but the workaround didn't work:
[dmadeira@splunkbothwf01 usspupl859.internal.unicreditgroup.eu-sh_hf_-20210921-145201-TjHeUfFT]$ grep -R 'category.SearchResultsFiles' .
./etc/log-local.cfg:category.SearchResultsFiles=ERROR

 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.

Is this impacting the customer's environment or is this a benign issue?",,,,,,,,,,,,,,,,,,,2021-09-22 10:33:24.683,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,none,,,,,,,,,,2021-09-22 10:33:24.683,2021-09-22 10:33:24.683,,,,,,Luca Stoppa,6053c2ec06cbba006a0ef899,,,,,,"22/Sep/21 3:30 AM;6036b7b12de69b006a170ca0;DIAGS

[https://downloadsvc.splunk.com/download/splunk/09-21-2021/uploadsvc-23case2681162-09-21-2021-USER-0030b00001vUFv3AAG-diag-usspupl859-2021-09-21_15-24-00.tar.gz]

[https://downloadsvc.splunk.com/download/splunk/09-21-2021/uploadsvc-86case2681162-09-21-2021-USER-0030b00001vUFv3AAG-diag-usspupl864-2021-09-21_15-24-00.tar.gz]

 ","22/Sep/21 3:33 AM;6036b7eb783a4600687c4be2;+*Jira Review*+
1. Review the Jira Summary (/)
2. Review Accuracy and Completeness (/)
3. Review the Description Field (/)
4. Review the Support Template Tab (/)",29/Sep/21 5:32 AM;6036b7b12de69b006a170ca0;[~accountid:6053b1ca86b0dd0071888a2e] do you have any update on this JIRA? many thanks,"29/Sep/21 3:41 PM;6053b1ca86b0dd0071888a2e;[~accountid:6036b7b12de69b006a170ca0] Sorry, I haven't yet had time to investigate this.. ",14/Oct/21 6:46 AM;6036b7b12de69b006a170ca0;[~accountid:6053b1ca86b0dd0071888a2e] let me know for further progress. Many thanks in advance.,"18/Oct/21 1:47 PM;6053b1ca86b0dd0071888a2e;So I tried to reproduce this like I did for the previous ticket and was unable to. (previously, this was occurring because queued jobs weren't populating the site id with its true value before being read, causing the invalid warning to pop up.) I think the circumstances of this might be slightly different. There are places in the code that I'll check, as it might be easy to spot an uninitialized site id. If that search is fruitless, I'll check on skynet to see if that will help narrow it down. I might come back with more questions about when the customer is seeing these messages. At any rate, will update again after some digging.",19/Oct/21 5:06 AM;6036b7b12de69b006a170ca0;[~accountid:6053b1ca86b0dd0071888a2e] thank you,"21/Oct/21 7:13 AM;6036b7b12de69b006a170ca0;SPL-196040, SPL-212495 [https://docs.splunk.com/Documentation/Splunk/8.1.5/ReleaseNotes/KnownIssues]

that was it.

they have tried the fix, but it didn't work",02/Nov/21 7:21 AM;6036b7b12de69b006a170ca0;[~accountid:6053b1ca86b0dd0071888a2e] do you have an update on this issue>?,"02/Nov/21 9:09 AM;6053a5692f452d006f82793b;Looking at {{usspupl859}} I can see it was restarted {{09-21-2021 09:06:43.471 +0200}} and the messages that match the customer's search string after this point are not from {{SearchResultsFiles}} rather from {{IntrospectionGenerator:resource_usage}}:{code}> cat ./diag-usspupl859.internal.unicreditgroup.eu-2021-09-21_15-24-00/log/splunkd.log{.{5..1},}|grep 'site_label\|My GUID' | cut -d ' ' -f4- | sed -ne '/My GUID/,$ p'|uniq -c
   1 INFO  ServerConfig [0 MainThread] - My GUID is 38E53660-0F4B-42A3-9F92-362D8F0F3B14
1338 WARN  IntrospectionGenerator:resource_usage [142806 ExecProcessor] -   SearchResultsFiles - Unable to parse site_label, label=invalid due to err=""Invalid site id: invalid""{code}

I can't tell when the other host was restarted, but all of the messages there are also from {{IntrospectionGenerator:resource_usage}}:{code}> cat ./diag-usspupl864-2021-09-21_15-24-00/log/splunkd.log{.{5..1},}|grep 'site_label\|My GUID'|cut -d ' ' -f4- | uniq -c
87126 WARN  IntrospectionGenerator:resource_usage [159368 ExecProcessor] -   SearchResultsFiles - Unable to parse site_label, label=invalid due to err=""Invalid site id: invalid""{code}

I suppose you can change {{IntrospectionGenerator:resource_usage}} to ERROR as well.","05/Nov/21 8:41 AM;6053a5692f452d006f82793b;[~accountid:6036b7b12de69b006a170ca0] is this jira attached to the right case? 2739472 is ""Health Report: showing red even after disabled""?",08/Nov/21 9:04 PM;6036b7b68ff09800716a026a;[~accountid:6053a4be81b82500685cf5c5] How are you. Can I ask if we have any ETA or planed version ?,"09/Nov/21 3:49 AM;6036b7b12de69b006a170ca0;[~accountid:6053a5692f452d006f82793b] I have corrected SFDC case attached to this JIRA ( you were right as it was wrongly attached ) -

 the right case is 2681162 for UniCredit","15/Nov/21 4:05 AM;6036b7b12de69b006a170ca0;[~accountid:6053a5692f452d006f82793b] can you explain which stanza field name and the field value in which config file, please? 

or confirm if the following is correct?

==========

Config file

$SPLUNK_HOME/etc/log-local.cfg

Stanza

[splunkd]

Field name = field value

category.resource_usage=ERROR

==========

 Also, as per [~accountid:6053a4be81b82500685cf5c5] question: Can I ask if we have any ETA or planed version ?

many thanks","15/Nov/21 1:43 PM;6036b7b12de69b006a170ca0;I asked the customer to follow [https://docs.splunk.com/Documentation/Splunk/8.2.3/Troubleshooting/WhatSplunklogsaboutitself#Use_Splunk_Web_to_manage_logging-level] so they can change {{IntrospectionGenerator:resource_usage}} to ERROR.

[~accountid:6053a5692f452d006f82793b] I will feedback to you as soon as I hear.","15/Nov/21 2:30 PM;6036b7b68ff09800716a026a;[~accountid:6053c2ec06cbba006a0ef899]  Can we get the expected fixed version ? I've been asked from customer.
Thank you.","15/Nov/21 10:35 PM;6053c2ec06cbba006a0ef899;Hello [~accountid:6036b7b68ff09800716a026a], sorry for my late answer. I was OOO last week, came back yesterday. I'll start working on this case today and will provide some info ASAP.","19/Nov/21 1:59 AM;6053c2ec06cbba006a0ef899;I was following the whole conversation in SPL-196040, and I have tried to implement the fix that was suggested there. While SPL-196040 fixed the SiteId default constructor issue specifically in SearchResultsFiles, the fact our customer still sees the same error probably means we still have invalid usages of SiteId default constructor (and that's why we still have the ""invalid"" site label).

I have tried to extend the search to the whole clustering code and I found few usages (code branches), where we could still potentially have ""invalid"" SiteIds.
{code:java}
struct BucketCSVStruct {
    BucketCSVStruct() : _bytes(0), _standalone(false), _frozen(false),
                        _postponeService(false), _genid(INVALID_GENID),
                        _replCount(0), _searchCount(0),
                        _earliestTime(0), _latestTime(0) {}
    // more fields in here …
    Clustering::SiteId _bucketSite;  // <—— HERE, NEVER INITIALISED
{code}
{code:java}
class MasterSpec {
public:
    MasterSpec( const SchemeHostPort &u, const Str &s ) : _uri(u), _secret(s), _multiSite(false) {}
    MasterSpec() : _multiSite(false) {}
    // …
private: 
	// …
    Clustering::SiteId _site; // <—— HERE, NEVER INITIALISED
{code}
{code:java}
bool GenerationResponse::peersFromConfigItem(const ConfigItem& ci, const Str& peerListType, Str& err) {
    ConfigItem::const_iterator cit = ci.find(peerListType);
    // …
    const ParamType &pt = cit->second;
    const ArgsList* argList = pt.getArgsList();
    FOREACH_CONST( ArgsList, git, (*argList) ) {
        const ArgsList* peerInfo = git->second.getArgsList();
        // ...
        Clustering::SiteId site;
        Str siteErr;
        if (sitei != peerInfo->end()) {
            if (!site.deserialize(sitei->second.getFirstValue(), false, siteErr)) {
                err << ""Invalid CMPeer site:"" << sitei->second.getFirstValue() << "", error="" << siteErr;
                return false;
            }
        } /*else: old master might not put this in response, So let new SH's not fail*/
         // <—- HERE THE ELSE PUSHES AN INVALID SiteId
        cmpg.push_back(CMGenerationPeer(guid
                         , sni->second.getFirstValue()
                         , hpi->second.getFirstValue()
                         , cmpStatus
                         , site));
    }
}
{code}
{code:java}
bool GenerationResponse::peersFromJsonNode(const JsonNode& jn, const Str& peerListType, Str& err) {
    const JsonNode gen_peers = jn[peerListType];
    // …
    JsonNode::const_iterator git = gen_peers.begin();
    for ( ; git != gen_peers.end() ; git++) {
	 // …
        const JsonNode peersInfo = *git;
        // …
        Str siteStr, siteErr;
        Clustering::SiteId site;
        if (toStr(peersInfo, CMConstants::SITE, siteStr, err)) {
            if (!site.deserialize(siteStr, false, siteErr)) {
                err << ""Invalid CMPeer Site: "" << site << "", error="" << siteErr;
                return false;
            }
        } /*else: no status, old master*/ // <— HERE WE PUSH AN INVALID SiteId

        cmpg.push_back( CMGenerationPeer(guid, peer, hp, cmpStatus, site));
    }
{code}
{code:java}
void BucketInfo_IndexVisitor::visitBucket(
    const Str & index,
    const DatabaseDirectoryManager::Bucket & bucket) {
    Clustering::SiteId site; // <— here we’re pushing an empty label for  cached site
    item[""originSite_cached""].insertStr(bucket.getOriginSite(site) ? site.label() : """");
}
{code}
{code:java}
        struct BucketLogElement{
            CMBucketId         bid;
            SlaveBucketFlags   sbf;
            uint64_t           sz;
            genid_t            effectivegenid;
            Clustering::SiteId site;
            BucketLogElement() {} // <—— INVALID SITE ID
            BucketLogElement( const CMBucketId& id, const SlaveBucketFlags& s, uint64_t z, genid_t g, Clustering::SiteId t )
            : bid( id ), sbf( s ), sz( z ), effectivegenid( g ), site( t )
            {}
        };
{code}
{code:java}
class CascadePlanPeerInfo {
  public:
    CascadePlanPeerInfo() {} // <—— INVALID SITE ID
    // more constructors/methods …
  private:
    // …
    Clustering::SiteId _siteId;
};
{code}
{code:java}
class CMSearchPeer {
    public:
      // …
        Clustering::SiteId _peerSite;
        CMSearchPeer() {}
{code}
{code:java}
class SHPAddPeerRequest FINAL : public SHPRequest {
  public:

    SHPAddPeerRequest()
        : _repPort(0),
          _repPortSSL(false),
          _mgmtPort(0),
          _addType(SHPAddPeerType::eUnset),
          _status(SHPPeer::eDown),
          _kvStorePort(0), _kvStoreHostname(),
          _adhocSH(false), _noArtifactReplications(false),
          _preferredCaptain(true),
          _pid(0),
          _cpuCores(0)
        {} <— INVALID SITE ID FOR THIS CONSTRUCTOR

    SHPAddPeerRequest(const Guid& guid_,
                   const Str& artifact_csv,
                   const SHPReplicationIds& reps,
                   const PortNumber& rep_port, const bool rep_port_ssl, 
                   const Str& server_name,
                   const Hostname& rep_addr,
                   const PortNumber& mgmt_port,
                   const SHPAddPeerType& add_type,
                   const SHPPeer::Status& status_,
                   const PortNumber& ssPort,
                   const Hostname& ssHostname,
                   const SHPSiteId& s,
                   bool preferredCaptain)
      : _site(s)
        {}

  private:
    SHPSiteId _site;
};
{code}
Now, it might be very hard to tell from the customer's logs what path the code followed, so I tried to implement the initial suggestion to have SiteId default constructor to create a ""default"" site instead of an ""invalid"". I have a fix that will be reviewed soon. ",25/Nov/21 4:02 AM;6036b7b12de69b006a170ca0;[~accountid:6053c2ec06cbba006a0ef899] Please let me know when this will be implemented. My customer still has the issue as you would assume. ,"13/Dec/21 7:21 AM;6053c2ec06cbba006a0ef899;Hi [~accountid:6036b7b12de69b006a170ca0], I’d like to point out that the screenshot our customer sent is weird. Setting ERROR level should result into those WARN messages to disappear. Can we please verify with our customer how the change was really implemented? Setting logger levels cannot be done with a deployer server because the file we need to change is under 

{noformat}etc/log-local.cfg {noformat}

a folder that cannot be written by the deployer server. Can we verify that, for each SH cluster node (it looks like the customer has 12 of them), etc/log-local.cfg exists and it contains

{noformat}[splunkd]
category.SearchResultsFiles=ERROR
category.IntrospectionGenerator:resource_usage=ERROR{noformat}



In the meantime I can say that -based on the diag provided- we have this weird error because the SearchResult file (info.csv) has already “invalid” set for the site_id. For example, for diag-usspupl859.internal.unicreditgroup.eu-2021-09-21_15-24-00, we can see that most searches have site=site0, while just few of them have “invalid”:

{noformat}lstoppa@C02DL3AAMD6R diag-usspupl859.internal.unicreditgroup.eu-2021-09-21_15-24-00 % find ./dispatch -type f -name ""*csv"" | xargs grep "",site0,"" | wc -l
   83083
lstoppa@C02DL3AAMD6R diag-usspupl859.internal.unicreditgroup.eu-2021-09-21_15-24-00 % find ./dispatch -type f -name ""*csv"" | xargs grep "",invalid,"" | wc -l
     155
{noformat}

This is an example of a good info.csv:

{noformat}// header:
""_sid"",<... many fields>,""site,"" <many more fields>
// values
<many fields>,"""",5288426,site0, <many more fields>{noformat}

Example of bad info.csv

{noformat}// header:
""_sid"",<... many fields>,""site,"" <many more fields>
// values
<many fields>, """",0,invalid,0, <many more fields>{noformat}

At the moment I’m trying to understand why we have wrong data in there. ","31/Dec/21 12:27 AM;6053c2ec06cbba006a0ef899;Hi [~accountid:6036b7b12de69b006a170ca0] , I have a question for you. Considering I kept failing at trying to reproduce the issue (what we provided till now is nothing more than an hack) I was wondering whether the customer would agree on running -just for a single SH- an instrumented build that will provide additional logging when the issue occurs.

I have the code ready, so I just need a confirmation from our customer.","11/Jan/22 12:50 AM;6036b8025ddf020069bff55d;Hi , i have a customer who is also seeing this issue since upgrading from 7.3.4 → 8.1.5   Case 280974 which i have added to the jira



the customer has

 7 Standalone Search heads and 4 search head clusters (including 1 ES and 1 ITSI cluster)

This issue began immediately after our upgrade from 7.3.4 to 8.1.5. Issue began on each search head as soon as it was upgraded.

Note there was a large time lag from when the Search heads were updated to when the clusters were updated.

The frequency varies wildly by search head (see chart below) so its feasibly search related, but I see not identifier in the error message to help correlate it to an individual search.

!frequent_off_error.png|width=1894,height=953!



i have told the customer about the known issue and the workaround,  and the customer does not feel that this is not a suitable workaround,  as this is reducing the verbosity of the SearchResultsFiles events  , which they are monitoring



the question is ,  is there an alternate workaround ( temporary) ,  or will there be a fix ?



customer has noted also all of their SH are seeing the issue , but some more that others , and  we have two diags one from server gridlog and another from the server redcube.


Their concerns I have are that both of these servers are similar in load and user base.
However, redcube seems to be generating the greatest number of the these errors by a large margin. The question is why?

the links to the diags are below  and is it possible to see why it is effecting one more than the other and we hope that this may give some help in finding the fix to the issue

gridlog diag      [https://downloadsvc.splunk.com/download/splunk/12-08-2021/uploadsvc-85case2809734-12-08-2021-USER-0034000000YhqPtAAJ-diag-gridlog-2021-12-07_17-11-44.tar.gz|https://downloadsvc.splunk.com/download/splunk/12-08-2021/uploadsvc-85case2809734-12-08-2021-USER-0034000000YhqPtAAJ-diag-gridlog-2021-12-07_17-11-44.tar.gz]

redcube diag   [https://downloadsvc.splunk.com/download/splunk/12-08-2021/uploadsvc-24case2809734-12-08-2021-USER-0034000000YhqPtAAJ-diag-redcube-2021-12-07_16-17-53.tar.gz|https://downloadsvc.splunk.com/download/splunk/12-08-2021/uploadsvc-24case2809734-12-08-2021-USER-0034000000YhqPtAAJ-diag-redcube-2021-12-07_16-17-53.tar.gz]

one last thing you mention that you have a debug build, is this for 8.1.5, as i can ask the customer if they want to implement this to see if this speed up the investigation 
","12/Jan/22 12:47 AM;6036b8025ddf020069bff55d;depending on what it involves, the customer could try out the debug version,  but it needs to be safe in a production enviroment","13/Jan/22 12:31 AM;6036b8025ddf020069bff55d;just to let you know the customer has reminded us that this is a production enviroment

   so they are going to need full install  and removal instructions

also in the event of all going wrong a way to roll back the changes



as you said earlier this is just a splunkd replacement, but please  let me know full instruction ","14/Jan/22 4:54 AM;6053c2ec06cbba006a0ef899;Hi, I have added the newly instrumented build to this JIRA.  

*1. INITIAL VERIFICATION:*

* please download the file splunkd_8_1_5_instrumented.gz and run the following commands: 
{noformat}gunzip splunkd_8_1_5_instrumented.gz
md5sum splunkd_8_1_5_instrumented{noformat}

It should show you this hash. This is some sort of integrity check.

{noformat}91fcb84580a1307b6c3ccff4d8819dbb  splunkd_8_1_5_instrumented{noformat}

*2. WHERE TO DEPLOY:*

we need a SH instance with version *8.1.5 where we can reproduce the problem*. The instrumented build was created on top of *8.1.5*. 

*3. HOW TO DEPLOY:*

*3.1 Enable an additional logger*

We first of all need to enable an additional logger. We need to add to *an existing* $SPLUNK_HOME/etc/log-local.cfg

{noformat}category.BacktraceLogger=DEBUG{noformat}

or create a new $SPLUNK_HOME/etc/log-local.cfg with the following content: 

{noformat}[splunkd]
category.BacktraceLogger=DEBUG{noformat}

*3.2 Deploy the new binary*

{noformat}cd $SPLUNK_HOME/bin
./splunk stop
mv splunkd splunkd.original
cp <path-where-was-extracted>/splunkd_8_1_5_instrumented splunkd
chmod a+rx-w splunkd
./splunk start{noformat}

*4. GATHERING LOGS*

We want to probably run few searches and monitor $SPLUNK_HOME/var/log/splunk/splunkd.log* until we see DEBUG entries for {{BacktraceLogger}}. 

Once they appear, I suggest to stop splunk, take a diag, and send it to us for further analysis, and undeploy the change.

*5. UNDEPLOY THE CHANGE*

{noformat}cd $SPLUNK_HOME/bin
./splunk stop
mv splunkd splunkd.instrumented
mv splunkd.original splunkd
chmod a+rx-w splunkd
./splunk start{noformat}

[~accountid:6036b8025ddf020069bff55d] please review these instructions and let me know if you have any question.","14/Jan/22 8:17 AM;5a848585a08cc5310a6cae0a;*+SFDC Case 2876269 Update:+*

[~accountid:6053c2ec06cbba006a0ef899], thanks a lot for all your insights. For my customer, changing

{code}
[splunkd]
category.SearchResultsFiles=ERROR
{code}

has improved things, but they are now getting warnings from a different component {{category.IntrospectionGenerator:resource_usage}}.

This being said, I have asked the customer now to change also the 2nd component on just one server for testing purposes:

{quote}
A. change log-local.cfg to contain: [splunkd]
category.SearchResultsFiles=ERROR
category.IntrospectionGenerator:resource_usage=ERROR B. save the change and restart splunk in order to apply changes.
C. monitor the server and confirm that warnings for both components are now gone.
D. if all warnings are gone for such first server, you can apply the same identical approach for all other affected servers.
{quote}

*+Actions:+*
*Customer:* to further change {{log-local.cfg}} and provide further feedback after the change.
*CMP Engineering:* to continue to work on the official fix.
*+Next Update:+* once there is further feedback from the customer and/or CMP Engineering..","19/Jan/22 9:45 AM;5a848585a08cc5310a6cae0a;*+SFDC Case 2876269 Update:+*

[~accountid:6053c2ec06cbba006a0ef899], after implementing the full change:

{code}
[splunkd]
category.SearchResultsFiles=ERROR
category.IntrospectionGenerator:resource_usage
{code}

{quote}We have done the change and now the WARN logs stopped generating (Uploaded the screenshot). We will be implementing similar config to other SHs as well.{quote}

*+Actions:+*
*Customer:* to implement the same change {{log-local.cfg}} on other affected instances.
*CMP Engineering:* to continue to work on the official fix.
*+Next Update:+* once there is further feedback from CMP Engineering.",20/Jan/22 11:59 PM;6053c2ec06cbba006a0ef899;[~accountid:6036b8025ddf020069bff55d] : have we heard anything from our customer? Was he/she able to deploy our instrumented build? ,"04/Feb/22 1:24 AM;6036b8025ddf020069bff55d;they have not implemented it yet, as the have some other priorities,  but will have to test on there test server 1st,  

however the customer had uploaded some diags for some other system that was seeing the issue and another from a system that is not

they have uploaded a diag from one of the indexers for reference (splkindxeda11).
It may be worth reviewing to see if there is an indication as to why the indexers in this environment are generating this error but the indexers in the other environment are not?

A diag from an indexer (splkindx100) that is NOT generating this error has also been attached.


[https://downloadsvc.splunk.com/download/splunk/01-20-2022/uploadsvc-94case2809734-01-20-2022-USER-0034000000YhqPtAAJ-diag-splkindx100-2022-01-20_10-57-23.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-20-2022/uploadsvc-94case2809734-01-20-2022-USER-0034000000YhqPtAAJ-diag-splkindx100-2022-01-20_10-57-23.tar.gz]

[https://downloadsvc.splunk.com/download/splunk/01-20-2022/uploadsvc-22case2809734-01-20-2022-USER-0034000000YhqPtAAJ-diag-splkindxeda11-2022-01-20_10-56-32.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-20-2022/uploadsvc-22case2809734-01-20-2022-USER-0034000000YhqPtAAJ-diag-splkindxeda11-2022-01-20_10-56-32.tar.gz]","09/Feb/22 1:13 AM;6053c2ec06cbba006a0ef899;Thanks [~accountid:6036b8025ddf020069bff55d] I’ll definitely have a look at those. Maybe they will show us some new hint.

I still believe, however, that having the customer to use our instrumented build should give us some new hint.","10/Feb/22 6:05 AM;6053c2ec06cbba006a0ef899;[~accountid:6036b8025ddf020069bff55d] I have checked both diags and indeed only in one of them you can see the problem, while in the other one no. Initially I thought about some different configuration change, however I couldn’t find a single meaningful difference. I wrote a small script that compared each config file and pretty much we can say both servers have the same config.

So I decided to moved to some data analysis and I think I might have a theory about it.

* I have gathered from both diags the UUID of each replicated bucket from IndefIf: 
{noformat}INFO  IndexerIf - Asked to add or update bucket manifest values, bid=_audit~197~65C39F8A-3FDA-4CD3-91B7-71BCBB9841B0{noformat}
* I have gathered all the repaired buckets (lines like this one): 
{noformat} OnlineFsck - Scheduled repair fsck; procId=0 idx=_audit bucketName=db_1642268574_1642268231_197_65C39F8A-3FDA-4CD3-91B7-71BCBB9841B0 kind={noformat}

I have sorted the full output of both and when trying to find common buckets (that were either replicated or repaired) I couldn’t find a single one. It looks like several buckets in splkindxeda11 exhibit the problem, while the ones from splkindx100 don’t.

*Results* for the “Scheduled repair” data set:

{noformat}lstoppa@C02DL3AAMD6R SPL212495 % grep ""Scheduled repair"" splkindx100/log/splunkd.log* | cut -d= -f4 | sort | uniq > ~/splkindx100_repair.txt    
lstoppa@C02DL3AAMD6R SPL212495 % grep ""Scheduled repair"" splkindxeda11/log/splunkd.log* | cut -d= -f4 | sort | uniq > ~/splkindxeda11_repair.txt
lstoppa@C02DL3AAMD6R SPL212495 % comm -21 ~/splkindx100_repair.txt ~/splkindxeda11_repair.txt                     
lstoppa@C02DL3AAMD6R SPL212495 % wc -l ~/splkindx*                                                                                          
    1884 /Users/lstoppa/splkindx100_repair.txt
     143 /Users/lstoppa/splkindxeda11_repair.txt
    2027 total{noformat}

*Results* for “add or update”:

{noformat}lstoppa@C02DL3AAMD6R SPL212495 % grep ""Asked to add or update bucket manifest values"" splkindx100/log/splunkd.log* | cut -d= -f2 | sort | uniq > ~/splkindx100_add_or_update.txt
lstoppa@C02DL3AAMD6R SPL212495 % grep ""Asked to add or update bucket manifest values"" splkindxeda11/log/splunkd.log* | cut -d= -f2 | sort | uniq > ~/splkindxeda11_add_or_update.txt 
lstoppa@C02DL3AAMD6R SPL212495 % comm -21 ~/splkindx100_add_or_update.txt ~/splkindxeda11_add_or_update.txt                                        
lstoppa@C02DL3AAMD6R SPL212495 % wc -l ~/splkindx100_add_or_update.txt ~/splkindxeda11_add_or_update.txt
    4783 /Users/lstoppa/splkindx100_add_or_update.txt
    6395 /Users/lstoppa/splkindxeda11_add_or_update.txt
   11178 total{noformat}

This shows (clearly?) that both servers have a completely different kind of buckets. The instrumented build I have prepared should tell us where/when those buckets have a sideid field set to “invalid”.

Now, I really wish the customer had a newer Splunk’s version because (I don’t know exactly when) but logging has been improved and now we see what specific thread was executing an offending 

{noformat}SearchResultsFiles Unable to parse site_label, label=invalid due to err=""Invalid site id: invalid{noformat}

Now, with the latest I have available, I see the thread that was calling the {{SearchResultsFiles}} logger.

{noformat} WARN  SearchResultsFiles [3913556 DispatchReaper] -{noformat}","15/Feb/22 3:16 AM;6036b8025ddf020069bff55d;Thanks for looking at the Diags , the customer has Finally installed the binary on the production box, but he says

“ I installed the binaries according to the instructions you provided on the production server redcube.
As you know this server generates a high number of the ""invalid site"" messages.
I let the splunkd run for 30min under normal search load.
However, I did not see any DEBUG events for category.BacktraceLogger=DEBUG .
I have upload a diag from the server.
I hope the additional diag is of value as I have rolled back the splunkd.instrumentation from the server.”



the diag that the customer has provided can be found at 

  [https://downloadsvc.splunk.com/download/splunk/02-15-2022/uploadsvc-36case2809734-02-15-2022-USER-0034000000YhqPtAAJ-diag-redcube-2022-02-14_16-30-15.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-15-2022/uploadsvc-36case2809734-02-15-2022-USER-0034000000YhqPtAAJ-diag-redcube-2022-02-14_16-30-15.tar.gz]

this is 1.6G in size",22/Feb/22 3:57 AM;6036b8025ddf020069bff55d;Hi is there any information i can pass on to the customer?,"23/Feb/22 12:53 AM;6053c2ec06cbba006a0ef899;[~accountid:6036b8025ddf020069bff55d] I have a couple of updates on this, they are not the best but still I believe it’s a step in the right direction. Now at I know why one SH has issues, while the other one not.

Between those there are only one subtle difference:

{noformat}/Users/lstoppa/Downloads/SPL212495/splkindxeda11/dispatch

lstoppa@C02DL3AAMD6R dispatch % find . -type f -name ""*.csv"" | xargs grep "",default,"" | cut -d: -f1 | wc -l
    6436
lstoppa@C02DL3AAMD6R dispatch % find . -type f -name ""*.csv"" | xargs grep "",site,"" | cut -d: -f1 | wc -l
    6442
lstoppa@C02DL3AAMD6R dispatch % find . -type f -name ""*.csv"" | xargs grep "",invalid,"" | cut -d: -f1 | wc -l
       6

lstoppa@C02DL3AAMD6R dispatch % cd ../../splkindx100/dispatch 
lstoppa@C02DL3AAMD6R dispatch % find . -type f -name ""*.csv"" | xargs grep "",default,"" | cut -d: -f1 | wc -l
    4113
lstoppa@C02DL3AAMD6R dispatch % find . -type f -name ""*.csv"" | xargs grep "",site,"" | cut -d: -f1 | wc -l   
    4113
lstoppa@C02DL3AAMD6R dispatch % find . -type f -name ""*.csv"" | xargs grep "",invalid,"" | cut -d: -f1 | wc -l
       0 {noformat}

{{splkindxeda11}} has, in the dispatch directory, 6 info.csv files (created as part of a scheduler run) that have “site” set to “invalid”. I took those files, tried to use them into a simulation (pretty much this was a small unit-test) of how SearchResultsFiles works and finally I was able to reproduce the issue. So I can confirm the issue is data-related.

I have tried to ingest a full day of data from a forwarder to a cluster (indexes) and a SHC, but I couldn’t really replicate the issue. I fear that trying to reproduce the issue in my DEV env won’t give us any result.

What I would like to do now:

* as you know, for 8.2.X there was already a fix implemented that somehow alleviated the issue (instead of thousands of those messages, we now get a couple hundreds at most). My idea would be to incorporate the fix from [https://splunk.atlassian.net/browse/SPL-196040|https://splunk.atlassian.net/browse/SPL-196040|smart-link] into 8.1.8 (or 8.1.9), so at least our customer can upgrade from 8.1.5 → 8.1.8/9 (a minor upgrade is probably safer and less scary than moving to a completely new version)
* because the issue is data-related it’s very hard to tell our customer exactly where to put an instrumented build. I want you to understand that this issue happens when trying to read a csv file that has site = invalid (as part of a scheduler run), however (and I just realised it too late) seeing this doesn’t tell us who wrote “invalid” to any of those cvs under $SPLUNK_HOME/etc/var/dispatch. For this reason I want to add some instrumented dormant code that we can enable immediately once we see the issue (this would simply require to enable/disable an extra logger) and that’s all we need. 

I hope this makes sense.","25/Feb/22 2:09 AM;6036b8025ddf020069bff55d;yes i does make some sense ,  so is there a new test binary , which identifies the wrote the csv file ?","15/Mar/22 2:59 AM;6036b8025ddf020069bff55d;how is it going,  do you have any feedback to my previous update ?
","15/Mar/22 3:23 AM;6053c2ec06cbba006a0ef899;[~accountid:6036b8025ddf020069bff55d] I have at the moment a couple of code changes that will help us identifying who is really writing those “invalid” site ids. I am planning to include that code to Splunk 8.1.X, 8.2.X, and of course Splunk 9.X. The idea is that this time we won’t need the customer to install/rollback instrumented versions, but rather they will only need to enable a newly-added logger. So to summarise the plan more in detail:

* for Splunk 8.1.X: I have applied the fix already introduced in SPlunk 8.2.X that should greatly reduce the number of invalid site ids written to disk. One additional merge request will add new instrumented code.
* for Splunk 8.2.X one merge request will add new instrumented code.
* for Splunk 9.X one merge request will add new instrumented code.

This way we can ask existing 8.1.X customers to upgrade to the newest minor version available (less risky that going directly to 9.0), same for existing 8.2.X customers.

9.X customers will have already the new instrumented code included as part of the first release (or at most in the next minor release).","17/Mar/22 2:37 AM;6053c2ec06cbba006a0ef899;[~accountid:6036b8025ddf020069bff55d] after a deep analysis with a Splunk architect, we believe that adding instrumented code to every future release might not be in the best interest of our customers.

I was wondering: would the customer be ok with retrying the instrumented build we sent him, but this time with a couple of changes:

* before installing the build we must disable (this way the log files won’t be filled with useless output from SearchResultsFiles) SearchResultsFiles and enable again our debug logger:
{noformat}[splunkd]
category.SearchResultsFiles=ERROR
category.IntrospectionGenerator:resource_usage=ERROR
category.BacktraceLogger=DEBUG{noformat}
* we place this time the instrumented build into two different machines: an indexer, and a SH
* remove all the current splunkd.log* (so we have clean logs)
* we wait until our logger will collect some meaningful data we can analyse further. 
* collect a new diag from both SH and indexer

Last time when we gave the customer our instrumented built I realised too late that we had to disable those logs. I think our logger data was probably overwritten by the SearchResultsFiles logger.

What do you think?","17/Mar/22 2:42 AM;6036b8025ddf020069bff55d;i can go back again to them,  just to say we are testing the logger code before with put it in the next minor release

what do you mean about 

before installing the build we must disable 



what do we disable ?","23/Mar/22 8:00 AM;6036b8025ddf020069bff55d;i asked the customer if they are willing to try the new action plan and they replied to say

“I cannot commit to helping with troubleshooting and debug process on my production servers. I would have to try to setup a test environment to reproduce the problem.
But this will take time.”

Also at the moment they say

“We are still getting a high level of noise from this issue and have had to reduce the logging level from WARN to ERROR as the recommended workaround.”



So  we are in a holding pattern until the build a test server, but we dont know when that will be available

if they are wiling upgrade, when is new logging be available ?  as this may be quicker ","23/Mar/22 10:05 AM;6053c2ec06cbba006a0ef899;[~accountid:6036b8025ddf020069bff55d] what new logging are you referring to? Unfortunately the only way we have is to ask them to try our instrumented build with the logger settings as we described. 

I think unfortunately the only way would be to ask them to implement the workaround.","24/Mar/22 7:52 AM;6036b8025ddf020069bff55d;sorry i was refering to you adding instrumented code to every future release

but when i re read this again you said that you will not be doing this.

Customer is running the work around,     so will give him a week or two  to see if they have replicated the problem. and  then they  can try the instrumented build ","01/Apr/22 1:14 AM;6036b8025ddf020069bff55d;just to report back, the customer is still working on a test system to replicate the issue on, they do not want to do this on their prod systems","07/Apr/22 8:44 AM;6036b8025ddf020069bff55d;the customer is still building the test system and i dont know when it will be available,

i think i must have pushed them a bit too far trying to get an ATA

One suggestion they have made is we could do a zoom meeting on with yourself to grab all of the  information you need

on the meeting,    is this something that is possible ?","16/May/22 7:19 AM;6036b8025ddf020069bff55d;they have deployed a test search head with direct access to the production indexers.
they are now attempting to reproduce the error.
It may take a little while longer to get that figured out.","16/May/22 4:33 PM;6036b7b68ff09800716a026a;[~accountid:6053c2ec06cbba006a0ef899] I have the same issue.
Can I ask what version will have the fix ?",22/May/22 4:44 PM;6036b7b68ff09800716a026a;[~accountid:6053c2ec06cbba006a0ef899]  Can I ask what version will have the fix ?,"23/May/22 1:02 AM;6053c2ec06cbba006a0ef899;[~accountid:6036b7b68ff09800716a026a] the issue still isn’t fixed. There was a tentative fix that was developed for 8.2.X, and the same fix has been back-ported to 8.1.8 (this should somehow mitigate the issue). At the moment we are working with [~accountid:6036b8025ddf020069bff55d]'s customer where our customer deployed an instrumented build that should help us understanding where the real problem.","15/Jun/22 12:14 AM;6036b8025ddf020069bff55d;Update from customer ,

I have been trying search combinations to see what works in trying to reproduce the problem.
Due to interruptions I am not quite there yet but have not abandoned the effort...
This week is a bit hectic due to .Conf22 but I'm hoping to make time starting next week.","15/Jun/22 10:25 AM;61d406430586a20069486775;[https://splunk.lightning.force.com/lightning/r/Case/5005a000029MxwGAAS/view|https://splunk.lightning.force.com/lightning/r/Case/5005a000029MxwGAAS/view|smart-link] For additional references, this customer is also experiencing the same issue with their ITSI Search Head. ","07/Jul/22 4:20 AM;6036b8025ddf020069bff55d;I have had an update from the customer

Since .Conf22 the big push has been to upgrade Splunk to v9.0 within our TEST environments.
I am looking to upgrade my test search head to v9.0 and try to reproduce the error there.
It seems that some scheduled searches cause the this error to happen more frequently.
The following screenshot shows some statistics to illustrate what I mean.
Can you help determine which scheduled searches I need to recreate in TEST to duplicate the issue?



!image001 (8a65a2d4-f45b-4702-9ed1-ada87aa3789c).png|width=1151,height=311!","01/Aug/22 1:54 AM;6036b8025ddf020069bff55d;[https://downloadsvc.splunk.com/download/splunk/07-29-2022/uploadsvc-3case2809734-07-29-2022-USER-0034000000YhqPtAAJ-diag-gridlog-2022-07-28_16-51-24.tar.gz|https://downloadsvc.splunk.com/download/splunk/07-29-2022/uploadsvc-3case2809734-07-29-2022-USER-0034000000YhqPtAAJ-diag-gridlog-2022-07-28_16-51-24.tar.gz]  here is the diag fromthe customer , the report

The diag file from my production search head (gridlog) which reports this error the most.
Note that the file is 6.5GB large.
The major reason for this is due to the dispatch directory but I chose to include it because I think it will help in investigating the problem.
The error is related in some form to the type of searching so I think it would be useful to have in the diag.",08/Aug/22 12:38 AM;6036b8025ddf020069bff55d;hi just checking on the Diag that was sent in last week,"24/Aug/22 6:23 AM;6053c2ec06cbba006a0ef899;[~accountid:6036b8025ddf020069bff55d] I was checking two “huge chunks” of loggings and I found that there are searches that repeats themselves:

{noformat}lstoppa@C02DL3AAMD6R log % grep ""07-28-2022 16:14"" scheduler.log | cut -d, -f1,4
07-28-2022 16:14:06.675 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;Jira_License_Status_7 days"", app=""search""
07-28-2022 16:14:08.374 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;Modified Tasktop Alert"", app=""search""
07-28-2022 16:14:17.040 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;ADMIN-SearchSummaryIndexSend2Centauri"", app=""search""
07-28-2022 16:14:31.934 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;CMDB_etx_esx_filers_mapping"", app=""search""
07-28-2022 16:14:36.926 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;Summarize Clearcase History"", app=""search""
07-28-2022 16:14:45.891 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;vcenter_clusters"", app=""search""
07-28-2022 16:14:47.915 -0700 INFO  SavedSplunker - savedsearch_id=""eckman;search;Falcon Double Automount - Open hosts"", app=""search""

07-28-2022 16:14:48.282 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;EMC-app-Isilon;EMC-Isilon-Users-Sid-lookup"", app=""EMC-app-Isilon""

07-28-2022 16:14:55.202 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;monitoringkubernetes;Monitoring Kubernetes: Cluster Critical: etcd does not have a leader"", app=""monitoringkubernetes""
07-28-2022 16:14:00.220 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;monitoringkubernetes;Monitoring Kubernetes: Cluster Critical: Kubernetes API High Number of 5xx"", app=""monitoringkubernetes""
07-28-2022 16:14:31.333 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;monitoringkubernetes;Monitoring Kubernetes: Cluster Warning: high container memory usage"", app=""monitoringkubernetes""
07-28-2022 16:14:35.228 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;monitoringkubernetes;Monitoring Kubernetes: Cluster Warning: unhealthy nodes"", app=""monitoringkubernetes""
07-28-2022 16:14:46.109 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;monitoringkubernetes;Monitoring Kubernetes: Events: Constant Warning"", app=""monitoringkubernetes""


Older dates for 16:14 PM (07/27 and 07/26)
lstoppa@C02DL3AAMD6R log % grep ""07-27-2022 16:14"" scheduler.log* | cut -d, -f1,4
scheduler.log.1:07-27-2022 16:14:06.347 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;Jira_License_Status_7 days"", app=""search""
scheduler.log.1:07-27-2022 16:14:08.598 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;Modified Tasktop Alert"", app=""search""
scheduler.log.1:07-27-2022 16:14:17.809 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;ADMIN-SearchSummaryIndexSend2Centauri"", app=""search""
scheduler.log.1:07-27-2022 16:14:32.171 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;monitoringkubernetes;Monitoring Kubernetes: Cluster Warning: high container memory usage"", app=""monitoringkubernetes""
scheduler.log.1:07-27-2022 16:14:32.372 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;CMDB_etx_esx_filers_mapping"", app=""search""
scheduler.log.1:07-27-2022 16:14:35.449 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;monitoringkubernetes;Monitoring Kubernetes: Cluster Warning: unhealthy nodes"", app=""monitoringkubernetes""
scheduler.log.1:07-27-2022 16:14:37.347 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;Summarize Clearcase History"", app=""search""
scheduler.log.1:07-27-2022 16:14:44.974 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;vcenter_clusters"", app=""search""
scheduler.log.1:07-27-2022 16:14:46.527 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;monitoringkubernetes;Monitoring Kubernetes: Events: Constant Warning"", app=""monitoringkubernetes""
scheduler.log.1:07-27-2022 16:14:48.575 -0700 INFO  SavedSplunker - savedsearch_id=""eckman;search;Falcon Double Automount - Open hosts"", app=""search""
scheduler.log.1:07-27-2022 16:14:48.946 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;EMC-app-Isilon;EMC-Isilon-Users-Sid-lookup"", app=""EMC-app-Isilon""
scheduler.log.1:07-27-2022 16:14:55.508 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;monitoringkubernetes;Monitoring Kubernetes: Cluster Critical: etcd does not have a leader"", app=""monitoringkubernetes""

lstoppa@C02DL3AAMD6R log % grep ""07-26-2022 16:14"" scheduler.log* | cut -d, -f1,4
scheduler.log.1:07-26-2022 16:14:05.740 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;monitoringkubernetes;Monitoring Kubernetes: Cluster Critical: etcd does not have a leader"", app=""monitoringkubernetes""
scheduler.log.1:07-26-2022 16:14:05.751 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;ADMIN-SearchSummaryIndexSend2Centauri"", app=""search""
scheduler.log.1:07-26-2022 16:14:05.894 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;monitoringkubernetes;Monitoring Kubernetes: Cluster Warning: unhealthy nodes"", app=""monitoringkubernetes""
scheduler.log.1:07-26-2022 16:14:05.904 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;monitoringkubernetes;Monitoring Kubernetes: Events: Constant Warning"", app=""monitoringkubernetes""
scheduler.log.1:07-26-2022 16:14:05.983 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;monitoringkubernetes;Monitoring Kubernetes: Cluster Warning: high container memory usage"", app=""monitoringkubernetes""
scheduler.log.1:07-26-2022 16:14:06.227 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;Summarize Clearcase History"", app=""search""
scheduler.log.1:07-26-2022 16:14:10.377 -0700 INFO  SavedSplunker - savedsearch_id=""nobody;search;CMDB_etx_esx_filers_mapping"", app=""search"" {noformat}

I think we can start with the kubernetes-related searches. Those should be run into the test SH peer the customer has added to his PROD env.

If this won’t help replicating the issue, I think we could deploy a different custom build that shows us where we set “invalid” SiteIDs. I checked the whole Splunk code base and found that there are really few places (like 3-4) in the code where we might hit the issue.","25/Aug/22 5:21 AM;6053c2ec06cbba006a0ef899;[~accountid:6036b8025ddf020069bff55d] : one more question. While we know the customer is trying to recreate the issue into one of his VMs, do you think he might agree on testing a new instrumented build? I am asking this because in the code it looks like there are only a couple of places where we might still use an invalid site id. Being able to figure out exactly what code path the customer is hitting, probably would result in a quick fix. 

We could go with both approaches in parallel (instrumented build and creating a repro environment).","09/Sep/22 12:20 AM;6036b8025ddf020069bff55d;Hi Luca,  the customer is happy to install the diagnostic binary into the test SH,  please provide install in so he can install it","27/Sep/22 3:51 AM;6036b8025ddf020069bff55d;hi the customer is getting cold feet about setting up the test sh in production they say

 I have been hesitating to add search load to my production environment.
As of late our indexing tier has been running on high CPU load primarily from search activity.
I would prefer to do this work in the TEST environment.
I think that given the volume that we currently have in TEST, we should be able to duplicate the issue there.
Let me know what your thoughts are on this?



i replied to them to say what ever makes them happy



i assume your tpatch does not care if it is in prod or test env, so log as they can see the error ?","27/Sep/22 4:11 AM;6053c2ec06cbba006a0ef899;[~accountid:6036b8025ddf020069bff55d] Yes, it doesn’t really matter as long as they can see the error in the test env. I might be wrong, but I never fully understood whether after setting up the additional searches in their TEST env they were able to replicate the error there as well.","25/Oct/22 2:24 PM;5f996bce58f26200723ff93b;This comment is left by python automation to maintain Severity alignment i.e., Setting the JIRA Severity to align with the highest priority of associated, open SFDC cases (of closed if all Closed) per New SLO guidelines. Please refer to the [Customer Issues SLO policy|https://docs.google.com/document/d/1aYef6u-9BnO4CBIgNYoCYhK-ejBdK1xMB_FmH4fHN3k]. If you have any concerns, share with [#help-customer-slo|https://slack.com/app_redirect?channel=C02KZJ51P4H]","21/Nov/22 3:11 AM;6036b8025ddf020069bff55d;With one thing or another the customer is not very responsive on this issue, and i think they will not be very us full  this side of Xmas as this is a production server and change controls will become frozen,    i was thinking if we can Drop them and if you can continue on  without them?  the case has now been open for 370Days …","23/Nov/22 12:27 AM;6036b8025ddf020069bff55d;[~accountid:6053c2ec06cbba006a0ef899] customer wants to check to see if we can do another webex with them in order to push this issue  further , he is finding it hard to schedule his time to work on this , so having a webex will help them, he is away this week, but would like to do this next week.","29/Jun/23 10:14 PM;557058:cef60de8-1390-451b-a8a2-aa7771e5dee3;is there any work being done on this? it was supposed to be fixed in the “next major release” after 8.2, we are now 9.1 ","05/Sep/23 2:35 AM;6036b7eb783a4600687c4be2;My cx on case *3302549* is getting these messages at a rate of 39 per second..per SH, at that level I believe it is not benign.",16/Oct/23 12:50 PM;6053a437695c3900707a5bc4;_Noah Indexing_ mission team was set incorrectly on this jira. This belongs to search or search infra (if issue is specific to SHC). Hence setting the mission team to Search Infrastructure. Pls adjust the mission team within search landscape if required. cc [~accountid:6053a3fb81b82500685ced67] [~accountid:5c6e33daa5f342215023f83e] !,"16/Oct/23 3:25 PM;6053a3fb81b82500685ced67;[~accountid:6053a437695c3900707a5bc4] , [~accountid:6053b1ca86b0dd0071888a2e] ,

Is this the same issue of “clustering id parsing” as in [SPL-196040] ?

Thanks,

Hongxun","16/Oct/23 4:55 PM;6053b1ca86b0dd0071888a2e;I think that this is the same symptom, but it doesn’t seem to be the same cause, as the code checked in for the previous ticket seems to still be in the repo. I don’t have context on this problem anymore, and it could stem from any potential use of one of search results info (iirc). Imo, this shouldn’t be assigned to me or the noah-orchestration team. If you do feel that someone from clustering should look at it, please cc their team.","01/Oct/24 2:00 PM;6053a3fb81b82500685ced67;[~accountid:6343d50e754fb6b373b8f017] ,

Can you help take a look at this ticket? It seems related to indexer clustering id parsing issue, as in [SPL-196040] .

Thanks,

Hongxun",,,,,,,,,,,,,,2012576,SPL-206170,Issues w/ no active SFDC ticket,In Progress,16/Nov/21 3:31 AM
[PUBLIC] [WLM][scootaloo] - Deletion of a workload pool is allowed if there is a 'disabled' rule that is related to that workload pool and this can cause errors if the rule is re-enabled later,SPL-204740,1974666,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,,,,Sonali Mahapatra,6053a35d06cbba006a0d9961,Sonali Mahapatra,6053a35d06cbba006a0d9961,24/Apr/21 2:49 PM,30/Jul/25 4:24 PM,,,10.0.x,8.1.2011 GA (Horseshoe),8.1.2012.1 GA (Icefall),8.1.2101.2 GA (Jungfrau),8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Workload Management,,,,,0,cf_systest,CMP_systest,known_issue_approved,known_issue_reviewed,psr_review_bugs_sys,Scootaloo_Review,systest_scootaloo,"build: 4e657ada6683 (8.2.0)

platform: Linux

WLM rules and pools are created.

 

a pool being able to be deleted, when associated rules is disabled. After that when rule is enabled again, there is error message shows up on page. So should we ever allow deleting the pool if rules is associated?

error messages and pool/rule configured are attached

note that When it is enabled, the pool can not be deleted. 

 ",,Andrea Hong,Andrew Brown,Christina Geng,Hongxun Liu,Kiran Shridhar,Pradeep Roy,Shalabh Goyal,Sonali Mahapatra,Sophia Barton,User known,Vimarsh Pandey,,,,,,,,,,,,,,,,,,,,,,,,,6053a969695c3900707a9628,557058:37b4518c-e757-440f-87b7-181f5f425e80,6053aaff3adeca0067edb953,6053a3fb81b82500685ced67,6053a73f4a91ad0068c8310e,5ebddcc3ed4a650b8e88c76f,6053a60cf180c300675e7e8b,6053a35d06cbba006a0d9961,6053bafc311e270068e447e5,unknown,5ebddcc4c5c6230baa7a88cb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-204735,,,,,,,,,,,,,,,,,,,,24/Apr/21 2:49 PM;smahapatra;Screen Shot 2021-04-24 at 12.39.48 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/2505037,24/Apr/21 2:49 PM;smahapatra;Screen Shot 2021-04-24 at 12.39.57 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/2505038,24/Apr/21 2:54 PM;smahapatra;error message after delete the pool.png;https://splunk.atlassian.net/rest/api/3/attachment/content/2505041,24/Apr/21 2:54 PM;smahapatra;rule status after pool is deleted.png;https://splunk.atlassian.net/rest/api/3/attachment/content/2505042,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5401a6fa,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,System,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,138412800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Wed May 05 21:26:29 UTC 2021,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise Services,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,4.0,16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ahong(ahong),andrewb(andrewb),cgeng(cgeng),hliu(hliu),kshridhar(kshridhar),pradeepr(pradeepr),sangp(JIRAUSER44904),sgoyal(sgoyal),smahapatra(smahapatra),sbarton(sbarton),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i7zg1b:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2021-04-25 03:54:04.804,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"To prevent this issue: When you delete a workload pool, please make sure that you delete any disabled workload rules that are associated with that workload pool.

To resolve the issue if you encounter this: Disable or delete the workload rule that is associated with a workload pool that does not exist anymore.",,,,,,,,,,2021-04-25 03:54:04.804,2021-04-25 03:54:04.804,,,,,,,,,,,,,24/Apr/21 8:54 PM;5ebddcc3ed4a650b8e88c76f;[~accountid:6053ac5e66c879006838c561] This needs triage. Thanks!,"26/Apr/21 10:21 AM;6053a3fb81b82500685ced67;[~accountid:6053a73f4a91ad0068c8310e],
Can you help fix this scootaloo related ticket?

Thanks,

Hongxun
","26/Apr/21 3:53 PM;6053ac5e66c879006838c561;[~accountid:6053a3fb81b82500685ced67] and [~accountid:6053a73f4a91ad0068c8310e], when you say ""fix"", does that mean this is a release blocker? (or can it go into a maintenance release) Can you double check the severity level. Thanks
cc: [~accountid:6053b95a66c8790068395481], [~accountid:6053aaff3adeca0067edb953]","27/Apr/21 11:00 AM;6053a73f4a91ad0068c8310e;[~accountid:6053a3fb81b82500685ced67] I spent time today looking into this today. Per https://splunk.atlassian.net/browse/SPL-188539, feature to disable rules was introduced in Horseshoe i.e after Rarity. So, this is a new feature in Scootaloo. 

Cc: [~accountid:6053bafc311e270068e447e5]","27/Apr/21 5:00 PM;6053a60cf180c300675e7e8b;In my view, we should not let a pool to be deleted if a related rule exists even though the rule is disabled. Else, once the rule is enabled after pool is deleted then it will run into issues like this.

For now, we can call this as known issue and list a workaround.","27/Apr/21 8:04 PM;6053a3fb81b82500685ced67;[~accountid:6053a60cf180c300675e7e8b],
A disabled rule is essentially a non-existent rule  in terms of effect on searches. It is a convenience that we provide to keep a disabled rule, so that we can enable the rule sometime in the future, or never.  Having a pool dependent on an ineffective rule might be an overreach. It makes a bit more sense to have a rule dependent on a pool. 

Either way, this should not be a blocker for scootaloo release.

Thanks,

Hongxun
","28/Apr/21 4:58 AM;6053a73f4a91ad0068c8310e;Thanks [~accountid:6053a3fb81b82500685ced67] and [~accountid:6053a60cf180c300675e7e8b] for sharing your feedback. I agree that this is not a blocker for Scootaloo. Also, I believe this is pre-existing since it was first implemented in Horseshoe. I'm going to unassign myself on that basis. Kindly re-assign it as necessary.

Cc: [~accountid:6053ac5e66c879006838c561] [~accountid:6053a66586b0dd007188092d]","28/Apr/21 7:51 AM;6053aaff3adeca0067edb953;[~accountid:6053a73f4a91ad0068c8310e] what do you mean pre-existing? do you mean Rarity? since this thread is discussing the issue for on-prem customers.

I agree with [~accountid:6053a60cf180c300675e7e8b] from the user experience perspective, it is not right to delete a pool with rule existence even it is disabled, but not deleted yet. What's the behavior of the customer enables the disabled rule without a pool? 

cc [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] since it might need to document it. ","28/Apr/21 9:12 AM;6053a60cf180c300675e7e8b;For release note purpose, I have few questions: [~accountid:6053a3fb81b82500685ced67] [~accountid:6053bafc311e270068e447e5]

1) If a user deletes a pool and then enables a rule that is related to the deleted pool, we show an error that workload rules could not be processed/read. Does it mean none of the WLM rules are processed or only the rule that has deleted pool is not processed?

2) To recover from this, does the customer just needs to delete or disable the rule that is related to the deleted pool?",28/Apr/21 9:17 AM;6053a73f4a91ad0068c8310e;[~accountid:6053aaff3adeca0067edb953] I meant its not introduced in Scootaloo but is present since it was implemented in Horseshoe Cloud Release. Hope this helps!,"28/Apr/21 10:44 AM;6053bafc311e270068e447e5;[~accountid:6053a60cf180c300675e7e8b] 
1) It means that when it is reading the rules config from disk and iterating over each rule, it will fail and stop reading/processing all subsequent rules once it reaches the one with the pool that doesn't exist anymore. But, there is *no* guarantee that any of the rules will work properly.
2) To recover, the customer can disable the rule with the deleted pool. Deleting it is not necessary.

For example, I tested this just now and let's say you have some rule1 that works and a rule2 associated with a deleted search pool. If rule2 is enabled after deleting its pool, if you edit rule1, that change will not be reflected when you search. Once you disable rule2 however, you can modify rule1 and see the changes and it all works properly. ","28/Apr/21 7:15 PM;6053aaff3adeca0067edb953;Removing this from the Scootaloo watch list, cc [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] requires the release note with the workaround. Thanks. cc [~accountid:6053ac5e66c879006838c561] [~accountid:6053b95a66c8790068395481]","29/Apr/21 8:51 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:6053aaff3adeca0067edb953] this is included in the customer-facing Scootaloo / 8.2.0 known issues list, including the helpful workaround that Shalabh provided. 

[https://docs.splunk.com/Documentation/Splunk/8.2.0/ReleaseNotes/Knownissues#Uncategorized_issues] (login required for this URL until May 12 GA)",05/May/21 1:00 PM;6053a969695c3900707a9628;[~accountid:6053a60cf180c300675e7e8b] - Please take a look at the summary and workaround. Can this be re-worded so that it's more straightforward for a customer to follow? ,05/May/21 1:06 PM;6053a60cf180c300675e7e8b;I updated the summary a bit. I don't think I can make any changes to the workaround. We can ask [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] in case he has any feedback/edits.,05/May/21 2:26 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;I think users will be able to follow the workaround if they need it. If we heard of confusion we can update it on the fly after release. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1260914,SPL-171944,MT-PR10 Tech Debt,To Do,24/Apr/21 2:49 PM
[PUBLIC] Summary page on monitoring console doesn't show correct RF/SF when not running on the CM.,SPL-203100,1938312,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Fixed,Sriram Ananth,6053b98160d39e006f8173ee,Ahmed Nasser Abdel Wahab,6036b882d3fc7c006809a688,Ahmed Nasser Abdel Wahab,6036b882d3fc7c006809a688,29/Mar/21 8:27 AM,30/Jul/25 4:24 PM,,28/Apr/21 5:48 PM,10.0.x,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.6.1,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Monitoring Console,,,,,0,AxpTriaged,,,,,,,"Customer set RF=2 and SF=2 on CM but when accessing summary page on MC, the RF is appearing as 3. 

Configurations are set correctly on CM and on Indexer Clustering: Status panel, it appears correctly as 2.

We tried it in our lab and MC is also showing RF = 3 and SF =2. It appears to be showing static info and not reflecting the actual configurations.

Attached screenshots showing the issue.

CM diag:

[https://downloadsvc.splunk.com/download/splunk/02-23-2021/uploadsvc-37case2124885-02-23-2021-USER-0030b000026MfFtAAK-diag-splunk-mgmt-clsmaster-2021-01-13_15-59-04.tar.gz]

MC diag:

[https://downloadsvc.splunk.com/download/splunk/03-28-2021/uploadsvc-89case2124885-03-28-2021-USER-0030b00001yM2GVAA0-diag-splunk-mgmt-lic-dmc-2021-03-25_15-37-08.tar.gz]

Please ignore any connectivity errors found in splunkd.log in MC diag as it has been resolved.",,Ahmed Nasser Abdel Wahab,Emmanuele La Porta,Jonah Reuter,Kriti Ashok,Sriram Ananth,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6036b882d3fc7c006809a688,6036b7ac5ddf020069bff0ff,6053a5692f452d006f82793b,6053a53945a3bb006816a1a7,6053b98160d39e006f8173ee,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-204960,,,,,,,,,,,29/Mar/21 8:23 AM;ahmedn;Screenshot 2021-03-29 at 12.46.43.png;https://splunk.atlassian.net/rest/api/3/attachment/content/2407183,29/Mar/21 8:23 AM;ahmedn;Screenshot 2021-03-29 at 12.48.17.png;https://splunk.atlassian.net/rest/api/3/attachment/content/2407184,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Vodafone Egypt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@40b8ee3a,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"o Have they made any changes to their environment just before the issue started?
No",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,34560000,,,,,,,,,,,,,,,,,,,,,,,,,Installed Version,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"o What is the customer environment for the issue? 
Indexer cluster and DMC

",,,,,,,,,,"o What errors are being reported?
No Errors. Summary page on DMC reporting wrong RF.",false,,,,,,,,,,,,No,,,,,,,,,,,,,"o What is the expectation when this problem is not there?
Summary page on DMC reports correct RF and SF figures",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• How long has the customer been tracking this issue and how frequent this issue is being hit?
For a while now",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Tue Aug 20 11:28:06 UTC 2024,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,2.0,14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ahmedn(ahmedn),elporta(elporta),garias(garias),jreuter(jreuter),ostapleton(JIRAUSER46162),sananth(sananth),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Feature/Usability Improvement,,Cust Managed Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i7tmvr:,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sriram Ananth,6053b98160d39e006f8173ee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,F. Hoffmann-La Roche AG,3077461,012400000005WzCAAU,P3,No,5005a00002GyrObAAJ,,Open,Customer Update,Standard,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2124885,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Monitor Indexer cluster from DMC. Summary page showing incorrect RF.,,,,,,,,,,,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,On Prem Premium,,,,,,,,,,,,,,,,2021-03-30 09:14:31.476,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3_*:*_1_*:*_605381898_*|*_6_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_1953807107_*|*_10039_*:*_1_*:*_66472939,,,,,,,,,,,,,,,,,,,,,,,,,,"Sustaining/Support template Accuracy  (data collected at same time, Support analysis filled in etc.)",Sustaining/Support template complete,Valid JIRA mandatory fields,Valid JIRA problem statement in JIRA Summary,,,,,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2021-03-30 09:14:31.476,2021-03-30 09:14:31.476,,,,,,Sriram Ananth,6053b98160d39e006f8173ee,,,,,,"30/Mar/21 2:14 AM;6036b7ac5ddf020069bff0ff;Hello Everyone,

 

Issue is reproduced on this instance:

This is the SH/DMC
 [http://10.224.150.77:62887/en-GB/account/login?return_to=%2Fen-GB%2Fapp%2Fsplunk_monitoring_console%2Fmonitoringconsole_landing]

 

And this is CM:
 [http://10.224.155.64:61898/en-GB/manager/system/clustering?tab=indexes]

 Checking the same summary page on the CM shows correct numbers:


http://10.224.155.64:61898/en-GB/app/splunk_monitoring_console/monitoringconsole_landing

 

USER/Pass

admin/Chang3d!

 

Initially RF&SF were 3/3 though shown number from the summary page were 3/2, and now I settled 2/2, and they are still 3/2 like if no changes.

It seems that this fails only for DMC machines configured outside the CM.

 

BR Emmanuele","30/Mar/21 2:55 AM;6036b807f6ccc0007044b3fd;Traffic cop, jira is fine, repro available","07/Apr/21 2:10 AM;6036b882d3fc7c006809a688;Hi [~accountid:6053b98160d39e006f8173ee]

Any updates on this?

 ","12/Apr/21 4:06 PM;6053b98160d39e006f8173ee;Hello, I am just looking into the issue now. I am unable to access the repro env, is this open for access? ","13/Apr/21 2:19 AM;6036b7ac5ddf020069bff0ff;Hello [~accountid:6053b98160d39e006f8173ee],

 

you are one day too late ;). Machine was killed yesterday morning.

Do try reproduce it yourself and let us know, start SF/RF 3.3 then reduce to 2.2.

 

Regards Emmanuele","18/Apr/21 5:12 AM;6036b882d3fc7c006809a688;Hi [~accountid:6053b98160d39e006f8173ee]

Any updates on this please? This ticket has been idle since 29th of March.

 ","21/Apr/21 5:35 AM;6036b882d3fc7c006809a688;Hi [~accountid:6053b98160d39e006f8173ee]

Appreciate your feedback on this ticket.","21/Apr/21 11:26 PM;6053b98160d39e006f8173ee;[~accountid:6036b882d3fc7c006809a688] I have replicated the issue and am in the process of investigating it. The first step is to determine if the issue is on the front end or on the backend. It appears that the 'Summary' page of the DMC which shows the IDXC configs under 'Deployment Topology' hits this REST endpoint to query for IDXC configs - '/services/cluster/config/config'. 

Now when I hit the same endpoint manually, I am getting RF=3 from the IDX nodes of the cluster, even though replication_factor is set to 2 on the CM. I confirmed that the CM is showing the right replication_factor by hitting the same endpoint on the CM. I am still investigating the issue and will provide another update shortly.","26/Apr/21 10:48 AM;6053b98160d39e006f8173ee;*+UPDATE+*

This appears to be a limitation in Splunk when hosting the MC on a SH. The REST endpoint used to query the deployment topology information like rep factor and search factor are valid only on a CM. The SH has no REST endpoints which can query the RF and SF from the CM. This can only be done on the CM.

From a frontend perspective, if the MC is hosted on an SH, it cannot directly query a CM for the RF SF info (Thanks [~accountid:557058:b69f0164-d20f-4759-be85-9d35952e8b44] ), it needs to happen on the backend. As mentioned there is no way currently, from the backend of the SH to query this.

Unfortunately, at this point, the only solution would be to host the MC on the CM node, as this will ensure the right RF SF details are populated. A design change/ new REST endpoint which enables SH to query CM will be required for this issue to be fixed correctly.

Please let me know if you have any follow up questions.

","27/Apr/21 2:35 AM;6036b882d3fc7c006809a688;Hi [~accountid:6053b98160d39e006f8173ee]

Thanks for the update. I will inform the customer and will let you know if there are any questions.

Thanks","28/Apr/21 5:51 AM;6036b882d3fc7c006809a688;Hi [~accountid:6053b98160d39e006f8173ee]

Customer understood the explanation and happy to close the case.

Thanks for your support.","28/Apr/21 1:59 PM;6053a5692f452d006f82793b;[~accountid:6053b98160d39e006f8173ee] can we create a jira to address this limitation and link/assign appropriately, then close this one?","28/Apr/21 5:48 PM;6053b98160d39e006f8173ee;[~accountid:6053a5692f452d006f82793b] Done, created SPL-204960 to track post-mortem of this. Closing out this JIRA","20/Aug/24 4:28 AM;60067377ee80bd006f6bcc78;The follow-up task (SPL-204960) was closed due to inactivity, so I will comment here instead. This still hasn’t been addressed as of Splunk Enterprise 9.3.0, so I just wanted to share what my customer reported: 

They had a discussion due to hardware sizing and were not 100% sure about their RF/SF. So they first checked their MC instance and were happy to immediately see 3/2. Based on this they made their calculations, which ended up wrong because in their 2 Indexer Clusters, they actually have 2/2. They are frustrated that the UI in the MC “cares more about displaying a fictional value” than actually making sure the information is correct. The explanation that the MC instance simply can’t query the information from the CM instance(s) of the Indexer Cluster(s) also isn’t really satisfactory, because then why bother at all to show this value in the MC UI? Especially since our own documentation states
”[As a general rule, you should dedicate the Splunk Enterprise instance running the manager node to that single purpose.|https://docs.splunk.com/Documentation/Splunk/9.3.0/Indexer/Systemrequirements#Additional_roles_for_the_manager_node]” This is also what our customers seem to follow (certainly the customers I have seen). 

It also does not leave a great impression that the issue has been open for 3.5 years now and the new releases still don’t fix it. IMO, the solution is simple: Remove displaying this value in the MC UI as long as the information cannot be correctly queried via REST from the CM instances by the MC instance. Currently it shows a wrong value for the grand majority of customers so causes more harm than good.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,28/Apr/21 5:48 PM
[PUBLIC] The splunkd process changes the local distsearch.conf on service start,SPL-203060,1935510,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Delivered,Robin Welch,6053abe445a3bb006816ecea,Robin Welch,6053abe445a3bb006816ecea,Robin Welch,6053abe445a3bb006816ecea,26/Mar/21 2:08 AM,30/Jul/25 4:26 PM,,26/Mar/21 8:56 AM,10.0.x,8.0.0 GA (Quake),8.0.1,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conf System,Migration,Search - Distributed,,,0,,,,,,,,"(see [SPL-202723|https://splunk.atlassian.net/browse/SPL-202723])

After upgrading from 7.2.7 to 8.1.2, customer has found that Splunk makes the following modifications to *distsearch.conf* each time the splunkd process starts:

* Removes any attributes which match default settings
* Removes user comments
* Reorders KV pairs alphanumerically within a stanza
* Reorders stanzas within the file

This has affected them the customer as described in the main ticket. I have explained to the customer the situation and (albeit that I intend to take the matter of configs being altered further as a separate issue) and they have accepted the explanation but have understandably queried why this is not documented, particularly since there is a specific change from 7.x to 8.x

Can the documentation specifically for 8.x be updated in the description of *distsearch.conf* ([here?|https://docs.splunk.com/Documentation/ITSI/4.8.1/Configure/distsearch.conf]) to reflect that this file may be altered on start-up?",,Andrew Brown,Edward Kostowski,Jo Hornsby,Jonah Reuter,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,557058:4a62ae54-d811-4a94-abdf-457ade60ee5b,557058:f87e2651-5132-4aed-8b96-f608ee969435,6053a5692f452d006f82793b,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,SPL-202723,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Apple Inc.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4fc90466,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,139536000,,,,,,,,,,,,,,,,,,Enterprise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,false,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Thu Apr 22 17:13:09 UTC 2021,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),ekostowski(ekostowski),jreuter(jreuter),rwelch(JIRAUSER42064),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Codefix,Documentation improvement - Customer Facing,Cust Managed Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i7t6kf:,,,,,,,"The code is making upgrade changes as part of the start of splunkd rather than as part of a separate migration process. This is arguably bad practice. nevertheless, it also does so in a rather disruptive manner, and while the behaviour meets internal functional requirements, it is understandably confusing and inconvenient for customers and can cause confusion to orchestration systems. In the short term, the fact that this is happening should at least be documented so that customers know to expect it. In the longer term a code fix is needed to stop this happening.",,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This is caused by code changing the order of config items and removing comments.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Closed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2021-03-26 15:51:52.282,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4_*:*_1_*:*_14425_*|*_5_*:*_1_*:*_1904380_*|*_6_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_22549063,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"There is no workaround. After upgrading to Splunk Enterprise 8.x, the splunkd process checks and modifies the local/distsearch.conf on each service start. The process will:
* Remove any settings that define default values already set in the /default/distsearch.conf file.
* Removes comments preceded by a hash.
* Reorders the KV pairs alphanumerically within a stanza.
* Reorders stanzas within the file.
",,,,,,,,,,2021-03-26 15:51:52.282,2021-03-26 15:51:52.282,,,,,,Robin Welch,6053abe445a3bb006816ecea,,,,,,"26/Mar/21 8:51 AM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;Flagging this as a Known Issue due to the poor code behavior, which also breaks the customer expected and documented behavior when setting configurations using .conf files in the etc/system/local path. 
The secondary consideration is:
* Is there a plan to change that behavior back?
  * Yes: Then it should remain a Known Issue.
  * No: Then we should consider updating the documentation as well.
* Does this change apply only to distsearch.conf in etc/system/local, or does it apply to etc/apps/$app_name/local as well?
* Is there a plan to do setting validation and modification for other configuration files in /local?","26/Mar/21 9:08 AM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;[~accountid:6053a5692f452d006f82793b], with the pending feature of full accounting of config files changes coming soon, is there a way for the customer to differentiate a change made by splunkd itself?
Thanks!","26/Mar/21 11:39 AM;6053a5692f452d006f82793b;In phase 1 which will be in scootaloo, we took the approach of catching all changes first. Since we are catching everything, even changes made with a text editor while splunk is not running, there is no attribution to the change. Determining who performed the action is to be delivered in a later phase, and attribution will obviously not be available for out of band/splunk down changes.",26/Mar/21 11:43 AM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;Thank you!,22/Apr/21 10:13 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;I plan to carry this forward as a published known issue for Scootaloo / 8.2.0. Just an FYI. No need to respond unless this should not happen. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,26/Mar/21 8:56 AM
"[PUBLIC] The license usage report tab name is Previous 60 days, but the reports run over the last 30 days",SPL-202682,1924040,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Delivered,Shanmugam Kailasam,6053a6c745a3bb006816b393,Kana Aiyoshi,6036b88d8ff09800716a0bf3,Kana Aiyoshi,6036b88d8ff09800716a0bf3,19/Mar/21 2:40 AM,30/Jul/25 4:25 PM,,03/May/21 7:48 AM,10.0.x,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Licensing,,,,,0,apac_review:feed09,apac_support:triaged_03,known_issue_approved,known_issue_reviewed,,,,"h3. [Problem description]

After upgrading to 8.1.1
 The SPL in license usage set only for 30days.

Even though the document below said,

 
{code:java}
""The License Usage report will change to ""Previous 60 Days"" if your Splunk Enterprise license stack is less than 100GB and is subject to conditional license enforcement.""
{code}
[https://docs.splunk.com/Documentation/Splunk/8.1.1/Admin/AboutSplunksLicenseUsageReportView#License_Usage_-_Previous_30_Days]

 

Moreover, the default value of the _internal's frozenTimePeriodInSecs is 2592000 seconds, which is 30 days not 60 days.
h3. [Environment]

Splunk enterprise: 8.1.1
h3. [Investigation by support]

I just confirmed all the SPLs in use for reports on License Usage in Splunk version:8.1.1.

 

On the monitoring console:
 Settings > Monitoring Console.
 Indexing > License Usage
 Select Historic License Usage.

Here is the one of SPLs in use for the reports on License Usage.

***************************************

 
{code:java}
(index=_internal host=so1 source=*license_usage.log* type=""RolloverSummary"" earliest=-30d@d) <-----
| eval _time=('_time' - 43200) 
| bin _time span=1d 
| stats latest(b) AS b by slave, pool, _time 
| timechart span=1d sum(b) AS ""volume"" fixedrange=false 
| join type=outer _time 
 [| search (index=_internal host=so1 source=*license_usage.log* type=""RolloverSummary"" earliest=-30d@d) <-----
 | eval _time=('_time' - 43200) 
 | bin _time span=1d 
 | dedup _time stack 
 | stats sum(stacksz) AS ""stack size"" by _time] 
| fields - _timediff 
| foreach ""*"" 
 [ eval <<FIELD>>=round('<<FIELD>>'/1024/1024/1024, 3) ] 
| fields - ""stack size"" 
| fields - ""pool size""{code}
 

***************************************
h3. [Troubleshooting steps performed]

N/A

 
h3. [Log files]

No log files because this can be easily confirmed by creating Splunk:8.1.1.

 
h3. [Expectation to Engineering]

1. Is this behavior is by design?
 2. If this behavior is not by design, Can you modify the document with the doc team?

 

**

I created the Jira#:DOCGUILD-6234 to the doc team but the doc team said I should raise a Jira to CMP first.

 ",Splunk enterprise: 8.1.1,Andrea Hong,Edward Kostowski,Izzy Park,Jo Hornsby,Kana Aiyoshi,Kriti Ashok,Miles Chang,Rui Chai,Shanmugam Kailasam,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a969695c3900707a9628,557058:4a62ae54-d811-4a94-abdf-457ade60ee5b,5a1ef9dc40207c40ef9126d7,557058:f87e2651-5132-4aed-8b96-f608ee969435,6036b88d8ff09800716a0bf3,6053a53945a3bb006816a1a7,6036b828d321ef0068d43d7d,6053a3ffe394c30069caaf10,6053a6c745a3bb006816b393,,,,,,,,,,,,,,,,,,,,,,,,,,,1200,0,,0%,1200,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DOCGUILD-6234,,,,,,,,12/Apr/21 2:53 PM;rchai;Screen Shot 2021-04-12 at 2.52.57 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/2456753,12/Apr/21 2:53 PM;rchai;Screen Shot 2021-04-12 at 2.53.06 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/2456754,29/Mar/21 9:02 AM;ekostowski;Untitled.m4v;https://splunk.atlassian.net/rest/api/3/attachment/content/2407317,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Ministry of Education, Culture, Sports, Science and Technology",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@76f86eb3,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"o Have they made any changes to their environment just before the issue started?
No",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,29.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,138412800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"o What is the customer environment for the issue? 
Linux, Splunk 8.1.1
o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,"o What errors are being reported?
N/A",false,,,,,,,,,,,,No,,,,,,,,,,,,,"o What is the expectation when this problem is not there?
License Usage should report 60 days.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• How long has the customer been tracking this issue and how frequent this issue is being hit?
N/A",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Wed May 05 20:12:13 UTC 2021,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CMPInsightsAmer,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,3.0,29.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ahong(ahong),ekostowski(ekostowski),ipark(ipark),kaiyoshi(kaiyoshi),kshridhar(kshridhar),mchang(mchang),rchai(rchai),skailasam(skailasam),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cust Managed Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Shanmugam Kailasam,6053a6c745a3bb006816b393,,,,,,,,,,,,,,,,,,0|i7vz2v:,,,,,,,Reopening to assign a story point to this ticket for tracking purposes. Will close it again afterwards. ,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Shanmugam Kailasam,6053a6c745a3bb006816b393,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Companies Registry,3213448,012400000005WzMAAU,P4,No,5005a00002VqSKFAA3,,Closed,Resolved - Work Around,Standard,Tier4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2160329,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CMPInsightsAmer-FY22Q1-S5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

1. Installed Splunk version:8.1.1.
2. On the monitoring console:
Settings > Monitoring Console.
Indexing > License Usage
Select Historic License Usage.

3. you can click ""open in search"" on panels and confirm ""earliest=-30d@d""

• If available, will customer upgrade to fixed version?
Need to ask

• If support is able to reproduce, share the setup.
Created in orca choosing debian linux.",,,,,,,,,,1.0,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.


--> Please look at the description.
",,,,,,,,,,,,,,,,,,,2021-03-22 03:47:18.71,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4_*:*_3_*:*_1299987067_*|*_5_*:*_3_*:*_2049260539_*|*_6_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_319185387_*|*_10039_*:*_1_*:*_238047813,,,,,,,,,,,,,,,,,,,,,,,,,,"Sustaining/Support template Accuracy  (data collected at same time, Support analysis filled in etc.)",Sustaining/Support template complete,Valid JIRA mandatory fields,Valid JIRA problem statement in JIRA Summary,,,,,,,,,,,,,,,,,,,,,,12.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2021-03-22 03:47:18.71,2021-03-22 03:47:18.71,,,,,,Shanmugam Kailasam,6053a6c745a3bb006816b393,,,,,,"21/Mar/21 8:47 PM;6036b828d321ef0068d43d7d;*1 Review the Jira Summary (/)* 
*2 Review Accuracy and Completeness (/)*
*3 Review the Description Field (/)*
*4 Review the Support Template Tab (/)* 
 
*Triage case to CMP (/)*

*+Comments to CMP+*

*+Comments to TSE+*
",25/Mar/21 1:21 PM;6053a73f4a91ad0068c8310e;I can confirm from code inspection as well as with my standalone test install that _Settings > Licensing > License Usage or Settings > Splunk Monitoring Console > Indexing > Licence Usage > Historic License Usage_ will aways show previous 30 days.,25/Mar/21 1:27 PM;6053a73f4a91ad0068c8310e;I see DOCGUILD-6234 is already logged to track updates or correction to DOCs. Nothing to do in SPLCORE. ,"25/Mar/21 5:16 PM;6036b88d8ff09800716a0bf3;Thank you [~accountid:6053a73f4a91ad0068c8310e] for the confirmation. I will check with the doc team in DOCGUILD-6234

 ","26/Mar/21 8:13 AM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;Thanks. It looks like there was a behavior change that wasn't communicated to me. 
https://docs.google.com/presentation/d/1PdPOOqCiGLjfajCq2jK2nW-CABqp1LF-mxGU80TFy-8/edit#slide=id.g82c9922c9a_0_32
Hi [~accountid:6053a3ffe394c30069caaf10], [~accountid:557058:7f736889-b6e6-4118-9404-366738728426], would you confirm that the License Usage report will never change from the 30 day default under a CLE license condition? 

Thanks!","29/Mar/21 8:53 AM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;Good day all!

I tested this on 8.1.2 this morning. Using an Enterprise license under 100G results in the License Usage report changing the tab name from 30 Days to 60 Days. The actual search doesn't change unfortunately. 

Please have another look.

Thanks!
",29/Mar/21 10:05 AM;6053a73f4a91ad0068c8310e;Thanks [~accountid:557058:4a62ae54-d811-4a94-abdf-457ade60ee5b] for chasing this. I appreciate your help. Can you share the license or the test system where this is reproducible or visible? I tested it with the internal license (50GB) which is technically <100GB too.,"29/Mar/21 10:18 AM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;Sure. It's a Developer License assigned to my name, and I'll send it to you via Slack. The key is the license has to appear as an Enterprise License Stack before the transition on the usage report takes place.","29/Mar/21 3:48 PM;6053a73f4a91ad0068c8310e;Thanks [~accountid:557058:4a62ae54-d811-4a94-abdf-457ade60ee5b]. I was able to reproduce the issue. As this feature was implemented tas part of SPL-178881, I've assigned this to [~accountid:6053a6c745a3bb006816b393] who was the lead. ","29/Mar/21 4:29 PM;6053a73f4a91ad0068c8310e;I and [~accountid:6053a6c745a3bb006816b393] had a discussion earlier. Our question is as below

What is the behaviour if the _internal index data age is increased from the 31 days to 60 days?","30/Mar/21 12:59 AM;6036b88d8ff09800716a0bf3;Hello [~accountid:6053a73f4a91ad0068c8310e] [~accountid:6053a6c745a3bb006816b393]  

The customer pointed out index=_internal will roll out to frozenbucket in 30days.
I also checked this setting in my lab on 8.1.1.
Should I still ask the customer to run the SPL to modify the time range:earliest=-60d@d latest=-31d@d even though all these searches use index=_internal?

Kindly let me know.

 
{code:java}
[_internal]
homePath = $SPLUNK_DB/_internaldb/db
coldPath = $SPLUNK_DB/_internaldb/colddb
thawedPath = $SPLUNK_DB/_internaldb/thaweddb
tstatsHomePath = volume:_splunk_summaries/_internaldb/datamodel_summary
maxDataSize = 1000
maxHotSpanSecs = 432000
frozenTimePeriodInSecs = 2592000 <--- 30days{code}
 ",30/Mar/21 1:18 AM;6053a73f4a91ad0068c8310e;[~accountid:6036b88d8ff09800716a0bf3] The ask was to verify whether the behaviour internally first before going to the customer. ,30/Mar/21 3:01 AM;6036b88d8ff09800716a0bf3;[~accountid:6053a73f4a91ad0068c8310e] Understood.  Kindly let me know when you need to get some info from the customer. ,"31/Mar/21 1:48 PM;6053a6c745a3bb006816b393;[~accountid:6036b88d8ff09800716a0bf3] yes, let's confirm before we suggest this fix. I don't remember we tested this case internally, so we will take that as next step before letting you know. Thanks!","12/Apr/21 2:54 PM;6053a3ffe394c30069caaf10;hi [~accountid:6053a6c745a3bb006816b393] / [~accountid:6053a73f4a91ad0068c8310e] / [~accountid:6036b88d8ff09800716a0bf3] I just manually verified this. By *extending the _internal index's frozenTime to 60 days*, and modifying the search SPL's time range to 60days, the license usage data over 30days can be searched/displayed successfully.

!Screen Shot 2021-04-12 at 2.52.57 PM.png|width=942,height=374!

!Screen Shot 2021-04-12 at 2.53.06 PM.png|width=936,height=372!

Having 2 screenshots above, after extending the _internal index's frozenTime. If search earliest time is -30days, results' earliest date is 04/19. If earliest time is -60days, then the results will start from 04/17","13/Apr/21 9:56 AM;6053a3ffe394c30069caaf10;per the discussion with [~accountid:6053a6c745a3bb006816b393] this should be okay to mark as resolved now and good to close it. 

cc [~accountid:6036b88d8ff09800716a0bf3] [~accountid:557058:4a62ae54-d811-4a94-abdf-457ade60ee5b] [~accountid:6053a73f4a91ad0068c8310e]","13/Apr/21 12:06 PM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;Sorry folks, I'm missing out on the resolution here. As I read it, changing the _internal retention sounds like a workaround. ","13/Apr/21 5:28 PM;6036b88d8ff09800716a0bf3;[~accountid:6053a3ffe394c30069caaf10] [~accountid:6053a6c745a3bb006816b393] 

Thank you for the update and the workaround. 

Isn't there any fix included for future version?

I think we just need to fix either one of them, fixing this _internal frozen period or fixing the UI.

Can you also update this on a release note as a known issue? ","13/Apr/21 10:42 PM;6053a6c745a3bb006816b393;Thanks [~accountid:6053a3ffe394c30069caaf10] for getting this verified.

Hi [~accountid:557058:4a62ae54-d811-4a94-abdf-457ade60ee5b] [~accountid:6036b88d8ff09800716a0bf3]

Yes, this is a short term workaround.

Also changing the default retention time of _internal from 30 to 60 just for CLE wouldn't make lots of sense for >100GB where 60 days CLE regime isn't applicable.   

| I think we just need to fix either one of them, fixing this _internal frozen period or fixing the UI.

I understand it's bit misleading without customer having to apply this quirky workaround, but changing the ""Previous 60 Days"" in the ""License Usage"" panel to something very ""dynamic"" might be an overkill if it's something that isn't super critical. The UI code might have to make a smart decision  to know the earliest timeline of either { license usage, retention time of _internal and/or the ET of the license__usage events).

I would leave that to [~accountid:5a1ef9dc40207c40ef9126d7] to respond or an UI engineer to comment on what it really take to make UI self-adjusting...

Also I will leave [~accountid:5a1ef9dc40207c40ef9126d7] and [~accountid:557058:4a62ae54-d811-4a94-abdf-457ade60ee5b] to comment on updating release notes as well as the documentation (for necessary workaround), until we have a final decision from UI point of view.

 

  

 ","14/Apr/21 11:35 AM;5a1ef9dc40207c40ef9126d7;Hi - 

A new story will be created to address a longer term solution to this issue. There was some discussion on potentially pulling it from a summary search (with little to no license capacity impact and disk space) that gets updated once a day, or making the chart ""dynamic"" - which at present I don't know of any feasible solutions given its expected user-value. ","14/Apr/21 12:34 PM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;[~accountid:6053a6c745a3bb006816b393] Please add or discuss the details necessary to implement the workaround in the Workaround field on this ticket. It looks like you'd need to override the defaults in a couple files (default/indexes.conf and app/default/savedsearches.conf) to get the workaround to work. I can review and edit what you place in there. Thank you! 


","21/Apr/21 11:10 AM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;Good day.
[~accountid:5a1ef9dc40207c40ef9126d7] Would you provide your perspective on if you feel this issue needs to be in the Known Issues, and if we need to provide a workaround?
[~accountid:6053a6c745a3bb006816b393] Can you provide the technical details necessary to implement a workaround? 
Thank you both!","21/Apr/21 1:07 PM;6053a6c745a3bb006816b393;[~accountid:557058:4a62ae54-d811-4a94-abdf-457ade60ee5b] et. al.

I see a couple of ways to address this technically.
 # To provide the context necessary per the licensing regime (30 days for non-CLE and 30 days otherwise), the retention period for _internal index has to be changed to 60 days by default. I will be bit reluctant in changing .conf defaults that has been stuck over a long period of time just to the benefit of few (CLE) customers. I don't know how this might mess with user experience, storage and existing toolchain of large customers to whom CLE is not at all a concern, so I would suggest checking with [~accountid:5d0995dcc184b50c22ff1c46] and/or [~accountid:557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e] . Pls note that, licensing regimes can switch anytime from CLE to Non-CLE or otherwise based on license expirations, whereas changes to defaults are more sticky.
 # Change the UI, so it plots the usage report based on the events that are available instead of statically saying 30 days or 60days. That way the tab name “Previous 60 days” under “Licensing » License Usage Reporting” should be modified to something like “Previous X days available data”. If customers wants to have more context, they can increase the retention time for _internal index and they always get X as they desire irrespective of license regime (30 or 60 or anything else). The UI needs to determine the number of days for which it “has” the license_usage events, and appropriately render both the tab name as well as the x-axis of the usage chart. This is a one time cost when the user clicks in ""Usage Report"" or refresh the page. What is the right X can be obtained from tsidx field of the _internal index. Below is a sample query and output on a devcluster machine that has < 1 day worth of data, but you get the idea. 

 
{quote}
|\| tstats max(_time) as latest min(_time) AS earliest WHERE index=_internal|
|\| eval start_time=strftime(earliest,""%m/%d/%y %H:%M:%S"")|
|\| eval end_time=strftime(latest,""%m/%d/%y %H:%M:%S"")|
|\| eval indexed_secs=(latest-earliest)|
|\| eval days=if(indexed_secs < 86400, 1, ceil(indexed_secs/86400))|
|\| table start_time end_time days|
{quote}
 
{quote}start_time                       end_time                           days

04/21/21 08:40:00     04/21/21 12:40:28              1
{quote}
 Hope this helps.","21/Apr/21 2:08 PM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;Looks great, thank you [~accountid:6053a6c745a3bb006816b393]. 
I discussed this ticket with Izzy and we are going to set this as a Known Issue, but not provide a public Workaround due to the requirements to change those defaults and the complexity.

Should support have another case about the issue, the support rep can relay the workaround options to the customer directly and let them choose to implement it if desired.","29/Apr/21 4:42 PM;6053a969695c3900707a9628;[~accountid:557058:4a62ae54-d811-4a94-abdf-457ade60ee5b], [~accountid:6053a68b009fee00694bf279]  - This issue was resolved with resolution = fixed. Can you please specify which release version in the fixVersion field? / [~accountid:5a1ef9dc40207c40ef9126d7]","03/May/21 7:47 AM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;Hello! There's a requirement when using [automation for the Release Notes|https://splunk.atlassian.net/wiki/display/PROD/How+to+add+or+remove+an+issue+from+the+automated+release+notes] that the resolution field is set to ""Fixed"" or ""Delivered."" I'll change the ticket to Delivered as there's no other ticket yet identified to fix the bug. Subsequently, there's no fix version identified yet either.","05/May/21 10:50 AM;6053a969695c3900707a9628;[~accountid:557058:4a62ae54-d811-4a94-abdf-457ade60ee5b] - Just to double check my understanding of this issue. There was no ""fix"" (code change) implemented but there is a workaround ""delivered"" to resolve the issue. And that workaround is what [~accountid:6053a6c745a3bb006816b393] has commented above on Apr 21 @1:07pm. Is this correct? 

 ","05/May/21 1:03 PM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;[~accountid:6053a969695c3900707a9628], yes that summarizes it nicely!","05/May/21 1:12 PM;6053a969695c3900707a9628;Confirmed we're ok to publish on known issues list without a workaround, per Edward's comment above. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,03/May/21 7:48 AM
[PUBLIC] Crash in search process in PrecacheUsersThread when max_searches_per_process is set lower than default,SPL-198284,1799309,Crash,Resolved,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Workaround,Jo Hornsby,557058:f87e2651-5132-4aed-8b96-f608ee969435,Angus Hsu,6053a74b90f288007008eabe,Angus Hsu,6053a74b90f288007008eabe,04/Dec/20 9:31 AM,30/Jul/25 4:26 PM,,17/Oct/22 11:37 AM,10.0.x,8.1.0 GA (Rarity),8.1.1,8.1.2,8.1.3,8.1.4,8.1.5,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Identity - Authentication (AuthN),Search - Core,,,,0,cmpkanban,mr_regression,,,,,,"I found a crash during I was testing 08187535c166

{code}
[build 08187535c166] 2020-12-03 19:52:36
Received fatal signal 6 (Aborted).
 Cause:
   Signal sent by PID 23620 running under UID 2023.
 Crashing thread: PrecacheUsersThread
 Registers:
    RIP:  [0x00007F2F9AFF2387] gsignal + 55 (libc.so.6 + 0x36387)
    RDI:  [0x0000000000005C44]
    RSI:  [0x0000000000005C4D]
    RBP:  [0x00007F2F957FE570]
    RSP:  [0x00007F2F957FE428]
    RAX:  [0x0000000000000000]
    RBX:  [0x000055FE946A1750]
    RCX:  [0xFFFFFFFFFFFFFFFF]
    RDX:  [0x0000000000000006]
    R8:  [0x0000000000000100]
    R9:  [0x00007F2F9AC00080]
    R10:  [0x0000000000000008]
    R11:  [0x0000000000000206]
    R12:  [0x000055FE9398EA10]
    R13:  [0x00007F2F9A433908]
    R14:  [0x00007F2F957FE6F8]
    R15:  [0x00007F2F957FE718]
    EFL:  [0x0000000000000206]
    TRAPNO:  [0x0000000000000000]
    ERR:  [0x0000000000000000]
    CSGSFS:  [0x0000000000000033]
    OLDMASK:  [0x0000000000000000]

 OS: Linux
 Arch: x86-64

 Backtrace (PIC build):
{code}",,Angus Hsu,Gregory Runyon,Jo Hornsby,Joseph Iriogbe,Kiran Shridhar,Kyle Heo,Maarten Hoogcarspel,Manjunath Karikatti,Michal Zax,User known,User known,User known,Wiriadi Wangsa,,,,,,,,,,,,,,,,,,,,,,,6053a74b90f288007008eabe,6036b881185376007024178f,557058:f87e2651-5132-4aed-8b96-f608ee969435,6036b79ed416ea00700722a2,6053a73f4a91ad0068c8310e,6036b7b1ac6e4e0069e92892,5c5a99db92db7b0b85249c1a,6053abab45a3bb006816ea7c,557058:c4b67098-296a-4cb7-809a-9cc75ae39995,unknown,unknown,unknown,6036b7ad4623c60069c040b7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-231587,,,,,,,,,,,,,,,,,,,,SPL-197414,,,,,,,04/Dec/20 9:27 AM;ahsu;crash-2020-12-03-19-52-36.log;https://splunk.atlassian.net/rest/api/3/attachment/content/2060042,04/Dec/20 9:28 AM;ahsu;diag-ip-10-202-9-177.ec2.splunkit.io-2020-12-04_01-13-03.tar.gz;https://splunk.atlassian.net/rest/api/3/attachment/content/2060043,10/Dec/21 12:04 AM;addon_com.servicerocket.jira.salesforce;image001.png;https://splunk.atlassian.net/rest/api/3/attachment/content/3423137,10/Dec/21 12:04 AM;addon_com.servicerocket.jira.salesforce;image001.png;https://splunk.atlassian.net/rest/api/3/attachment/content/3423136,10/Dec/21 12:04 AM;addon_com.servicerocket.jira.salesforce;image001.png;https://splunk.atlassian.net/rest/api/3/attachment/content/3423139,10/Dec/21 12:04 AM;addon_com.servicerocket.jira.salesforce;image001.png;https://splunk.atlassian.net/rest/api/3/attachment/content/3423138,06/Apr/22 8:39 PM;frankl;uploadsvc-69case2943109-04-06-2022-USER-0030b00001yMJSfAAO-20220406_pudc-sps-006_splunkd-6.core.78407.gdb.log;https://splunk.atlassian.net/rest/api/3/attachment/content/4048109,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Domino's Pizza LLC,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3858df81,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,95472000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,false,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Thu Sep 15 13:59:08 UTC 2022,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CMPEntUsability,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,7.0,12.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ahsu(ahsu),frankl(JIRAUSER52506),jhornsby(jhornsby),jiriogbe(jiriogbe),kheo(kheo),ugwozdecki(JIRAUSER50064),mhoogcarspel(mhoogcarspel),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Codefix,,Cust Managed Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Jo Hornsby,557058:f87e2651-5132-4aed-8b96-f608ee969435,,,,,,,,,,,,,,,,,,0|i78pu7:,,,,,,,"Race needs to be fixed, but it's pretty-much inconsequential in a search process that's shutting down.  Fix tracked by SPIN-OFF.",,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Jo Hornsby,557058:f87e2651-5132-4aed-8b96-f608ee969435,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Race of PrecacheUsersThread thread trying to do stuff while search process is shutting down.,,,,Code change,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Telstra Corporation Limited,3166305,012400000005WzCAAU,P3,No,5005a00002HSAmKAAX,,Open,Customer Update,Standard,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2021-04-23 11:18:34.973,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10001_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_36784959395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Set limits.conf back to default, by removing any override of max_searches_per_process.

For example:
<pre>
[search]
max_searches_per_process=1
</pre>
to
<pre>
[search]
</pre>",,,,,,,,,,2021-04-23 11:18:34.973,2021-04-23 11:18:34.973,,,,,,Jo Hornsby,557058:f87e2651-5132-4aed-8b96-f608ee969435,,,,,,"04/Dec/20 11:37 AM;6053a74b90f288007008eabe;I ran a specious search for several times but I didn't get crash with ldap auth enable and max_searches_per_process = 1
{code}
search (action=alert_* earliest_time=-7d index=_audit splunk_server=local (NOT expiration=* OR expiration>1607025155) search sid trigger_time) | stats sum(deleted) AS deleted, list(alert_actions) AS alert_actions, list(severity) AS severity, list(expiration) AS expiration list(ss_name) AS ss_name, list(ss_app) AS ss_app, list(ss_user) AS ss_user list(digest_mode) AS digest_mode, list(triggered_alerts) AS triggered_alerts BY sid, trigger_time | search (NOT deleted=* OR deleted=0) | fields - deleted
{code}","23/Apr/21 4:18 AM;557058:f87e2651-5132-4aed-8b96-f608ee969435;*Update for [SFDC-2207119|https://splunk.my.salesforce.com/5005a00001lXxUO]*

In an effort to get hi-fidelity information out of the [{{coredump}}|https://downloadsvc.splunk.com/download/splunk/04-15-2021/uploadsvc-46case2207119-04-15-2021-USER-0030b00002FqItiAAF-splunk_support_2207119_core_dumpt.tgz], I did the following:
(Note that core gathering was based on the instructions here: [https://splunk.atlassian.net/wiki/display/~williaml/How+to+generate+coredump]; see this [Slack thread|https://splunk.slack.com/archives/C8YPU7BNY/p1617904441255500] for more background.)
# Installed CentOS Linux 7.9.2009 in a VM.
** I picked this version because it was the latest version of CentOS 7, which should match with the customer's OS version fairly well; from the diag (specifically a crash log and KV Store logs) we could see it was RHEL 7.2.
# Downloaded unstripped version of 8.1.3.
# Installed various _debuginfo_ packages, as instructed by GDB when I loaded the {{coredump}}.
# Got copies of some of the shared libraries from the same box that we got the {{coredump}} from---this was based on the following messages from GDB:
{noformat}
warning: .dynamic section for ""/lib64/librt.so.1"" is not at the expected address (wrong library or version mismatch?)
warning: .dynamic section for ""/lib64/libdl.so.2"" is not at the expected address (wrong library or version mismatch?)
warning: .dynamic section for ""/lib64/libm.so.6"" is not at the expected address (wrong library or version mismatch?)
warning: .dynamic section for ""/lib64/libc.so.6"" is not at the expected address (wrong library or version mismatch?)
warning: .dynamic section for ""/lib64/ld-linux-x86-64.so.2"" is not at the expected address (wrong library or version mismatch?)
{noformat}

Et voilà:
{noformat}
[jhornsby@spl-198284 ~]$ ls -l Downloads
total 3408
lrwxrwxrwx. 1 jhornsby jhornsby      79 Apr 23 11:51 ld-linux-x86-64.so.2 -> uploadsvc-37case2207119-04-15-2021-USER-0030b00002FqItiAAF-ld-linux-x86-64.so.2
lrwxrwxrwx. 1 jhornsby jhornsby      68 Apr 23 11:51 libc.so.6 -> uploadsvc-93case2207119-04-15-2021-USER-0030b00002FqItiAAF-libc.so.6
lrwxrwxrwx. 1 jhornsby jhornsby      69 Apr 23 11:50 libdl.so.2 -> uploadsvc-33case2207119-04-15-2021-USER-0030b00002FqItiAAF-libdl.so.2
lrwxrwxrwx. 1 jhornsby jhornsby      68 Apr 23 11:51 libm.so.6 -> uploadsvc-90case2207119-04-15-2021-USER-0030b00002FqItiAAF-libm.so.6
lrwxrwxrwx. 1 jhornsby jhornsby      69 Apr 23 13:36 librt.so.1 -> uploadsvc-97case2207119-04-15-2021-USER-0030b00002FqItiAAF-librt.so.1
-rw-rw-r--. 1 jhornsby jhornsby   19520 Apr 23 10:54 uploadsvc-33case2207119-04-15-2021-USER-0030b00002FqItiAAF-libdl.so.2
-rw-rw-r--. 1 jhornsby jhornsby  164440 Apr 23 10:54 uploadsvc-37case2207119-04-15-2021-USER-0030b00002FqItiAAF-ld-linux-x86-64.so.2
-rw-rw-r--. 1 jhornsby jhornsby 1141560 Apr 23 10:54 uploadsvc-90case2207119-04-15-2021-USER-0030b00002FqItiAAF-libm.so.6
-rw-rw-r--. 1 jhornsby jhornsby 2112384 Apr 23 10:54 uploadsvc-93case2207119-04-15-2021-USER-0030b00002FqItiAAF-libc.so.6
-rw-rw-r--. 1 jhornsby jhornsby   44096 Apr 23 13:35 uploadsvc-97case2207119-04-15-2021-USER-0030b00002FqItiAAF-librt.so.1
[jhornsby@spl-198284 ~]$ gdb splunk/bin/splunkd
GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-120.el7
Copyright (C) 2013 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type ""show copying""
and ""show warranty"" for details.
This GDB was configured as ""x86_64-redhat-linux-gnu"".
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>...
Reading symbols from /home/jhornsby/splunk/bin/splunkd...done.
(gdb) set sysroot /no/such/dir
(gdb) set solib-search-path ~/splunk/lib:~/Downloads:/lib64
(gdb) core ~/ccpp-2021-04-14_sample/coredump 
[New LWP 50581]
[New LWP 50576]
[New LWP 50577]
[New LWP 50582]
[New LWP 50574]
[New LWP 50578]
[New LWP 50580]
[New LWP 50540]
[New LWP 50843]
Missing separate debuginfo for /home/jhornsby/Downloads/uploadsvc-97case2207119-04-15-2021-USER-0030b00002FqItiAAF-librt.so.1
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/1c/d06cbe6869f4c30e418dee0a22b06da53d63fd.debug
Missing separate debuginfo for /home/jhornsby/Downloads/uploadsvc-33case2207119-04-15-2021-USER-0030b00002FqItiAAF-libdl.so.2
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/de/ca756a631284eee020e3ae1cadce9bdfb9914d.debug
Missing separate debuginfo for /home/jhornsby/Downloads/uploadsvc-90case2207119-04-15-2021-USER-0030b00002FqItiAAF-libm.so.6
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/32/d3a284434ce1b95063a0410f363c81e345080d.debug
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib64/libthread_db.so.1"".
Missing separate debuginfo for /home/jhornsby/Downloads/uploadsvc-93case2207119-04-15-2021-USER-0030b00002FqItiAAF-libc.so.6
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/be/868c2a4b7a0234e58f3031e5105b3b8a5b4bc5.debug
Missing separate debuginfo for /home/jhornsby/Downloads/uploadsvc-37case2207119-04-15-2021-USER-0030b00002FqItiAAF-ld-linux-x86-64.so.2
Try: yum --enablerepo='*debug*' install /usr/lib/debug/.build-id/f9/af7ba309f063d3bf9657a21436b4dcac03cf07.debug
Core was generated by `[splunkd pid=28797] se'.
Program terminated with signal 6, Aborted.
#0  0x00007f5fd94f15f7 in raise () from /home/jhornsby/Downloads/uploadsvc-93case2207119-04-15-2021-USER-0030b00002FqItiAAF-libc.so.6
(gdb) bt
#0  0x00007f5fd94f15f7 in raise () from /home/jhornsby/Downloads/uploadsvc-93case2207119-04-15-2021-USER-0030b00002FqItiAAF-libc.so.6
#1  0x00007f5fd94f2e28 in abort () from /home/jhornsby/Downloads/uploadsvc-93case2207119-04-15-2021-USER-0030b00002FqItiAAF-libc.so.6
#2  0x00007f5fdbee0bf0 in thread_fatal_error (funcname=funcname@entry=0x7f5fde0cd05c ""pthread_mutex_lock"", rval=<optimized out>)
    at /opt/splunk/src/util/Thread.cpp:227
#3  0x00007f5fdbee0c76 in checkPtRes (rval=<optimized out>, funcname=0x7f5fde0cd05c ""pthread_mutex_lock"")
    at /opt/splunk/src/util/Thread.cpp:292
#4  checkPtRes (rval=<optimized out>, funcname=0x7f5fde0cd05c ""pthread_mutex_lock"") at /opt/splunk/src/util/Thread.cpp:289
#5  PthreadMutexImpl::lock (this=<optimized out>) at /opt/splunk/src/util/Thread.cpp:624
#6  0x00007f5fdcdd4cb9 in lock (this=0x7f5fd89a1d48) at /opt/splunk/src/util/Mutex.h:203
#7  ScopedMutex (mutex=..., this=<synthetic pointer>) at /opt/splunk/src/util/Mutex.h:203
#8  remove (key=..., this=0x7f5fd89a1cd0) at /opt/splunk/src/util/SimpleCache.h:298
#9  AuthenticationProviderLDAP::cacheUserInfo(UserInfo const*, bool) ()
    at /opt/splunk/src/framework/auth/AuthenticationProviderLDAP.cpp:428
#10 0x00007f5fdcde27e5 in LoadLDAPUsersThread::loadUsersForStrategy(LDAPStrategy const&, AuthenticationProviderLDAP*, unsigned long*) const
    () at /opt/splunk/src/framework/auth/LoadLDAPUsersThread.cpp:281
#11 0x00007f5fdcde3b6b in LoadLDAPUsersThread::_internal_main() () at /opt/splunk/src/framework/auth/LoadLDAPUsersThread.cpp:55
#12 0x00007f5fdc153881 in PrecacheUsersThread::ImplThread::main (this=0x7f5fd89f13c0)
    at /usr/local/packages/gcc-9.1.0/include/c++/9.1.0/bits/unique_ptr.h:357
#13 0x00007f5fdcf00be7 in Thread::callMain (arg=0x7f5fd89f13c0) at /opt/splunk/src/util/Thread.cpp:2639
#14 0x00007f5fd9885dc5 in __nptl_deallocate_tsd () at pthread_create.c:171
#15 0x00007f5fd89f13d8 in ?? ()
#16 0x06669175beb859dd in ?? ()
#17 0x0666871ad8e259dd in ?? ()
#18 0x0000000000000000 in ?? ()
(gdb) 
{noformat}
","23/Apr/21 8:00 AM;557058:f87e2651-5132-4aed-8b96-f608ee969435;[~accountid:6036b79ed416ea00700722a2], I still have more I want to look at here, but one thing I have noticed is that they have {{max_searches_per_process}} set to {{1}}.  Do we know why that is?  That's generally a bad idea these days (there were a few issues with it being set to the default back in the day, but that was years ago now).  If they don't have a good reason, I would recommend that they do not override the default, and I imagine they'll find that the frequency of the crashes is reduced.

I should also note that I expect that the candidate fix for SPL-203946 will eliminate this crash.  But I would like to see if I can repro.
","23/Apr/21 8:30 AM;6036b79ed416ea00700722a2;[~accountid:557058:f87e2651-5132-4aed-8b96-f608ee969435] Thanks Jo. I will ask them the reason they have max_searches_per_process set to 1. I will let them know that we recommend not overriding the default.
- It's just weird that it generates an insane amount of crash logs but the system is not crashing.

I'll keep you posted with their response","18/Oct/21 1:50 AM;5c5a99db92db7b0b85249c1a;Had a SplunkTrust member report the same crash on [public slack,|https://splunk-usergroups.slack.com/archives/CDE623ETD/p1634298485081100] after upgrade to 8.2 (from unknown), stopped after removing max_searches_per_process = 1 too.

{code}
 Backtrace (PIC build):
  [0x00007F3717EAA387] gsignal + 55 (libc.so.6 + 0x36387)
  [0x00007F3717EABA78] abort + 328 (libc.so.6 + 0x37A78)
  [0x000056184ACE73AA] ? (splunkd + 0x12B13AA)
  [0x000056184ACE7430] ? (splunkd + 0x12B1430)
  [0x000056184BBDA219] _ZN26AuthenticationProviderLDAP13cacheUserInfoEPK8UserInfob + 793 (splunkd + 0x21A4219)
  [0x000056184BBE7D45] _ZNK19LoadLDAPUsersThread20loadUsersForStrategyERK12LDAPStrategyP26AuthenticationProviderLDAPPm + 3989 (splunkd + 0x21B1D45)
  [0x000056184BBE90CB] _ZN19LoadLDAPUsersThread14_internal_mainEv + 267 (splunkd + 0x21B30CB)
  [0x000056184AF5A031] _ZN19PrecacheUsersThread10ImplThread4mainEv + 17 (splunkd + 0x1524031)
  [0x000056184BD05F87] _ZN6Thread8callMainEPv + 135 (splunkd + 0x22CFF87)
  [0x00007F3718249EA5] ? (libpthread.so.0 + 0x7EA5)
  [0x00007F3717F729FD] clone + 109 (libc.so.6 + 0xFE9FD)
{code}","03/Jan/22 10:04 PM;6036b7b1ac6e4e0069e92892;Hi [~accountid:557058:f87e2651-5132-4aed-8b96-f608ee969435]  [https://splunk.atlassian.net/browse/SPL-216418|https://splunk.atlassian.net/browse/SPL-216418|smart-link] has been resolved as a dup of this ticket but the customer has default value for max_searches_per_process = 500. 

diag: [https://downloadsvc.splunk.com/download/splunk/12-14-2021/uploadsvc-54case2859378-12-14-2021-USER-0030b00001yMJSfAAO-diag-pudc-sps-002.networks.in.telstra.com.au-2021-12-14_01-36-32.tar.gz|https://downloadsvc.splunk.com/download/splunk/12-14-2021/uploadsvc-54case2859378-12-14-2021-USER-0030b00001yMJSfAAO-diag-pudc-sps-002.networks.in.telstra.com.au-2021-12-14_01-36-32.tar.gz]

As this ticket has been publicized and is included in the release note as a known issue with an workaround, I was wondering whether the workaround(max_searches_per_process = 500) is still valid or any other workaround is available for this symptom?  Thanks.","06/Jan/22 7:46 AM;557058:f87e2651-5132-4aed-8b96-f608ee969435;Hi [~accountid:6036b7b1ac6e4e0069e92892].  I think it could be argued that this issue is different as it’s a crash in a search process where it doesn’t even make sense for this thread to be running.

That said, the race is likely semantically identical in both cases.  I’ll see what I can do to fix it.","06/Apr/22 8:39 PM;6154ccf378e5e400701deb04;Hi [~accountid:557058:f87e2651-5132-4aed-8b96-f608ee969435] ,

Same customer who raise the case with Kyle before has upgrade to 8.2.5. While they still have the max_searches_per_process = 500, one of the SH crashed with the exact same stacktrace during rolling restart.

{noformat}Core was generated by `splunkd --under-systemd --systemd-delegate=yes -p 8089 _internal_launch_under_s'.
Program terminated with signal 6, Aborted.
#0  0x0000154bc84e4387 in raise () from /lib64/libc.so.6
#0  0x0000154bc84e4387 in raise () from /lib64/libc.so.6
#1  0x0000154bc84e5a78 in abort () from /lib64/libc.so.6
#2  0x0000563e42c85478 in ?? ()
#3  0x0000563e42c85536 in ?? ()
#4  0x0000563e43b959c1 in AuthenticationProviderLDAP::cacheUserInfo(UserInfo const*, bool) ()
#5  0x0000563e43ba9005 in LoadLDAPUsersThread::loadUsersForStrategy(LDAPStrategy const&, AuthenticationProviderLDAP*, unsigned long*) const ()
#6  0x0000563e43baa38b in LoadLDAPUsersThread::_internal_main() ()
#7  0x0000563e42f387eb in PrecacheUsersThread::ImplThread::main() ()
#8  0x0000563e43da4d27 in Thread::callMain(void*) ()
#9  0x0000154bc8883ea5 in start_thread () from /lib64/libpthread.so.0
#10 0x0000154bc85ac96d in clone () from /lib64/libc.so.6{noformat}

I see the affected version now has 8.2.5 here and customer has collect core file as well as gdb for us to review. No crash logs actually the SH didn’t generate any crash logs. 

[^uploadsvc-69case2943109-04-06-2022-USER-0030b00001yMJSfAAO-20220406_pudc-sps-006_splunkd-6.core.78407.gdb.log]

Core: [https://downloadsvc.splunk.com/download/support/04-06-2022/uploadsvc-11case2943109-04-06-2022-USER-0030b00001yMJSfAAO-splunkd-6.core.78407.gz|https://downloadsvc.splunk.com/download/support/04-06-2022/uploadsvc-11case2943109-04-06-2022-USER-0030b00001yMJSfAAO-splunkd-6.core.78407.gz]

Diag: [https://downloadsvc.splunk.com/download/support/03-31-2022/uploadsvc-23case2943109-03-31-2022-USER-0030b00001yMJSfAAO-diag-pudc-sps-006.networks.in.telstra.com.au-2022-03-30_22-44-08.tar.gz|https://downloadsvc.splunk.com/download/support/03-31-2022/uploadsvc-23case2943109-03-31-2022-USER-0030b00001yMJSfAAO-diag-pudc-sps-006.networks.in.telstra.com.au-2022-03-30_22-44-08.tar.gz]

Thought this can help you to better analyse this. 

Regards,

Frank","12/Apr/22 4:14 PM;6154ccf378e5e400701deb04;Hi [~accountid:557058:f87e2651-5132-4aed-8b96-f608ee969435] ,

Hope you are well.

Are you able to help us to have a look at the below or would like us to raise a new jira?

Regards,

Frank","13/Apr/22 4:28 AM;557058:f87e2651-5132-4aed-8b96-f608ee969435;Hi [~accountid:6154ccf378e5e400701deb04].  Yeah, let’s go ahead and create a new Jira.  It may well be that a fix will be the same for both issues, but I think it’d be useful to have the Mothership crash logged for posterity, at least.","15/Sep/22 6:47 AM;61325c25ae44ae0070db8b3e;Hi [~accountid:557058:f87e2651-5132-4aed-8b96-f608ee969435] I found this Jira while analysing another crash in the same thread and I have a theory about this crash:

AuthenticationManager owns the domain objects that the {{LoadLDAPUsersThread}} is operating on and the object might be deleted while the thread is still running and processing them.

During the shutdown {{AuthenticationManager}} signals the {{AuthenticationProviderLDAP}} and deletes the objects.

{noformat}bool AuthenticationManager::signalShutdown()
{
    gLogger.info() << ""event=signalShutdown, shutdown auth manager"";

#ifdef WITH_LDAP
    AuthenticationProviderLDAP::signalShutdown();
#endif //WITH_LDAP
#ifdef WITH_SAML
    AuthenticationProviderSAML::signalShutdown();
#endif //WITH_SAML
    shutdownLoadUsers.unregisterHandler(this);

    // SingletonUtil doesn't expose any custom destructor, so we do cleanup here
    // As per Paul Lucas:
    // ""Having your singleton destructor do clean-up is problematic because
    // you have no control over when your destructor will run during shutdown.
    // Resources it might rely on may already have been destroyed.
    // Better is to implement a shutdown handler over which you have control of when stuff is cleaned up.""

    deleteAllDomainObjects();
    return true;
}
{noformat}

{{AuthenticationProviderLDAP::signalShutdow}} is meant to stop the thread:

{noformat}bool AuthenticationProviderLDAP::signalShutdown()
{
    ScopedConditionMutex lock(_mutex);
    if (_ldapThread) {
        _ldapThread->stopThat(true);
        _ldapThread = nullptr;
        return false;
    }
    return true;
}{noformat}

and when the thread is running it is just setting the flag without waiting for thread to finish which leads to a race between deleting the objects and operating on them

{noformat}void LoadLDAPUsersThread::_internal_stopThat(bool isShutdown)
{
    gLogger.debug() << ""LoadLDAPUsersThread: Stopping due to ""
                    << (isShutdown ? ""shutdown"" : ""reload"");
    _shutdown |= isShutdown ? 2 : 1;
}{noformat}

hope this helps, unfortunately my crash seems to be unrelated 😞 ","15/Sep/22 6:59 AM;557058:f87e2651-5132-4aed-8b96-f608ee969435;Hi [~accountid:61325c25ae44ae0070db8b3e].  Cheers.  Yeah, I guess my experience working on [https://splunk.atlassian.net/browse/SPL-197414|https://splunk.atlassian.net/browse/SPL-197414|smart-link] made me reluctant to attempt a new fix, especially as the process must be shutting down and it’s a search process.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,17/Oct/22 11:37 AM
[PUBLIC] [SmartStore_on_GCP] Parallel upload is not supported in gcp-sse-kms encryption mode,SPL-193389,1655388,Bug,Reopened,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,Yuan Xu,6053b72645a3bb0068176b30,TOM HU,5a6692ec2eed6b2d0236cbf5,TOM HU,5a6692ec2eed6b2d0236cbf5,10/Aug/20 8:34 PM,30/Jul/25 4:25 PM,,,10.0.x,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.2008 GA (Fuji),8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,S2,,,,,0,gcp_s2,z0928,,,,,,"With build 8021deff7d66(stack: gcp-spl182068-test) uploading buckets to GCS with encryption gcp-sse-kms got multiple errors.

Steps to reproduce:
 # Configure volume with sse-kms encryption
 # Create index under the volume and ingest data
 # Roll buckets to warm to trigger bucket upload
 # Search for logs about bucket upload, there are many errors

Volume & index configuration used in my test:
{code:java}
[volume:sse-kms-new]
storageType = remote
path = gs://s2testing
remote.gs.project_id = s2-encryption-kms
remote.gs.credential_file = s2-encryption-kms-e3b16995402e.json
remote.gs.encryption = gcp-sse-kms
remote.gs.gcp_kms.locations = us-central1
remote.gs.gcp_kms.key_ring = thu-keyring
remote.gs.gcp_kms.key = thu-key-test

[idx-sse-kms-0810]
remotePath = volume:sse-kms-new/$_index_name
homePath = $SPLUNK_DB/idx-sse-kms-0810/db
coldPath = $SPLUNK_DB/idx-sse-kms-0810/colddb
thawedPath = $SPLUNK_DB/idx-sse-kms-0810/thaweddb
maxGlobalRawDataSizeMB = 100000
maxGlobalDataSizeMB = 0
maxTotalDataSizeMB = 0
frozenTimePeriodInSecs = 764000
{code}
Below query shows errors from the beginning of one bucket uploading:

[index=_internal source=*splunkd.log cache_id=""*idx-sse-kms-0810~5~2721E446-7C19-4D66-9BB3-AEF67BD5398B*""|https://gcp-spl182068-test.stg.splunkcloud.com/en-GB/app/search/search?q=search%20index%3D_internal%20source%3D*splunkd.log%20cache_id%3D%22bid%7Cidx-sse-kms-0810~5~2721E446-7C19-4D66-9BB3-AEF67BD5398B%7C%22&display.page.search.mode=smart&dispatch.sample_ratio=1&workload_pool=standard_perf&earliest=&latest=1597113224.328&display.general.type=events&display.page.search.tab=statistics&display.prefs.events.offset=40&sid=1597115425.1164]

 
08-11-2020 02:33:11.494 +0000 ERROR CacheManager - action=upload, cache_id=""bid|idx-sse-kms-0810~5~2721E446-7C19-4D66-9BB3-AEF67BD5398B|"", status=failed, reason=""HTTP Error 3: Permanent error in ComposeObject: \{\n  ""error"": {\n    ""code"": 400,\n    ""message"": ""Compose operation does not support Cloud KMS keys."",\n    ""errors"": [\n      {\n        ""message"": ""Compose operation does not support Cloud KMS keys."",\n        ""domain"": ""global"",\n        ""reason"": ""invalid""\n      }\n    ]\n  }\n}\n [INVALID_ARGUMENT]"", elapsed_ms=1388
 ",,Bharath Aleti,Stephen Goodman,Sundar Vasan,TOM HU,Yuan Xu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:b1e398c5-b004-4a33-8152-18daa36cda4b,5d0d153711233f0c4ca0aa17,6053a7722f452d006f82906f,5a6692ec2eed6b2d0236cbf5,6053b72645a3bb0068176b30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,0,,0%,3600,0,,Global,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DOCGUILD-6208,SPL-193099,,,,,,,11/Aug/20 11:16 PM;yxu;Screen Shot 2020-08-11 at 11.15.47 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1694911,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@37aaf906,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,157248000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,Tue Sep 29 18:57:30 UTC 2020,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NoahIndexing,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,1.0,12.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,jisenberg(jisenberg),sgoodman(sgoodman),svasan(svasan),thu(thu),yxu(yxu),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i6n3in:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1. Configure volume with sse-kms encryption
2. Create index under the volume and ingest data
3. Roll buckets to warm to trigger bucket upload
4. Search for logs about bucket upload, there are many errors",,,,,,,,,,1.0,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page ""Filing UI Bugs - 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs""?

 *  For a server bug, did you file this JIRA following the confluence page ""Filing Server Bugs -
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs""?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2020-08-11 04:13:09.009,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In the volumes using gcp-sse-kms encryption mode, specify ""remote.gs.upload_chunk_size = 0"" to disable parallel upload.",,,,,,,,,,2020-08-11 04:13:09.009,2020-08-11 04:13:09.009,,,,,,Yuan Xu,6053b72645a3bb0068176b30,,,,,,"10/Aug/20 9:13 PM;6053b72645a3bb0068176b30;The error message is ""*Compose operation does not support Cloud KMS keys*"". This is due to a limitation in GCS. Per [their doc #1 |https://cloud.google.com/storage/docs/encryption/customer-managed-keys#restrictions]:
{quote}You cannot use the JSON API [Compose Object method|https://cloud.google.com/storage/docs/json_api/v1/objects/compose] when one or more of the source objects are encrypted with a customer-managed encryption key.
{quote}
And [their doc #2 |https://cloud.google.com/storage/docs/composite-objects#composing]:
{quote}The compose operation creates a new composite object whose contents are the concatenation of a given sequence of source objects. The source objects all must:
 - Have the same storage class.
 - Be stored in the same Cloud Storage bucket.
 - NOT use customer-managed encryption keys.{quote}
Basically parallel upload does not work in GCP_SSE_KMS mode, i.e. customer-managed encryption key.

The workaround is to set ""remote.gs.upload_chunk_size = 0"" to disable parallel upload in the volumes using GCP_SSE_KMS mode. In the future when GCS adds support, we can re-enable it by removing that line. cc [~accountid:557058:b1e398c5-b004-4a33-8152-18daa36cda4b] [~accountid:6053a7722f452d006f82906f]

We probably need to document it as a known issue [~accountid:5d0d153711233f0c4ca0aa17].
  ","11/Aug/20 12:06 PM;5d0d153711233f0c4ca0aa17;[~accountid:6053b72645a3bb0068176b30]: The way to document this, or any ticket, as a known issue is to:
 # Edit the ""Document as"" field to ""Include in release notes""
 # Add the workaround, if any, to the ""workaround field""
 # Edit the Summary field to be customer-friendly (for example, we don't want to include ""S2"" in the summary field)
 # Edit the ""Affects version"" and ""Fix version"" fields, if necessary

The release notes will automatically pull in any tickets with the ""include in release notes"" marker and add workaround info, if any, for the releases noted by ""Affects version""

Also, I'm not sure that bugs closed as ""workaround"" will get picked up by the system.  I think they might need to stay open for that to happen.",11/Aug/20 1:04 PM;6053b72645a3bb0068176b30;Thanks [~accountid:5d0d153711233f0c4ca0aa17].  Made the suggested changes to the ticket.,"11/Aug/20 2:42 PM;5d0d153711233f0c4ca0aa17;[~accountid:6053b72645a3bb0068176b30]: Also, if this (as it seems) will be a long-term issue when using gcp-sse-kms, I can also document it in the wikidoc topic on GCP encryption.  For that matter, perhaps we should also note it in the spec file. 

However, if this is likely to be fixed in a release or two, then the known issues section of the release note is probably sufficient.","11/Aug/20 2:54 PM;6053b72645a3bb0068176b30;[~accountid:5d0d153711233f0c4ca0aa17] it is a limitation in GCP, not from our side.  I pinged their support engineers in the support channel - they will speak to their PMs and get back to us. ","11/Aug/20 11:16 PM;6053b72645a3bb0068176b30;Per the suggestion of GCP support engineers, filed a feature request on GCP's partner issue tracking website. Asked them to deliver it in 3/6 months.

!Screen Shot 2020-08-11 at 11.15.47 PM.png|thumbnail!","12/Aug/20 9:50 AM;5d0d153711233f0c4ca0aa17;[~accountid:6053b72645a3bb0068176b30]: The workaround states: ""In the volumes using gcp-sse-kms encryption mode, specify ""remote.gs.upload_chunk_size = 0"" to disable parallel upload.""  Is 0 the default for that setting?","12/Aug/20 10:17 AM;6053b72645a3bb0068176b30;[~accountid:5d0d153711233f0c4ca0aa17] default is 32mb.   https://git.splunk.com/projects/SPLCORE/repos/main/browse/cfg/bundles/README/indexes.conf.spec.in#2549
","17/Aug/20 4:57 PM;6053a7722f452d006f82906f;[~accountid:6053b72645a3bb0068176b30] This is the issue that I mentioned early August in slack (and we opened up SPL-193099 at that time).  FYI, I followed up with our google contact via email at that time and he mentioned that they'll be doing planning at the end of the month and will let us know at that time for a date.

They also have a multi-part upload feature coming up that won't have this limitation. No timelines yet for this feature.",17/Aug/20 5:33 PM;6053b72645a3bb0068176b30;SPL-193099  was converted to a test task by [~accountid:5a6692ec2eed6b2d0236cbf5] - that's why he created this ticket to track the issue.  ,24/Sep/20 11:59 AM;557058:2d2a7f1d-c9e1-4f3c-95c5-24ed73d0b69d;SevLevelCleanupJI,29/Sep/20 11:57 AM;557058:2d2a7f1d-c9e1-4f3c-95c5-24ed73d0b69d;SevLevelCleanupJI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1119206,SPL-163043,SmartStore native support for GCP,To Do,11/Aug/20 1:01 PM
"[PUBLIC] [millisecond] Subsecond search - When you update metric.timestampResolution via the UI, it is not updated on the search head index.conf file. This does not affect search functionality. ",SPL-192936,1643399,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,,Cuong Dong,6053a57786b0dd007187fe97,Yuanhang Qin,6053a4ad686bf5007045aa59,Yuanhang Qin,6053a4ad686bf5007045aa59,30/Jul/20 12:10 PM,30/Jul/25 4:24 PM,,,10.0.x,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Metric Store,,,,,0,z0928,,,,,,,"log in cloud stack (indexer cluster) as sc_admin user

create metric index with metric.timestampResolution as second 

verify in index.conf

 
{noformat}
[second-to-milli]
coldPath = $SPLUNK_DB/second-to-milli/colddb
datatype = metric
frozenTimePeriodInSecs = 1
homePath = $SPLUNK_DB/second-to-milli/db
maxGlobalRawDataSizeMB = 10
thawedPath = $SPLUNK_DB/second-to-milli/thaweddb
{noformat}
 

go to edit index page, update the Timestamp Resolution to milliseconds and save 

indexes.conf has updated on all indexers but not updated on the search head

 

 ",,Cuong Dong,Manu Jose,Yuanhang Qin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a57786b0dd007187fe97,5f7671cd837bb80068281d0b,6053a4ad686bf5007045aa59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@674d0a58,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,157248000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Tue Sep 29 18:57:27 UTC 2020,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MetricsSearch,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,cdong(cdong),jisenberg(jisenberg),mjose(mjose),yqin(yqin),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i6lfcn:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2020-08-04 20:28:47.75,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2020-08-04 20:28:47.75,2020-08-04 20:28:47.75,,,,,,,,,,,,,"04/Aug/20 1:28 PM;6053a57786b0dd007187fe97;This affects only SH; indexers are updated correctly (confirmed by [~accountid:6053a4ad686bf5007045aa59]). 

From what I discussed with #cloud-sme (https://splunk.slack.com/archives/C9NQXRT7X/p1596571501385200), SH are not expected to have up-to-date indexes.conf; SHs just send searches to indexers to run.

In sh_indexes_manager.py, the index on SH is intended to be temporary without any data
{noformat}
        # Call the create index endpoint, this is intended to be a small
        # index because no customer data should be sent directly to the
        # searchhead. This is a temporary solution that will be replaced later.
        # See SPL-148905 for more details.
        try:
            self.searchHeadApi.createIndex(...)
{noformat}

[~accountid:5f7671cd837bb80068281d0b]: should this bug be documented as a known issue with editing metric resolution after creating index? ",04/Aug/20 1:48 PM;5f7671cd837bb80068281d0b;[~accountid:6053a57786b0dd007187fe97] Does the SH does anything with the search based on the indexes.conf timestamp resolution ?,"04/Aug/20 6:14 PM;6053a57786b0dd007187fe97;""metric.timestampResolution"" in indexes.conf is only used for indexing. If the indexers are updated correctly, we set TSIDX files' flags to millisecond and searches are handled according to TSIDX files' flags. We don't use ""metric.timestampResolution"" in indexes.conf for search.","17/Aug/20 3:55 PM;6053a4ad686bf5007045aa59;documented as know issue It affects only SH’s temporary index, but indexers are not affected. marked it as a minor issue ",24/Sep/20 12:37 PM;557058:2d2a7f1d-c9e1-4f3c-95c5-24ed73d0b69d;SevLevelCleanupJI,29/Sep/20 11:57 AM;557058:2d2a7f1d-c9e1-4f3c-95c5-24ed73d0b69d;SevLevelCleanupJI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1535009,SPL-188350,[Frontend]Support for sub-second data storage and retrieval ,To Do,30/Jul/20 12:10 PM
[PUBLIC] tsidxWritingLevel and other fields are set empty after updating index in UI,SPL-192792,1640060,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,,,Yuanhang Qin,6053a4ad686bf5007045aa59,Yuanhang Qin,6053a4ad686bf5007045aa59,28/Jul/20 6:30 PM,30/Jul/25 4:26 PM,,,10.0.x,8.0.x,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conf System,REST API,,,,0,,,,,,,,"build with commit: 561bedfc8524

1; create a metric index with Timestamp Resolution = S

in index.conf
{noformat}
[second_metric]
coldPath = $SPLUNK_DB/second_metric/colddb
datatype = metric
enableDataIntegrityControl = 0
enableTsidxReduction = 0
homePath = $SPLUNK_DB/second_metric/db
maxTotalDataSizeMB = 512000
thawedPath = $SPLUNK_DB/second_metric/thaweddb{noformat}
run query: | rest /services/configs/conf-indexes | fields title, tsidxWritingLevel 

the tsidxWritingLevel  of second_metric is 1

 

2, edit index, update Timestamp Resolution to the millisecond 

in index.conf
{noformat}
[second_metric]
coldPath = $SPLUNK_DB/second_metric/colddb
datatype = metric
enableDataIntegrityControl = 0
enableTsidxReduction = 0
homePath = $SPLUNK_DB/second_metric/db
maxTotalDataSizeMB = 512000
thawedPath = $SPLUNK_DB/second_metric/thaweddb
archiver.enableDataArchive = 0
bucketRebuildMemoryHint = 0
compressRawdata = 1
enableOnlineBucketRepair = 1
hotBucketStreaming.deleteHotsAfterRestart = 0
hotBucketStreaming.removeRemoteSlicesOnRoll = 0
hotBucketStreaming.reportStatus = 0
hotBucketStreaming.sendSlices = 0
metric.enableFloatingPointCompression = 1
metric.stubOutRawdataJournal = 1
metric.timestampResolution = ms
minHotIdleSecsBeforeForceRoll = 0
rtRouterQueueSize = 
rtRouterThreads = 
selfStorageThreads = 
suspendHotRollByDeleteQuery = 0
syncMeta = 1
tsidxWritingLevel = 
{noformat}
run query: | rest /services/configs/conf-indexes | fields title, tsidxWritingLevel 

 

the tsidxWritingLevel  of second_metric is empty 

 

please verify if its as design. 

 ",,Andrew Fager,Bharath Aleti,Cuong Dong,Manu Jose,Ray Ranga [X],Yuanhang Qin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5d8538dd519d370d960ddd1c,557058:b1e398c5-b004-4a33-8152-18daa36cda4b,6053a57786b0dd007187fe97,5f7671cd837bb80068281d0b,613259d3b1894f0071943fd9,6053a4ad686bf5007045aa59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@1accd789,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,132451200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Tue Jul 13 16:17:14 UTC 2021,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NoahIndexing,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,afager(afager),baleti(baleti),cdong(cdong),jwescott(jwescott),mjose(mjose),rranga(rranga),yqin(yqin),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i4um8v:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2020-08-04 18:14:05.57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2020-08-04 18:14:05.57,2020-08-04 18:14:05.57,,,,,,,,,,,,,29/Jul/20 1:54 PM;6053a4ad686bf5007045aa59;cc [~accountid:6053ae7a2f452d006f82df6a] ,04/Aug/20 11:14 AM;6053a57786b0dd007187fe97;It seems the UI updates index.conf and leaves quite a few fields blank.,"04/Aug/20 11:08 PM;6053a4ad686bf5007045aa59;[~accountid:6053a57786b0dd007187fe97] reproducible in build 5cdb540af703

and it also happens to the event index, looks like all index type issue, can you help to assign to the correct team?

cc [~accountid:5f7671cd837bb80068281d0b]",05/Aug/20 7:14 AM;5f7671cd837bb80068281d0b;Is this the response from the endpoint trying to edit index configurations? [~accountid:6053a57786b0dd007187fe97] can you double check?,"05/Aug/20 12:47 PM;6053a57786b0dd007187fe97;This is a UI bug. The UI sends API request with the following payload. ""&tsidxWritingLevel="" is in the payload, setting it blank. UI should only send fields that are  changed by UI instead of setting ""tsidxWritingLevel"" and other fields blank

bq. output_mode=json&archiver.enableDataArchive=0&archiver.maxDataArchiveRetentionPeriod=0&bucketRebuildMemoryHint=0&coldPath.maxDataSizeMB=0&coldToFrozenDir=&coldToFrozenScript=&compressRawdata=1&enableDataIntegrityControl=0&enableOnlineBucketRepair=1&enableTsidxReduction=0&fileSystemExecutorWorkers=5&frozenTimePeriodInSecs=188697600&homePath.maxDataSizeMB=0&hotBucketStreaming.deleteHotsAfterRestart=0&hotBucketStreaming.extraBucketBuildingCmdlineArgs=&hotBucketStreaming.removeRemoteSlicesOnRoll=0&hotBucketStreaming.reportStatus=0&hotBucketStreaming.sendSlices=0&hotBucketTimeRefreshInterval=10&journalCompression=gzip&maxBloomBackfillBucketAge=30d&maxConcurrentOptimizes=6&maxDataSize=100&maxGlobalDataSizeMB=0&maxGlobalRawDataSizeMB=0&maxHotBuckets=auto&maxHotIdleSecs=0&maxHotSpanSecs=7776000&maxMemMB=5&maxMetaEntries=1000000&maxTimeUnreplicatedNoAcks=300&maxTimeUnreplicatedWithAcks=60&maxTotalDataSizeMB=512000&maxWarmDBCount=300&metric.compressionBlockSize=1024&metric.enableFloatingPointCompression=1&metric.maxHotBuckets=auto&metric.splitByIndexKeys=&metric.stubOutRawdataJournal=1&metric.tsidxTargetSizeMB=1500&minHotIdleSecsBeforeForceRoll=0&minRawFileSyncSecs=disable&minStreamGroupQueueSize=2000&partialServiceMetaPeriod=0&processTrackerServiceInterval=1&quarantineFutureSecs=2592000&quarantinePastSecs=77760000&rawChunkSizeBytes=131072&repFactor=0&rotatePeriodInSecs=60&rtRouterQueueSize=&rtRouterThreads=&selfStorageThreads=&serviceInactiveIndexesPeriod=60&serviceMetaPeriod=25&splitByIndexKeys=&streamingTargetTsidxSyncPeriodMsec=5000&suspendHotRollByDeleteQuery=0&syncMeta=1&throttleCheckPeriod=15&tsidxTargetSizeMB=1500&tsidxWritingLevel=&waitPeriodInSecsForManifestWrite=60&warmToColdScript=

For comparison, the POST request to create a new index from UI doesn't have this problem. The fields in the payload are the fields in the UI:
bq. output_mode=json&name=test3&datatype=event&homePath=&coldPath=&thawedPath=&maxTotalDataSizeMB=512000&maxDataSize=auto&coldToFrozenDir=&enableDataIntegrityControl=0&enableTsidxReduction=0","05/Aug/20 1:06 PM;6053a57786b0dd007187fe97;[~accountid:5f7671cd837bb80068281d0b]: this is a UI bug that affects both event and metric indexes. What team/area should we assign to? (I found ""Splunk UI"" team and ""End User Experience"" area but I'm not sure if they are the right team)","05/Aug/20 1:16 PM;5f7671cd837bb80068281d0b;According to the doc it should be *Administrator Experience,* CC *[~accountid:613259d3b1894f0071943fd9]* Not sure about the team. ** ",05/Aug/20 2:32 PM;613259d3b1894f0071943fd9;[~accountid:5f7671cd837bb80068281d0b] - is this issue seen on-premise or on EC?,05/Aug/20 2:40 PM;5f7671cd837bb80068281d0b;Both EC and on-premise. ,05/Aug/20 2:42 PM;613259d3b1894f0071943fd9;I am going to lean on [~accountid:5ba3e8119477482ee0fac66d] to provide insights whether this is something his team should look at or someone else.,06/Aug/20 11:06 AM;5ba3e8119477482ee0fac66d;Yes. This is a bug that our team should look at when it gets prioritized. Updated backlog area and mission team accordingly.,"20/May/21 8:58 AM;5d8538dd519d370d960ddd1c;All configuration changes in the UI send 2 calls to perform an update:  One GET request to retrieve the current values and a list of fields that the backend accepts to be changed. The second is a POST request that will send all fields that were retrieved and allowed to be changed. This standard behavior for all configuration endpoints forces the UI to not hold any information regarding what is a default value and what is a previously set value. Because of this, the UI sends all received fields that are marked as ""changeable"" back to the API on every update. When the UI resends the values, the backend assumes those are all being set, even if the value was a default value, and sets every field. 

To illustrate what the UI does with cURL 

{code}
# create index (POST)
curl -k -u admin:pass https://localhost:8089/services/data/indexes -d name=myindex

# pull fields (GET)
curl -k -u admin:changeme https://localhost:8089/services/data/indexes/myindex -G -d output_mode=json
# response truncated
{
  ""required"": [],
  ""optional"": [
    ""archiver.enableDataArchive"",
    ""archiver.maxDataArchiveRetentionPeriod"",
     … other fields   
    ""tsidxWritingLevel"",
    ""waitPeriodInSecsForManifestWrite"",
    ""warmToColdScript""
  ],
  ""wildcard"": []
}
""content"": {
    ""archiver.enableDataArchive"": false,
    ""archiver.maxDataArchiveRetentionPeriod"": 0,
    … other fields
    ""tsidxWritingLevel"": null,
    ""waitPeriodInSecsForManifestWrite"": 60,
    ""warmToColdScript"": """"
  }

{code}

When updating the index (even if no changes are made) the UI will send all fields and values in {{content}} that are allow-listed in the {{optional}} array. For example

{code}
MIME Type: application/x-www-form-urlencoded; charset=UTF-8
output_mode=json
archiver.enableDataArchive=0
archiver.maxDataArchiveRetentionPeriod=0
… 
tsidxTargetSizeMB=1500
tsidxWritingLevel
waitPeriodInSecsForManifestWrite: 60
warmToColdScript
{code}

Any fields without an {{=}} and a right-hand value  can be read as {{=<empty>}}.

In this way, on updates, all ""changeable"" fields are being sent back to the backend with default values in most cases. The API interprets this as set values and writes out to the indexes.conf file. All subsequent calls no longer use the default values.

Furthermore, there is no way for the UI to 'remove' a field once it has been set because the API does not support this operation. Fields can only be set to 'null' (empty) from the API, not removed.

We recommend that the backend handle setting unset values the same as default values. The UI can't know which fields have been previously set and which are default values. If the API sends null (like for tsidxWritingLevel), null will be sent back. It is the responsibility of the backend to handle these values appropriately.

The UI is working as designed. If the issue is that the backend sends {{tsidxWritingLevel = 2}} when {tsidxWritingLevel}} is unset, then it should do the same for {{tsidxWritingLevel = <empty>}}, or it send the default value of 2 in the original request so the UI sends it back.
","01/Jul/21 8:00 AM;557058:b1e398c5-b004-4a33-8152-18daa36cda4b;Isn't it sub-optimal to update ALL settings when the end user is making a single/isolated change.  

[~accountid:5f7671cd837bb80068281d0b]/[~accountid:5ba3e8119477482ee0fac66d] What are the next steps to resolve this ""known"" issue.","01/Jul/21 8:09 AM;5ba3e8119477482ee0fac66d;[~accountid:5d8538dd519d370d960ddd1c] thanks for the detailed explanation.

Is there no way for the UI to detect _which_ values have changed and only send back the ""dirty"" fields rather than re-POSTing the entire payload?","09/Jul/21 7:56 AM;5d8538dd519d370d960ddd1c;[~accountid:5ba3e8119477482ee0fac66d],  there are ways to do it, but it would be a fundamental shift in how the frontend handles all configuration changes. The way you could do it would be to have the frontend maintain the same list of defaults that the backend does (and put mechanisms in place to ensure they don't get out of sync), and use that list instead of the GET to filter to the dirty fields. In my mind that would be untenable given the volume of configuration pages affected and the extensibility of the platform. 
 The solution we suggested of having the backend filter the defaults from the save set seemed like a smaller lift to fix this issue.","13/Jul/21 9:17 AM;5ba3e8119477482ee0fac66d;[~accountid:5d8538dd519d370d960ddd1c] – sorry, I'm not understanding how defaults play into this. Your comment above says ""The second is a POST request that will send all fields that were retrieved and allowed to be changed.""

What I'm suggesting is that we _don't_ send ""all fields that were retrieved"" but just fields that we changed.

How does this relate to defaults?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/May/21 9:01 AM
[PUBLIC] The .deb installation package will fail if dpkg version doesn't support an .xz compressed control file.,SPL-191850,1618553,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Workaround,Edward Kostowski,557058:4a62ae54-d811-4a94-abdf-457ade60ee5b,Edward Kostowski,557058:4a62ae54-d811-4a94-abdf-457ade60ee5b,Edward Kostowski,557058:4a62ae54-d811-4a94-abdf-457ade60ee5b,10/Jul/20 1:08 PM,30/Jul/25 4:26 PM,,15/Jul/20 12:53 PM,10.0.x,8.0.10,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Installer,Upgrade,,,,0,,,,,,,,Issue was triaged and closed in ticket https://splunk.atlassian.net/browse/FAST-15548. This ticket is for the Known Issues page.,Splunk .deb installer package 8.0.5.,Andrew Brown,Edward Kostowski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,557058:4a62ae54-d811-4a94-abdf-457ade60ee5b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,FAST-15548,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@30bbbe45,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,157334400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Tue Sep 29 00:29:07 UTC 2020,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),ekostowski(ekostowski),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Eng Effectiveness,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Edward Kostowski,557058:4a62ae54-d811-4a94-abdf-457ade60ee5b,,,,,,,,,,,,,,,,,,0|i6hqvr:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,0.0,,,," *  For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

 *  For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

 *  Provide  splunk diags and accompanying data (pstacks, hardump, tcpdump, crashlog, strace, etc..) within the same timeframe. Can refer to this page for details on how to collect accompanying data https://confluence.splunk.com/display/SUP/Advanced+Troubleshooting+-+pstack+and+dumps+collection

 *  In your own words, describe the area affected based on your analysis.  Affected area could be splunk functionality you suspect or issues related to memory, performance or authentication etc.

 *  Provide evidence of the attempted steps/config tuning tried at customer env to resolve the issue 


 * What is the Support Request/Questions on the resolution of this issue? eg. is RCA needed, config/tuning recommendation needed.
",,,,,,,,,,,,,,,,,,,2020-07-15 18:00:48.26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4_*:*_1_*:*_16549_*|*_5_*:*_1_*:*_430368628_*|*_6_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_745676,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Update dpkg to version 1.17.6 or later.,,,,,,,,,,2020-07-15 18:00:48.26,2020-07-15 18:00:48.26,,,,,,Edward Kostowski,557058:4a62ae54-d811-4a94-abdf-457ade60ee5b,,,,,,"10/Jul/20 1:25 PM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;[https://metadata.ftp-master.debian.org/changelogs/main/d/dpkg/stable_changelog]

dpkg (1.17.6)

* Add support for .deb archives with a control member not compressed
 (control.tar) or compressed with xz (control.tar.xz).","15/Jul/20 11:00 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;This is now listed as a known issue for 8.0.5, but it's resolved as fixed with no fix version. It'll get picked up in filters that also make a known issue for all future 8.0.x releases and also Rarity and beyond. Is that the intent, or does this problem go away at some point?","15/Jul/20 12:26 PM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;Reference: dpkg is a linux distro-level tool. We can choose to drop the issue and workaround on the next major release, and I’m ok with that.

But, the issue with old versions of dpkg and compressed control files will apply to all releases going forward. The workaround chosen wasn't to change Splunk’s build process, but instead to have the customer update dpkg. 
","28/Sep/20 5:29 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:6053a969695c3900707a9628], confirming that this issue shows up in the 8.1.0 (Rarity) known issues list. (As well as known issues lists for 8.0.5 and 8.0.6, which are also listed in the affects versions list.)

For Rarity see [https://docs.splunk.com/Documentation/Splunk/8.1.0/ReleaseNotes/Knownissues#Upgrade_issues|https://docs.splunk.com/Documentation/Splunk/8.1.0/ReleaseNotes/Knownissues#Upgrade_issues] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,15/Jul/20 12:53 PM
[PUBLIC] Users are able to create/clone knowledge objects into apps where they lack permissions,SPL-186365,1521156,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,Farzad Salim,6053b249f180c300675f09a3,Luiz Perini,6036b8046bc3f300699cec92,Luiz Perini,6036b8046bc3f300699cec92,14/Apr/20 12:41 PM,30/Jul/25 4:25 PM,,,10.0.x,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Manager Pages - Misc,Security,,,,0,SUSTAINING_FRONTEND,z0928,,,,,,"h1. Updated 17 April 2020

Scope of this bug fix: 

The following Knowledge Objects list destination apps for which the user does not have permission on create and/or on clone: 
 * Field Transformation
 * Eventtype
 * Lookup Definition,
 * Macros,
 * UI, 
 * Field Extraction,
 * Lookup Table

Expected behaviour: The user should only see the new app in destination apps list. 

See below for more details. 

---------------------------------

 
h1. Description

1) Setting the permissions of the apps is not reflecting in the Cloning page for the knowledge objects. If you limit the permission for the apps, we still can see the Apps when cloning objects for certain knowledge objects.

2) The Cloning page for the Knowledge Objects are not consistency, we see different apps being listed in the Cloning pages, as below.

3) The issue was firstly reported in SPL-159600 and SPL-186365, but the JIRAs did not cover all lack of consistency and permission for all KO.
h1. Search, Report and Alerts

1) and Go to Settings > Create a New Report into the new_app and save it.  In settings > Search, Report and Alerts, in the new_app, new_report click on Clone and the App dropbox will show only the new_app, which is correct

!image-2020-04-14-21-31-31-461.png|width=530,height=276!  

Expected: This is correct, since the user is only able to see the new app in the dropdown. 
h1. Data Models

1) Go to Settings > Create a new Datamodel and In Settings > Datamodels, check the App dropdown 

!image-2020-04-14-21-35-52-534.png|width=384,height=331!

Expected: This is correct, since the user is only able to see the new app in the dropdown. 
h1. EventType

1) Create an EventType and clone the eventype, check the destination app

!image-2020-04-14-21-38-56-397.png|width=584,height=350!

Expected: This is not correct, the user should only be able to see the new app, and not the additional apps for which the user does not have read or write access. 
h1. Tag

1) Create a new tag and try to clone, there is no option to Destination app

!image-2020-04-14-21-45-10-991.png!

Expected: user should be able to select a Destination App at the time of tag creation and tag cloning. In both cases, the user should only see the new app in the dropdown. 
h1. Fields Aliases, Calculated Fields, Transformation, Sourcetype Renaming, Workflow Actions

1) Create a the knowledge object and clone it

!image-2020-04-14-21-52-44-634.png|width=658,height=460!

Expected: Both when creating and cloning a field transformation, the user should only see the new app in the dropdown list. Instead, the user sees apps for which it does not have read/write access. 
h1. Field Extraction

1) Create Field Extraction, there is no option to clone it

!image-2020-04-14-21-56-25-693.png|width=624,height=220!

Expected: The user should only see the new app when creating a field extraction. There is no clone option for field extraction, which is inconsistent with other knowledge objects. 
h1. Lookup Table

1) create a lookup table, not able to clone it. No option to clone the lookup table

!image-2020-04-14-21-59-23-739.png|width=549,height=170!

Expected: The user should only see the new app when creating a lookup table. There is no clone option for lookup table, which is inconsistent with other knowledge objects. 
h1. Lookup Definition and Automatic Lookup

1) Create the lookup definition, clone the lookup definition and check the Destination app

!image-2020-04-14-22-00-50-016.png|width=538,height=299!

Expected: Both when creating and cloning a lookup definition, the user should only see the new app in the dropdown list. Instead, the user sees apps for which it does not have read/write access. 
h1. User Interface and Macros

1) Create a new UI  or a Macro and try to clone it

!image-2020-04-14-22-04-55-099.png|width=462,height=405!

 

Expected: Both when creating and cloning macros, the user should only see the new app in the dropdown list. Instead, the user sees apps for which it does not have read/write access. 

 ",,Alonso Montero,Andrew Brown,Andy Rowsell,Kelly Snyder,Kriti Ashok,Laura Leaverton,Luiz Perini,Maarten Hoogcarspel,Marbeli Gonzalez Portillo,Mark Worden,Muni Tripathi,Sangamesh Sajjan (C),Sophia Xu,Thomas Chimento,User known,User known,User known,User known,User known,Victor Ebken,,,,,,,,,,,,,,,,6036b8946bc3f300699cf2cd,557058:37b4518c-e757-440f-87b7-181f5f425e80,6036b7eb783a4600687c4bdd,6036b88f18537600702417ec,6053a53945a3bb006816a1a7,5ed7ce93f2f6dd0a9bf81823,6036b8046bc3f300699cec92,5c5a99db92db7b0b85249c1a,5cc99645f345850e7271204f,6053a5de695c3900707a6eca,6053b95681b82500685dde4d,712020:a7dc656d-cb65-46b4-96c8-a96181de09ef,5d38c1ff0f478b0c1b8e46f2,5a7a10adbc5c58611529e15a,unknown,unknown,unknown,unknown,unknown,557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-186544,,,,,,,,29/Apr/25 11:19 PM;558a7a5c-7e81-44e2-80a0-37e008996c4e;ba2e05c4-1bdc-4425-9879-0486cc9d665a.png;https://splunk.atlassian.net/rest/api/3/attachment/content/8121081,14/Apr/20 1:31 PM;lperini;image-2020-04-14-21-31-31-461.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1424818,14/Apr/20 1:35 PM;lperini;image-2020-04-14-21-35-52-534.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1424819,14/Apr/20 1:38 PM;lperini;image-2020-04-14-21-38-56-397.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1425220,14/Apr/20 1:45 PM;lperini;image-2020-04-14-21-45-10-991.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1425222,14/Apr/20 1:52 PM;lperini;image-2020-04-14-21-52-44-634.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1425224,14/Apr/20 1:56 PM;lperini;image-2020-04-14-21-56-25-693.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1425225,14/Apr/20 1:59 PM;lperini;image-2020-04-14-21-59-23-739.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1425228,14/Apr/20 2:00 PM;lperini;image-2020-04-14-22-00-50-016.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1425231,14/Apr/20 2:03 PM;lperini;image-2020-04-14-22-03-49-657.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1425233,14/Apr/20 2:04 PM;lperini;image-2020-04-14-22-04-55-099.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1425234,14/Apr/20 2:05 PM;lperini;image-2020-04-14-22-05-47-780.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1425235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,British Broadcasting Corporation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@29adf512,,,,,,,,,,,,,,,,,,,,,,,,,,,11.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Functional,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"o Have they made any changes to their environment just before the issue started?
No change made before the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,42.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,62294400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"o What is the customer environment for the issue? 
Splunk 8.0.1 - SHC and Indexer Cluster. I did not collect all the details of the topology, as i could reproduce the issue win a single standalone instance.


o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,"o What errors are being reported?
THere is a lack of consistency when cloning knowledge objects. Differentss Destination Apps are shown when cloning KO",false,,,,,,,,,,,,No,,,,,,,,,,,,,"o What is the expectation when this problem is not there?
The destination app should be consistency and the changes made in the App permissions should reflect the destination apps.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• How long has the customer been tracking this issue and how frequent this issue is being hit?
For more than 1 year, there were another Jira in the past and did not address the problem - SPL-174778",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Wed Oct 04 05:36:11 UTC 2023,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Core Security,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,12.0,46.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),arowsell(arowsell),bohanlon(bo'hanlon),coreyf(coreyf),fsalim(JIRAUSER47423),garias(garias),jilama(JIRAUSER47359),jisenberg(jisenberg),lleaverton(lleaverton),lperini(lperini),mhoogcarspel(mhoogcarspel),marbelig(marbelig),mworden(mworden),mcosio(JIRAUSER45844),mtripathi(mtripathi),pchang(pchang),aef4deb5-e054-4c0e-b5c7-40ac2d7722a8(aef4deb5-e054-4c0e-b5c7-40ac2d7722a8),sophiax(sophiax),tchimento(tchimento),yzeng(yzeng),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Security Services,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i10j0n:,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Corey Ferguson,5ecd736e9286180c304ce82c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Closed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1) orca create 
2) log as administrator
3) create a user and assign ONLY user role (Settings > User > New User)
4) create an new app - new_app (Apps > Manage Apps > Create App)
5) update permissions of the new_app, set Read to Power role and Read/Write to User.
6) update permissions of the 
* splunk_internal_metrics, 
* alert_log_event, 
* splunk_archiver, 
* splunk_enterprise_on_docker, 
* splunk_httpinput, 
* alert_webhook 
- set Read to Power and Write to Power
6) Open a new window browser and log with the new user

After prepare the environment, check the JIRA description

",,,,,,,,,,,,,,,,,,,,,3.0,,,,"• For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

• For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

• Describe the steps taken to perform analysis and provide evidence to support the problem statement
. If available, provide the events that match the timeline with supporting logs.

• Provide the logs and accompanying data with reference to the confluence page Filing UI/Server Bugs, and all the relevant documentation based on your analysis to support your observation and theories.

• In your own words, describe the area affected based on your analysis.  Affected area could be component or issues related to memory, performance or authentication etc. Please specify.

Steps to Reproduce:
.If the issue is reproducible,provide the steps.
Is this issue reproducible on latest maintenance version or latest release?

. If available, will customer upgrade to fixed version?
If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,2020-04-15 09:39:24.064,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,23.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2020-04-15 09:39:24.064,2020-04-15 09:39:24.064,,,,,,Corey Ferguson,5ecd736e9286180c304ce82c,,,,,,"15/Apr/20 2:39 AM;6036b807f6ccc0007044b3fd;Traffic cop, all is correct",15/Apr/20 11:24 AM;5ed7ce93f2f6dd0a9bf81823;assigning for triage,"15/Apr/20 2:32 PM;5d38c1ff0f478b0c1b8e46f2;Hi [~accountid:6036b8046bc3f300699cec92], I am in the process of trying to reproducing this issue. Could you confirm that under ""Search, Report and Alerts"", Step 1, that the user with the 'user' role should be creating the Report? I went through steps 1-6 under ""Steps to reproduce"" and logged in as the 'user' role user. However I was not able to create a Report. 

I am using an orca instance, version 8.0.2.1","15/Apr/20 3:26 PM;6036b8046bc3f300699cec92;[~accountid:5d38c1ff0f478b0c1b8e46f2] sorry, just an small correction: In the  update permissions of the new_app, set Read to Power and User roles and Write to User. I have updated the reproduction steps.","15/Apr/20 4:19 PM;5d38c1ff0f478b0c1b8e46f2;Thanks [~accountid:6036b8046bc3f300699cec92], I was able to create the knowledge objects (KO)after changing the user's permissions. I added some ""expected"" comments after each use case. Let me know if they are valid. 

Looks like this item has 4 main categories
 * KOs that list correct apps when creating and cloning - Reports, Data Models
 * KOs that don't allow you to select a Destination App when creating or cloning - Tags 
 * KOs that list the incorrect apps (lists apps for which the user does not have permissions) on create and clone - Field Transformation, Lookup Definition, Macros, UI
 * KOs that list the incorrect apps on create and do not offer a clone option - Field Extraction, Lookup Table

The desired behaviour for all KOs is that of Data Models and Reports:
 * option to select the Destination App on create and clone 
 * the listed Apps conform to the user permissions 
 * the option to clone exists 

 ",15/Apr/20 4:28 PM;5ed7ce93f2f6dd0a9bf81823;[~accountid:5a7a10adbc5c58611529e15a] this customer issue looks like an enhancement request.  Do you agree this would be Core Sec team?  We can convert to a Story for your prioritization.,"16/Apr/20 12:55 AM;6036b8046bc3f300699cec92;Thanks [~accountid:5d38c1ff0f478b0c1b8e46f2] and [~accountid:5ed7ce93f2f6dd0a9bf81823].
Just wanted to add an comment: The behaviour is impacting the customer at this moment, as the users are still able to clone KO. They understand there is failure in the security, due to the fact that you can still clone knowledge objects to an app that you were not supposed to have access, even if the KO cloned is created as private (which is the reproduced case).","16/Apr/20 4:17 PM;5d38c1ff0f478b0c1b8e46f2;Thanks for the feedback [~accountid:6036b8046bc3f300699cec92]. Would it be fair to split this into a bug and a feature request? I propose the following: 
 * restrict this bug to fixing the permissions issues only
 ** KOs that list the incorrect apps on create and/or on clone: Field Transformation, Lookup Definition, Macros, UI, Field Extraction, Lookup Table

 
 * Create a separate feature enhancement for:
 ** adding destination app to KOs that don't have a list option (eg. Tags)
 ** adding clone option for KOs that don't have a clone option (eg. Field Extraction, Lookup Table)

cc: [~accountid:5ed7ce93f2f6dd0a9bf81823], [~accountid:5a7a10adbc5c58611529e15a]","16/Apr/20 4:29 PM;5ed7ce93f2f6dd0a9bf81823;Thanks [~accountid:5d38c1ff0f478b0c1b8e46f2], that sounds like a good plan.","17/Apr/20 12:22 AM;6036b8046bc3f300699cec92;Thanks [~accountid:5d38c1ff0f478b0c1b8e46f2] and [~accountid:5ed7ce93f2f6dd0a9bf81823]. Just to set the expectations with the customer, I understand the EH will be reviewed by Splunk PM, so there is no visibility when it will take place. What about the Bug, is there anything that I can share with the customer, please?","17/Apr/20 10:50 AM;5d38c1ff0f478b0c1b8e46f2;Hi [~accountid:6036b8046bc3f300699cec92], I modified the bug description to reduce the scope of this issue. 

This bug has now been triaged and is in the pipeline to be worked on by a developer. I wanted to confirm that the customer has an on prem deployment and they are currently on 8.0.x? 

 

I've created a story for Core Security on the feature enhancements: https://splunk.atlassian.net/browse/SPL-186544

cc: [~accountid:5a7a10adbc5c58611529e15a], [~accountid:5ed7ce93f2f6dd0a9bf81823]",21/Apr/20 10:28 PM;5e1cde1de7319c0caaefb75d;Bulk changed priority from medium to unknown. For more information please refer to PT-1721.,"22/Apr/20 1:22 AM;6036b8046bc3f300699cec92;Hi [~accountid:5d38c1ff0f478b0c1b8e46f2], yes the customer is On Prem and currently on version 8.0.x.

Please let me know Hello, the next steps for the Jira, please? Is there anything that I need collect from the customer environment?","23/Apr/20 11:20 AM;5d38c1ff0f478b0c1b8e46f2;Hi [~accountid:6036b8046bc3f300699cec92], there is nothing else needed from the customer at this time. I am currently looking into potential fixes and will update you of any upcoming patches once it becomes available. ","28/Apr/20 11:21 PM;6036b8046bc3f300699cec92;[~accountid:5d38c1ff0f478b0c1b8e46f2] [~accountid:6053a698311e270068e36404] Hello, I need to set some expectations about this bug with the customer. I know it is difficult to precise when a fix will be released for that, but could you give us some idea when the fix will be implemented for that issue? Which release or version roughly? Thank you in advance",29/Apr/20 12:16 PM;6053a698311e270068e36404;8.0.4 is cut off. This fix might go in 8.0.5. We don't have a date right now.,"29/Apr/20 12:17 PM;6036b8046bc3f300699cec92;Thanks for the information, I am sharing that with the customer","13/May/20 4:46 PM;5ecd736e9286180c304ce82c;There are two parts to this fix:

1) Fix the API’s such that that properly throw errors when this is attempted (the root security issue)
2) Fix the frontend experience such that these app’s aren’t suggested as clone destinations (an experience issue)

I will go ahead and start on the fix for #2. [~accountid:6053a5de695c3900707a6eca] will help find a resource for #1.","14/May/20 2:11 PM;5ecd736e9286180c304ce82c;Working with [~accountid:6053b95681b82500685dde4d] to help with backend components.

I created three subtasks to divide up the work.","29/May/20 2:09 PM;6053b95681b82500685dde4d;Discussed with [~accountid:5ecd736e9286180c304ce82c]. The core issue from backend perspective is that KOs can be created by a user for an app for which it does not have explicit permission.

Looked at saved searches and event types, it is explicitly allowed to be created by any user without having the capabilities. Implementation indicates that it is by design. I'll be further investigating the reason for this choice.",09/Jul/20 2:08 PM;5ed7ce93f2f6dd0a9bf81823;[~accountid:6053b95681b82500685dde4d] any further progress with this?,"09/Jul/20 4:07 PM;6053b95681b82500685dde4d;Existing backend behavior is as designed. It requires further research whether to change the behavior keeping backward compatibility in mind. For some of the objects UI is using filters to get expected behavior. It is worth exploring if similar filters can be applied to other objects.

In any case, it looks like a significant effort to change backend behavior. Reassigned to [~accountid:6053a5de695c3900707a6eca] to route it appropriately.",10/Jul/20 9:59 AM;6053a5de695c3900707a6eca;Assigning to [~accountid:5a7a10adbc5c58611529e15a] for prioritization.,06/Aug/20 7:50 AM;5a7a10adbc5c58611529e15a;The customers listed in this ticket have a status of Closed/Resolved. This is not a security issue. As a feature enhancement this will need to be submitted to ideas.splunk.com where we can see if there is sufficient customer interest to prioritize it above the list of other requested changes. ,"06/Aug/20 8:39 AM;6036b8046bc3f300699cec92;Hi [~accountid:5a7a10adbc5c58611529e15a], thanks for the update, I would like to share what we found during the investigation of this case. 
if a user is intentionally configured to has a limited read/write permission up to a level where the app should be blocked but still, this same user has power enough to clone objects for this certain app, it does not sound like a enhancement request, there is a lack of consistency in the security restrictions, where the user was set to not see the target app, but he still can copy things to it, regardless harmless it might sound. The customer opened the case, as they are experiencing this lack of security from their environment, As you reviewed the case, he could cleared explain and demonstrate it as a bug during the sessions he had with Support Team. The case is marked as status resolved, because we opened the Bug for the issue. I also provided the steps to reproduce the bug. Please let me know if you need anything else, or more information about it. 
thank you.
CC:[~accountid:5c5a99db92db7b0b85249c1a]",29/Sep/20 10:46 AM;557058:2d2a7f1d-c9e1-4f3c-95c5-24ed73d0b69d;SevLevelCleanupJI,"14/Oct/20 1:44 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:5a7a10adbc5c58611529e15a] we've been including this in the public Known Issues list for 8.0.x versions. Should we continue to do that and treat this as a bug? if so, should we also carry this forward into the 8.1.x versions as a public known issue? ","09/Nov/21 4:52 AM;6036b7eb783a4600687c4bdd;Hi [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80]/ [~accountid:6053b249f180c300675f09a3] we still have customer's encountering this issue. I was wondering where we are with this and whether it is going to be fixed?

CC:[~accountid:60373ae52de69b006a1c6b4e]

 ","18/Nov/21 1:23 AM;60373ae52de69b006a1c6b4e;Hello [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80]  and [~accountid:6053b249f180c300675f09a3], any news regarding this issue? It is still present even on new Splunk versions. Thanks,","06/Dec/21 10:03 AM;6053b249f180c300675f09a3;[~accountid:611be770be6c4300703f3c7e] FYI: Can we have Oncall review and determine the effort/complexity. 

[~accountid:60373ae52de69b006a1c6b4e] [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] Can you let us know if this is currently an escalation from the customer and how many customers are being impacted. 

Reading through the comments, it looks like to have been reviewed by the team and considered to be a feature request and not a bug fix. Our team is currently in the midst of resolving Sec_critical and Sec_high vulnerabilities for 9.0. Unless this can be addressed using on_call resources we’ll have to put in in our backlog and revisit when other high priority security vulnerabilities are resolved. 



[~accountid:6053a74994d7b90069f8435f] [~accountid:6053b95681b82500685dde4d] fyi",06/Dec/21 10:11 AM;6053b95681b82500685dde4d;This is a feature request. This falls into the domain of app management and should be routed accordingly to the right team.,22/Dec/21 5:47 AM;5cc99645f345850e7271204f;Is there an expected ETA for the resolution of this issue or providing this capability in a new release? We had another request from a customer on this.,"22/Dec/21 6:06 AM;557058:9161deac-4565-42f2-8b02-d9d6e8a88621;[~accountid:6053b95681b82500685dde4d] can you help us (support and PSE team) understand the rational for considering this a feature enhancement versus a security vulnerability?

Do we have an expected ETA for resolution? Or even a Date-for-a-date?","22/Dec/21 11:45 AM;6053b95681b82500685dde4d;[~accountid:557058:9161deac-4565-42f2-8b02-d9d6e8a88621] This is a security vulnerability that requires feature work. In other words, the access control is not enforced properly for KOs is a feature gap. In the description there are 9+ different categories of KOs and each needs to be looked at for proper access control.","27/Dec/21 2:51 AM;557058:9161deac-4565-42f2-8b02-d9d6e8a88621;Thanks [~accountid:6053b95681b82500685dde4d] . That doesn’t answer my question unfortunately. Let me rephrase as it seems like I am being unclear.

I am trying to understand the *method by which you differentiate bug fixing versus feature work*. I know different teams have different [definitions of done|https://splunk.atlassian.net/wiki/spaces/PROD/pages/313614222325/Definition+of+Done], and think about sustaining versus feature enhancement differently. I work in DevOps Markets, and I am just covering for [~accountid:6108d946fc68c10069d91dcf] , so the definitions of done may be different to the ones you use.

The support definition would consider this a bug fix versus a feature enhancement because this relates to the expected and desired behaviour of existing functionality. We might consider this feature enhancement work is this was an extension to an existing feature, or a net-new capability. Does that make sense?



Can you give us an idea of when this work could be deliverable? Even a date for a date would be helpful?
e.g. an ETA on a spike to scope the work required.",04/Jan/22 10:37 AM;6053b95681b82500685dde4d;[~accountid:557058:9161deac-4565-42f2-8b02-d9d6e8a88621] I don’t have much to add here. cc [~accountid:611be770be6c4300703f3c7e] [~accountid:6053b249f180c300675f09a3] for prioritization and follow up. ,"04/Jan/22 10:52 AM;6053b249f180c300675f09a3;[~accountid:557058:9161deac-4565-42f2-8b02-d9d6e8a88621] This is a valid issue and we’ll need to address it. However, the team is currently in the midst of resolving Sec_critical and Sec_high vulnerabilities for 9.0. Given the fix is involved, we’ll have to put in in our backlog and revisit when current high priority security vulnerabilities are resolved. I expect it to be reviewed in Q2. ","10/Jan/22 6:27 AM;557058:9161deac-4565-42f2-8b02-d9d6e8a88621;Copy that [~accountid:6053b249f180c300675f09a3] . Thanks for the update. Will speak again in Q2.
Just to confirm, currently you are categorising this as sec_med?","11/Jan/22 2:53 AM;60373ae52de69b006a1c6b4e;Hi team, is it also possible to include it on the latest release notes to share with the customer?","11/Jan/22 5:03 AM;5c5a99db92db7b0b85249c1a;[~accountid:60373ae52de69b006a1c6b4e] It’s already in the release notes, only one that I saw missing from the Affects versions was 8.2.4 so I added that
feel free to add other affected version that were missed 

[https://splunk.atlassian.net/wiki/spaces/PROD/pages/313635214470/How+to+use+Release+Notes+Automation|https://splunk.atlassian.net/wiki/spaces/PROD/pages/313635214470/How+to+use+Release+Notes+Automation|smart-link] ","15/Mar/22 5:31 AM;557058:9161deac-4565-42f2-8b02-d9d6e8a88621;Hi [~accountid:6053b249f180c300675f09a3] , I am following up on this like we agreed in Jan. Is there any update please?","29/Mar/22 5:24 AM;557058:9161deac-4565-42f2-8b02-d9d6e8a88621;[~accountid:6053b249f180c300675f09a3] Sorry to be chasing on this. Do we have a date, or a date for a date please?",28/Jul/22 3:27 PM;6053b249f180c300675f09a3;[~accountid:6046a61aaf9849007236b939] can you please review and let me know if this is already covered in Identity roadmap? Thanks,"29/Jul/22 2:01 PM;6046a61aaf9849007236b939;Hi [~accountid:6053b249f180c300675f09a3] , no, i wasnt aware of the issue. TY for the ping

[~accountid:557058:9161deac-4565-42f2-8b02-d9d6e8a88621] , [~accountid:6036b8046bc3f300699cec92] 

# Do you know how many customers and users are affected by this issue?  
# how many support tickets do we receive about this problem per week or month?


[~accountid:611be770be6c4300703f3c7e] ,  [~accountid:611be0f6c2f3a50069e22502] 

This looks like a bug that we might want to fix. Can you please assign someone to do a quick review and give us a rough estimate on the size to fix?

Thanks,

CC: [~accountid:557058:f61ef450-6067-4c35-b85a-bef69cfd54e5] ",02/Aug/22 6:51 AM;557058:9161deac-4565-42f2-8b02-d9d6e8a88621;Thanks for the reply [~accountid:6046a61aaf9849007236b939] . Tagging [~accountid:6164c889c7bea400699a7e2f] from my team as he is now assigned to this area.,"03/Oct/23 10:36 PM;712020:a7dc656d-cb65-46b4-96c8-a96181de09ef;[~accountid:6046a61aaf9849007236b939] [~accountid:6036b8046bc3f300699cec92] [~accountid:6053b249f180c300675f09a3]  Hi everyone,

I have a customer who has requested an update on this SPL. Is there a workaround or fix for this?

Any update would be helpful.

Thank you",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,06/Aug/20 7:53 AM
"[PUBLIC] When generating LISPY for field values that are numbers (""[ EQ field 1 ]""), the values aren't deduplicated, which can cause slowdowns in certain scenarios",SPL-183259,1460211,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,Unknown,Won't Fix,Chamara Wickramanayake,6053a4be81b82500685cf5c5,Maarten Hoogcarspel,5c5a99db92db7b0b85249c1a,Maarten Hoogcarspel,5c5a99db92db7b0b85249c1a,12/Feb/20 5:44 AM,30/Jul/25 4:24 PM,,20/Mar/20 10:08 AM,10.0.x,7.2.0 GA (OrangeSwirl),7.2.1,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.2.9.1,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.4.1,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search,,,,,0,z0928,,,,,,,"When generating LISPY for field values that are numbers, the values aren't deduplicated, which can cause slowdowns in certain scenarios

Reproduces on 8.0.1:

eval macro ""foreach_count"":
{code}
mvjoin(mvrange(0,$count$),"" "")
{code}

Simple example with strings, you can see the lispy terms are deduplicated
{code}
index=_internal 
 [| makeresults 
 | foreach `foreach_count(100)` 
 [| eval something=mvappend(something,""something"")] 
 | format 
 | eval search=replace(replace(search,""="",""::""),""\"""","""")]
{code}

Generated LISPY:
{code}
02-12-2020 13:34:28.837 INFO UnifiedSearch - base lispy: [ AND index::_internal something::something ]
{code}

Simple example with numbers, you can see the lispy terms are NOT deduplicated:
{code}
index=_internal 
 [| makeresults 
 | foreach `foreach_count(100)` 
 [| eval something=mvappend(something,""1"")] 
 | format 
 | eval search=replace(replace(search,""="",""::""),""\"""","""")]
{code}

Generated LISPY:
{code}
02-12-2020 13:33:02.844 INFO UnifiedSearch - base lispy: [ AND index::_internal [ OR [ EQ something 1 ] [ EQ something 1 ] [ EQ something 1 ] [ EQ something 1 ] [ EQ something 1 ] [ EQ something 1 ] [ EQ something 1 ] [ EQ something 1 ] [ EQ something 1 ] [ EQ something 1 ] ... [ EQ something 1 ] ] ]
{code}
 
Theses searches take a lot longer to run, in customer case like 50x slower.

In real scenarios, this is most likely going to be on data that has INDEXED_EXTRACTIONS (csv, json) for example.

Customer description, with a bit more detail (using INDEXED=true for example):
{quote}
If you set the field to INDEXED=true, the LISPY created isn't optimized in that same way.

The Expanded index search is exactly the same as in CASE 1 above.

But the base lispy is:
[ AND index::field_test [ OR [ EQ globalcallid_callid 2564322 ] [ EQ globalcallid_callid 2564322 ] [ EQ globalcallid_callid
2564322 ] [ EQ globalcallid_callid 2564322 ] [ EQ globalcallid_callid 2564322 ] [ EQ globalcallid_callid 2564322 ]
... LOTS AND LOTS AND LOTS OF REPEATING...
[ EQ globalcallid_callid 2564323 ] [ EQ globalcallid_callid 2564322 ] [ EQ globalcallid_callid 2564322 ]
[ EQ globalcallid_callid 2564321 ] [ EQ globalcallid_callid 2564320 ] ] ]

Key items to note:
The LISPY does not dedupe away those repeated (same) globalcallid_callid fields, but just includes the repetition.

Speed - the search takes more than a minute to run (about 70 seconds usually, or about 100x as slow). This isn't good!


CASE 3:
So I thought, let's try to confirm it's the duplication (e.g. non-optimization) of the LISPY in the INDEXED=true case that's causing this.

index=""field_test"" [search index=""field_test"" globalCallID_callId=256432* | stats count by globalCallID_callId | fields globalCallID_callId]

turns into
Expanded index search = (index=""field_test"" (globalCallID_callId=""2564320"" OR globalCallID_callId=""2564321"" OR globalCallID_callId=""2564322"" OR globalCallID_callId=""2564323"" OR globalCallID_callId=""2564324"" OR globalCallID_callId=""2564325"" OR globalCallID_callId=""2564327"" OR globalCallID_callId=""2564329""))

and then
base lispy: [ AND index::field_test [ OR [ EQ globalcallid_callid 2564320 ] [ EQ globalcallid_callid 2564321 ] [ EQ globalcallid_callid 2564322 ] [ EQ globalcallid_callid 2564323 ] [ EQ globalcallid_callid 2564324 ] [ EQ globalcallid_callid 2564325 ] [ EQ globalcallid_callid 2564327 ] [ EQ globalcallid_callid 2564329 ] ] ]

Key items to note:
LISPY is now deduped. Still the same syntax, just instead of 500kb of LISPY it's 3 KB (and 500 KB is a guess, dunno, but it's BIG).

Speed - it runs in 2.5 seconds or so over the exact same data.{quote}",,Antony Wang,Chamara Wickramanayake,Maarten Hoogcarspel,Niclas Andersson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:9b250150-4703-4e0c-a02b-ad536bb0d408,6053a4be81b82500685cf5c5,5c5a99db92db7b0b85249c1a,6036b7996bc3f300699ce83a,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Forest County Potawatomi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3efe3a4d,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SEC_NONE,,,,,,,,,false,,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,143251200,,,,,,,,,,,,,,,,,,,,,,,,,Installed Version,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,Reproduces on 8.0.1 standalone,,,,,,,,,,"None, just much slower searches",false,,,,,,,,,,,,No,,,,,,,,,,,,,"LIPSY terms to be deduped, regardless of if it's numbers or strings",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,consistent,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Thu Mar 11 11:07:49 UTC 2021,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dawang(dawang),cwickramanayake(cwickramanayake),garias(garias),jisenberg(jisenberg),mhoogcarspel(mhoogcarspel),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Documentation improvement - Customer Facing,,Sustaining,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Chamara Wickramanayake,6053a4be81b82500685cf5c5,,,,,,,,,,,,,,,,,,0|i5tsd3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chamara Wickramanayake,6053a4be81b82500685cf5c5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Closed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"index=_internal 
 [| makeresults 
 | foreach `foreach_count(100)` 
 [| eval something=mvappend(something,""1"")] 
 | format 
 | eval search=replace(replace(search,""="",""::""),""\"""","""")]",,,,,,,,,,,,,,,,,,,,,0.0,,,,See description ,,,,,,,,,,,,,,,,,,,2020-02-12 13:47:06.924,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3_*:*_1_*:*_345262011_*|*_4_*:*_1_*:*_13088_*|*_5_*:*_2_*:*_1567042972_*|*_10001_*:*_2_*:*_1296552288_*|*_10039_*:*_1_*:*_172210,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,11.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Dedup values in search before, for example:

instead of 
<pre>
index=""field_test"" [search index=""field_test"" globalCallID_callId=1234* | fields globalCallID_callId]
</pre>
add a stats or dedup in the subsearch:
<pre>
index=""field_test"" [search index=""field_test"" globalCallID_callId=123* | stats values(globalCallID_callId) AS globalCallID_callId | mvexpand globalCallID_callId ]
</pre>

If that list is still large and you're seeing the slowdown, consider moving the filtering to a | where after the initial search, for example:
<pre>
index=""field_test"" globalCallID_callId=* | where [search index=""field_test"" globalCallID_callId=123* | stats values(globalCallID_callId) AS globalCallID_callId | mvexpand globalCallID_callId ]
</pre>",,,,,,,,,,2020-02-12 13:47:06.924,2020-02-12 13:47:06.924,,,,,,Chamara Wickramanayake,6053a4be81b82500685cf5c5,,,,,,"12/Feb/20 5:47 AM;6036b807f6ccc0007044b3fd;Traffic Cop, all fine","28/Feb/20 2:33 AM;6053a4be81b82500685cf5c5;Hi [~accountid:5c5a99db92db7b0b85249c1a].

 I think this is a corner case right? If all below conditions are met the issue arises? 
 # It is an indexed field
 # it is comparing it to an numeric quantity
 # The exact same comparison is being repeated many time

Likelihood of meeting all these 3 are rare to justify for a fix. This will not be straightforward fix too and its a new optimisation that needs to be added to our parsenodecmp.","02/Mar/20 2:24 AM;5c5a99db92db7b0b85249c1a;Correct, and I would say best practice is to dedup in the subsearch before dumping the filter into search.
The less corner case version that I can think of would be the automatic reverse lookup ","20/Mar/20 10:03 AM;5c5a99db92db7b0b85249c1a;Can the resolution be ""Won't Fix"" as you've acknowledged that this is an issue in the product but you won't be fixing this at this point please? 

Also I'm trying to get this into the release notes ",20/Mar/20 10:07 AM;6053a4be81b82500685cf5c5;yes I am changing it to wont Fix.,29/Sep/20 8:53 AM;557058:2d2a7f1d-c9e1-4f3c-95c5-24ed73d0b69d;SevLevelCleanupJI,25/Jan/21 11:24 AM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Closing as part of 6+ months old resolved bugs bulk clean up by QA Metrics Team. CC [~accountid:5ebddcc35862510b798f5abd] [~accountid:5d3547e5c2db730c59b25bdc] [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408].,11/Mar/21 3:07 AM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Rolling back Assignee & Developer from Jan 25th bulk closure operation. CC [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408]. Assignee: cwickramanayake. Developer: cwickramanayake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,20/Mar/20 10:08 AM
[PUBLIC] geostats provides incorrect results for lower zoom levels when split BY has a higher cardinality than globallimit.,SPL-181573,1429670,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,Unknown,Workaround,Chamara Wickramanayake,6053a4be81b82500685cf5c5,Maarten Hoogcarspel,5c5a99db92db7b0b85249c1a,Maarten Hoogcarspel,5c5a99db92db7b0b85249c1a,10/Jan/20 3:40 AM,30/Jul/25 4:25 PM,,30/Mar/20 3:22 AM,10.0.x,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.2.9.1,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search,,,,,0,z0928,,,,,,,"geostats provides incorrect results for lower zoom levels when split BY has a higher cardinality than globallimit

Reproduces on 8.0.1, with attached csv

search to have the totals per zoom level, without the split:
{code}
| inputlookup location-1000.csv 
| geostats count 
| eval zoom_level=mvindex(split(geobin,""_""),3) 
| stats sum(count) BY zoom_level
{code}
in this example: 852 

search to show the issue:
{code}
| inputlookup location-1000.csv 
| rex field=City mode=sed ""s/\'//g"" 
| geostats count BY City 
| eval zoom_level=mvindex(split(geobin,""_""),3) 
| eval total=0 
| fillnull 
| foreach * 
    [| eval total=if(""<<FIELD>>""=""geobin"" OR ""<<FIELD>>""=""latitude"" OR ""<<FIELD>>""=""longitude"" OR ""<<FIELD>>""=""zoom_level"" OR ""<<FIELD>>""=""total"",total,tonumber(total)+tonumber('<<FIELD>>'))] 
| stats count sum(total) BY zoom_level
{code}
This results in more and more total per zoom level, until you hit the highest one (zoom level 9, highest split) and then it matches the total without the split BY again

You also get the message in the job inspector: 
{code}
The split by field City has a large number of unique values 264. Chart column set will be trimmed to 10. Use globallimit argument to control column count.
{code}

if you then add ""globallimit=264"" the results are correct again:
{code}
| inputlookup location-1000.csv 
| rex field=City mode=sed ""s/\'//g"" 
| geostats globallimit=264 count BY City 
| eval zoom_level=mvindex(split(geobin,""_""),3) 
| eval total=0 
| fillnull 
| foreach * 
    [| eval total=if(""<<FIELD>>""=""geobin"" OR ""<<FIELD>>""=""latitude"" OR ""<<FIELD>>""=""longitude"" OR ""<<FIELD>>""=""zoom_level"" OR ""<<FIELD>>""=""total"",total,tonumber(total)+tonumber('<<FIELD>>'))] 
| stats count sum(total) BY zoom_level
{code}
","Reproduces on 8.0.1 standalone
Customer is running 7.2 ",Antony Wang,Chamara Wickramanayake,Jeffrey Chang,Maarten Hoogcarspel,Sudha lakshmisha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:9b250150-4703-4e0c-a02b-ad536bb0d408,6053a4be81b82500685cf5c5,557058:52c801cc-dc38-4687-bd76-6aeaab916f3c,5c5a99db92db7b0b85249c1a,557058:5fa06468-e0e4-4d85-b25f-e01185474f22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10/Jan/20 3:49 AM;mhoogcarspel;location-1000 (2).csv;https://splunk.atlassian.net/rest/api/3/attachment/content/1247070,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Davis Wright Tremaine LLP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@61656b65,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SEC_MEDIUM,,,,,,,,,false,,13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,143251200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,"Reproduces on 8.0.1 standalone
Customer is running 7.2",,,,,,,,,,"Job Inspector: 
The split by field City has a large number of unique values <number>. Chart column set will be trimmed to 10. Use globallimit argument to control column count.",false,,,,,,,,,,,,No,,,,,,,,,,,,,Correct results for each zoom level,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,consistently,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Thu Mar 11 11:13:47 UTC 2021,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,1.0,13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dawang(dawang),cwickramanayake(cwickramanayake),garias(garias),jisenberg(jisenberg),jchang(jchang),mhoogcarspel(mhoogcarspel),pchang(pchang),slakshmisha(slakshmisha),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Documentation improvement - Customer Facing,,Sustaining,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Chamara Wickramanayake,6053a4be81b82500685cf5c5,,,,,,,,,,,,,,,,,,0|i5pjl3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Chamara Wickramanayake,6053a4be81b82500685cf5c5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Closed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,See Description,,,,,,,,,,,,,,,,,,,,,0.0,,,,See description,,,,,,,,,,,,,,,,,,,2020-01-13 23:41:25.904,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3_*:*_1_*:*_579135469_*|*_4_*:*_1_*:*_4204592535_*|*_5_*:*_2_*:*_547904330_*|*_10001_*:*_1_*:*_1040021374_*|*_10039_*:*_1_*:*_535691251,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"- Increase globallimit to the value of ""unique values"" number mentioned in the warning message: <br>
""The split by field <field> has a large number of unique values <number>. Chart column set will be trimmed to 10. Use globallimit argument to control column count.""

- Use very high globallimit in geostats and post process after if needed

- Don't use BY in geostats

- Use lower cardinality BY and/or higher globallimit in geostats",,,,,,,,,,2020-01-13 23:41:25.904,2020-01-13 23:41:25.904,,,,,,Chamara Wickramanayake,6053a4be81b82500685cf5c5,,,,,,13/Jan/20 3:41 PM;557058:52c801cc-dc38-4687-bd76-6aeaab916f3c;[~accountid:5c5a99db92db7b0b85249c1a] Please assign to support regional reviewer per https://splunk.atlassian.net/wiki/display/HSSR/Creating+Splunk+Core+Product+Jiras .,"16/Jan/20 8:28 AM;6036b807f6ccc0007044b3fd;Traffic cop review, all fine","24/Jan/20 10:06 AM;6053a4be81b82500685cf5c5;Hi [~accountid:5c5a99db92db7b0b85249c1a],

The root cause to not get all the counts is the series restriction which is set by globallimit which default globallimit set to 10. This converts the seach by adding top+<globallimit>
{code:java}
geobin City sum(count) IN top10
{code}
I think customer can either disable by settting globallimit=0 which is riskier and UI can get flooded. As you suggested preferable approach will be to set the globallimit to the  max cardinality provided by the message 
{code:java}
The split by field City has a large number of unique values 264. Chart column set will be trimmed to 10. Use globallimit argument to control column count.
{code}
 ","10/Feb/20 10:26 AM;5c5a99db92db7b0b85249c1a;Hold on a moment :)

The globallimit is just for how many named fields there are, the OTHER field should still tally up to the same total.

I think this shows that the named fields are fine, just not all non-named totals are added to OTHER:
{code:java}
| inputlookup location-1000.csv 
| rex field=City mode=sed ""s/\'//g"" 
| geostats count BY City 
| eval zoom_level=mvindex(split(geobin,""_""),3) 
| eval total=0 
| fillnull 
| foreach * 
    [| eval total=if(""<<FIELD>>""=""geobin"" OR ""<<FIELD>>""=""latitude"" OR ""<<FIELD>>""=""longitude"" OR ""<<FIELD>>""=""zoom_level"" OR ""<<FIELD>>""=""total"",total,tonumber(total)+tonumber('<<FIELD>>'))] 
| stats count sum(total) AS total sum(OTHER) AS OTHER_in_total BY zoom_level
| eval named_in_total=total-OTHER_in_total
{code}
The green is the named fields, and they show the same total regardless of zoom level, blue contains both OTHER and the named fields, and the discrepancy is in OTHER:

!image-2020-02-10-18-26-04-404.png!",09/Mar/20 3:25 AM;5c5a99db92db7b0b85249c1a;Any updates on this? Do you agree with my last update [~accountid:6053a4be81b82500685cf5c5]? ,"12/Mar/20 4:27 AM;6053a4be81b82500685cf5c5;Yes i Agree [~accountid:5c5a99db92db7b0b85249c1a], Let me take another look at this.","18/Mar/20 10:58 AM;6053a4be81b82500685cf5c5;Hi [~accountid:5c5a99db92db7b0b85249c1a],

 As discussed on slack this is the way how geostats is working right now. The default globalimit(10) flag is limiting the elements in a column. It will make Rows containing other values that aren't the top 10 for that field to be dropped.  This is the difference in the totals and OTHERS field you see in the results.

 ",27/Mar/20 3:22 PM;557058:5fa06468-e0e4-4d85-b25f-e01185474f22;[~accountid:6053a4be81b82500685cf5c5] you have picked Documentation for this JIRA PPIc so could you please open a doc JIRA with details on what needs to be documented please?  and link the JIRA through Customer Issue Spinoff - thanks.,30/Mar/20 3:10 AM;5c5a99db92db7b0b85249c1a;Customer was OK to close the case and track via release notes.,21/Apr/20 10:26 PM;5e1cde1de7319c0caaefb75d;Bulk changed priority from medium to unknown. For more information please refer to PT-1721.,29/Sep/20 8:02 AM;557058:2d2a7f1d-c9e1-4f3c-95c5-24ed73d0b69d;SevLevelCleanupJI,26/Jan/21 11:38 AM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Closing as part of 6+ months old resolved bugs bulk clean up by QA Metrics Team. CC [~accountid:5ebddcc35862510b798f5abd] [~accountid:5d3547e5c2db730c59b25bdc] [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408].,11/Mar/21 3:13 AM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Rolling back Assignee & Developer from Jan 25th bulk closure operation. CC [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408]. Assignee: cwickramanayake. Developer: cwickramanayake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,30/Mar/20 3:22 AM
[PUBLIC] The splunktcp and splunktcp-ssl stanzas are not reloadable in inputs.conf,SPL-179528,1393492,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P1-Immediate,As Designed,Richard Wang,6053a68b009fee00694bf279,Kashyap Jotwani,6053a6d394d7b90069f83e26,Kashyap Jotwani,6053a6d394d7b90069f83e26,13/Nov/19 9:25 AM,30/Jul/25 4:24 PM,,04/Jun/20 11:08 AM,10.0.x,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.2001 GA (Aspen),8.0.2003 GA (Balfour),8.0.2004 GA (Chimney Rock),8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.0.x,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DMC,,,,,0,CF_cloud_triaging,E-13,V-5,,,,,"As part of the Reloadable bundle push feature, inputs.conf is now reloadable instead of needing a rolling-restart

However, some stanza's in inputs.conf like
{code}
[splunktcp://22242]
connection_host = ip
{code}

{code}
[splunktcp-ssl://22242]
connection_host = ip

[SSL]
password = password
rootCA = $SPLUNK_HOME/etc/auth/cacert.pem
serverCert = $SPLUNK_HOME/etc/auth/server.pem
{code}


Are not reloadable. Discussed with [~accountid:6053a68b009fee00694bf279], it might be that TcpInput Handlers are not reloadable.
Hence, with the reloadable bundle push feature, if a user has the above settings in inputs.conf and pushes from the Cluster Master, reload happens but the TcpInput Processors are not working until a restart is done. 
And there's no indication to the user that a restart is needed

[~accountid:6053a68b009fee00694bf279] pls Add if I have missed something",,Amrit Bath,Emily Cheung,Eric Woo,Howard (Shih-Chen) Chin,Kashyap Jotwani,Kevin Cable [X],Michael Lin,Richard Wang,Shruti Anand,Srinivas Bobba,srv- ssc-gitlab,Steven Roback,Suketu Shah,User known,,,,,,,,,,,,,,,,,,,,,,5d0995dcc184b50c22ff1c46,6053a74994d7b90069f8435f,5f55997610d187006f15c025,557058:30e41e2b-e9d1-427a-ae34-cf6659ef443a,6053a6d394d7b90069f83e26,613249214a98da0069ede63d,557058:66096a91-634f-4762-882d-fbd9a1c51674,6053a68b009fee00694bf279,5fefe7eb332c3a0107caf133,6053b9162f452d006f83555b,613254346fa73c006a9e37be,557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc,6053a66586b0dd007188092d,unknown,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5d6bee3d,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,On Prem,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,Not looked at yet,,,,,,151113600,,,,,,,,,,,,,,,,,,Enterprise,Enterprise Cloud,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Thu Dec 10 08:12:31 UTC 2020,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Rolling Restart,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,18.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,echeung(echeung),kjotwani(kjotwani),kcable(kcable),jwang(jwang),sanand(sanand),sbobba(sbobba),srv-ssc-gitlab(srv-ssc-gitlab),sroback(sroback),wshi(swindy),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i5jyd3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Richard Wang,6053a68b009fee00694bf279,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,13.0,,,,,,,,,,,0.0,,,,"• For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

• For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

• Describe the steps taken to perform analysis and provide evidence to support the problem statement
. If available, provide the events that match the timeline with supporting logs.

• Provide the logs and accompanying data with reference to the confluence page Filing UI/Server Bugs, and all the relevant documentation based on your analysis to support your observation and theories.

• In your own words, describe the area affected based on your analysis.  Affected area could be component or issues related to memory, performance or authentication etc. Please specify.

Steps to Reproduce:
.If the issue is reproducible,provide the steps.
Is this issue reproducible on latest maintenance version or latest release?

. If available, will customer upgrade to fixed version?
If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,2019-11-13 19:09:26.649,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_4_*:*_6143408648_*|*_3_*:*_8_*:*_923666214_*|*_6_*:*_1_*:*_0_*|*_10001_*:*_4_*:*_10472236810_*|*_10039_*:*_1_*:*_88892360,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019-11-13 19:09:26.649,2019-11-13 19:09:26.649,,,,,,Richard Wang,6053a68b009fee00694bf279,,,,,,"13/Nov/19 11:09 AM;6053a68b009fee00694bf279;per conversation w/ [~accountid:6053a35c60d39e006f807b35], we need to factor in the inconsistency b/t dry run and real push for inputs.conf reload vs restart decision.",13/Nov/19 12:27 PM;5fefe7eb332c3a0107caf133;[~accountid:6053a68b009fee00694bf279] assuming this is only for tcp handler we are safe to have this go out for cloud as per discussion with [~accountid:557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e]. For on prem customers I assume since its a one time initial change if we have a fix for this out post 8.0.1 in the immediate release we should be good there too (need to confirm this with [~accountid:557058:b1e398c5-b004-4a33-8152-18daa36cda4b]) ,"14/Nov/19 4:33 AM;6053a6d394d7b90069f83e26;[~accountid:6053a68b009fee00694bf279] and I tested a few more scenarios:
 *Single Instance:*

Monitor stanza is reloadable
{code:java}
[monitor://]
 {code}
Tcp stanza is reloadable
{code:java}
[tcp]
 {code}
Next we checked {{splunktcp}} stanza, which is also reloadable
{code:java}
[splunktcp://]
{code}
*Clustered Deployment*
 checked {{splunktcp}} stanza, which is also reloadable
{code:java}
[splunktcp://]
{code}
But, {{splunktcp-ssl}} with {{SSL}} stanza's reloading doesn't seem to be working. In these stanza, which param is not reloadable needs further debugging
{code:java}
[splunktcp-ssl://] 
[SSL]
{code}",14/Nov/19 10:00 AM;613249214a98da0069ede63d;Scope review 11/14:  Will try to get this in 802 but needs to be documented in Release Notes,18/Nov/19 2:29 AM;557058:0367aeb8-777e-4e9f-a4de-ecbe323cebcc;Seems this issue also happens in current branch.,"19/Dec/19 9:50 AM;613249214a98da0069ede63d;CC [~accountid:6053a68b009fee00694bf279]
",16/Mar/20 12:12 PM;5fefe7eb332c3a0107caf133;[~accountid:613249214a98da0069ede63d] all the changes are reverted from balfour so targetting CR now,31/Mar/20 4:03 PM;6053a74994d7b90069f8435f;[~accountid:6053a68b009fee00694bf279] - can you include this in your ERD?,"28/Sep/20 1:02 PM;6053b9162f452d006f83555b;[~accountid:557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc] I believe we have documented this (what stanza's are supported) in the Splunk Clouds docs, 
can we please ensure this is also called out in the rarity docs.","28/Sep/20 5:19 PM;557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc;Hi [~accountid:6053b9162f452d006f83555b]. Yes, we documented the specific indexes.conf stanzas that are reloadable here:

[https://docs.splunk.com/Documentation/SplunkCloud/8.1.2008/User/PrivateApps#Stanza-level_reload_triggers_for_inputs.conf]

And this will also be included in the Rarity docs. See SPL-191352

 ",09/Dec/20 3:23 AM;613254346fa73c006a9e37be;[Kashyap Jotwani|https://cd.splunkdev.com/kjotwani] mentioned this issue in [a commit|https://cd.splunkdev.com/bitbucket_repos/qa/-/commit/ac91e31aefeaa4ab139949132397fbdc0408960f] of [bitbucket_migration / qa|https://cd.splunkdev.com/bitbucket_repos/qa] on branch [SPL_179430|https://cd.splunkdev.com/bitbucket_repos/qa/-/tree/SPL_179430]:{quote}Marking test out as SplunkBug due to SPL-179528{quote},09/Dec/20 3:23 AM;613254346fa73c006a9e37be;[Kashyap Jotwani|https://cd.splunkdev.com/kjotwani] mentioned this issue in [a commit|https://cd.splunkdev.com/bitbucket_repos/qa/-/commit/2a43af685203eb47911cb2b37a3588d29fdbdcac] of [bitbucket_migration / qa|https://cd.splunkdev.com/bitbucket_repos/qa]:{quote}Merge pull request #18223 in SPLCORE/qa from SPL_179430 to develop{quote},"09/Dec/20 3:29 AM;613254346fa73c006a9e37be;[Nikitha Janapareddy|https://cd.splunkdev.com/rnjanapareddy] mentioned this issue in [a commit|https://cd.splunkdev.com/bitbucket_repos/qa/-/commit/99ef9b22721c5e0b9f0e8b36f56c9c426238a074] of [bitbucket_migration / qa|https://cd.splunkdev.com/bitbucket_repos/qa]:{quote}Revert ""Marking test out as SplunkBug due to SPL-179528""{quote}",09/Dec/20 3:29 AM;613254346fa73c006a9e37be;[Kashyap Jotwani|https://cd.splunkdev.com/kjotwani] mentioned this issue in [a commit|https://cd.splunkdev.com/bitbucket_repos/qa/-/commit/390a017de67487c00b616aad477a72e5949d1d18] of [bitbucket_migration / qa|https://cd.splunkdev.com/bitbucket_repos/qa]:{quote}Merge pull request #19909 in SPLCORE/qa from tests/revert-RR-balfour to develop{quote},09/Dec/20 4:10 AM;613254346fa73c006a9e37be;[Freddy Tan|https://cd.splunkdev.com/ftan] mentioned this issue in [a commit|https://cd.splunkdev.com/bitbucket_repos/qa/-/commit/711a6c058501401715d4907157018ac01cf5e7e5] of [bitbucket_migration / qa|https://cd.splunkdev.com/bitbucket_repos/qa]:{quote}skip due to bug SPL-179528{quote},09/Dec/20 4:10 AM;613254346fa73c006a9e37be;[Freddy Tan|https://cd.splunkdev.com/ftan] mentioned this issue in [a commit|https://cd.splunkdev.com/bitbucket_repos/qa/-/commit/15d4c2340dccaaccda98e32189d8498d435f6f8c] of [bitbucket_migration / qa|https://cd.splunkdev.com/bitbucket_repos/qa]:{quote}Merge pull request #18799 in SPLCORE/qa from bugfix/ftan/skip-case to sustain/quake{quote},09/Dec/20 4:19 AM;613254346fa73c006a9e37be;[Jeffrey Chang|https://cd.splunkdev.com/jchang] mentioned this issue in [a commit|https://cd.splunkdev.com/bitbucket_repos/qa/-/commit/3a44395450252bb7bd82144b7772b46b78429140] of [bitbucket_migration / qa|https://cd.splunkdev.com/bitbucket_repos/qa]:{quote}Merge pull request #18980 in SPLCORE/qa from sustain/quake to release/quake{quote},"10/Dec/20 12:12 AM;613254346fa73c006a9e37be;[Nikitha Janapareddy|https://cd.splunkdev.com/rnjanapareddy] mentioned this issue in [a commit|https://cd.splunkdev.com/bitbucket_repos/qa/-/commit/ca1a809b5de9c811e44fbe0dd440cccce7e39ecf] of [bitbucket_migration / qa|https://cd.splunkdev.com/bitbucket_repos/qa]:{quote}Revert ""Revert ""Marking test out as SplunkBug due to SPL-179528""""{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,04/Jun/20 11:08 AM
[PUBLIC]  [Cascading Replication] Bundle replication takes longer than expected time for indexers that have bundleEnforcerBlacklist configured ,SPL-177447,1353627,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,Bing Zhou,615b2dd89cdb9300726ea48e,jshao,557058:3161bbb4-d09d-483c-886e-5f7fa44534f2,jshao,557058:3161bbb4-d09d-483c-886e-5f7fa44534f2,03/Oct/19 5:20 AM,30/Jul/25 4:25 PM,,,10.0.x,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Bundle Replication Enhancement,,,,,0,caused_flakiness,quake_punt_ok,quake_systest_preqac,requires-triage,systest_quake,,,"Steps to reproduce:
 # set a distribute search(3 SHC, 7 Indexers IDC, 1 standalone indexer) with cascading replication
 # create a kv-store based lookup
 # use a script to make a kv store collection data keep updating
 # create a scheduled savesearch to run a search with the lookup every 1 minutes
 # check the bundle status on all the peers, you can find the bundle replication every 2 mins
 # change the standalone indexer's bundleEnforcerBlacklist to block the bundle replication on this peer.
 # check the result.

 

Result:
 # the bundle no longer replicate to the standalone indexer, since the bundle is blacklisted
 # check the splunk log on the standalone indexer, it tells *WARN*.
{code:java}
10-03-2019 18:37:48.080 +0800 WARN  CascadingUploadHandler - Failed to apply delta for search-head=A513C830-58BC-45B4-A236-88A64F695C6D with filePath=/root/splunk/var/run/searchpeers/A513C830-58BC-45B4-A236-88A64F695C6D-1570098912-1570099031.delta and currCheckSum=12928352177236211333 and currTimestamp=1570099031 and prevChecksum=6689478038752077008 and prevTimestamp=1570098912 and exception=File apps/search_app/local/collections.conf in knowledge bundle is either not in white list or else excluded by black list. Bundle /root/splunk/var/run/searchpeers/A513C830-58BC-45B4-A236-88A64F695C6D-1570099031.1d94e88d20c6ce0d.tmp will be removed.
{code}

 # check the splunkd log on the SH, it tells *Error* like bellow
{code:java}
10-03-2019 04:55:34.405 -0700 ERROR CascadePlan - planId=6EE094C0-1666-4075-8401-212AA32DE0DF state updated to payload_send_failed.{code}

 # Check the replication status on SH, it tells ""apply_failed"" on that peer
 # Check the bundle status on the other peers, the replication period extend from 2 mins to 18 mins, because the SH will retry to replication the bundle to the failed standalone indexer.

Actually, the bundleEnforcerBlacklist is a feature, it should not be treated directly as a failure on Search head, so there should not retry and block the next replication during that period. If there are more than 1 blacklisted indexers, the period might be even longer.

 

And anther point is, when fixing this issue, in cascading mode,  please consider that the blacklisted indexer also have possibility to be chosen as an ""receiver"" and then behave as  a ""sender""","Quake, 4bee050a3013",Bhavin Thaker,Christina Geng,jshao,Manikanthreddy Kudumula (C),Shalabh Goyal,Suketu Shah,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a4ef60d39e006f808dd6,6053aaff3adeca0067edb953,557058:3161bbb4-d09d-483c-886e-5f7fa44534f2,6389243e9e48f2b9a6157dad,6053a60cf180c300675e7e8b,6053a66586b0dd007188092d,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Global,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-111542,,,,,,,,03/Oct/19 5:18 AM;jshao;Screen Shot 2019-10-03 at 8.08.42 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1092177,03/Oct/19 5:18 AM;jshao;bundle status on other peer.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1092179,03/Oct/19 5:18 AM;jshao;bundles on standalone peer.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1092178,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@16a18b3e,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,68083200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,Fri Jul 28 18:10:43 UTC 2023,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise Services,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,3.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,abrown(abrown),bthaker(bthaker),bzhou(JIRAUSER53110),cgeng(cgeng),jshao(jshao),kcable(kcable),57fe36df-e272-4e92-8ce1-1b2782fe7f71(57fe36df-e272-4e92-8ce1-1b2782fe7f71),ruochenz(ruochenz),2aea4c9c-d59e-41a8-9cf7-d012e3cb0c08(2aea4c9c-d59e-41a8-9cf7-d012e3cb0c08),sgoyal(sgoyal),suketus(suketus),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i0mmcn:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Rebecca Zhang,6053a65360d39e006f809da2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PR-10-S64,PR-10-S65,PR-10-S66,SHC-XRDR-FY23Q4-S6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,0.0,,,,"• For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

• For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

• Describe the steps taken to perform analysis and provide evidence to support the problem statement
. If available, provide the events that match the timeline with supporting logs.

• Provide the logs and accompanying data with reference to the confluence page Filing UI/Server Bugs, and all the relevant documentation based on your analysis to support your observation and theories.

• In your own words, describe the area affected based on your analysis.  Affected area could be component or issues related to memory, performance or authentication etc. Please specify.

Steps to Reproduce:
.If the issue is reproducible,provide the steps.
Is this issue reproducible on latest maintenance version or latest release?

. If available, will customer upgrade to fixed version?
If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,2019-10-03 19:58:01.586,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019-10-03 19:58:01.586,2019-10-03 19:58:01.586,,,,,,Rebecca Zhang,6053a65360d39e006f809da2,,,,,,03/Oct/19 12:58 PM;613249214a98da0069ede63d;CC [~accountid:6053a4ef60d39e006f808dd6],"03/Oct/19 6:20 PM;6053a65360d39e006f809da2;I think this is a valid concern, but can be punted to post-Quake at this time, because:

1. This problem only happens on indexers who have `bundleEnforcerBlacklist` configured. When indexer receives a bundle that includes any file marked as bundleEnforcerBlacklist, the entire bundle will be rejected. This is the designed behavior implemented in a feature pre-Quake.

2. The customer impact when this problem happens is that replication time takes a bit longer to finish. But I don't think the end-to-end replication time will increase proportionally with the number of affected indexers.

3. The fix for this problem is risky for Quake because it requires implementing a way to tell the failures are because of `bundleEnforcerBlacklist`, not the other failures that worth retry. The fix will be a behavior change.

I suggest to document this and punt to post-Quake.

CC [~accountid:6053a4ef60d39e006f808dd6], [~accountid:6053a60cf180c300675e7e8b], [~accountid:6053a66586b0dd007188092d] Can you approve the punt to post-Quake? ","04/Oct/19 11:10 AM;6053a65360d39e006f809da2;For developers who will be working on this jira, I think the following changes are needed:
 # Detect it is because of ""bundleEnforcerBlacklist"" set on indexers that caused bundle being rejected.
 # Update the state machine on search head accordingly
 # Let search head NOT to retry in this case",04/Oct/19 11:15 AM;613249214a98da0069ede63d;[~accountid:6053a4ef60d39e006f808dd6] [~accountid:6053a60cf180c300675e7e8b] [~accountid:6053a66586b0dd007188092d] ^^. Agree to punt this?,04/Oct/19 12:07 PM;6053a4ef60d39e006f808dd6;Agree on punt. Edited Issue to add to Release Notes.,"04/Oct/19 1:06 PM;6053a60cf180c300675e7e8b;+1 on punt ok

Not very significant customer impact and risky fix.",04/Oct/19 1:07 PM;6053a66586b0dd007188092d;Yes good to punt from Quake to reduce the risk of affecting the CR functionality with new changes ,"03/May/21 3:11 PM;6053aaff3adeca0067edb953; [~accountid:6053a60cf180c300675e7e8b] [~accountid:5f7671cd837bb80068281d0b] our tests are flaky due to this bug, do you think we could assign the developer to fix this any time soon? or we want to close this ticket and remove the tests from full regression? Thanks.","03/May/21 4:16 PM;6053a60cf180c300675e7e8b;[~accountid:6053aaff3adeca0067edb953] - we should review which tests are impacted and why are they flaky. In terms of customer priority, this is not a high priority issue.","15/Nov/22 9:31 AM;62c2a8697273faf658f024d8;adding a new label ""Blocking_Functional_Test"" to the issue, for tracking all the SPL issues associated with the ARTs automation triaging tool","28/Nov/22 2:17 PM;557058:4240d4ce-c6e5-4077-a73c-949f1ad0796b;Bulk-adding label ""requires-triage"" to indicate we need to review this item.  Please triage and fix P0/P1, and at least triage everything else.","19/Dec/22 9:59 AM;6053a60cf180c300675e7e8b;Cascading replication is not used in Splunk Cloud and even for CMP customer base, it is rarely used. Product telemetry show <15 customers using it across all production, test, dev environments. 

This bug is a corner case performance bug that we can defer.","19/Dec/22 3:43 PM;615b2dd89cdb9300726ea48e;Blocking_Functional_Test label was added to this ticket due to linked issue [https://splunk.atlassian.net/browse/TNT-9561|https://splunk.atlassian.net/browse/TNT-9561|smart-link]. 

Based on my triaging of that ticket, the failed test case is not related to cascading replication, but normal knowledge bundle replication. Also the flakiness for that ticket can be resolved by increasing waiting time of bundle replication from 180 seconds to 300 seconds. 

As such, I think we can remove the Blocking_Functional_Test from this ticket. ","28/Jul/23 12:46 AM;6389243e9e48f2b9a6157dad;Hi [~accountid:615b2dd89cdb9300726ea48e] 

I have similar case going on - where I would like to know if users get this health error they have to add this parameter in the Distsearch.conf or we need to remove it from  file. 

I can see the suggestion in splunk docs.

“Bundle replication takes longer than expected time for indexers”
[https://docs.splunk.com/Documentation/Splunk/9.0.4/ReleaseNotes/KnownIssues#Monitoring_Console_issues|https://docs.splunk.com/Documentation/Splunk/9.0.4/ReleaseNotes/KnownIssues#Monitoring_Console_issues] 

I am bit confused, could you please help me with the answer.","28/Jul/23 11:10 AM;615b2dd89cdb9300726ea48e;Hi [~accountid:6389243e9e48f2b9a6157dad] , sorry I’m not an expert in this area. Maybe you want to reach out to [~accountid:6053a3fb81b82500685ced67] 's team for suggestions. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1260914,SPL-171944,MT-PR10 Tech Debt,To Do,04/Oct/19 11:13 AM
"[PUBLIC] [WLM] Under heavy search workload, the search memory usage estimation may be higher than actual usage",SPL-177144,1348290,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Cannot Reproduce,Mateusz Ksyta,712020:15cd66a8-d0ee-4316-9f03-ca3bb9bccd38,Kashyap Jotwani,6053a6d394d7b90069f83e26,Kashyap Jotwani,6053a6d394d7b90069f83e26,26/Sep/19 6:03 PM,30/Jul/25 4:25 PM,,04/Mar/25 3:47 AM,10.0.x,7.3.0 GA (PinkiePie),8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Workload Management,,,,,0,Enterprise_FY20Q4_top10,quake_punt_ok,,,,,,"While doing WLM Cloud Use Case testing, we observed that there were search processes exceeding the cgroup memory limits for the Search Category.

This was observed on the cloud stacks:
https://wlm-cloud-quake.stg.splunkcloud.com

Particularly on indexer:
idx-i-0be727143631f08b7.wlm-cloud-quake.stg.splunkcloud.com

Test setup:
Index Cluster with 3 indexers, 1SH and 1CM
2 cpu cores and 13GB RAM on indexers
Search Category: 65% memory usage

Scheduled searches: 100+
Cron: 1-2 mins
base_max_searches=12
max_searches_per_cpu=20
Total concurrent searches that can be run = (12 + 8*20) = 86

Forwarders are continuously sending data to the indexers and scheduler saved searches are running on a cron.
2 HWF are oneshotting 10TB files and forwarding data
100GB * 100 times oneshot with 2 HWFs 

{code}
[STG]01:01:28 root@idx-i-0be727143631f08b7 /sys/fs/cgroup/memory/system.slice/splunk.service/search # cat memory.limit_in_bytes
9398910976
{code}
9398 *MB*

Swap Space is disabled
{code}
[STG]01:02:06 root@idx-i-0be727143631f08b7 /sys/fs/cgroup/memory/system.slice/splunk.service/search # cat /proc/meminfo
MemTotal:       15657096 kB
MemFree:          176132 kB
MemAvailable:    3745688 kB
Buffers:          173332 kB
Cached:          3092744 kB
SwapCached:            0 kB
Active:         13053384 kB
Inactive:        1368888 kB
Active(anon):   11209260 kB
Inactive(anon):   109016 kB
Active(file):    1844124 kB
Inactive(file):  1259872 kB
Unevictable:        7004 kB
Mlocked:            7036 kB
SwapTotal:             0 kB
SwapFree:              0 kB
{code}

As seen from the screenshot below, the searches on indexer {{idx-i-0be727143631f08b7.wlm-cloud-quake.stg.splunkcloud.com}} are using ~11000+ MB continuously even though the limit for search category cgroup is ~9300MB

 !Screen Shot 2019-09-26 at 5.57.01 PM.png|! 

{code}
[general]
enabled = true
ingest_pool = ingest_pool
default_pool = default_search_pool
workload_pool_base_dir_name = splunk

[workload_category:ingest]
cpu_weight = 20
mem_weight = 100

[workload_category:search]
cpu_weight = 75
mem_weight = 65

[workload_category:misc]
cpu_weight = 5
mem_weight = 5

[workload_pool:default_search_pool]
cpu_weight = 80
mem_weight = 100
category = search
default_category_pool = 1

[workload_pool:vip_search_pool]
cpu_weight = 20
mem_weight = 100
category = search
default_category_pool = 0

[workload_pool:ingest_pool]
cpu_weight = 100
mem_weight = 100
category = ingest
default_category_pool = 1

[workload_pool:misc_pool]
cpu_weight = 100
mem_weight = 100
category = misc
default_category_pool = 1
{code}

{code}
           ├─31977 [splunkd pid=32023] [search-launcher] [process-runner]
           ├─31979 [splunkd pid=32023] [search-launcher] [process-runner]
           ├─32023 splunkd --under-systemd --systemd-delegate=yes -p 8089 _internal_launch_under_systemd
           ├─32068 [splunkd pid=32023] splunkd --under-systemd --systemd-delegate=yes -p 8089 _internal_launch_under_systemd [process-runner
           ├─32151 [splunkd pid=32023] [search-launcher] [process-runner]
           ├─32211 /opt/splunk/bin/splunkd instrument-resource-usage -p 8089
           ├─32293 [splunkd pid=32023] search --id=remote_sh-i-0fda5bd6ed4f16991.wlm-cloud-quake.stg.splunkcloud.com_scheduler__admin__search__RMD5726b94f313332e62_at_1569545760_893 --maxbuckets=0 --ttl=60 --maxout=0 --maxtime=0 --lookups=1 --streaming --sidtype=normal --outCsv=true --acceptSrsLevel=1 --user=admin --pro --roles=admin:power:use
           ├─32299 [splunkd pid=32023] [search-launcher] [process-runner]
           ├─32377 [splunkd pid=32023] search --id=remote_sh-i-0fda5bd6ed4f16991.wlm-cloud-quake.stg.splunkcloud.com_scheduler__admin__search__RMD5764156635d848e5e_at_1569545760_894 --maxbuckets=0 --ttl=60 --maxout=0 --maxtime=0 --lookups=1 --streaming --sidtype=normal --outCsv=true --acceptSrsLevel=1 --user=admin --pro --roles=admin:power:use
           ├─32380 [splunkd pid=32023] search --id=remote_sh-i-0fda5bd6ed4f16991.wlm-cloud-quake.stg.splunkcloud.com_scheduler__admin__search__RMD568c34b212a016839_at_1569545760_895 --maxbuckets=0 --ttl=60 --maxout=0 --maxtime=0 --lookups=1 --streaming --sidtype=normal --outCsv=true --acceptSrsLevel=1 --user=admin --pro --roles=admin:power:use
           ├─32390 [splunkd pid=32023] [search-launcher] [process-runner]
           ├─32394 [splunkd pid=32023] [search-launcher] [process-runner]
           ├─32577 [splunkd pid=32023] search --id=remote_sh-i-0fda5bd6ed4f16991.wlm-cloud-quake.stg.splunkcloud.com_scheduler__admin__search__RMD59260984a90adc0e2_at_1569545760_896 --maxbuckets=0 --ttl=60 --maxout=0 --maxtime=0 --lookups=1 --streaming --sidtype=normal --outCsv=true --acceptSrsLevel=1 --user=admin --pro --roles=admin:power:use
           └─32578 [splunkd pid=32023] [search-launcher] [process-runner]

Sep 27 00:56:44 idx-i-0be727143631f08b7 python2.7[30288]: cloud_apps 2019-09-27 00:56:44,252 +0000, level=INFO, pid=30288, tid=140120701110016, comp=router, loc=router:148, appNames=""['system', 'splunk_datasets_addon', 'user-prefs', 'splunk_instrumentation', 'introspection_generator_addon', '100-whisper', 'splunk_gdi', 'launcher', 'splunk_httpinput', 'splunk_internal_metrics', 'sample_app', 'alert_webhook', 'search', '100-whisper-common', 'legacy', 'cloud_administration', 'splunk_metrics_workspace', '100-whisper-indexer', 'alert_logevent', 'learned', 'dmc', 'appsbrowser', '100-s2-config']"", status=""success"", propsConfs=""[]"", action=""findPropsConfsWithTouchFile""
Sep 27 00:56:44 idx-i-0be727143631f08b7 python2.7[30288]: cloud_apps 2019-09-27 00:56:44,252 +0000, level=INFO, pid=30288, tid=140120701110016, comp=router, loc=router:163, status=""noTouchFile"", action=""rePropagatePropsConfs""
Sep 27 00:56:44 idx-i-0be727143631f08b7 python2.7[30288]: cloud_apps 2019-09-27 00:56:44,252 +0000, level=INFO, pid=30288, tid=140120701110016, comp=__main__, loc=sh_sourcetypes_monitor:48, status=""success"", action=""main""
Sep 27 00:57:43 idx-i-0be727143631f08b7 python2.7[443]: cloud_apps 2019-09-27 00:57:43,679 +0000, level=INFO, pid=443, tid=140242849216256, comp=__main__, loc=sh_sourcetypes_monitor:24, status=""run"", action=""main""
Sep 27 00:57:43 idx-i-0be727143631f08b7 python2.7[443]: cloud_apps 2019-09-27 00:57:43,815 +0000, level=INFO, pid=443, tid=140242849216256, comp=router, loc=router:156, status=""run"", action=""rePropagatePropsConfs""
Sep 27 00:57:43 idx-i-0be727143631f08b7 python2.7[443]: cloud_apps 2019-09-27 00:57:43,819 +0000, level=INFO, pid=443, tid=140242849216256, comp=router, loc=router:123, status=""run"", action=""findPropsConfsWithTouchFile""
Sep 27 00:57:43 idx-i-0be727143631f08b7 python2.7[443]: cloud_apps 2019-09-27 00:57:43,823 +0000, level=INFO, pid=443, tid=140242849216256, comp=router, loc=router:78, action=""getAllAppDirName"", appNames=""['system', 'splunk_datasets_addon', 'user-prefs', 'splunk_instrumentation', 'introspection_generator_addon', '100-whisper', 'splunk_gdi', 'launcher', 'splunk_httpinput', 'splunk_internal_metrics', 'sample_app', 'alert_webhook', 'search', '100-whisper-common', 'legacy', 'cloud_administration', 'splunk_metrics_workspace', '100-whisper-indexer', 'alert_logevent', 'learned', 'dmc', 'appsbrowser', '100-s2-config']""
Sep 27 00:57:43 idx-i-0be727143631f08b7 python2.7[443]: cloud_apps 2019-09-27 00:57:43,824 +0000, level=INFO, pid=443, tid=140242849216256, comp=router, loc=router:148, propsConfs=""[]"", status=""success"", action=""findPropsConfsWithTouchFile"", appNames=""['system', 'splunk_datasets_addon', 'user-prefs', 'splunk_instrumentation', 'introspection_generator_addon', '100-whisper', 'splunk_gdi', 'launcher', 'splunk_httpinput', 'splunk_internal_metrics', 'sample_app', 'alert_webhook', 'search', '100-whisper-common', 'legacy', 'cloud_administration', 'splunk_metrics_workspace', '100-whisper-indexer', 'alert_logevent', 'learned', 'dmc', 'appsbrowser', '100-s2-config']""
Sep 27 00:57:43 idx-i-0be727143631f08b7 python2.7[443]: cloud_apps 2019-09-27 00:57:43,827 +0000, level=INFO, pid=443, tid=140242849216256, comp=router, loc=router:163, status=""noTouchFile"", action=""rePropagatePropsConfs""
Sep 27 00:57:43 idx-i-0be727143631f08b7 python2.7[443]: cloud_apps 2019-09-27 00:57:43,831 +0000, level=INFO, pid=443, tid=140242849216256, comp=__main__, loc=sh_sourcetypes_monitor:48, status=""success"", action=""main""
{code}


CM search query:
{code}
index=""_introspection"" host=idx-i* sourcetype=splunk_resource_usage component=PerProcess data.process_type=""search"" 
| eval args = 'data.args' 
| eval pid = 'data.pid' 
| eval ppid = 'data.ppid' 
| eval elapsed = 'data.elapsed' 
| eval mem_used = 'data.mem_used' 
| eval mem = 'data.mem' 
| eval pct_memory = 'data.pct_memory' 
| eval pct_cpu = 'data.pct_cpu' 
| eval sid = 'data.search_props.sid' 
| eval app = 'data.search_props.app' 
| eval label = 'data.search_props.label' 
| eval type = 'data.search_props.type' 
| eval mode = 'data.search_props.mode' 
| eval user = 'data.search_props.user' 
| eval role = 'data.search_props.role' 
| eval label = if(isnotnull('data.search_props.label'), 'data.search_props.label', """") 
| eval provenance = if(isnotnull('data.search_props.provenance'), 'data.search_props.provenance', ""unknown"") 
| eval search_head = case(isnotnull('data.search_props.search_head') AND 'data.search_props.role' == ""peer"", 'data.search_props.search_head', isnull('data.search_props.search_head') AND 'data.search_props.role' == ""head"", ""_self"", isnull('data.search_props.search_head') AND 'data.search_props.role' == ""peer"", ""_unknown"") 
| bin _time span=10s 
| eval process=process."" "".args 
| stats latest(mem_used) AS mem_used_dedup by _time, data.process_type, pid, host 
| stats sum(mem_used_dedup) AS sum_mem_used by _time, data.process_type, host
| timechart minspan=10s Perc90(sum_mem_used) AS ""Perc90 of resource usage"" by host 
| eval search_category_limit = 9081 
| eval max_memory = 14000
{code}

",,Aaron Zhang,Andrew Brown,Bhavin Thaker,David Choi,Hongxun Liu,Justin Moore (C),Kashyap Jotwani,Li Zhao,Mateusz Ksyta,Megha Lakshminarayan,Octavio Di Sciullo,Shalabh Goyal,Steven Roback,Suketu Shah,User known,zora zhou,,,,,,,,,,,,,,,,,,,,5a1f2da6c2f0444e6979d7cd,557058:37b4518c-e757-440f-87b7-181f5f425e80,6053a4ef60d39e006f808dd6,5c9a111eba8bf7223028aaef,6053a3fb81b82500685ced67,6053a6d0f180c300675e87d4,6053a6d394d7b90069f83e26,6053a24e66c879006838534d,712020:15cd66a8-d0ee-4316-9f03-ca3bb9bccd38,6053c4e1686bf50070470cd5,557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e,6053a60cf180c300675e7e8b,557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc,6053a66586b0dd007188092d,unknown,557058:39d2845b-c454-4bc4-a5e0-f42521a819af,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,SPL-177326,,,,,,,,,,,,,,,,,,,,,,,,,,,26/Sep/19 5:57 PM;kjotwani;Screen Shot 2019-09-26 at 5.57.01 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/1081248,26/Sep/19 8:51 PM;kjotwani;max_mem_usage_bytes_idx-i-013f7ee9456976ea2.wlm-cloud-quake.stg.splunkcloud.com;https://splunk.atlassian.net/rest/api/3/attachment/content/1081617,26/Sep/19 8:51 PM;kjotwani;max_mem_usage_bytes_idx-i-0608523a1ef83cf40.wlm-cloud-quake.stg.splunkcloud.com;https://splunk.atlassian.net/rest/api/3/attachment/content/1081618,26/Sep/19 8:51 PM;kjotwani;max_mem_usage_bytes_idx-i-0be727143631f08b7.wlm-cloud-quake.stg.splunkcloud.com;https://splunk.atlassian.net/rest/api/3/attachment/content/1081616,26/Sep/19 9:19 PM;kjotwani;usage_in_bytes_idx-i-013f7ee9456976ea2.wlm-cloud-quake.stg.splunkcloud.com;https://splunk.atlassian.net/rest/api/3/attachment/content/1081659,26/Sep/19 9:19 PM;kjotwani;usage_in_bytes_idx-i-0608523a1ef83cf40.wlm-cloud-quake.stg.splunkcloud.com;https://splunk.atlassian.net/rest/api/3/attachment/content/1081660,26/Sep/19 9:19 PM;kjotwani;usage_in_bytes_idx-i-0be727143631f08b7.wlm-cloud-quake.stg.splunkcloud.com;https://splunk.atlassian.net/rest/api/3/attachment/content/1081658,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5bf47f4,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,36.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Splunk Cloud,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,155952000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Wed Oct 14 23:44:06 UTC 2020,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,WLM,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,7.0,36.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),hliu(hliu),jumoore(jumoore),kjotwani(kjotwani),lzhao(lzhao),9981794a-6150-41b2-a27e-61ce9a2112e0(9981794a-6150-41b2-a27e-61ce9a2112e0),odisciullo(odisciullo),sgoyal(sgoyal),sroback(sroback),suketus(suketus),yuwang(ywang2),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|if3uf3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mateusz Ksyta,712020:15cd66a8-d0ee-4316-9f03-ca3bb9bccd38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ALBIANT - IT,3504525,012400000005WzMAAU,P2,No,5005a00003134ZgAAI,,Closed,Unresolved,Standard,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,WLM-CiscoFY25Q3-S3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,"• For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

• For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

• Describe the steps taken to perform analysis and provide evidence to support the problem statement
. If available, provide the events that match the timeline with supporting logs.

• Provide the logs and accompanying data with reference to the confluence page Filing UI/Server Bugs, and all the relevant documentation based on your analysis to support your observation and theories.

• In your own words, describe the area affected based on your analysis.  Affected area could be component or issues related to memory, performance or authentication etc. Please specify.

Steps to Reproduce:
.If the issue is reproducible,provide the steps.
Is this issue reproducible on latest maintenance version or latest release?

. If available, will customer upgrade to fixed version?
If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,2019-09-27 01:10:39.806,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_1_*:*_32976000379_*|*_3_*:*_3_*:*_181454032_*|*_10001_*:*_2_*:*_136810297557_*|*_10039_*:*_1_*:*_9175738,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Mateusz Ksyta,,,,,,,,,,,,,,,2019-09-27 01:10:39.806,2019-09-27 01:10:39.806,,,,,,Mateusz Ksyta,712020:15cd66a8-d0ee-4316-9f03-ca3bb9bccd38,,,,,,"26/Sep/19 6:10 PM;6053a66586b0dd007188092d;Thanks [~accountid:6053a6d394d7b90069f83e26] for finding the issue. Looks like a perf issue.
cc [~accountid:6053a4ef60d39e006f808dd6] [~accountid:6053a60cf180c300675e7e8b]",26/Sep/19 8:01 PM;6053a60cf180c300675e7e8b;Good find. This should affect PP as well if I am not mistaken. ,"26/Sep/19 8:29 PM;6053a3fb81b82500685ced67;[~accountid:6053a6d394d7b90069f83e26], [~accountid:6053a60cf180c300675e7e8b],

This looks to me more like a dashboard issue.

The current dashboard of workload management is using introspection to calculate the memory usage of workload pool, which is just an estimation of usage, since it is not a snapshot of the workload pool (cgroup) memory usage. In introspection, we take a list of process IDs, and try to get memory information for each process, sequentially. The memory usage is changing during the process of collecting memory usage for a long list of processes.

A better way is through the cgroup memory accounting directly. 

There are two readings which are useful to us, memory.usage_in_bytes, and memory.max_usage_in_bytes. They are recording the current memory usage and max memory usage of this cgroup.

 

[~accountid:6053a6d394d7b90069f83e26], please help do some experiments, ""watching"" these two values and dump them into some file. We can then check these two values during the search running period. These two values should be within the limit we set in memory.limit_in_bytes.

 

Thanks,

 

Hongxun

 ","26/Sep/19 8:50 PM;6053a3fb81b82500685ced67;After [~accountid:6053a6d394d7b90069f83e26] collects the memory.usage_in_bytes, and we can draw the ""real"" graph of the memory usage. If that confirms our guess, un-reliable dashboard reading, we can punt this one out of quake. 

thanks,

 

Hongxun

 ","26/Sep/19 9:22 PM;6053a6d394d7b90069f83e26;[~accountid:6053a3fb81b82500685ced67]: Attached the {{memory.usage_in_bytes}} and {{memory.max_usage_in_bytes}} from each indexer.
Looks like it's at or within limits of the {{memory.memory.limit_in_bytes}}
Please double check","26/Sep/19 9:38 PM;6053a3fb81b82500685ced67;[~accountid:6053a6d394d7b90069f83e26],

Thanks for the quick collecting and posting the memory.usage_in_bytes for the indexers. I checked the usage information. They are within the limit. So in terms of the workload management feature, we are fine.

The issue helps us understand the limitation of memory usage coming from the dashboard, which is further coming from the introspection log, especially when there are a large number of processes involved. In this case, dashboard is as good as a trend indication, not an accurate snapshot reading.

I am punting this out of quake.

 

We could try to improve the memory reading for dashboard, in future release. 

 

thanks,

 

Hongxun

 

 ",26/Sep/19 9:41 PM;6053a60cf180c300675e7e8b;Thanks guys for quick triaging of this issue (y),"27/Sep/19 1:14 PM;6053a6d0f180c300675e87d4;{{Hello [~accountid:6053a3fb81b82500685ced67], [~accountid:6053a6d394d7b90069f83e26],}}
 
 
{{The below stack(s) have been launched }}{{for}} {{longer than 48 hours.  Dev stacks normally should not be running when not in use.  Please consider turning it off when not in use.  If you no longer require the stack, please let us know. If you need an exception to have a }}{{long}} {{running stack please let us know why.  Please reply within 48 hours.}}
 
{{If we haven't heard back from you and the stacks have not been shut down, we will terminate the stack.}}
 
wlm-cloud-os
wlm-cloud-quake
 
 
{{Thanks,}}
 
{{Justin}}
{{Delivery and Business Operations Team}}","27/Sep/19 5:25 PM;557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e;{quote}
The current dashboard of workload management is using introspection to calculate the memory usage of workload pool, which is just an estimation of usage, since it is not a snapshot of the workload pool (cgroup) memory usage. In introspection, we take a list of process IDs, and try to get memory information for each process, sequentially. The memory usage is changing during the process of collecting memory usage for a long list of processes.
{quote}
Alright. If we are _confident_ that is the root cause, we should really open a bug or story against our introspection implementation to improve it and capture an _actual_ snapshot of the system at a given point in time. Would you agree [~accountid:6053a3fb81b82500685ced67]?","27/Sep/19 11:36 PM;6053a3fb81b82500685ced67;[~accountid:557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e], [~accountid:6053a6d394d7b90069f83e26],

I continued the investigation today. There are some new findings regarding the big difference between cgroup accounting and introspection accounting.

The difference is mainly from how memory cgroup does the charge/accounting.

In the memory cgroup controller, we have this setting:

 
{code:java}
memory.move_charge_at_immigrate

allows moving charges associated with a task along with task migration. Charging is a way of giving a penalty to cgroups which access shared pages too often. These penalties, also called charges, are by default not moved when a task migrates from one cgroup to another. The pages allocated from the original cgroup still remain charged to it; the charge is dropped when the page is freed or reclaimed. With memory.move_charge_at_immigrate enabled, the pages associated with a task are taken from the old cgroup and charged to the new cgroup. The following example shows how to enable memory.move_charge_at_immigrate:
echo 1 > /cgroup/memory/lab1/memory.move_charge_at_immigrate

Charges are moved only when the moved task is a leader of a thread group. If there is not enough memory for the task in the destination cgroup, an attempt to reclaim memory is performed. If the reclaim is not successful, the task migration is aborted.{code}
 
 Further from the kernel documentation:
{code:java}
This feature is disabled by default. It can be enabled (and disabled again) by writing to memory.move_charge_at_immigrate of the destination cgroup. If you want to enable it: # echo (some positive value) > memory.move_charge_at_immigrate Note: Each bits of move_charge_at_immigrate has its own meaning about what type of charges should be moved. See 8.2 for details. Note: Charges are moved only when you move mm->owner, in other words, a leader of a thread group. Note: If we cannot find enough space for the task in the destination cgroup, we try to make space by reclaiming memory. Task migration may fail if we cannot make enough space. Note: It can take several seconds if you move charges much.{code}
 
 In Splunk, all new processes are created/forked by ""process runner"", which is running in ""ingest pool"". When process runner forked a search process, that search process inherits its parent's cgroup setup, which is ""ingest pool"". So the initial memory charge for the new search process happens in ""ingest pool"". We then move that search process into a search pool. Since the setting of ""memory.move_charge_at_immigrate"" is false, the initial memory charge/accounting for search process is still in ingest pool. After search process is moved into search pool, all new memory operations are accounted in search pool. Since the search pool does not charge/account for the initial memory usage for the search process, that results in the memory usage difference between ""memory.usage_in_bytes"" and introspection based memory reading.
  
 Introspection based memory reading is from ""/proc/search_pid/stat"", which is basically the RSS of the search process. All of the search processes are sharing the same ""splunkd"" binary. There are some memory shared by different search processes. Those shared memory will be repeatedly counted in introspection based memory reading, since RSS includes the share memory portion. This makes introspection based memory reading larger than the real memory usage.
  
 For cgroup memory controller, it avoids the duplicate memory page accounting. In this sense, it is more accurate than the RSS based reading. 
 Even when a search process is first forked in the ""ingest pool"", the ""ingest pool"" memory usage barely changes, since most of the pages are shared and are already loaded into memory. So the ""search process"" is not adding meaningful weight on the ingest pool at the initial/fork stage.
  
 So far, the memory cgroup based account seems more accurate in terms real memory usage in that specific workload pool (cgroup).
  
 We do have the option to add the support to for the ""memory.move_charge_at_immigrate"". We can continue to do some experiments ([~accountid:6053a6d394d7b90069f83e26]). So far, the potential performance hit to migrate the memory charge/accounting is big concern. We need performance team to help evaluate the pros and cons of adding this support. ([~accountid:6053a24e66c879006838534d])
{code:java}
Note: If we cannot find enough space for the task in the destination cgroup, we try to make space by reclaiming memory. Task migration may fail if we cannot make enough space. Note: It can take several seconds if you move charges much.{code}
 
  
 Thanks,
  
 Hongxun
  ","27/Sep/19 11:44 PM;6053a3fb81b82500685ced67;[~accountid:6053a6d394d7b90069f83e26],

Can you help do an experiment on the cloud setup? 

We can manually do this for all pools  (memory cgroup) on the indexer

echo 1 > /sys/fs/cgroup/memory/system.slice/splunk.service/pool_path/memory.move_charge_at_immigrate



Then we can run the setup again, and check the dashboard for the memory usage. See it could contain the memory usage (based on introspection) under the limit.

 

Thanks,

 

Hongxun

 ","28/Sep/19 10:38 PM;6053a3fb81b82500685ced67; 

[http://re-bamboo.sv.splunk.com/browse/OD-MBCW-485]","29/Sep/19 6:51 PM;6053a3fb81b82500685ced67;[~accountid:6053a24e66c879006838534d],

Can you help use the build at [http://re-bamboo.sv.splunk.com/browse/OD-MBCW-485] ?

The main focus is on ""any"" performance regression again the current ""release/quake"" build, in terms of data ingestion, and search throughput. 

The build I bonked above is adding ""memory accounting migration"" when we are moving a process from one pool to another pool. 

We can talk about the details offline.

 

Thanks,

 

Hongxun

 ","29/Sep/19 10:04 PM;5af5d6f3a0e6e6142d1f8592;[~accountid:6053a3fb81b82500685ced67], please expect a delay as we are in Chinese National Holiday. :)

For the memory accounting problem, I think it is not easy to get accurate number because the RSS includes shared lib, probably considering calculating USS + PSS?","29/Sep/19 10:19 PM;6053a3fb81b82500685ced67;[~accountid:5af5d6f3a0e6e6142d1f8592],

You are right. The dashboard memory usage is based on RSS, which overestimates the memory usage for the shared memory pages. In search pool, all the processes are sharing the same image, which can show a much bigger number than the real memory usage, in that pool.

When are you guys back from holiday?

thanks,

 

Hongxun

 ","29/Sep/19 11:32 PM;5af5d6f3a0e6e6142d1f8592;[~accountid:6053a3fb81b82500685ced67], just quickly synced with [~accountid:6053a24e66c879006838534d], he could help to validate the performance. However, could you please show me the concern on this change?

My quick thoughts on this:
 # Ingest category usually is set to 100% memory, so accounting migration may not do better or worse to ingest.
 # We may not have monitor rule based on memory consumption, so the monitored statistics seems not to be critical for search abort/move feature.","29/Sep/19 11:40 PM;6053a3fb81b82500685ced67;[~accountid:5af5d6f3a0e6e6142d1f8592], [~accountid:6053a24e66c879006838534d],

The concern is the accuracy of the cgroup memory accounting.

For example, when we move a search p1 from pool1 to pool2. The memory usage of p1 is first accounted in pool1. When p1 is moved into pool2, the initial memory allocation of p1 is still account in pool1, not accounted in pool2. pool2 will only start to account memory usage of p1 when p1 starts to allocate/release memory.

 

by setting ""memory.move_charge_at_immigrate"", we are forcing cgroup to re-account all memory usage of p1 to pool2. This is ""very expensive"" operation. That is why we need your help to evaluate the performance impact.

 

Thanks,

 

Hongxun

 ","30/Sep/19 6:21 AM;6053a24e66c879006838534d;[~accountid:6053a3fb81b82500685ced67],  I can run  workload management regression on your build.  

As you said  ' by setting ""memory.move_charge_at_immigrate"" ,  we are forcing cgroup to re-account all memory usage of p1 to pool2 .  ' , is that meaning we need to set this env parameter and enable the workload rule to move long run search by search time ? 

Offline talk would be helpful, ping me on slack when you got time. 

Thanks , Li ","01/Oct/19 1:23 AM;6053a24e66c879006838534d;[~accountid:6053a3fb81b82500685ced67] , I compared with your build and quake , and looks there's no regression

Conclusion:
As we expected, compared to quake release, the ingest and search performance are degradation,  search process memory usage is grown up .
Because of the change is smaller than 3% we think there's no regression issue.

1. Search count  is reduced 3% (worse).  
2. Search runtime is increased %2  (worse) .
3. Ingest throughput is reduced %1 (worse) . 
4. Search process memory usage is increased +1% (worse).


Details could be found here:
https://splunk.atlassian.net/wiki/display/~lzhao/SPL-177144+validation%3A+Under+Heavy+Search+Workloads%2C+Search+processes+are+exceeding+the+Search+Category+Memory+limits",01/Oct/19 1:23 AM;6053a24e66c879006838534d;If here's any more tests needed please let me know.,01/Oct/19 8:49 AM;6053a60cf180c300675e7e8b;We will need to include this in release notes.,"01/Oct/19 9:16 AM;5af5d6f3a0e6e6142d1f8592;[~accountid:6053a24e66c879006838534d], thanks for the tests. Do we have a valid way to make sure if the enablement of {color:#4c9aff}memory.move_charge_at_immigrate{color} take effect?

cc [~accountid:6053a3fb81b82500685ced67]","01/Oct/19 9:38 AM;6053a3fb81b82500685ced67;[~accountid:5af5d6f3a0e6e6142d1f8592], [~accountid:6053a24e66c879006838534d],

I did some manual testing, moving around some processes, with and without enabling memory.move_charge_at_immigrate.

When the setting is enabled, the target pool will increase a lot when we move a big memory print process into it. Otherwise, the target pool will not show memory usage increasing (memory.usage_in_bytes).

Another effect is, moving around the process is much slower when enabling this setting.

 

thanks,

 

Hongxun

 ","01/Oct/19 9:43 AM;557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e;{quote}
We will need to include this in release notes.
{quote}

In addition to that, we should probably annotate any Monitoring Console dashboards that show the aggregated memory usage of a pool against the limit of that pool, so admins can be informed that it's possible and normal for the aggregated usage to go slightly above the set limit.","01/Oct/19 10:37 AM;6053a3fb81b82500685ced67;[~accountid:557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e],

You are right. the aggregation usage in the dashboard is treating every search process as independent, which in fact is not. They share the same binary. Some memory are shared across them. By simply adding everyone's RSS together, we are overestimating the memory usage in that pool.

Thanks,

 

Hongxun

 ","01/Oct/19 12:51 PM;6053a3fb81b82500685ced67;[~accountid:557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc],

Can we add some documentation in the workload management section, mentioning the memory usage information of workload pools in dashboard (monitoring console) is an estimation, which is the simple addition of all processes in the pool? So the memory shared among the processes are repeatedly counted, which tends to overestimate the memory usage in that workload pool.

 

Thanks,

 

Hongxun

 ","01/Oct/19 7:05 PM;557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc;hi [~accountid:6053a3fb81b82500685ced67], 

I've added a note to the WLM docs based on your comment. See:

[https://docs.splunk.com/Documentation/Splunk/8.0.0/Workloads/Monitorworkloadmanagement#CPU_and_memory_usage]

Please review. Thanks!","02/Oct/19 3:20 PM;6053a3fb81b82500685ced67;[~accountid:557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e],

Please also help review the document written by [~accountid:557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc].

[https://docs.splunk.com/Documentation/Splunk/8.0.0/Workloads/Monitorworkloadmanagement#CPU_and_memory_usage]

Thanks,

 

Hongxun

 ","02/Oct/19 6:01 PM;557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e;The addendum looks good to me. 

Again, I would suggest adding this information in the dashboard itself, under the panel showing memory usage per pool.",02/Oct/19 6:16 PM;6053a60cf180c300675e7e8b;[~accountid:557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e] we should rather fix it in next release (post quake). We are not making any changes to Quake anymore unless blocker/critical bug.,02/Oct/19 6:18 PM;557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e;That's OK.,14/Oct/20 1:49 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;I don't see any signs that we've fixed this in a Cloud First release or in Rarity. [~accountid:6053a60cf180c300675e7e8b] should I carry this forward as a known issue in the Rarity 8.1.0 release notes? ,14/Oct/20 2:36 PM;6053a60cf180c300675e7e8b;[~accountid:6053a3fb81b82500685ced67] Do you know how much can this overestimation be? Is it 10-20% or can be be 2x?,14/Oct/20 2:39 PM;6053a60cf180c300675e7e8b;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] No this is a carry forward. Can you please check the language for public note?,"14/Oct/20 3:36 PM;6053a3fb81b82500685ced67;[~accountid:6053a60cf180c300675e7e8b],
The overestimation can be around 10% to 20%. It should not amount to 2x.

thanks,

Hongxun
","14/Oct/20 4:44 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;Thank you, gentlemen. The wording of the summary field looks fine to me. Based on another Rarity conversation I'm involved in, I'm sure that legal won't want us to put any percentages into writing unless they are backed by hard, repeatable data. Being non-specific is probably fine for the known issues list. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1260914,SPL-171944,MT-PR10 Tech Debt,Done,04/Mar/25 3:47 AM
[PUBLIC] [WLM] Workload management fails to enable for addition of a pool with 1% cpu and 1% memory,SPL-177008,1345624,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,,Hongxun Liu,6053a3fb81b82500685ced67,zora zhou,557058:39d2845b-c454-4bc4-a5e0-f42521a819af,zora zhou,557058:39d2845b-c454-4bc4-a5e0-f42521a819af,25/Sep/19 12:07 AM,30/Jul/25 4:25 PM,,,10.0.x,7.3.0 GA (PinkiePie),8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Workload Management,,,,,0,quake_punt_ok,systest_quake,WLM,,,,,"1. setup a single instance 
2. enable systemd
3. create many pools, enable wlm successfully
{noformat}
[workload_pool:default_search_pool]
category = search
cpu_weight = 100
default_category_pool = 1
mem_weight = 100

[workload_pool:default_ingest_pool]
category = ingest
cpu_weight = 100
default_category_pool = 1
mem_weight = 100

[workload_pool:report_acceleration_pool1]
category = search
cpu_weight = 100
default_category_pool = 0
mem_weight = 100

[workload_pool:report_acceleration_pool2]
category = search
cpu_weight = 100
default_category_pool = 0
mem_weight = 100

[workload_pool:datamodel_acceleration_pool1]
category = search
cpu_weight = 100
default_category_pool = 0
mem_weight = 100

[workload_pool:datamodel_acceleration_pool2]
category = search
cpu_weight = 100
default_category_pool = 0
mem_weight = 100

[workload_pool:summary_index_pool1]
category = search
cpu_weight = 100
default_category_pool = 0
mem_weight = 100

[workload_pool:summary_index_pool2]
category = search
cpu_weight = 100
default_category_pool = 0
mem_weight = 100

[workload_pool:search_time_range_pool1]
category = search
cpu_weight = 100
default_category_pool = 0
mem_weight = 100

[workload_pool:search_time_range_pool2]
category = search
cpu_weight = 100
default_category_pool = 0
mem_weight = 100

[workload_pool:user_pool1]
category = search
cpu_weight = 100
default_category_pool = 0
mem_weight = 100

[workload_pool:user_pool2]
category = search
cpu_weight = 100
default_category_pool = 0
mem_weight = 100

[workload_pool:system_user_pool1]
category = search
cpu_weight = 100
default_category_pool = 0
mem_weight = 100

[workload_pool:system_user_pool2]
category = search
cpu_weight = 100
default_category_pool = 0
mem_weight = 100

[general]
enabled = 1
{noformat}
4. add a pool whose cpu and memory is 1%, the wlm failed to enable.
{noformat}
Failed to setup workload pools in workload configuration. Check the status of workload management preflight checks for additional information.
{noformat}
5. delete the pool, the wlm can be enabled successfully..

Env to reproduce: 
https://sv3-orca-0313e0b2.sv.splunk.com:8000/en-US/manager/system/workload_management. (admin/changed)",,Bhavin Thaker,Hongxun Liu,Jo Hornsby,Kashyap Jotwani,Shalabh Goyal,Vera Qin [X],Zhenghui Xie,zora zhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a4ef60d39e006f808dd6,6053a3fb81b82500685ced67,557058:f87e2651-5132-4aed-8b96-f608ee969435,6053a6d394d7b90069f83e26,6053a60cf180c300675e7e8b,613259952aa2800068a4a8cf,6053b6be695c3900707b2c98,557058:39d2845b-c454-4bc4-a5e0-f42521a819af,,,,,,,,,,,,,,,,,,,,,,,,,,,,28800,28800,,0%,28800,28800,,Global,,,,,,,,,,,,,SPL-177009,,,,,,,SPL-176710,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3399060e,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,189216000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,Thu Sep 26 00:19:30 UTC 2019,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise Services,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,bthaker(bthaker),hliu(hliu),zzhou(zzhou),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,zora zhou,557058:39d2845b-c454-4bc4-a5e0-f42521a819af,,,,,,,,,,,,,,,,,,0|i5dbbb:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2019-09-26 00:17:28.028,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019-09-26 00:17:28.028,2019-09-26 00:17:28.028,,,,,,Hongxun Liu,6053a3fb81b82500685ced67,,,,,,"25/Sep/19 5:17 PM;6053a4ef60d39e006f808dd6; 

Based on Slack conversation with [~accountid:6053a3fb81b82500685ced67], here is a summary of justification for the punt2Rarity:

Q1) Does the error happen only for 1% setting? Yes
 Q2) Does 2% and above setting work? Yes
 Q3) Is this a regression from earlier releases? NoQ4) Should we enable this issue for addition to Release Notes? Yes

 

Updated JIRA issue: Document as: Include in Release Notes.

 ","25/Sep/19 5:19 PM;6053a3fb81b82500685ced67;This is a boundary checking issue.

The total CPU weight is more than 1000. When we give a pool with 1 CPU weight, our internal calculation is treating it as 0 (zero). Then the workload management verification will fail since it sees we are trying to set a 0 value to CPU shares. For this setup, it will only happen for CPU weight 1. If we give it CPU weight 2, it will pass.

this is not a regression. it is also in previous release 7.3.x.

 We can document this as a known issue.

The fix for this issue to relax the checking, and make 1 CPU weight work when we have many pools with total CPU weight more than 1000.

 

 

 

thanks,

 

Hongxun

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1260914,SPL-171944,MT-PR10 Tech Debt,To Do,25/Sep/19 12:07 AM
[PUBLIC] Multiple SH Clustering with single deployer can't use datamodel summary sharing,SPL-176812,1342138,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,,Sophia Zhu,5cdb19327aecfb0fea319d56,Sophia Zhu,5cdb19327aecfb0fea319d56,Sophia Zhu,5cdb19327aecfb0fea319d56,20/Sep/19 10:42 AM,30/Jul/25 4:24 PM,,,10.0.x,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search - Datamodel Acceleration,,,,,1,StructuredSearch_Reviewed,,,,,,,"In a system with multiple search head clusters but a single deployer, all clusters will be given the same knowledge object configurations. This means you can't designate a cluster as the summary owner/source, because the acceleration.source_guid setting will also get deployed to itself and currently there is no logic to check/ignore the setting if its referencing its own ID. This is not an issue where there is a separate deployer per cluster, or for single search head instances.",,James Ervin,Ketan Pawar,Matthew Ness,Shane Evans,Sophia Zhu,User known,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:ab939132-f05e-4f5f-8bec-6939a8133dcc,557058:84a600ad-55da-4237-b002-71f4acb2ec9d,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,5cd34132b588780fd3da382f,5cdb19327aecfb0fea319d56,unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@233186c6,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,122860800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Tue Nov 02 02:11:55 UTC 2021,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,StructuredSearch,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,jervin(jervin),mness(mness),sevans(sevans),szhu(szhu),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i54ia7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,False,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,0.0,,,,"• For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

• For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

• Describe the steps taken to perform analysis and provide evidence to support the problem statement
. If available, provide the events that match the timeline with supporting logs.

• Provide the logs and accompanying data with reference to the confluence page Filing UI/Server Bugs, and all the relevant documentation based on your analysis to support your observation and theories.

• In your own words, describe the area affected based on your analysis.  Affected area could be component or issues related to memory, performance or authentication etc. Please specify.

Steps to Reproduce:
.If the issue is reproducible,provide the steps.
Is this issue reproducible on latest maintenance version or latest release?

. If available, will customer upgrade to fixed version?
If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,2019-09-20 20:02:04.241,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019-09-20 20:02:04.241,2019-09-20 20:02:04.241,,,,,,,,,,,,,20/Sep/19 1:02 PM;557058:89ace195-adc2-4f25-9924-ded7c8f10bb6;Now this sounds like a Known Issue! [~accountid:5cd34132b588780fd3da382f] what say you? ,22/Sep/19 11:43 AM;5cd34132b588780fd3da382f;Yes. This should go in the release notes as a limitation.,"01/Sep/21 11:45 AM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;[~accountid:5cd34132b588780fd3da382f] , [~accountid:6053a2e3686bf50070459604] - something for Structured Search?","01/Nov/21 7:11 PM;5cdb19327aecfb0fea319d56;If I recall correctly, this ticket was filed for a last minute issue that was uncovered by QA so there wasn't time to fix it (although presumably it hasn't been a problem in the last two years). We could either document it as a limitation [~accountid:557058:89ace195-adc2-4f25-9924-ded7c8f10bb6], or I can see if I can just the issue (essentially disable the feature if source_guid is the same as the machine's guid- I don't think this should be terribly difficult to do).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2393303,SPL-220931,DMA improvements backlog,To Do,20/Sep/19 10:42 AM
[PUBLIC] Offline rebuild of unsearchable bucket may lead to stale information in dbinspect searches,SPL-176514,1338261,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,,,John Nguyen,557058:fdb81fd4-ec55-42da-8e7d-863a3555dd8a,John Nguyen,557058:fdb81fd4-ec55-42da-8e7d-863a3555dd8a,16/Sep/19 1:08 PM,30/Jul/25 4:26 PM,,,10.0.x,7.2.0 GA (OrangeSwirl),7.3.0 GA (PinkiePie),8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Indexing,S2,,,,0,,,,,,,,"It appears offline rebuild of a unsearchable bucket does not update the .bucketManifest. At startup, the DDM uses these values and if S2 is enabled for migration, we'll upload the bucket with invalid manifest. -This is problematic (currently) in S2 as it fails bootstrapping validation.- Misspoke about failing bootstrapping. Bootstrapping works. Download works. Its just that dbinspect is incorrect.

https://splunk.atlassian.net/browse/CO-132896

Repro:
 1) Index data to splunk
 2) roll bucket
 3) shutdown splunk
 4) delete search files for the bucket (everything except the journal.gz)
 5) delete entry in .bucketManifest
 6) start splunk. This should rescan & rebuild the .bucketManifest based on files in db
 7) Confirm .bucketManifest shows 0 values for raw size, event count, host count, source count, source type count, but valid journal size.
 ie
{noformat}
""_internal~0~57F238A8-ED60-4A18-A5F9-0FEF09D72EFB"",""db_1568310518_1568299727_0"",0,0,0,0,0,45056,1568663140,0,"""",0,36727
{noformat}
8) stop splunk
 9) rebuild bucket
 10) start splunk
 11) Confirm .bucketManifest still shows values for unsearchable bucket (same as before)
 ie
{noformat}
""_internal~0~57F238A8-ED60-4A18-A5F9-0FEF09D72EFB"",""db_1568310518_1568299727_0"",0,0,0,0,0,45056,1568663140,0,"""",0,36727
{noformat}",,Bharath Aleti,David Choi,Gaurav Gupta,John Nguyen,Mahek Chheda,Rajpal Bal,Sai Sajja,Stephen Goodman,Tommy Nguyen,Yuan Xu,,,,,,,,,,,,,,,,,,,,,,,,,,557058:b1e398c5-b004-4a33-8152-18daa36cda4b,5c9a111eba8bf7223028aaef,6053ac5ae394c30069cb0d7f,557058:fdb81fd4-ec55-42da-8e7d-863a3555dd8a,6053a58f60d39e006f8094e7,557058:f9466031-49a3-4131-8cb7-7d482a9eb937,5f57bd53d2c77e0075a4cc40,5d0d153711233f0c4ca0aa17,6053b8c9695c3900707b4296,6053b72645a3bb0068176b30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CO-132896,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@63d36845,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,189388800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Mon Sep 23 21:09:57 UTC 2019,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NoahIndexing,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,jnguyen(jnguyen),ssajja(ssajja),sgoodman(sgoodman),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i5c5w7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,0.0,,,,"• For a UI bug, did you file this JIRA following the confluence page ""Filing UI Bugs - 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs""?

• For a server bug, did you file this JIRA following the confluence page ""Filing Server Bugs -
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs""?

• Describe the steps taken to perform analysis and provide evidence to support the problem statement
. If available, provide the events that match the timeline with supporting logs.

• Provide the logs and accompanying data with reference to the confluence page Filing UI/Server Bugs, and all the relevant documentation based on your analysis to support your observation and theories.

• In your own words, describe the area affected based on your analysis.  Affected area could be component or issues related to memory, performance or authentication etc. Please specify.

Steps to Reproduce:
.If the issue is reproducible,provide the steps.
Is this issue reproducible on latest maintenance version or latest release?

. If available, will customer upgrade to fixed version?
If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,2019-09-20 18:17:55.083,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019-09-20 18:17:55.083,2019-09-20 18:17:55.083,,,,,,,,,,,,,"17/Sep/19 1:24 PM;557058:fdb81fd4-ec55-42da-8e7d-863a3555dd8a;If offline rebuild is performed, while splunk is still stopped, remve the entry from the .bucketManifest file. At splunk startup, it will rebuild the .bucketManifest from directory contents. This must be correct before enabling S2 (in migration scenario).

Confirm non-zero values before proceeding.","20/Sep/19 11:09 AM;557058:fdb81fd4-ec55-42da-8e7d-863a3555dd8a;[~accountid:557058:b1e398c5-b004-4a33-8152-18daa36cda4b]/[~accountid:5d0d153711233f0c4ca0aa17]
We should document the potential issue of migrating buckets rebuilt from thawed buckets.

See  CO-132896 ","20/Sep/19 11:17 AM;5d0d153711233f0c4ca0aa17;I've updated the JIRA with a ""Document As: Include in Release Notes"" field, so this will go into the known issues section of the release notes until it gets fixed or otherwise resolved.","20/Sep/19 12:22 PM;5d0d153711233f0c4ca0aa17;[~accountid:557058:fdb81fd4-ec55-42da-8e7d-863a3555dd8a]: I'm not completely clear whether this point is relevant to the issue at hand, but we don't support thawing buckets into SmartStore indexes: https://docs.splunk.com/Documentation/Splunk/7.3.1/Indexer/SmartStoredataretention#Thawing_data_and_SmartStore","20/Sep/19 4:12 PM;557058:fdb81fd4-ec55-42da-8e7d-863a3555dd8a;[~accountid:5d0d153711233f0c4ca0aa17] - This isn't thawing buckets into an S2 index. This is thawing buckets into a non-S2 index and then later doing a migration to S2.
","20/Sep/19 5:14 PM;5d0d153711233f0c4ca0aa17;Do we need to change the summary for this JIRA so that the implications are more evident to customers, as it's the summary that appears in the known issues secction of the release notes?","23/Sep/19 1:43 PM;5f57bd53d2c77e0075a4cc40;Perhaps need to do the below:
1. prevent customer from triggering the rebuild in offline mode
2. document the same","23/Sep/19 2:09 PM;5d0d153711233f0c4ca0aa17;As this is listed as a bug, the best approach in terms of documentation would be to update the summary field (aka, the title of the JIRA) to make that point clear in the known issues section of the release notes 

How about if I changed the summary to: ""Cannot perform offline rebuilds of unsearchable buckets.""

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Sep/19 1:08 PM
[PUBLIC] SmartStore: Migration uploads of auto_high_volume buckets can fail indefinitely due to an XFS bug,SPL-176447,1337060,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Won't Fix,,,John Nguyen,557058:fdb81fd4-ec55-42da-8e7d-863a3555dd8a,John Nguyen,557058:fdb81fd4-ec55-42da-8e7d-863a3555dd8a,13/Sep/19 1:07 PM,30/Jul/25 4:24 PM,,05/Dec/22 10:03 AM,10.0.x,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,S2,,,,,0,,,,,,,,"Many migrations have encountered upload timeouts due to having too many concurrent uploads (and using auto_high_volume buckets). All the upload threads are contending for the same disk & network I/O, so they repeatedly fight and all fail their uploads. 

Should revisit max_concurrent_uploads and/or max_http_connections and consider if lower defaults would be sufficient for all scenarios.

Also revisit our backoff mechanism to see how it handles timeouts.

Revisit downloads as well.",,Da Xu,Igor Stojanovski,John Nguyen,Sai Sajja,Stephen Goodman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5c77260b8a38a065324ba55b,6053a8d9311e270068e37dd9,557058:fdb81fd4-ec55-42da-8e7d-863a3555dd8a,5f57bd53d2c77e0075a4cc40,5d0d153711233f0c4ca0aa17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,SPL-140351,,,,,,,,,,CO-94408,SPL-172369,SPL-165765,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7dc25a87,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,88387200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,Mon Dec 05 18:03:27 UTC 2022,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Indexing,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dxu(dxu),istojanovski(istojanovski),jnguyen(jnguyen),ssajja(ssajja),sgoodman(sgoodman),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i5c0fz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,UNUM Group,3044986,012400000005WzMAAU,P3,No,5005a00002ESRhhAAH,,Closed,Resolved - Work Around,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"• If the issue is reproducible, provide the steps and the access of the reproducible testing environment

• If available, will customer upgrade to fixed version?

• If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,,,0.0,,,,"• For a UI bug, did you file this JIRA following the confluence page ""Filing UI Bugs - 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs""?

• For a server bug, did you file this JIRA following the confluence page ""Filing Server Bugs -
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs""?

• Describe the steps taken to perform analysis and provide evidence to support the problem statement
. If available, provide the events that match the timeline with supporting logs.

• Provide the logs and accompanying data with reference to the confluence page Filing UI/Server Bugs, and all the relevant documentation based on your analysis to support your observation and theories.

• In your own words, describe the area affected based on your analysis.  Affected area could be component or issues related to memory, performance or authentication etc. Please specify.

Steps to Reproduce:
.If the issue is reproducible,provide the steps.
Is this issue reproducible on latest maintenance version or latest release?

. If available, will customer upgrade to fixed version?
If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,2019-09-14 13:09:20.555,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_100991554190_*|*_10039_*:*_1_*:*_866631508,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Before migration, lower the max_concurrent_uploads setting in server.conf to 2.

After migration, revert the setting to the default of 8.",,,,,,,,,,2019-09-14 13:09:20.555,2019-09-14 13:09:20.555,,,,,,,,,,,,,14/Sep/19 6:09 AM;5c77260b8a38a065324ba55b;we should consider adding some throttling as well. its still possible a smaller number of concurrent uploads will saturate available network bandwidth and cause other issues... see the many customer cases associated with https://splunk.atlassian.net/browse/SPL-129080 (clustering replication throttling),"23/Sep/19 2:06 PM;557058:fdb81fd4-ec55-42da-8e7d-863a3555dd8a;We may not take action regarding changing defaults, but we should probably document the workaround somewhere.

cc [~accountid:5d0d153711233f0c4ca0aa17]",23/Sep/19 2:13 PM;5d0d153711233f0c4ca0aa17;[~accountid:557058:fdb81fd4-ec55-42da-8e7d-863a3555dd8a]: What is the workaround?,23/Sep/19 3:44 PM;557058:fdb81fd4-ec55-42da-8e7d-863a3555dd8a;The workaround is to lower the max_concurrent_uploads setting in server.conf during migration if uploads are failing repeatedly due to timeout.,"21/Oct/19 1:51 PM;5f57bd53d2c77e0075a4cc40;5xx errors are retryable (with exponential backoff).
Need to verify if that is the case with RetryableClientTransaction.
If that is the case, then need to investigate why the backoffs are not helping us.","23/Sep/20 4:17 PM;5f57bd53d2c77e0075a4cc40;Hit this issue earlier due to slow disk io as a result of XFS issues.
https://splunk.atlassian.net/browse/SPL-165765?focusedCommentId=2942895&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-2942895
https://splunk.atlassian.net/browse/SPL-165765?focusedCommentId=2999863&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-2999863

This can happen due to slow disk io or network. AFAIK we have not encountered this issue since last year.","30/Nov/22 12:51 PM;6053a8d9311e270068e37dd9;[~accountid:5f57bd53d2c77e0075a4cc40] , should we close this issue, if you think it’s not worthwhile spending on this?",05/Dec/22 10:03 AM;6053a8d9311e270068e37dd9;Closing this ticket as “won’t fix” since this hasn’t been brought up as an issue more recently.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,05/Dec/22 10:03 AM
[PUBLIC] [systemd] Root unable to run splunk cli if SPLUNK_OS_USER is set,SPL-174406,1302041,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,,,Ted Xiao,6053a58c45a3bb006816a5a0,Ted Xiao,6053a58c45a3bb006816a5a0,05/Aug/19 8:08 PM,30/Jul/25 4:24 PM,,,10.0.x,7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Admin - CLI,,,,,0,cftt_reviewed,oper_bug_bash,,,,,,"Similar issue as SPL-109254 

To reproduce:
- enable boot-start as the user 'nginx'  splunkforwarder/bin/splunk help enable boot-start -user nginx -systemd-managed 1
- systemctl start SplunkForwarder
- log in to root
- splunkforwarder/bin/splunk list monitor

Splunk will throw an error stating permission denied.

I would like to run splunkforwarder as low privileged user nginx and monitor /var/log/nginx logs, it is is very inconvenient of unable to run splunk under root, for example

- switch to nginx user to run splunk command
- switch to root to restart splunk
- config does not work, has to switch to nginx user run splunk command agin
- switch to root to restrt splunk

Note: nginx does not have sudo privileges",,Chris Green,Connor Goodwolf (C) [X],Jessie Evans,Joshua Rodman,Mark Go,Mitch Blank,Mustafa Ahamed,Nithya Janarthanan [X],Priti Marappan,Sundar Vasan,Ted Xiao,,,,,,,,,,,,,,,,,,,,,,,,,604a8d29bfef95006be55e4b,6132428e98a977006b122bb3,6053b81fe394c30069cb90c5,61324b38be9e4d00697d5cc7,557058:636d227d-9e08-4111-8699-d8f94d6b20cd,6053b370e394c30069cb5cca,557058:3babe893-a658-4d4c-936c-bc1a13c889e1,612455864f292300692b9329,6053a5db311e270068e35be4,6053a7722f452d006f82906f,6053a58c45a3bb006816a5a0,,,,,,,,,,,,,,,,,,,,,,,,,0,0,,,0,0,,Global,,,,,,,,,,,,,,,,,,,,SPL-109254,,,,,,,,,,,,,,,,,,,,06/Aug/19 7:58 PM;txiao;strace.txt.gz;https://splunk.atlassian.net/rest/api/3/attachment/content/995564,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@18e0a58f,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,29030400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Tue Oct 22 18:15:09 UTC 2024,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,mblank(mblank),txiao(txiao),007e21cd-7a2d-4f69-90ee-0eb50d6f5965(007e21cd-7a2d-4f69-90ee-0eb50d6f5965),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i57293:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2019-08-06 17:08:05.809,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019-08-06 17:08:05.809,2019-08-06 17:08:05.809,,,,,,,,,,,,,"06/Aug/19 10:08 AM;6053b370e394c30069cb5cca;If someone reproduces this it would be useful to capture the {{strace -o strace.out -f splunkforwarder/bin/splunk list monitor}}

","06/Aug/19 7:58 PM;6053a58c45a3bb006816a5a0;Attached  [^strace.txt.gz] 
{code}
setresuid(500, 500, 500)          = 0
29555 mkdir(""/root/.splunk"", 0755)      = -1 EACCES (Permission denied)
{code}","22/Oct/24 11:15 AM;62835c84c217e200698c237f;Comcast brought this up in a case today.  From a Support perspective, it would be really nice if we could just catch this scenario and throw a meaningful error, instead of a stack trace ending with “Please file a case online at [+http://www.splunk.com/page/submit_issue+|http://www.splunk.com/page/submit_issue]”.  

I don’t see many other cases directly about this.  It’s hard to tell how often customers hit this, but I feel like I have run into it before and most customers sort of muddle through and figure it out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,05/Aug/19 8:08 PM
[PUBLIC] [wlm] timezone isn't stored for start_time/end_time of rule schedule every_day/every_week/every_month ,SPL-173449,1288104,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,As Designed,Vera Qin [X],613259952aa2800068a4a8cf,zora zhou,557058:39d2845b-c454-4bc4-a5e0-f42521a819af,zora zhou,557058:39d2845b-c454-4bc4-a5e0-f42521a819af,19/Jul/19 10:39 AM,30/Jul/25 4:26 PM,,26/Jul/19 10:01 AM,10.0.x,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Workload Management,,,,,0,z0928,,,,,,,"*Current behavior:*

If schedule=time_range, the conf stores the timezone, so that when user change timezone, the schedule isn't effect
If schedule=every_day/every_time/every_week, the conf doesn't store the timezone, so when user change timezone, the effect time range is changed.
{noformat}
root@ip-10-202-20-37:/home/splunker# cat /tmp/splunk/etc/apps/search/local/workload_rules.conf
[workload_rule:tmp_rule_search_type]
action = move
predicate = search_type=adhoc AND runtime>30
workload_pool = test_pool_bbx1wh_0

[workload_rule:rule2]
end_time = 2019-07-19T18:00:00+00:00
predicate = index=_internal
schedule = time_range
start_time = 2019-07-19T17:00:00+00:00
workload_pool = test_pool_bbx1wh_1

[workload_rules_order]
rules = tmp_rule_search_type,rule2,rule3

[workload_rule:rule3]
end_time = 18
predicate = index=_audit
schedule = every_day
start_time = 17
workload_pool = test_pool_bbx1wh_2
{noformat}
",,Andrea Hong,Andrew Brown,Hongxun Liu,Kashyap Jotwani,Leon Li,Shalabh Goyal,Steven Roback,Vera Qin [X],zora zhou,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a969695c3900707a9628,557058:37b4518c-e757-440f-87b7-181f5f425e80,6053a3fb81b82500685ced67,6053a6d394d7b90069f83e26,6053a6c060d39e006f80a26e,6053a60cf180c300675e7e8b,557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc,613259952aa2800068a4a8cf,557058:39d2845b-c454-4bc4-a5e0-f42521a819af,,,,,,,,,,,,,,,,,,,,,,,,,,,28800,0,,0%,28800,0,,Global,,,,,,,,,,,,,,,,,,,,SPL-173259,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4dafb4e7,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,o Have they made any changes to their environment just before the issue started?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,157248000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,"o What is the customer environment for the issue? 

o If available, provide Splunk topology diagram, host name with IP mapping.
",,,,,,,,,,o What errors are being reported?,,,,,,,,,,,,,No,,,,,,,,,,,,,o What is the expectation when this problem is not there?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,• How long has the customer been tracking this issue and how frequent this issue is being hit?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Tue Sep 29 20:16:50 UTC 2020,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise Services,,,,,,,,,,,,,,,,,Not looked at yet,,,,,,,,,,,,,,,,,,,,,,,0.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ahong(ahong),andrewb(andrewb),jisenberg(jisenberg),wqin(wqin),zzhou(zzhou),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,Vera Qin [X],613259952aa2800068a4a8cf,,,,,,,,,,,,,,,,,,0|i556bz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not looked at yet,,,,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,"• For a UI bug, did you file this JIRA following the confluence page “Filing UI Bugs – 
 https://confluence.splunk.com/display/PROD/Filing+UI+Bugs”?

• For a server bug, did you file this JIRA following the confluence page “Filing Server Bugs –
 https://confluence.splunk.com/display/PROD/Filing+Server+Bugs”?

• Describe the steps taken to perform analysis and provide evidence to support the problem statement
. If available, provide the events that match the timeline with supporting logs.

• Provide the logs and accompanying data with reference to the confluence page Filing UI/Server Bugs, and all the relevant documentation based on your analysis to support your observation and theories.

• In your own words, describe the area affected based on your analysis.  Affected area could be component or issues related to memory, performance or authentication etc. Please specify.

Steps to Reproduce:
.If the issue is reproducible,provide the steps.
Is this issue reproducible on latest maintenance version or latest release?

. If available, will customer upgrade to fixed version?
If support is able to reproduce, share the setup.",,,,,,,,,,,,,,,,,,,2019-07-19 20:33:54.284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3_*:*_1_*:*_6958455_*|*_5_*:*_1_*:*_0_*|*_10001_*:*_2_*:*_592537025_*|*_10039_*:*_1_*:*_3008001,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019-07-19 20:33:54.284,2019-07-19 20:33:54.284,,,,,,Vera Qin [X],613259952aa2800068a4a8cf,,,,,,"19/Jul/19 10:46 AM;557058:39d2845b-c454-4bc4-a5e0-f42521a819af;[~accountid:6053a3fb81b82500685ced67], please confirm that if we need to handle the timezone for every_day/every_week/every_month","19/Jul/19 1:33 PM;613259952aa2800068a4a8cf;In every day/week/month schedule, if the time slot changes depending on timezone changes, there will be issue that time will cross 2 days, and we cannot show it on UI. So we will keep the design and document it for now, and improve it in the future.

And time_range doesn't have the issue, as it can show different date and time on UI.

cc [~accountid:6053a3fb81b82500685ced67] [~accountid:6053a60cf180c300675e7e8b]","25/Jul/19 11:54 AM;6053a969695c3900707a9628;[~accountid:557058:b493c6e6-693a-43ea-ba7a-02e4a72fb1dc] - Can you please check if closing this ticket will affect getting the release notes posted for Quake. And if it's ok to do so, please go ahead and close. Thanks!","21/Aug/20 4:06 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;I was doing some cleanup and removed the ""document as include in release notes"" field because a Clone of this Jira was marked as fixed in 8.0.0. Based on the comments this one might not be the same thing as the Clone so I'm undoing my previous change. This will continue to appear in 8.0.x known issues ",29/Sep/20 1:16 PM;557058:2d2a7f1d-c9e1-4f3c-95c5-24ed73d0b69d;SevLevelCleanupJI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,946081,SPL-152877,Workload Management - Rule Enhancements,Done,26/Jul/19 10:01 AM
[PUBLIC] SmartStore standalone instance + Monitoring Console: Bootstrapping panel needs to reflect the standalone bootstrapping process,SPL-168314,1198786,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,,Douglas Rapp,6053aba7695c3900707aafc5,Tommy Nguyen,6053b8c9695c3900707b4296,Tommy Nguyen,6053b8c9695c3900707b4296,26/Mar/19 2:19 PM,30/Jul/25 4:25 PM,,,10.0.x,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,S2,,,,,0,indexing_bug_bash,,,,,,,"S2 standalone + monitoring console currently only reflects the status of bootstrapping in a indexer clustering environment.
S2 standalone hits different codepaths and logs differently while bootstrapping(bootstrap status endpoint are different too)

",,Andrew Brown,Douglas Rapp,Jo Hornsby,Stephen Goodman,Tommy Nguyen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,6053aba7695c3900707aafc5,557058:f87e2651-5132-4aed-8b96-f608ee969435,5d0d153711233f0c4ca0aa17,6053b8c9695c3900707b4296,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@72fa04ae,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,202435200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Thu Apr 25 16:29:34 UTC 2019,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NoahIndexing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),drapp(drapp),echeung(echeung),tonguyen(tonguyen),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i52ajb:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2019-03-27 17:28:19.889,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019-03-27 17:28:19.889,2019-03-27 17:28:19.889,,,,,,,,,,,,,27/Mar/19 10:28 AM;6053a74994d7b90069f8435f;[~accountid:6053aba7695c3900707aafc5] - can you take a look?,"27/Mar/19 10:41 AM;6053aba7695c3900707aafc5;It doesn't look like there are any obvious ways to report bootstrapping in a non-cluster. I think the question for PP is whether we should remove the panel from the instance dashboard, or just leave it in its current inoperable state and document it. In a future release we might try to make it work, although it sounds like that will involve adding real code to report single instance bootstrapping.","27/Mar/19 11:02 AM;6053b8c9695c3900707b4296;{quote}I think the question for PP is whether we should remove the panel from the instance dashboard, or just leave it in its current inoperable state and document it
{quote}
[~accountid:5d0d153711233f0c4ca0aa17] had already updated this ticket so that this issue will be documented in the release notes, so I think we should be fine for PP.

 

[~accountid:6053a8d9311e270068e37dd9] did you say we should target this for 7.3.x?

 
{quote}In a future release we might try to make it work, although it sounds like that will involve adding real code to report single instance bootstrapping.
{quote}
We do have some information regarding bootstrapping in standalone, but I think it will still require some work.

If you hit the following endpoint(index specific) on standalone, you can find the bootstrapping state for that index:
{noformat}
/services/data/indexes/<index>/_retrieve_recreate_status{noformat}
^ With this example, you'll need to know the index name at search time and I'm not sure if there is an endpoint for standalone which just returns all indexes being bootstrapped and their statuses.

 

In metrics.log, I do see the following log lines, but not sure how helpful it'll be for reporting the status. It doesn't even contain the index name.
```

03-26-2019 09:53:20.695 -0700 INFO  Metrics - group=*cachemgr*_bucket, register_existing=37081, open_buckets=0
```","08/Apr/19 1:02 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;Please use customer-facing names, not internal codes, in descriptions of JIRAs that are tagged to be included in the release notes. I've updated this one. ",25/Apr/19 9:29 AM;6053a74994d7b90069f8435f;Bulk updating Mission Team from PR-22-TeamTakTak to Indexing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1025629,SPL-159732,SmartStore hardening,To Do,26/Mar/19 2:19 PM
[PUBLIC] The data preview for the Add Data workflow does not display for Log to Metrics source types,SPL-160286,1032838,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,,,Matthew Ness,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,Matthew Ness,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,19/Sep/18 5:40 PM,30/Jul/25 4:24 PM,,,10.0.x,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Preview,Metric Store,,,,0,,,,,,,,This bug was created so that this issue appears in the release notes for OrangeSwirl and later. Close it when task SPL-157693 is closed. ,,David Marquardt,Manu Jose,Matthew Ness,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a2e3686bf50070459604,5f7671cd837bb80068281d0b,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-157693,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@645ec1b6,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,221184000,,,,,,,,,,,,,,,,,,Enterprise,Enterprise Cloud,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Thu Sep 20 18:14:27 UTC 2018,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MetricsSearch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dmarquardt(dmarquardt),mness(mness),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This field is here for our customer to understand the work that is required to be done.,,,,,,,,,,,,,,,,,,,,,,,,,0|i488pr:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,"CVSS Score:
ProdSec Business Score:
Identified in Release:
Splunk Advisory Ref:
Ref ID:",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2018-09-20 18:14:27.324,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2018-09-20 18:14:27.324,2018-09-20 18:14:27.324,,,,,,,,,,,,,19/Sep/18 5:41 PM;557058:89ace195-adc2-4f25-9924-ded7c8f10bb6;Close this bug when SPL-157693 is closed. ,20/Sep/18 11:14 AM;6053a2e3686bf50070459604;FYI [~accountid:5f7671cd837bb80068281d0b],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,999040,SPL-157688,Logs to Metrics UI enhancements ,To Do,19/Sep/18 5:40 PM
[PUBLIC] A timeout or slow response when accessing Splunk Web Licensing page ,SPL-158658,1010825,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Delivered,Harendra Rawat,6053a6d694d7b90069f83e4f,Andrew Lee,6036b7b05ddf020069bff121,Andrew Lee,6036b7b05ddf020069bff121,13/Aug/18 8:29 AM,30/Jul/25 4:25 PM,,14/Aug/18 9:49 AM,10.0.x,6.6.5,8.1.0 GA (Rarity),8.1.1,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Licensing,Manager Pages - Misc,,,,0,,,,,,,,"Customer is experiencing read timeouts when accessing settings -> licensing, previously this would take ~30 seconds to minutes to load and customer was okay with this behavior as they did not need to access this page frequently. However, when accessing this page last week they were experiencing the read timeout. According to the customer, this is the only page that they are experiencing this with. 
{noformat}
<response> 
<messages> 
<msg type=""ERROR"">Read Timeout</msg> 
</messages> 
</response> 
{noformat}
Customer is running 8 cores, ~8GB Ram, no high CPU usage or RAM usage

Detected 8 (virtual) CPUs, 8 CPU cores, and 7871MB RAM

 

Indexers that are connecting to this LM are running 6.6.3 

 

Previous JIRAs related to this issue --SPL-145717--, --SPL-144110--

Diag

[https://carabiner.splunk.com/supportdiags/files/case1076584-08-09-18-USER-0034000001RDPbdAAH-1672834495-diag-fldcvpsla5466-2018-08-09_13-47-17.tar.gz]

Attached are Pstacks as well as a flamegraph and HAR",,Andrew Lee,Antony Wang,Becky Simmons,Edward Kostowski,Gregory Runyon,Harendra Rawat,Horace Hsu,Howard (Shih-Chen) Chin,Izzy Park,Jacob Crabb,Jeffrey Chang,Jo Hornsby,Ragu Kantamaneni,Victor Ebken,,,,,,,,,,,,,,,,,,,,,,6036b7b05ddf020069bff121,557058:9b250150-4703-4e0c-a02b-ad536bb0d408,6036b837d3fc7c006809a2d7,557058:4a62ae54-d811-4a94-abdf-457ade60ee5b,6036b881185376007024178f,6053a6d694d7b90069f83e4f,6053b9c094d7b90069f913c4,557058:30e41e2b-e9d1-427a-ae34-cf6659ef443a,5a1ef9dc40207c40ef9126d7,6036b8356bc3f300699ceea0,557058:52c801cc-dc38-4687-bd76-6aeaab916f3c,557058:f87e2651-5132-4aed-8b96-f608ee969435,5c6f638def824a130638dd10,557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SCSM-515,,,,,,,,13/Aug/18 2:03 PM;alee;29case1076584-08-13-2018-USER-0034000001RDPbdAAH-splunk_messages.csv;https://splunk.atlassian.net/rest/api/3/attachment/content/527838,13/Aug/18 8:24 AM;alee;51case1076584-08-10-2018-USER-0034000001RDPbdAAH-pstack.tar;https://splunk.atlassian.net/rest/api/3/attachment/content/527503,13/Aug/18 8:26 AM;alee;83case1076584-08-10-2018-USER-0034000001RDPbdAAH-fldcvpsla5466.wdw.disney.com.har;https://splunk.atlassian.net/rest/api/3/attachment/content/527501,13/Aug/18 8:24 AM;alee;flamegraph.svg;https://splunk.atlassian.net/rest/api/3/attachment/content/527502,13/Aug/18 2:12 PM;alee;pstacksUpdated813.tar;https://splunk.atlassian.net/rest/api/3/attachment/content/527846,13/Aug/18 2:06 PM;alee;strace.tar;https://splunk.atlassian.net/rest/api/3/attachment/content/527841,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Walt Disney Parks and Resorts U.S., Inc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3a9cbcaa,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,115948800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Thu Jan 20 22:17:15 UTC 2022,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,alee(alee),dawang(dawang),ekostowski(ekostowski),hrawat(hrawat),hhsu(hhsu),vebken(vebken),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sustaining,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Harendra Rawat,6053a6d694d7b90069f83e4f,,,,,,,,,,,,,,,,,,0|i44p9z:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HSBC Digital Transformation,3489105,012400000005WzMAAU,P2,No,5005a0000312iuZAAQ,,Closed,Resolved,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1076584,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SusDv-S6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2018-08-13 16:54:01.959,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_91228149,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A timeout or slow performance of the license management page is caused by a build-up of historical license warning messages, which are processed every time the page is accessed.  Can be verified by running this search on the License Manager:  

| rest splunk_server=local /services/licenser/messages 

If a high value is returned for that end point, you are likely affected.  Log a support ticket with Splunk to obtain a license reset key, and apply the key to clear out any historical license warning messages. After the reset license is applied, the license management pages should load normally.",,,,,,,,,,2018-08-13 16:54:01.959,2018-08-13 16:54:01.959,,,,,,,,,,,,,"13/Aug/18 9:54 AM;6053b9c094d7b90069f913c4;*+[TC update]+*
Hi [~accountid:557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792],
Could you please check this ticket since it should be fixed in 6.6.3(SPL-144110)?
Thank you!",13/Aug/18 10:43 AM;6053a6d694d7b90069f83e4f;[~accountid:6053b9c094d7b90069f913c4] customer is on 6.6.5 and SPL-144110 is fixed for 6.6.3. Also [~accountid:557058:30187f71-0711-4ef0-bea5-7b52e05e23bd] verified it.,13/Aug/18 10:45 AM;6053a6d694d7b90069f83e4f;[~accountid:6036b7b05ddf020069bff121] please have the customer apply the workaround.,"13/Aug/18 11:49 AM;6036b7b05ddf020069bff121;[~accountid:6053a6d694d7b90069f83e4f] 

This workaround is already set in their splunk-launch.conf

TZ=:/etc/localtime","13/Aug/18 1:09 PM;557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792;It looks like they have a high number of messages (likely from warnings) which is something that caused comcast to experience the slow responses even after applying the TZ workaround.
{noformat}
splunkd_access.log:127.0.0.1 - blaig003 [09/Aug/2018:13:47:20.669 -0400] ""GET /services/licenser/messages?offset=15750&count=50 HTTP/1.0"" 200 99254 - - - 1762ms
splunkd_access.log:127.0.0.1 - blaig003 [09/Aug/2018:13:47:22.503 -0400] ""GET /services/licenser/messages?offset=15800&count=50 HTTP/1.0"" 200 99245 - - - 1628ms
splunkd_access.log:127.0.0.1 - blaig003 [09/Aug/2018:13:47:24.199 -0400] ""GET /services/licenser/messages?offset=15850&count=50 HTTP/1.0"" 200 99275 - - - 1680ms
splunkd_access.log:127.0.0.1 - blaig003 [09/Aug/2018:13:47:25.945 -0400] ""GET /services/licenser/messages?offset=15900&count=50 HTTP/1.0"" 200 99309 - - - 1715ms
splunkd_access.log:127.0.0.1 - blaig003 [09/Aug/2018:13:47:27.728 -0400] ""GET /services/licenser/messages?offset=15950&count=50 HTTP/1.0"" 200 99167 - - - 2829ms
splunkd_access.log:127.0.0.1 - blaig003 [09/Aug/2018:13:47:30.611 -0400] ""GET /services/licenser/messages?offset=16000&count=50 HTTP/1.0"" 200 99163 - - - 1702ms
splunkd_access.log:127.0.0.1 - blaig003 [09/Aug/2018:13:47:32.604 -0400] ""GET /services/licenser/messages?offset=16050&count=50 HTTP/1.0"" 200 99264 - - - 1640ms
splunkd_access.log:127.0.0.1 - blaig003 [09/Aug/2018:13:47:34.291 -0400] ""GET /services/licenser/messages?offset=16100&count=50 HTTP/1.0"" 200 99228 - - - 1710ms
splunkd_access.log:127.0.0.1 - blaig003 [09/Aug/2018:13:47:36.062 -0400] ""GET /services/licenser/messages?offset=16150&count=50 HTTP/1.0"" 200 99329 - - - 1739ms
{noformat}

See ticket SPL-144651, we should gather the output of the endpoint to see what exactly they are. We can try to clear messages from the bulletin board manager or we may have to apply a reset license in order to clear the messages (as they might be warnings).
","13/Aug/18 1:13 PM;6036b7b05ddf020069bff121;[~accountid:557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792]
{noformat}
| rest splunk_server=local /services/licenser/messages{noformat}

Correct? And see how many events we return? ","13/Aug/18 1:16 PM;6053a6d694d7b90069f83e4f;[~accountid:6036b7b05ddf020069bff121] from the pstack it does not look like the setting is being used. Can you ask customer to run strace?

With the workaround we should have zero tzset calls in pstacks.

1) start strace 
{code}
strace  -ff -e trace=mmap,munmap,open,close,link,unlink,stat,statfs  -p<main splunkd pid>  -o strace.main.out
{code}
2) re-pro issue
3) once timeout hit, stop strace

Repeat another re-pro for pstack.
","13/Aug/18 1:27 PM;6036b7b05ddf020069bff121;[~accountid:6053a6d694d7b90069f83e4f] [~accountid:557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792] I have requested strace, rest endpoint results, and updated pstacks ",13/Aug/18 1:36 PM;6053a6d694d7b90069f83e4f;[~accountid:6036b7b05ddf020069bff121] the workaround is in place since 2017 ( strace will confirm). So it's likely warnings that are slowing down.,"13/Aug/18 2:04 PM;6036b7b05ddf020069bff121;Looks like they have ~16.7k messages for that endpoint, a license reset key would be the best way to clear these messages taking a look at the other JIRA?

Also have attached the messages, strace, and updated pstacks","13/Aug/18 2:06 PM;6053a6d694d7b90069f83e4f;[~accountid:6036b7b05ddf020069bff121] that's right. From the strace, TZ setting is working. It's the warnings that needs to be cleared. ","14/Aug/18 9:13 AM;6036b7b05ddf020069bff121;[~accountid:6053a6d694d7b90069f83e4f] [~accountid:557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792] Applying the reset worked thank you for the assistance. 

For clarification, when we load the Settings -> Licensing and make that rest endpoint call for 

/services/licenser/messages

It looks like some of these have a create date of almost a year ago. Is it the case that the customer is not clearing these messages out through the GUI on the individual indexers or on the LM? Or do we store these messages permanently and the only way that we can clear them out is by applying the License Reset?",14/Aug/18 9:49 AM;6053a6d694d7b90069f83e4f;By applying the License Reset.,24/Jan/21 6:11 PM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Closing as part of 6+ months old resolved bugs bulk clean up by QA Metrics Team. CC [~accountid:5ebddcc35862510b798f5abd] [~accountid:5d3547e5c2db730c59b25bdc] [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408].,11/Mar/21 10:05 AM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Rolling back Assignee & Developer from Jan 25th bulk closure operation. CC [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408]. Assignee: hrawat. Developer: None,"20/Jan/22 2:13 PM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;Prior listed workaround:
Set TZ environment variable in $SPLUNK_HOME/etc/splunk-launch.conf

TZ=:/etc/localtime

Or whatever timezone customer want to set.
TZ=<timezone>
Example TZ=US/Pacific

splunk stop;
splunk start;

Note: Please don't run splunk restart","20/Jan/22 2:17 PM;557058:4a62ae54-d811-4a94-abdf-457ade60ee5b;[~accountid:6036b7b05ddf020069bff121] , [~accountid:6036b8356bc3f300699ceea0]  I’ve updated this ticket to prepare it as a Known Issue. Please review the workaround (pulled from the KBase article.) If it meets your approval, just flip the “Document As” field to “include in release notes”

Thanks\!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,14/Aug/18 9:49 AM
"[PUBLIC] After running the ""clean userdata"" command, admin is unable to login with msg ""No users exist. Please set up a new user.""",SPL-153403,951857,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Workaround,Nabeel Samad,557058:ed3e020e-a234-45d4-b0a7-d06c59f954dc,Mark Go,557058:636d227d-9e08-4111-8699-d8f94d6b20cd,Mark Go,557058:636d227d-9e08-4111-8699-d8f94d6b20cd,13/Apr/18 1:32 PM,30/Jul/25 4:25 PM,,13/Apr/18 1:49 PM,10.0.x,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Identity - Authentication (AuthN),Login Page,Security,Upgrade,,0,,,,,,,,"After running the ""clean userdata""  or ""clean all"" CLI commands, admin is unable to login and ""No users exist. Please set up a new user."" is displayed.

This will also occur on upgrade if an admin runs the clean commands on an 7.0.x or older Splunk instance before upgrading to 7.1.0.

{code}
$ splunk/bin/splunk login -auth admin:changeme
No users exist. Please set up a user.
Login Failed
{code}

 !Screen Shot 2018-04-13 at 1.04.03 PM.png|thumbnail! 

Workaround:
Create a $SPLUNK_HOME/etc/system/local/user-seed.conf and restart Splunk
{code}
[user_info]
PASSWORD = <yourpassword>
{code}",,Andrew Brown,Mark Go,Nabeel Samad,Rajpal Bal,Sunny Choi,Veruska Oropeza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,557058:636d227d-9e08-4111-8699-d8f94d6b20cd,557058:ed3e020e-a234-45d4-b0a7-d06c59f954dc,557058:f9466031-49a3-4131-8cb7-7d482a9eb937,6053a40806cbba006a0da0e9,557058:5c6547b3-454a-4c3b-929e-a3699ad7dfd8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,,,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13/Apr/18 1:32 PM;mgo;Screen Shot 2018-04-13 at 1.04.03 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/440417,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NetDocuments,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@2e118940,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,233971200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Thu Apr 26 02:43:40 UTC 2018,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PR-12-Pirates,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),mgo(mgo),nsamad(nsamad),voropeza(voropeza),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Nabeel Samad,557058:ed3e020e-a234-45d4-b0a7-d06c59f954dc,,,,,,,,,,,,,,,,,,0|i3vq3j:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PR-12-S26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2018-04-20 17:30:03.71,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4_*:*_1_*:*_9354_*|*_5_*:*_1_*:*_1040772975_*|*_6_*:*_2_*:*_992737_*|*_10039_*:*_1_*:*_20277,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Create a $SPLUNK_HOME/etc/system/local/user-seed.conf and restart Splunk

<div class=""samplecode""> 
[user_info]<br>
PASSWORD = <yourpassword><br>
</div>",,,,,,,,,,2018-04-20 17:30:03.71,2018-04-20 17:30:03.71,,,,,,Nabeel Samad,557058:ed3e020e-a234-45d4-b0a7-d06c59f954dc,,,,,,"20/Apr/18 10:30 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:557058:636d227d-9e08-4111-8699-d8f94d6b20cd] because text inside square brackets is suppressed from being published in the customer-facing release notes, the title of this one looks a bit confusing. It comes out as ""After running clean , admin is unable to login...""

I don't think there is a way to escape the bracket characters to make them show up in the published version. Would it be appropriate to change the title to something like ""After running the clean command, admin is unable to login..."" or would that be incorrect?","20/Apr/18 2:55 PM;557058:636d227d-9e08-4111-8699-d8f94d6b20cd;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] Thanks for the heads up on the formatting; that does seem pretty confusing without the brackets. I changed it the title to 'After running the ""clean userdata"" command, admin is unable to login...'

I figure keeping ""userdata"" in there is one extra keyword that might help searches and customers should be able to figure out that ""all"" includes ""userdata"".","20/Apr/18 2:59 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;Looks great, thanks [~accountid:557058:636d227d-9e08-4111-8699-d8f94d6b20cd]",25/Apr/18 2:52 PM;557058:5c6547b3-454a-4c3b-929e-a3699ad7dfd8;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] - are we good to close this?,"25/Apr/18 3:02 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;So we're never going to fix this, and only provide a workaround? If so, we can close it, but know that it will live on as a known issue forever in the release notes.","25/Apr/18 3:31 PM;557058:636d227d-9e08-4111-8699-d8f94d6b20cd;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] Yup, no fix, just the work around.","25/Apr/18 4:29 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;Ok, thanks. We will carry this one forward to the 7.1.1 release notes and beyond. ","25/Apr/18 5:58 PM;557058:ed3e020e-a234-45d4-b0a7-d06c59f954dc;{quote} So we're never going to fix this, and only provide a workaround? 
{quote}
 Tbh, I don't think this was a bug in the first place. This behavior is by design. If you follow the discussion in SPL-152151, in both pre 7.1 and 7.1:
 * if users run {{splunk clean all|userdata}} (CLI help clearly warns that this will delete _all_ users); Splunk will clean up the {{etc/passwd}} file and all user data.
 * Pre 7.1, when you restart, Splunk would've seeded with the default {{admin:changeme}} pair.

But that behavior is now removed. If you remove all users and don't seed new admin credentials; in 7.1, Splunk will start up and will still continue indexing/forwarding data. But it will not create an admin user, since admin credentials were not provided. We've added a docs section which mentions this pitfall.  Additionally, we could make it even more clear that {{clean userdata}} is going to delete the admin user which would need to be recreated.

 

What do you think? ","25/Apr/18 7:29 PM;557058:636d227d-9e08-4111-8699-d8f94d6b20cd;Yeah, I agree with Nabeel that this isn't really a bug. I basically just filed this for informational purposes because several sustaining engs thought it would be something a lot of customers could run into and customers are more likely to read over the release notes than go through the docs pages.","25/Apr/18 7:43 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;No problem at all, guys, I just wanted to make sure that was the intent. It's really helpful to call these things out to customers and provide them the way to get around the problem/change. Much appreciated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,13/Apr/18 1:49 PM
"[PUBLIC] After installing Splunk on Windows using msiexec and the ""GENRANDOMPASSWORD=1"" option (and if generated password ends with backslash) admin is unable to login with msg ""No users exist. Please set up a new user.""",SPL-152330,940388,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,,,,zora zhou,557058:39d2845b-c454-4bc4-a5e0-f42521a819af,zora zhou,557058:39d2845b-c454-4bc4-a5e0-f42521a819af,20/Mar/18 1:35 AM,30/Jul/25 4:26 PM,,,10.0.x,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Security,,,,,0,CF_d247,CF_triaging,d69,NL_triaging,,,,"After installing Splunk on Windows using msiexec and the ""GENRANDOMPASSWORD=1"" option admin is unable to login with msg ""No users exist. Please set up a new user.""

{noformat}
C:\Windows\system32\msiexec.exe /i splunk-7.1.0-935b28aa68d8-x64-release.msi /l install_splunk-7.1.0-935b28aa68d8-x64-release.msi.log AGREETOLICENSE=Yes /quiet MINPASSWORDLOWERCASELEN=1 MINPASSWORDUPPERCASELEN=1 MINPASSWORDLEN=1 MINPASSWORDSPECIALCHARLEN=1 GENRANDOMPASSWORD=1 MINPASSWORDDIGITLEN=1
{noformat}

If the generated password ends with backslash, the login will fail with error: ""No users exist. Please set up a user."", e.g.

{noformat}
[user_info]
USERNAME=admin
PASSWORD=eX3\
{noformat}",,Emily Cheung,Jo Hornsby,Mark Go,Nabeel Samad,User known,User known,Wim Colgate,zora zhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a74994d7b90069f8435f,557058:f87e2651-5132-4aed-8b96-f608ee969435,557058:636d227d-9e08-4111-8699-d8f94d6b20cd,557058:ed3e020e-a234-45d4-b0a7-d06c59f954dc,unknown,unknown,557058:ef43a796-f89f-4463-801b-30ded73fb41e,557058:39d2845b-c454-4bc4-a5e0-f42521a819af,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Global,,,,,,,,,,,,,,,,,,,,SPL-151992,,,,,,,,,,,,,SPL-148838,,,,,,,20/Mar/18 1:59 AM;zzhou;Screen Shot 2018-03-20 at 16.59.09.png;https://splunk.atlassian.net/rest/api/3/attachment/content/423652,20/Mar/18 1:40 AM;zzhou;diag-ip-0ADE19BE-2018-03-20_09-11-25.tar.gz;https://splunk.atlassian.net/rest/api/3/attachment/content/423649,20/Mar/18 1:41 AM;zzhou;helmut.log;https://splunk.atlassian.net/rest/api/3/attachment/content/423650,20/Mar/18 1:41 AM;zzhou;install_splunk-7.1.0-935b28aa68d8-x64-release.msi.log;https://splunk.atlassian.net/rest/api/3/attachment/content/423651,20/Mar/18 1:39 AM;zzhou;splunk.log;https://splunk.atlassian.net/rest/api/3/attachment/content/423647,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@2ffee05e,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,169862400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,Thu May 07 00:16:21 UTC 2020,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Core Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,echeung(echeung),nsamad(nsamad),wcolgate(wcolgate),zzhou(zzhou),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Security Services,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i13ejz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PR-12-S66,PR-12-S67,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2018-03-20 15:25:02.037,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Create a $SPLUNK_HOME/etc/system/local/user-seed.conf and restart Splunk

<div class=""samplecode""> 
[user_info]<br>
PASSWORD = <yourpassword><br>
</div>",,,,,,,,,,2018-03-20 15:25:02.037,2018-03-20 15:25:02.037,,,,,,,,,,,,,20/Mar/18 8:25 AM;557058:ef43a796-f89f-4463-801b-30ded73fb41e;cc [~accountid:557058:ed3e020e-a234-45d4-b0a7-d06c59f954dc]. probably missing an escape for backslash? [~accountid:557058:39d2845b-c454-4bc4-a5e0-f42521a819af] the password in the log file when generating a random password is as designed.,"20/Mar/18 10:23 AM;557058:ed3e020e-a234-45d4-b0a7-d06c59f954dc;Some more context:

*Why?*

The backslash ""\"" is a special character in the conf system wherein it's used as a separator. If the random password generator creates a password with the slash *at the end*, the conf system treats it differently. This bug already existed *before* the password mgt changes.

*Fix*

We fixed a similar situation in Linux by making {{user-seed.conf}} accept hashed passwords instead of just plaintext. The Linux fix (---SPL-148838---) missed porting this change to the Windows installer.

The fix here is pretty simple but I think this is a minor bug. It will only happen when users generate a random password on Windows and that password happens to have a backslash in the end.

 cc: [~accountid:5a7a10adbc5c58611529e15a], [~accountid:6053a40806cbba006a0da0e9]",20/Mar/18 10:59 AM;557058:ed3e020e-a234-45d4-b0a7-d06c59f954dc;I discussed this with [~accountid:6053a40806cbba006a0da0e9] and [~accountid:5a7a10adbc5c58611529e15a] – we will tackle this in a later release.,"20/Mar/18 4:06 PM;557058:ed3e020e-a234-45d4-b0a7-d06c59f954dc;After conferring with [~accountid:557058:ef43a796-f89f-4463-801b-30ded73fb41e] a solution could be
 *  Beefing up {{ExecCmd()}} in {{ca/common/util.cpp}} to capture a command's stdout. 
 * Using the new {{ExecCmd()}}, in the installer's {{GenRandomPassword}} deferred action; generate a random password like we do now.
 * Append the password to {{splunk.log}} as we do now.
 * Calling out to splunkd to hash the password like {{splunkd hash-passwd $PASSWD}}.
 * Writing the hashed password to user-seed.conf under the {{HASHED_PASSWORD}} field.

 

Sample code to capture stdout of a child process ({{ExternalProcessGroup::start_internal())}}
{code:java}
	} else {
		sinfo.dwFlags |= STARTF_USESTDHANDLES;
		if (initProcThreadAttrList != nullptr) {
			// Case 2: [SPL-83066] Vista added the ability to pass an
			// explicit list of handles to inherit, so we can actually
			// do the right thing.  For details, see:
			//   http://blogs.msdn.com/b/oldnewthing/archive/2011/12/16/10248328.aspx
			// note that the include files are different depending on  windows version:
			// WinBase.h on Windows Vista, Windows 7, Windows Server 2008, and Windows Server 2008 R2 (include Windows.h);
			// Processthreadsapi.h on Windows 8 and Windows Server 2012
			rv = FALSE;
			sinfo.cb = sizeof(sinfoex);
			SIZE_T size_of_attr_list = 0;
			if (initProcThreadAttrList(nullptr, 1, 0, &size_of_attr_list) || GetLastError() == ERROR_INSUFFICIENT_BUFFER) {
				sinfoex.lpAttributeList = (PPROC_THREAD_ATTRIBUTE_LIST) HeapAlloc(GetProcessHeap(), 0, size_of_attr_list);
				if (sinfoex.lpAttributeList != nullptr) {
					if (initProcThreadAttrList(sinfoex.lpAttributeList, 1, 0, &size_of_attr_list)) {
						HANDLE to_inherit[3];
						unsigned cnt = 0;
						if (sinfo.hStdInput != INVALID_HANDLE_VALUE)
							to_inherit[cnt++] = sinfo.hStdInput;
						if (sinfo.hStdOutput != INVALID_HANDLE_VALUE)
							to_inherit[cnt++] = sinfo.hStdOutput;
						if (sinfo.hStdError != INVALID_HANDLE_VALUE)
							to_inherit[cnt++] = sinfo.hStdError;
						assert(cnt > 0);
						assert(cnt <= ARRAY_ELEMENTS(to_inherit));
						if (updateProcThreadAttr(sinfoex.lpAttributeList, 0, PROC_THREAD_ATTRIBUTE_HANDLE_LIST, to_inherit, cnt * sizeof(HANDLE), nullptr, nullptr)) {
							rv = ospath_CreateProcess(nullptr, wargstr, nullptr, nullptr, TRUE,
								 CREATE_DEFAULT_ERROR_MODE | CREATE_NEW_PROCESS_GROUP | CREATE_UNICODE_ENVIRONMENT | EXTENDED_STARTUPINFO_PRESENT,
								 ebufptr, wdir, &sinfo, &ps->pinfo);
							if (!rv)
								gLogger.debug() << ""Failed to create process -- error is "" << GetLastError();
						}
						deleteProcThreadAttrList(sinfoex.lpAttributeList);
					}
					HeapFree(GetProcessHeap(), 0, sinfoex.lpAttributeList);
				}
			}{code}",13/Dec/18 3:32 PM;6053a74994d7b90069f8435f;Doesn't need to be targeted for PinkiePie,"15/Jan/19 1:46 PM;557058:ef43a796-f89f-4463-801b-30ded73fb41e;[~accountid:557058:ed3e020e-a234-45d4-b0a7-d06c59f954dc], pinkiepie and later has captured stdout/stderr in the installer. (actually so does nightlight and orange swirl -- for a separate issue than this one). ",25/Apr/19 9:11 AM;6053a74994d7b90069f8435f;Bulk updating Mission Team from PR-12-Pirates to Core Security,06/May/20 5:16 PM;6053a74994d7b90069f8435f;Bulk updating ProductBacklogArea from Enterprise to Security Services,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1297861,SPL-174184,Epic - technical debt repayment for security,To Do,20/Mar/18 1:35 AM
[PUBLIC] [CUSTOMER] Unable to access some settings/manager pages (data model editor) if starting from the setup page of a non-visible app,SPL-146820,894426,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,Nick Filippi,557058:7297dded-b84a-45a6-a1c9-ee6f9d7fa579,Sarah Moir,557058:ead33141-9902-4d93-9add-b226d2beed55,Sarah Moir,557058:ead33141-9902-4d93-9add-b226d2beed55,29/Nov/17 5:09 PM,30/Jul/25 4:26 PM,,,10.0.x,7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Manager Pages - Misc,,,,,0,,,,,,,,"Reproduced with CIM 4.9.1 with 7.0.1. A customer also reported in Splunk Answers with 4.9.1 on 7.0.0: https://answers.splunk.com/answers/593663/common-information-model-cim-data-model-editor-mis.html I encouraged him to file a bug but his support contract is still under renewal. Because I was able to reproduce this internally, I'm filing it. 

If you try to view certain manager pages after starting in the context of a non-visible app, the manager pages 404 because they return to the app context. This example uses CIM because it's what I used to reproduce and is a non-visible app.

# Install CIM on Enterprise.
# Go to Manage Apps  and click ""Set Up"" for the Splunk_SA_CIM 
## Note that the URL is in the app context: splunk-es/en-US/app/Splunk_SA_CIM/cim_setup?action=edit
# Click Settings > Data models
## Note that the URL is in the manager context: splunk-es/en-US/manager/Splunk_SA_CIM/data_model_manager
# Click a data model to edit it.
## Note that the URL changes back to the app context: splunk-es/en-US/app/Splunk_SA_CIM/data_model_editor
# Observe a 404:  !Screen Shot 2017-11-29 at 5.19.08 PM.png|thumbnail! 
# Even worse, when you click the ""open this in search"" from the 404, you don't find any results because it defaults to searching a log_level of ERROR even though this error was logged with a log_level of INFO: ""[5a1f5c79517f00702f7f90] error:133 - Masking the original 404 message: 'App ""Splunk_SA_CIM"" does not support direct UI access. ' with 'Page not found!' for security reasons""

If you start in the search and reporting app (or another visible app) it works fine.

Is there a way to redirect someone to a visible app context so this doesn't fail so epically and mysteriously in the future? Or some other workaround? 
",,Dhananjay Koshe [X],Jo Hornsby,Nick Filippi,Robin Pille,Sarah Moir,Sunny Changediya,Sunny Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,613244734a98da0069edb123,557058:f87e2651-5132-4aed-8b96-f608ee969435,557058:7297dded-b84a-45a6-a1c9-ee6f9d7fa579,557058:1f98d237-c864-4552-a352-e0178d4b860a,557058:ead33141-9902-4d93-9add-b226d2beed55,6053a5d194d7b90069f83323,557058:bc56650e-dd7f-4322-a130-700dc7ba1a1f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CIM-989,,,,,,,,,,,,,,,,,,,,,,,,,,,,29/Nov/17 5:19 PM;smoir;Screen Shot 2017-11-29 at 5.19.08 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/369479,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@79deff24,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,143164800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Thu Mar 11 23:55:13 UTC 2021,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SCS Logs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dkoshe(dkoshe),nfilippi(nfilippi),rpille(rpille),smoir(smoir),schangediya(schangediya),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DevPlatform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i3my3b:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2017-11-30 16:38:44.409,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Navigate to a visible app, such as the search and reporting app, and access the Splunk settings pages from that app context.",,,,,,,,,,2017-11-30 16:38:44.409,2017-11-30 16:38:44.409,,,,,,,,,,,,,"30/Nov/17 8:38 AM;557058:1f98d237-c864-4552-a352-e0178d4b860a;[~accountid:613244734a98da0069edb123], does this fall under Splunk UI, UI framework, or App Framework? If so, could you help get this triaged?","14/Dec/17 4:37 PM;613244734a98da0069edb123;[~accountid:557058:79277858-c49f-47fe-be0b-d81b27100ea0] and [~accountid:557058:7297dded-b84a-45a6-a1c9-ee6f9d7fa579] – how and when do we want to handle this?

This has been a forever behavior in Splunk.","20/Dec/17 12:39 PM;557058:7297dded-b84a-45a6-a1c9-ee6f9d7fa579;[~accountid:613244734a98da0069edb123] Not sure what the correct behavior should be.  Given that future product bundles will not have any guaranteed apps installed, we cannot fall back to any specific app context when accessing manager pages.  Perhaps we need to support manager/settings pages that are independent of the app-context...or if manager/settings becomes it's own app, then fallback to that.","20/Dec/17 1:53 PM;613244734a98da0069edb123;Looking through it a bit... maybe it's possible that we create a manager page for data_model_editor, and link to that from the data_model_manager page ?? /manager context is agnostic the the viewable property of app.",11/Mar/21 3:55 PM;6053a5d194d7b90069f83323;Is this request still being tracked ? We have customer issue reporting same issue. CIM-989 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,29/Nov/17 5:09 PM
[PUBLIC] Distributed environment requires index defined on search head for log event alerts,SPL-146802,894191,Bug,Reopened,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,,,Eve Meelan,5bbd110923099e22ab555cb0,Eve Meelan,5bbd110923099e22ab555cb0,29/Nov/17 10:53 AM,30/Jul/25 4:26 PM,,,10.0.x,6.4.0 GA (Galaxy),6.4.1,6.4.10,6.4.11,6.4.2,6.4.3,6.4.4,6.4.5,6.4.6,6.4.7,6.4.8,6.4.9,6.5.0 GA (Ivory),6.5.1,6.5.10,6.5.2,6.5.3,6.5.4,6.5.5,6.5.6,6.5.7,6.5.8,6.5.9,6.6.0 GA (Kimono),6.6.1,6.6.10,6.6.11,6.6.12,6.6.2,6.6.3,6.6.4,6.6.5,6.6.6,6.6.7,6.6.8,6.6.9,7.0.0 GA (Minty),7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,Alerting,Search - Distributed,,,,0,alerting,distributedsearch,SUST_TC_SKIP,workgroupC,,,,"It seems that when a customer with a distributed search environment tries to set up a log event alert action, they must specify the path of the index that lives on the search head. This is the case even when they have set up their environment correctly (i.e. they have turned off indexing on the search head - I've confirmed this with the customer). Here's a summary from Answers:

{color:#d04437}The summarized data forwarding to indexers works fine. Other internal logs forwarding to indexers work fine. But, the Alert Action Log event alone fails.{color}

{color:#d04437}For this to work -- We have to define index in Search Head as well.{color}

{color:#d04437}Note: the data wont be stored on Search Head, eventually the Alert Action's Log event will be forwarded to indexer. But the definition on SH is must.{color}

{color:#d04437}Not sure if it is a bug in Splunk or it is working as expected.{color}

{color:#d04437}I have added a comment in Alert documentation and a discussion is on with splunk folks :){color}

 ",,Andrew Brown,Eve Meelan,Hiroatsu Noboru,Jack Guo,Jeffrey Chang,Jo Hornsby,Jonathan Statt,Joshua Newlan,Leo Liou,Nick Filippi,Stephen Goodman,tvu,User known,Wythe Lin,Zofnat Shy,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,5bbd110923099e22ab555cb0,6036b7ddac6e4e0069e92a8b,6053a4a366c8790068386da5,557058:52c801cc-dc38-4687-bd76-6aeaab916f3c,557058:f87e2651-5132-4aed-8b96-f608ee969435,6053b95a66c8790068395481,557058:2129cc96-e228-4a9a-b5bf-da0fae4e6e8d,5ad8089dbaaf152a765686e7,557058:7297dded-b84a-45a6-a1c9-ee6f9d7fa579,5d0d153711233f0c4ca0aa17,5cc893082ba2de10052c131a,unknown,5fda4d9e6420890141370cb0,6053add6695c3900707ac8b7,,,,,,,,,,,,,,,,,,,,,0,0,,,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,REAL-NOT,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@52b015af,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,19.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,147484800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Thu Jan 21 03:26:05 UTC 2021,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,19.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),emeelan(emeelan),hnoboru(hnoboru),jchang(jchang),jstatt(jstatt),cweiliou(cweiliou),clin(clin),zshy(zshy),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i3mwpz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Group Operations Service Platform (GOSP),3642823,012400000005WzMAAU,P3,No,5005a0000316CyoAAE,,Closed,Resolved,Standard,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unknown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2018-02-15 00:41:51.106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2018-02-15 00:41:51.106,2018-02-15 00:41:51.106,,,,,,Eve Meelan,5bbd110923099e22ab555cb0,,,,,,29/Nov/17 2:21 PM;5bbd110923099e22ab555cb0;https://answers.splunk.com/answers/405204/how-can-i-set-up-the-log-event-alert-action-in-a-d.html,"29/Nov/17 2:24 PM;5bbd110923099e22ab555cb0;Email response from customer:

*From:* Mahesh <[onlybytes@gmail.com|mailto:onlybytes@gmail.com]>
*Date:* Wednesday, November 29, 2017 at 4:57 AM
*To:* Eve Meelan <[emeelan@splunk.com|mailto:emeelan@splunk.com]>
*Subject:* Re: Customer comment on Splunk Documentation

 

Hi Eve Meelan,

I did further analysis and found this

The alert_logevent app that is responsible for sending log events to an index uses REST Endpoint

  url = '%s/services/receivers/simple?%s' % (settings.get('server_uri'), urlencode(query))

in this case, the server_uri is that of Search Head's. 

Hence i think it is necessary to define the index in Searh Head. Even though the data will be sent to the index present in an indexer.

There are many posts in Splunk Answers forum around this topic. 

Thanks,

Mahesh

 

On Wed, Nov 29, 2017 at 2:48 PM, Mahesh <[onlybytes@gmail.com|mailto:onlybytes@gmail.com]> wrote:

Hi Eve Meelan,

Thanks a lot for your response. 

The issue still exists.

Before I explain the issue in detail, i would like to make it clear that we do not store data in search head. We follow distributed deployment and all the data is stored in the indexers.

All our summary searches that are running in search head are forwarding data to indexers.

 In short - Everything is working as intended except for the 'Alert Action - Log Event'.

 

For 'Alert Action - Log Event', i should also define index in Search Head (index definition in indexer alone is not sufficient). When an alert is triggered, the data gets forwarded to indexer and is indexed only when the corresponding index definition is present in Search Head also.

 

Thanks,

Mahesh","14/Feb/18 4:41 PM;5fda4d9e6420890141370cb0;Hi [~accountid:5bbd110923099e22ab555cb0],
I noticed the SFDC status is closed now.
Do you still need anything from sustaining?
If not, could we close this ticket?
Thanks","15/Feb/18 9:58 AM;5bbd110923099e22ab555cb0;[~accountid:5fda4d9e6420890141370cb0] I guess. It doesn't look like anyone fixed anything, but I've changed the documentation to state the work-around. ","20/Feb/18 1:36 AM;6053add6695c3900707ac8b7;[~accountid:5bbd110923099e22ab555cb0] can you explain why this was marked as closed won't fix?

Is this the intended behaviour?

Not sure what happened between ""We will fix this"" and closing it as ""won't fix"".

The SFDC case was closed as a workaround does not mean the customer is not expecting a fix. 

In addition the answer notes this bug as a reference but if it is not going to be fixed what is the point? 

I have a customer asking when it is going to be fixed as he saw the answer post.

JIRA SPL-145546 can provide ideas for fixing. Its in a different area of the product but basically the same issue of not getting the list of available indexes from the Indexers layer in a distributed environment.

 ","20/Feb/18 7:54 AM;5bbd110923099e22ab555cb0;[~accountid:6053add6695c3900707ac8b7] Sustaining asked me to close it, so I did. The SFDC case was closed then. It looks like one was recently opened again (after I closed this bug). If you want me to re-open it, to whom should I assign it?","20/Feb/18 9:36 AM;5bbd110923099e22ab555cb0;Also, my Answer stated that this bug was looking into whether the behavior was expected or not expected - not that we'd fix the behavior. ",27/Feb/18 7:02 AM;6053add6695c3900707ac8b7;Thank you [~accountid:5bbd110923099e22ab555cb0] for clarifying. I will discuss with the sustaining manager locally and will take it from there.,"28/Feb/18 3:17 AM;6053b95a66c8790068395481;[~accountid:5bbd110923099e22ab555cb0] This JIRA needs the workaround field populated and closed as workaround to correctly show in the release notes under known issues

 

[~accountid:6053add6695c3900707ac8b7] If this is acceptable then great. If not this needs a new support created JIRA to ask for a proper bug fix, referring to this JIRA. As this has been the case since 6.6.0, it may be a design change that needs referral to a PM.","28/Feb/18 8:48 AM;5bbd110923099e22ab555cb0;[~accountid:6053b95a66c8790068395481] Thanks for the clarification, I will re-close the bug. ","27/Jun/19 9:45 PM;6036b7ddac6e4e0069e92a8b;Hello [~accountid:5bbd110923099e22ab555cb0]

My customer hit this issue, and asking if this is going to be fixed or not.

According to the above comment, I understand we're not going to fix this unless the customer demands, right?

If my understanding is right, would you update the answers page? As the customer thinks it will be fixed in next release.

----

Hi folks, 
This issue will be fixed in our next release.

Thank you for your patience.

-Eve Meelan

----","27/Jun/19 9:47 PM;6036b7ddac6e4e0069e92a8b;Hello [~accountid:5bbd110923099e22ab555cb0]

Sorry the below is the URL.

https://answers.splunk.com/answers/405204/how-can-i-set-up-the-log-event-alert-action-in-a-d.html","28/Jun/19 7:01 AM;5bbd110923099e22ab555cb0;[~accountid:6036b7ddac6e4e0069e92a8b] - https://splunk.atlassian.net/browse/SPL-161629 led me to believe the issue was fixed. If it was not, I think that is the bug to re-open. ","01/Jul/19 7:57 AM;6036b7ddac6e4e0069e92a8b;Hello [~accountid:5bbd110923099e22ab555cb0]

Thank you very much for your reply.

I'm a bit confused. I understand SPL-145546 (and SPL-161629) and this JIRA is different.

Because this number is still sitting on the known issue page. 

[https://docs.splunk.com/Documentation/Splunk/7.2.1/ReleaseNotes/KnownIssues]

[https://docs.splunk.com/Documentation/Splunk/7.3.0/ReleaseNotes/KnownIssues]

Whist SPL-145546, SPL-161629 are listed on the release note as a fix issue.

[https://docs.splunk.com/Documentation/Splunk/7.3.0/ReleaseNotes/Fixedissues]

 

BTW, my customer doesn't need a fix for this JIRA. ",08/Jul/19 7:07 AM;5bbd110923099e22ab555cb0;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] - I don't know how to handle this. Can you offer some clarity?,"08/Jul/19 9:26 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:6036b7ddac6e4e0069e92a8b], SFDC indicates that your customer encountered this bug on Splunk Enterprise 7.1.3. Is that correct? 

[~accountid:5bbd110923099e22ab555cb0], do you know whether the fixes in 7.2.1 (SPL-161629) and 7.3.0 (SPL-145546) also fix the specific log event alerts problem that you describe in this JIRA? This one hasn't been marked as a duplicate of either of those, so I don't understand whether the problem described in this JIRA is independent or not.","08/Jul/19 9:59 AM;5bbd110923099e22ab555cb0;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] - I honestly don't remember as it was over a year ago. I just remember documenting the workaround. The comments by [~accountid:6053b95a66c8790068395481] seem to mean that closing the bug was the right thing to do. This whole thing is very confusing. I think -SPL-161629-  fixed the issue and they just didn't link to this. 

[~accountid:6036b7ddac6e4e0069e92a8b], I think you'll need to go to that bug and talk with the assigned engineer to find out if it's fixed and if it's supposed to be linked to this bug. This was just for the release notes. ","22/Jul/19 5:22 PM;557058:52c801cc-dc38-4687-bd76-6aeaab916f3c;Added SUST_TC_SKIP label, since we are looking for documentation update here, no action for sustaining.
Feel free to update if I'm wrong.","20/Jan/21 7:26 PM;5ad8089dbaaf152a765686e7;[~accountid:5bbd110923099e22ab555cb0] and [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80],
My customer is using 7.3.3 and they were hitting this same issue as well at case:2046293.
They had to workaround current issue by creating the same index in SH side even in 7.3.3
Please let me know if you have any other questions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,27/Jun/19 9:47 PM
[PUBLIC] Uninstall app dialog does not show the app name correctly when the app doesn't have the label,SPL-143981,844399,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,,,Mei-Fang Huang [X],613255586e5e1e0071f06835,Mei-Fang Huang [X],613255586e5e1e0071f06835,14/Aug/17 2:30 PM,30/Jul/25 4:24 PM,,,10.0.x,7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DMC,,,,,0,oper_bug_bash,,,,,,,"Repro steps:
 # Install an private app that doesn't have app label
 # Go to app listing page and uninstall the app

Results:
 # Uninstall in process dialog doesn't show app name
 # Uninstall confirm dialog show 'null' as app name

Expected:

If app label doesn't exist, fall back to app id. ",,Andrea Hong,Mei-Fang Huang [X],,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a969695c3900707a9628,613255586e5e1e0071f06835,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Aug/17 2:29 PM;mfhuang;Screen Shot 2017-08-14 at 2.23.54 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/327686,14/Aug/17 2:29 PM;mfhuang;Screen Shot 2017-08-14 at 2.25.40 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/327685,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@10d810ba,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,160790400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Thu Aug 20 05:04:34 UTC 2020,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Operability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ahong(ahong),echeung(echeung),mfhuang(mfhuang),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i3f333:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2019-05-06 17:02:40.493,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019-05-06 17:02:40.493,2019-05-06 17:02:40.493,,,,,,,,,,,,,15/Aug/17 2:07 PM;613255586e5e1e0071f06835;This is not a regression. Public app has the same issue. ,06/May/19 10:02 AM;6053a74994d7b90069f8435f;Bulk updating Mission Team to Operability,"19/Aug/20 10:04 PM;6053a969695c3900707a9628;Removed fixVersion Rarity since the Rarity release branch has been cut as of Aug 17th. If you would like to include a fix for this bug for the release, please request a [Release Exception|https://splunk.atlassian.net/wiki/display/PROD/Release+Exception+Request]. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,14/Aug/17 2:30 PM
[PUBLIC] [cloud] Report acceleration is broken for users with a configured role-based access filter,SPL-143947,844024,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,,,Cliff Xuan,6053ae81695c3900707ad0a1,Cliff Xuan,6053ae81695c3900707ad0a1,14/Aug/17 2:38 AM,30/Jul/25 4:25 PM,,,10.0.x,8.0.0 GA (Quake),8.1.0 GA (Rarity),8.2.0 GA (Scootaloo),8.2.2109(Pilatus),8.2.2112(Butterfinger),8.2.2201(Crunch),8.2.2202(Dove),8.2.2203(Emerald),9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,~Deprecated - Cloud,Search - Report Acceleration,,,,0,coresearch_reviewed_FY22Q1,found_on_cloud,searchpipeline_cleanup_apr2022,StructuredSearch_Reviewed,systest_minty,,,"With a power user, after enable acceleration for reports on search head, the ""Summary Status"" always shows the progress is 0%, and there are no acceleration related messages (like ""Using summaries for search, summary_id=XXX"") in search jobs.

I can reproduce this issue on stackmakr/victor stacks, but cannot reproduce this with on-prem environments.

*Reproduce steps:*
1. Apply a stackmakr/victor stack
2. Create a new power user, and add ""_internal"" index for power user, so we can use internal data to produce this issue
3. Login as power user, create a report like ""index=_internal | stats count by host"", enable acceleration for report, use ""1 day"" for summary range
4. Wait and check the ""Report Acceleration Summaries"" status, and run the search ""index=_internal | stats count by host"", check if there are acceleration related messages in search  jobs.





",,Ashish Mathew,Bei Li,Bharath Aleti,Chris Pride,Chris Vervais,Cliff Xuan,David Marquardt,Dhananjay Koshe [X],Ed Hunsinger,Izzy Park,James Ervin,Karthik Sabhanatarajan,Matthew Ness,nromito,Octavio Di Sciullo,Rama Gopalan [X],Shane Evans,Steve Zhang,Sunny Choi,User known,Victor Ebken,Xiaowei Wang,,,,,,,,,,,,,,5cd341229435c90fd6efce8f,5cd341243e6fd40fe82c5536,557058:b1e398c5-b004-4a33-8152-18daa36cda4b,557058:761616db-db41-4d02-bf30-e81f409685ed,5af8dc35d1d9445cd3a5f5e6,6053ae81695c3900707ad0a1,6053a2e3686bf50070459604,613244734a98da0069edb123,6026d41337bd64006a4a87bc,5a1ef9dc40207c40ef9126d7,557058:ab939132-f05e-4f5f-8bec-6939a8133dcc,611be44c883cef0077c90366,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,611bdefb4016870069185d34,557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e,613256e96e5e1e0071f078ad,5cd34132b588780fd3da382f,5af8dc2ef2f8f0082b77630a,6053a40806cbba006a0da0e9,unknown,557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792,5cd341218ad0f60dca0071cd,,,,,,,,,,,,,,,,,,,,,Global,,,,,,,,,,,,,,,,,,,,,,,,,SPL-244897,,,,SPL-177442,,,,SPL-177442,,,,,,,05/Sep/17 7:37 PM;cxuan;search.log;https://splunk.atlassian.net/rest/api/3/attachment/content/336123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7d74d677,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,35.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,107481600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,Thu Apr 28 19:44:27 UTC 2022,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,StructuredSearch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,35.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,amathew(amathew),baleti(baleti),cvervais(cvervais),cxuan(cxuan),dmarquardt(dmarquardt),dkoshe(dkoshe),ehunsinger(ehunsinger),ipark(ipark),jervin(jervin),lbosquezgonzalez(JIRAUSER51601),odisciullo(odisciullo),rgopalan(rgopalan),sevans(sevans),schoi(schoi),xwang(xwang),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Ashish Mathew,5cd341229435c90fd6efce8f,,,,,,,,,,,,,,,,,,0|i0petz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2017-08-14 15:25:12.067,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,25.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017-08-14 15:25:12.067,2017-08-14 15:25:12.067,,,,,,Ashish Mathew,5cd341229435c90fd6efce8f,,,,,,"14/Aug/17 8:25 AM;6053a40806cbba006a0da0e9;cc [~accountid:6026d41337bd64006a4a87bc], triage?","14/Aug/17 3:18 PM;6026d41337bd64006a4a87bc;[~accountid:557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e] as a Cloud Products person, can you triage this please?",14/Aug/17 3:34 PM;5af8dc35d1d9445cd3a5f5e6;This should be in the administration area most likely. Role based access control and report acceleration are product features and not specific and exclusive cloud features. ,"14/Aug/17 3:50 PM;557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e;{quote}
With a power user, after enable acceleration for reports on search head, the ""Summary Status"" always shows the progress is 0%, and there are no acceleration related messages (like ""Using summaries for search, summary_id=XXX"") in search jobs.
{quote}
[~accountid:6053ae81695c3900707ad0a1], do note that it can take up to 10 minutes for a report acceleration that was just created to kick in for the first time.

In your tests, have you found that the summary status remains at 0% even, say, 20 minutes after creating the report acceleration?","14/Aug/17 5:23 PM;613256e96e5e1e0071f078ad;I see the following in search.log in the summary created by a power user in the Cloud that doesn't work
{noformat}

INFO DispatchThread - Getting summary ID for summaryHash=NSdad20f064f790381
08-15-2017 00:18:34.266 INFO DispatchThread - Did not find a usable summary_id, setting info._summary_mode=none, not modifying input summary_id=877B9EF2-C814-49C6-8208-C9DD1FF1C536_search_power_user_NSdad20f064f790381{noformat}
 

vs. one prem power user wher it does manager to find a summary id
{noformat}
08-14-2017 17:20:01.751 INFO  DispatchThread - Getting summary ID for summaryHash=NS973f3154c8039d24
08-14-2017 17:20:01.865 INFO  DispatchThread - Found usable summary_id=190BF483-A193-4A4E-AF37-42F879A3BAB5_search_power_user_NS973f3154c8039d24
08-14-2017 17:20:01.875 INFO  DispatchThread - Found existing Normalized summary=190BF483-A193-4A4E-AF37-42F879A3BAB5_search_power_user_NS973f3154c8039d24{noformat}
 

cc [~accountid:6053a2e3686bf50070459604]

 

 ","14/Aug/17 6:42 PM;6053ae81695c3900707ad0a1;[~accountid:557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e] Yes, I have wait longer than 20mins to reproduce this issue. In fact, I have a cloud stack with acceleration reports and the status remains 0% over nights.","15/Aug/17 8:43 PM;557058:b1e398c5-b004-4a33-8152-18daa36cda4b;Based on [~accountid:613256e96e5e1e0071f078ad]'s analysis, i am deferring this to search technology team for further investigation",05/Sep/17 2:41 PM;613244734a98da0069edb123;[~accountid:557058:761616db-db41-4d02-bf30-e81f409685ed] / [~accountid:6053a2e3686bf50070459604] who is investigating this? Is this a Minty blocker?,05/Sep/17 2:56 PM;6053a2e3686bf50070459604;[~accountid:5cd341229435c90fd6efce8f]/[~accountid:5cd341218ad0f60dca0071cd]/[~accountid:5cd341243e6fd40fe82c5536] - Can MT04 investigate this issue to see if we have a potential blocker for Minty? RA being DOA would be pretty unfortunate,"05/Sep/17 3:59 PM;5cd341229435c90fd6efce8f;Where might I apply for a cloud stack ? The hashes are different. Even though they are different instances the hashes should most likely have been the same for the same search. We literally just hash a normalized version of the remote search string so it not be as brittle as changing

{code}

litsearch index=_internal

{code}

to 

{code}

litsearch index =_internal

{code}

but it's not too far off either. 

[~accountid:613256e96e5e1e0071f078ad] do you still have the search.logs ? ",05/Sep/17 4:44 PM;5cd341218ad0f60dca0071cd;[~accountid:6053ae81695c3900707ad0a1] we also need your search.logs from the cloud environment,"05/Sep/17 7:37 PM;6053ae81695c3900707ad0a1;[~accountid:5cd341229435c90fd6efce8f] [~accountid:5cd341218ad0f60dca0071cd] I have an old stackmakr environment, which you can used to reproduce this issue.

*Stack info:*
 [https://sh1.csa-single-sh-1502936275.splunkcloud.com|https://sh1.csa-single-sh-1502936275.splunkcloud.com/]
 admin username/password: admin/WhisperAdmin250,
 power user username/password: power/password
 for more details for this stack, check: [http://cloudstacks.splunk.com/api/v1/stacks?request_id=1502936275]

And attached one search log here [^search.log]","06/Sep/17 10:17 AM;5cd341229435c90fd6efce8f;Search is 
{code}
index=_internal | stats count by host
{code}

When I run the search an admin the remoteSearch is
{code}
litsearch index=_internal | addinfo type=count label=prereport_events | fields keepcolorder=t ""host"" ""prestats_reserved_*"" ""psrsvd_*"" | prestats count by host
{code}

and it can pick up RAs correctly
{code}
09-06-2017 17:08:41.759 INFO  DispatchThread - Getting summary ID for summaryHash=NS0ff7a6ef349ebf5f
09-06-2017 17:08:41.767 INFO  DispatchThread - Found usable summary_id=E78189B9-C95D-4797-956B-8F5228675BEC_search_power_NS0ff7a6ef349ebf5f
{code}

But when I run as power user an extra {{NOT index=*_archive}} is auto-magically added to the remoteSearch ?
{code}
litsearch (index=""_internal"" NOT index=*_archive) | addinfo type=count label=prereport_events | fields keepcolorder=t ""host"" ""prestats_reserved_*"" ""psrsvd_*"" | prestats count by host
{code}

This causes a summaryID mismatch and we don't pick up the existing RAs
{code}
09-06-2017 17:06:13.373 INFO  DispatchThread - Getting summary ID for summaryHash=NS487519b264a1aa6b
09-06-2017 17:06:13.437 INFO  DispatchThread - Did not find a usable summary_id, setting info._summary_mode=none, not modifying input summary_id=E78189B9-C95D-4797-956B-8F5228675BEC_search_power_NS487519b264a1aa6b
{code}

[~accountid:6053a2e3686bf50070459604] Any idea who is adding this extra extra {{NOT index=*_archive}}  and why we don't use the existing index access permissions system ?","06/Sep/17 10:41 AM;613244734a98da0069edb123;[~accountid:5cd341229435c90fd6efce8f] look into this file, which may explain the mysterious behavior:

{{cfg/bundles/cloud_administration/default/authorize.conf}}
{code:java}
[default]
srchFilter = NOT index=*_archive{code}
As to why it's doing this, it lies with an old feature added to cloud, called data archival. For every index X that has archiving enabled, would create a shadow index X_archive where data would be rolled off to upon reaching data retention time. This was implemented to prevent searching over archived indexes.",06/Sep/17 11:18 AM;613256e96e5e1e0071f078ad;I don't what keys were pressed and how it got reassigned. Ashish back to you.,"06/Sep/17 12:52 PM;6053a2e3686bf50070459604;Ah! This is pretty straightforward. That user has a configured role-based search filter, so the RA is correctly not being used.

In general RA summaries are not usable across roles with different role-based search filters, as fundamentally the results that have been cached by RA correspond to a different search that could have different results.

Now in this *particular* case, the filter happens to be index based, and wouldn't actually affect the correctness of the results *within* a bucket *within* an individual index, but RA isn't that smart (and has never been). It's based upon a hash of the remote search string, and they just don't match.

Removing fixVersion of Minty because that behavior is not a core regression (unless [~accountid:6053ae81695c3900707ad0a1] or someone can prove me wrong). Possible workarounds include:

* Using the standard method of index access filters of {{srchIndexesAllowed}} as [~accountid:5cd341229435c90fd6efce8f] alluded to earlier (which isn't ideal as it can't do a wild carded negation, as this case wants)
* Create and accelerate the same search but owned as the power user, and each other role that has a different role-based search filter. While this seems absurd at first, remember that fundamentally RA is creating a cache of results, and different searches necessitate different caches",06/Sep/17 3:56 PM;6053a2e3686bf50070459604;RA working as designed considering the role-based search filters that are configured,"06/Sep/17 4:08 PM;613244734a98da0069edb123;[~accountid:6053a2e3686bf50070459604] the role based search filters are configured by splunk, and the big question is if the end customer can comprehend this behavior.

I think we probably should just drop that search filter, which I believe is not in the production for any customer, per my knowledge. May need to consult with [~accountid:6053a52f45a3bb006816a155] and some folks like [~accountid:5af5d6f43630716452acf743] regarding why this filter was added at the power role level in the first place.","06/Sep/17 4:27 PM;6053a2e3686bf50070459604;[~accountid:613244734a98da0069edb123] - probably makes sense now that the feature is completely unused, but we'd have to check with the people who created it and see if there are still active users","06/Sep/17 4:32 PM;6053a2e3686bf50070459604;Actually now that I re-read the bug summary, it appears that this bug isn't about sharing RA summaries between users with differing roles, but rather the power user that creates the accelerated report cannot use it *themselves*! That's something different. It's unclear to me whether that in particular is a regression.

This would imply that RA is broken for any user with a configured role-based access filter (*not* cloud specific, easy to repro with a user with an RBAC search filter). [~accountid:6053ae81695c3900707ad0a1]/[~accountid:5cd341229435c90fd6efce8f] can you test whether this is a regression? If we've moved around the code for when we check summaries and when we add the search filter, this could easily regress.

If this is a regression, we should fix for Minty.","06/Sep/17 7:12 PM;6053ae81695c3900707ad0a1;[~accountid:6053a2e3686bf50070459604] [~accountid:5cd341229435c90fd6efce8f] I have tried to enable similar RBAC search filter on some old splunk instances (dash, kimono), this issue can also be reproduced, this is not a regression.

*Reproduce steps:*
 1. Install one fresh on-prem single splunk instance
 2. Edit ""etc/system/local/authorize.conf"", add following stanza
{noformat}
[role_power]
srchIndexesAllowed = *;_internal
srchFilter = NOT index=main
{noformat}
3. Add a power user and login, try to enable a simple report acceleration with search ""index=_internal | stats count by host""
 4. Check ""Summary Status"" and run ad-hoc search ""index=_internal | stats count by host"", check report acceleration is not utilized in search log.",07/Sep/17 12:05 PM;6053a2e3686bf50070459604;Thanks [~accountid:6053ae81695c3900707ad0a1] - we should look at how we could fix this post-Minty,"21/Mar/18 8:39 PM;6053ae81695c3900707ad0a1;I see srchFilter is removed in latest nightlight branch, and report acceleration works fine for power user in nightlight branch now.
 *Cloud build 55c5941b695f*
{noformat}
(.pyenv1) -bash-4.2$ cat etc/apps/cloud_administration/default/authorize.conf
# Cloud-specific capbilities needed for file upload workflow
# (See: AMI-3039, SEC-4631)
[capability::edit_upload_and_index]
[capability::edit_tcp_stream]
[capability::indexes_list_all]

[role_admin]
edit_upload_and_index = enabled
edit_tcp_stream = enabled
indexes_list_all = enabled

[role_index-manager]
edit_indexer_cluster = enabled
edit_sourcetypes = enabled
indexes_edit = enabled
indexes_list_all = enabled
list_indexer_cluster = enabled
list_inputs = enabled
dispatch_rest_to_indexers = enabled
{noformat}
*Cloud build 0d18fdd6d500*
{noformat}
(.pyenv1) -bash-4.2$ cat splunkcloud2/splunk/etc/apps/cloud_administration/default/authorize.conf
# Cloud-specific capbilities needed for file upload workflow
# (See: AMI-3039, SEC-4631)
[capability::edit_upload_and_index]
[capability::edit_tcp_stream]
[capability::indexes_list_all]

[role_admin]
edit_upload_and_index = enabled
edit_tcp_stream = enabled
indexes_list_all = enabled

[role_index-manager]
edit_indexer_cluster = enabled
edit_sourcetypes = enabled
indexes_edit = enabled
indexes_list_all = enabled
list_indexer_cluster = enabled
list_inputs = enabled
dispatch_rest_to_indexers = enabled

[default]
srchFilter = NOT index=*_archive
{noformat}","10/Oct/19 10:54 AM;5cd34132b588780fd3da382f;[~accountid:6053ae81695c3900707ad0a1] I am not familiar with how cloud builds differ from on-premise, or if they still do. Can you confirm whether this is still an issue for the cloud team?","11/Oct/19 7:23 PM;6053ae81695c3900707ad0a1;[~accountid:5cd34132b588780fd3da382f], the srchFilter setting is already removed in cloud builds, so this issue is no longer exist in cloud build. 
Anyway, this is a splunk product issue - ""When srchFilter is applied with power user role, report acceleration will stop working with power user"".
You can check my previous comments for how to reproduce this on a single on-prem instance, https://splunk.atlassian.net/browse/SPL-143947?focusedCommentId=1758890&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-1758890","25/Oct/19 12:51 PM;5cd34132b588780fd3da382f;I'm a bit unclear as to when the {{srchFilter}} setting would be used, and whether there might be a legitimate need for the appended SPL to impact the use of Report Acceleration, such as the Report containing indexes which are restricted to the role with the {{srchFilter}} applied; which would complicate removing it from the final search. Hoping [~accountid:557058:ab939132-f05e-4f5f-8bec-6939a8133dcc] can comment on the validity of this use case.

Edit: Updated to ask James to take a look.","28/Feb/20 8:49 AM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;I confirmed that when I set up a report with acceleration as a particular power user, the report summary does get created, but the power user cannot use the summary because the summary is based on the actual search (e.g., {{search index=_internal | stats count.}}

When the power user then attempts to run this search, we fail the hash comparison because the initial search string is modified by the {{srchFilter}} setting:
{code:java}
02-28-2020 08:19:09.994 INFO  StatsProcessorV2 - Instantiating Stats function count for key=, alias=count
02-28-2020 08:19:09.994 INFO  DispatchThread - BatchMode: allowBatchMode: 1, conf(1): 1, timeline/Status buckets(0):0, realtime(0):0, report pipe empty(0):0, reqTimeOrder(0):0, summarize(0):0, statefulStreaming(0):0
02-28-2020 08:19:09.994 INFO  DispatchThread - required fields list to add to remote search = host,prestats_reserved_*,psrsvd_*
02-28-2020 08:19:09.994 INFO  SearchParser - PARSING: fields keepcolorder=t ""host"" ""prestats_reserved_*"" ""psrsvd_*""
02-28-2020 08:19:09.994 INFO  DispatchCommandProcessor - summaryHash=a7848872aeaa9e9c summaryId=F9F580C3-C536-4CBF-AF79-6F7AFBC6F5CC_search_test_a7848872aeaa9e9c remoteSearch=litsearch (index=_internal NOT index=main) | addinfo  type=count label=prereport_events | fields  keepcolorder=t ""host"" ""prestats_reserved_*"" ""psrsvd_*"" | prestats  count by host
02-28-2020 08:19:09.994 INFO  DispatchCommandProcessor - summaryHash=NSe38aa3794b36bbf0 summaryId=F9F580C3-C536-4CBF-AF79-6F7AFBC6F5CC_search_test_NSe38aa3794b36bbf0 remoteSearch=litsearch (index=_internal NOT index=main) | addinfo type=count label=prereport_events | fields keepcolorder=t ""host"" ""prestats_reserved_*"" ""psrsvd_*"" | prestats count by host
02-28-2020 08:19:09.994 INFO  DispatchThread - Getting summary ID for summaryHash=NSe38aa3794b36bbf0
02-28-2020 08:19:10.062 INFO  DispatchThread - Did not find a usable summary_id, setting info._summary_mode=none, not modifying input summary_id=F9F580C3-C536-4CBF-AF79-6F7AFBC6F5CC_search_test_NSe38aa3794b36bbf0
 {code}
In a way this makes sense, since an accelerated report could be viewed by many users - therefore, the acceleration probably should not respect the {{srchFilter}} setting; instead, we would want the {{srchFilter}} to be applied after the accelerated results were retrieved, somehow. However this would only apply for a search which was *shared*. In this case, this probably should not apply since the search is *private.* But then the same question arises - would we want the actually accelerated data to respect the user's {{srchFilter}} setting? What if the user's {{srchFilter}} changes? Then the previously accelerated data would become obsolete and need to be regenerated.

The difficulties of sharing search results among different users are noted in the original Report Acceleration ERD: [https://splunk.atlassian.net/wiki/display/PROD/Report+Acceleration] (see the ""Summary sharing"" section).

I'm not sure of the best way to fix this, but it seems like work that we would undertake in our investigation of improved search caching strategies, instead of making a small point fix to address this one case.",11/Jan/21 10:37 AM;5cd34132b588780fd3da382f;Lowering priority as it doesn't seem any customers are complaining about this. [~accountid:557058:ab939132-f05e-4f5f-8bec-6939a8133dcc] the original description suggested this was a version-dependent issue. Can you comment on whether that is expected due to some change?,"11/Jan/21 11:21 AM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;[~accountid:5cd34132b588780fd3da382f], no, this is just a problem with report acceleration. The instructions in Cliff's message still work to reproduce the issue on Icefall: https://splunk.atlassian.net/browse/SPL-143947?focusedCommentId=1758890&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-1758890.

TL;DR: If you have a RBAC search filter, you can create accelerated reports, but can't use the summaries for your own accelerated reports.

 ",04/Nov/21 7:18 AM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;Added affectsVersions as this issue is still assumed to occur on more recent versions than Quake.,"14/Dec/21 2:49 PM;5a1ef9dc40207c40ef9126d7;[~accountid:5cd34132b588780fd3da382f] - Was this ever re-visited? Looks like another customer reported this issue. Also, is this supposed to be treated as a bug or an enhancement?","14/Dec/21 2:59 PM;5cd34132b588780fd3da382f;This has not been revisited lately, but we do have a KR this quarter to reduce tech debt. [~accountid:557058:ab939132-f05e-4f5f-8bec-6939a8133dcc] perhaps someone can look into this in a near-term sprint?","14/Dec/21 3:26 PM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;[~accountid:6053a2e3686bf50070459604] , just to verify, do we feel report acceleration more properly resides with the Structured Search team or with Search Generation?","04/Apr/22 3:03 PM;557058:ab939132-f05e-4f5f-8bec-6939a8133dcc;Moving to Structured Search as this seems like a specific issue related to search filters in conjunction with acceleration, not with search filters specifically. CC [~accountid:6053a2e3686bf50070459604]  FYI, let me know if that’s incorrect.",28/Apr/22 12:44 PM;6126b3d8ec0a83006a42d40a;[~accountid:557058:89ace195-adc2-4f25-9924-ded7c8f10bb6] to review how to address this in docs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1910423,SPL-202130,[DMA] Performance Optimization,To Do,06/Sep/17 4:32 PM
"[PUBLIC]  [Windows]""Splunkd daemon is not responding"" when edit local windows event log collection",SPL-143111,834482,Bug,Resolved,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Won't Fix,Wim Colgate,557058:ef43a796-f89f-4463-801b-30ded73fb41e,YW,557058:eadbf820-cf9a-4bd8-82e1-0c23958c2fc8,YW,557058:eadbf820-cf9a-4bd8-82e1-0c23958c2fc8,13/Jul/17 8:16 PM,30/Jul/25 4:25 PM,,09/Dec/22 11:52 AM,10.0.x,6.6.0 GA (Kimono),7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,REST API,Search - Splunkweb,,,,0,Blocking_Functional_Test,regression_product_bug,requires-triage,ShanghaiCoreUIQA,Windows_Team_2.0,,,"# install a Windows version splunk, in my test, I used the zip version.
 # login as admin and goes to Settings --> Data inputs --> Event log collections–> edit
 # click ""add all"", then click the save button.
 # an error message shows up at the top as: ""Splunkd daemon is not responding..."" and errors with code 15009 show up in the splunkd.log.

Sometimes, the save succeeded, then repeat step 2 and 3 will reproduce this issue. And after this, the web on port 8000 and rest services on 8089 are still availible. No crash log for this.

 ","Splunk Version 6.6.2 Splunk Build 4b804538c686

Splunk Version6.7.0  Splunk Build60b1ee5e6074

Windows 2012R2

 ",Blaine Wastell,Izzy Park,Jian Zhang,Jo Hornsby,Scott Lu,Sunny Choi,Wim Colgate,YW,,,,,,,,,,,,,,,,,,,,,,,,,,,,5cbf5cf63d24270e75a8f74a,5a1ef9dc40207c40ef9126d7,5af8dc2f82ddd25cccb8bfbe,557058:f87e2651-5132-4aed-8b96-f608ee969435,5aa2e5f5d0c9c42eefad8009,6053a40806cbba006a0da0e9,557058:ef43a796-f89f-4463-801b-30ded73fb41e,557058:eadbf820-cf9a-4bd8-82e1-0c23958c2fc8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Global,,,,,,,,,,,SPL-159994,,,,,,,,,,,,,,,,,TNT-456,,,,,,,,,,,,13/Jul/17 8:14 PM;yowang;event-log-error.png;https://splunk.atlassian.net/rest/api/3/attachment/content/315832,13/Jul/17 8:14 PM;yowang;resource_usage-6.6.2.log;https://splunk.atlassian.net/rest/api/3/attachment/content/315830,13/Jul/17 8:15 PM;yowang;resource_usage.log;https://splunk.atlassian.net/rest/api/3/attachment/content/315828,13/Jul/17 8:14 PM;yowang;splunkd-6.6.2.log;https://splunk.atlassian.net/rest/api/3/attachment/content/315831,13/Jul/17 8:15 PM;yowang;splunkd.log;https://splunk.atlassian.net/rest/api/3/attachment/content/315829,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4303ff2d,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,88041600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,Fri Dec 09 16:30:18 UTC 2022,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Operability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ysu(ysu),abrown(abrown),bwastell(bwastell),bkovacevic(bkovacevic),echeung(echeung),ipark(ipark),2aea4c9c-d59e-41a8-9cf7-d012e3cb0c08(2aea4c9c-d59e-41a8-9cf7-d012e3cb0c08),slu(slu),schoi(schoi),wcolgate(wcolgate),yowang(yowang),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wim Colgate,557058:ef43a796-f89f-4463-801b-30ded73fb41e,,,,,,,,,,,,,,,,,,0|i3djxz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Wim Colgate,557058:ef43a796-f89f-4463-801b-30ded73fb41e,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Restricted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2017-07-17 22:10:40.902,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10001_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_36797572573,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017-07-17 22:10:40.902,2017-07-17 22:10:40.902,,,,,,Wim Colgate,557058:ef43a796-f89f-4463-801b-30ded73fb41e,,,,,,17/Jul/17 3:10 PM;6053a40806cbba006a0da0e9;assuming this is in your area [~accountid:5cbf5cf63d24270e75a8f74a],17/Jul/17 3:55 PM;5cbf5cf63d24270e75a8f74a;[~accountid:557058:761616db-db41-4d02-bf30-e81f409685ed] It looks like a mission team in your area works on the Search - Splunkweb component. Does this belong to your area?,01/Sep/17 11:19 AM;557058:ef43a796-f89f-4463-801b-30ded73fb41e;There is no crash; and it looks like the ExecProcessor was in fact attempting to register/run all the windows event channels in the list. So it is probably the case where the timeout is too short for processing all those windows event channel entries. I would suggest changing the timeout as an experiment.,"23/Aug/18 11:48 PM;5aa2e5f5d0c9c42eefad8009;[~accountid:557058:eadbf820-cf9a-4bd8-82e1-0c23958c2fc8] Can we try use wait_for_condition on clicking the save button? As Wim suggested, the message might be gone after a while.","12/Sep/18 5:40 PM;5c05d10cc22785142f67ff97;[~accountid:557058:ef43a796-f89f-4463-801b-30ded73fb41e], I tried to change the default timeout value of the rest proxy from 30 to 200 and it solved the problem on my windows instance. However I don't think changing the timeout value is a good solution because it will apply to all the requests going through the rest proxy.

The root cause might be that it takes too much time for the original request to respond when the log list is very long. The request took approximately 70 seconds on my windows instance and a bug has been fired to track this: https://splunk.atlassian.net/browse/SPL-159994

I would like to set SPL-159994 as the blocker of this issue.","10/Jan/19 11:12 AM;5ac6aaab0658540c0debafcd;This seems to be an Enterprise issue, and updating Product Backlog area accordingly",06/May/19 9:58 AM;6053a74994d7b90069f8435f;Bulk updating Mission Team to Operability,"21/Apr/20 7:52 PM;557058:eadbf820-cf9a-4bd8-82e1-0c23958c2fc8;Some update on Windows 2019, this test is passed on Windows 2016, but failed on Windows 2019.",22/Apr/20 8:31 AM;557058:ef43a796-f89f-4463-801b-30ded73fb41e;[~accountid:557058:eadbf820-cf9a-4bd8-82e1-0c23958c2fc8] does it still fail if your change the timeout value? ,"23/Apr/20 1:22 AM;557058:eadbf820-cf9a-4bd8-82e1-0c23958c2fc8;[~accountid:557058:ef43a796-f89f-4463-801b-30ded73fb41e], 200s works for our test, since there's already a bug filed by [~accountid:5c05d10cc22785142f67ff97] for tracking this issue. I will add a ProductBug mark for the test.","15/Nov/22 9:32 AM;62c2a8697273faf658f024d8;adding a new label ""Blocking_Functional_Test"" to the issue, for tracking all the SPL issues associated with the ARTs automation triaging tool","28/Nov/22 2:17 PM;557058:4240d4ce-c6e5-4077-a73c-949f1ad0796b;Bulk-adding label ""requires-triage"" to indicate we need to review this item.  Please triage and fix P0/P1, and at least triage everything else.",07/Dec/22 2:26 PM;5a1ef9dc40207c40ef9126d7;[~accountid:557058:ef43a796-f89f-4463-801b-30ded73fb41e] - could you triage this please? ty,"07/Dec/22 5:24 PM;557058:ef43a796-f89f-4463-801b-30ded73fb41e;There is nothing in code that we can do here to make it any faster.  This is strictly a testing harness issue, right?",09/Dec/22 8:30 AM;5a1ef9dc40207c40ef9126d7;Well then. Perhaps we propose to close this as won’t do? [~accountid:5d0995dcc184b50c22ff1c46] ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,09/Dec/22 11:52 AM
[PUBLIC] Upload modal should use size=large File element,SPL-141982,817029,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,,,Michael Papale,557058:bcc33dd6-d888-4703-9af4-9cff17d0a86b,Michael Papale,557058:bcc33dd6-d888-4703-9af4-9cff17d0a86b,24/May/17 11:32 AM,30/Jul/25 4:26 PM,,,10.0.x,7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DMC,,,,,0,oper_bug_bash,,,,,,,"In the upload UI, we use a size=<default> File element. The user has to make sure to drag file the to the element exactly.

We should use the size=large File instead.",,Andrea Hong,Dhananjay Koshe [X],Michael Papale,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a969695c3900707a9628,613244734a98da0069edb123,557058:bcc33dd6-d888-4703-9af4-9cff17d0a86b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@664c12fe,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,160790400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Thu Aug 20 05:01:34 UTC 2020,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Operability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ahong(ahong),dkoshe(dkoshe),echeung(echeung),mpapale(mpapale),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i3ay0f:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SG-2,,,,,,,,Restricted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DevEcosystemSprint1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2017-08-21 15:55:56.351,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017-08-21 15:55:56.351,2017-08-21 15:55:56.351,,,,,,Michael Papale,557058:bcc33dd6-d888-4703-9af4-9cff17d0a86b,,,,,,"24/May/17 11:53 AM;557058:bcc33dd6-d888-4703-9af4-9cff17d0a86b;Meantime, the workaround is

<Modal.Body style=\{\{ overflow: 'hidden' }}> // otherwise removing bled-in bootstrap styles cause overflow

and

<File style=\{\{ position: 'relative' }} > // otherwise upload SVG image shows up in a weird place

finally

Some kind of hack to remove the bootstrap styles from the <label> within the File component.

 ",21/Aug/17 8:55 AM;613244734a98da0069edb123;[~accountid:5d3756fd276fe40c53371f66] please triage it.. Mike's no more with company. Taking it out of Minty as well.,06/May/19 9:55 AM;6053a74994d7b90069f8435f;Bulk updating Mission Team to Operability,"19/Aug/20 10:01 PM;6053a969695c3900707a9628;Removed fixVersion Rarity since the Rarity release branch has been cut as of Aug 17th. If you would like to include a fix for this bug for the release, please request a [Release Exception|https://splunk.atlassian.net/wiki/display/PROD/Release+Exception+Request]. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Aug/17 9:10 AM
"[PUBLIC] DataModel Editor - when child object has same name as inherited field, inherited field does not show in the inherited fields list. ",SPL-141693,812152,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,As Designed,Manoj Pawar,557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d,Valentine Prisiajni,6036b79bf032740068734426,Valentine Prisiajni,6036b79bf032740068734426,09/May/17 11:16 AM,30/Jul/25 4:24 PM,,17/May/17 11:20 AM,10.0.x,6.5.0 GA (Ivory),6.5.1,6.5.2,6.5.3,6.5.4,6.6.0 GA (Kimono),6.6.1,6.6.10,6.6.11,6.6.12,6.6.2,6.6.3,6.6.4,6.6.5,6.6.6,6.6.7,6.6.8,6.6.9,7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,6.5.5,,,,,,,,Data Model - UI,,,,,0,may15may28,SUSTAINING_FRONTEND,,,,,,"When creating a data model, if the name of the child has the same name as an inherited field, the inherited field does not show in the UI upon creation of the child. 

Steps to reproduce:

1. Create a DataModel with index=_internal

2. Add 2 fields (example, current_size and build)

3. Create a child object under the parent and call it 'current_size'.

4. Notice that only the build field shows in the inherited fields. 


Current workaround:

1.Call the child object something else upon creation, i.e: 'asdf' as the name. 

2. Notice that both inherited fields show up. Then rename the object to the field name 'current_size'.

3. Notice that the field stays in the inherited fields list. 


",6.5.x SHC,Andrew Brown,Anil Nagavaram,Antony Wang,Jo Hornsby,Kishore Vasudeva,Manoj Pawar,Simon Fishel,Sunny Wang,Valentine Prisiajni,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,5bbbd7d2d517b631d894a23d,557058:9b250150-4703-4e0c-a02b-ad536bb0d408,557058:f87e2651-5132-4aed-8b96-f608ee969435,557058:86ec36cd-ed38-4da0-9ca8-d452b6d025d8,557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d,557058:e5764eeb-1543-470e-8844-5997eb32f42f,557058:bc56650e-dd7f-4322-a130-700dc7ba1a1f,6036b79bf032740068734426,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15/May/17 3:20 PM;mpawar;datamodel.png;https://splunk.atlassian.net/rest/api/3/attachment/content/292830,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Palantir Technologies Inc.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@34397a86,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,9.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,143251200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Thu Mar 11 08:04:08 UTC 2021,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,9.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),dawang(dawang),jisenberg(jisenberg),mpawar(mpawar),sfishel(sfishel),vprisiajni(vprisiajni),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sustaining,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Manoj Pawar,557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d,,,,,,,,,,,,,,,,,,0|i3a75b:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Closed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,481205,,,,,,,,,,,,,,,,,,,,Restricted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2017-05-15 22:19:24.702,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_664177061_*|*_10039_*:*_1_*:*_27295410,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017-05-15 22:19:24.702,2017-05-15 22:19:24.702,,,,,,,,,,,,,"15/May/17 3:19 PM;557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d;The reason why this happens is because, we get multiple (two) fields with same fieldname (current_size).

When i created a current_size as a child of root, i got following response:
{code:java}""fields"": [
    {
        ""fieldName"": ""current_size"",
        ""owner"": ""root_s"",
        ""type"": ""number"",
        ""fieldSearch"": """",
        ""required"": false,
        ""multivalue"": false,
        ""hidden"": false,
        ""editable"": true,
        ""displayName"": ""current_size"",
        ""comment"": """"
    },
    {
        ""fieldName"": ""bytes"",
        ""owner"": ""root_s"",
        ""type"": ""number"",
        ""fieldSearch"": """",
        ""required"": false,
        ""multivalue"": false,
        ""hidden"": false,
        ""editable"": true,
        ""displayName"": ""bytes"",
        ""comment"": """"
    },
    {
        ""fieldName"": ""current_size"",
        ""owner"": ""root_s.current_size"",
        ""type"": ""objectCount"",
        ""fieldSearch"": """",
        ""required"": false,
        ""multivalue"": false,
        ""hidden"": false,
        ""editable"": false,
        ""displayName"": ""current_size"",
        ""comment"": """"
    }
]
{code}
As you can see that two fields have same value for the fieldname attribute which is *""fieldName"": ""current_size""*. So, what UI code does is, it checks if there is already a field in the list with a particular name ansd then instead of adding it as new, it overwrites that existing field with properties of new field. So, eventually UI takes following list of fields to render on the page:
{code:java}""fields"": [
    {
        ""fieldName"": ""bytes"",
        ""owner"": ""root_s"",
        ""type"": ""number"",
        ""fieldSearch"": """",
        ""required"": false,
        ""multivalue"": false,
        ""hidden"": false,
        ""editable"": true,
        ""displayName"": ""bytes"",
        ""comment"": """"
    },
    {
        ""fieldName"": ""current_size"",
        ""owner"": ""root_s.current_size"",
        ""type"": ""objectCount"",
        ""fieldSearch"": """",
        ""required"": false,
        ""multivalue"": false,
        ""hidden"": false,
        ""editable"": false,
        ""displayName"": ""current_size"",
        ""comment"": """"
    }
]
{code}
Note: fields which are prefixed with root field (e.g. ""root_s.current_size"") are used for the parent-to-child structure on the left pane and not for the inherited fields.

So, in terms of renaming field which was mentioned in the description:

The reason why this works is because, renaming only renames the display name and not the field name.
 I created a child called ""test"" and then renamed it to ""current_size"" and this is how we got the fields information.

Before rename:
{code:java}""fields"": [
    {
        ""fieldName"": ""current_size"",
        ""owner"": ""root_s"",
        ""type"": ""number"",
        ""fieldSearch"": """",
        ""required"": false,
        ""multivalue"": false,
        ""hidden"": false,
        ""editable"": true,
        ""displayName"": ""current_size"",
        ""comment"": """"
    },
    {
        ""fieldName"": ""bytes"",
        ""owner"": ""root_s"",
        ""type"": ""number"",
        ""fieldSearch"": """",
        ""required"": false,
        ""multivalue"": false,
        ""hidden"": false,
        ""editable"": true,
        ""displayName"": ""bytes"",
        ""comment"": """"
    },
    {
        ""fieldName"": ""test"",
        ""owner"": ""root_s.test"",
        ""type"": ""objectCount"",
        ""fieldSearch"": """",
        ""required"": false,
        ""multivalue"": false,
        ""hidden"": false,
        ""editable"": false,
        ""displayName"": ""current_size"",
        ""comment"": """"
    }
]
{code}
After rename:
{code:java}""fields"": [
    {
        ""fieldName"": ""current_size"",
        ""owner"": ""root_s"",
        ""type"": ""number"",
        ""fieldSearch"": """",
        ""required"": false,
        ""multivalue"": false,
        ""hidden"": false,
        ""editable"": true,
        ""displayName"": ""current_size"",
        ""comment"": """"
    },
    {
        ""fieldName"": ""bytes"",
        ""owner"": ""root_s"",
        ""type"": ""number"",
        ""fieldSearch"": """",
        ""required"": false,
        ""multivalue"": false,
        ""hidden"": false,
        ""editable"": true,
        ""displayName"": ""bytes"",
        ""comment"": """"
    },
    {
        ""fieldName"": ""test"",
        ""owner"": ""root_s.test"",
        ""type"": ""objectCount"",
        ""fieldSearch"": """",
        ""required"": false,
        ""multivalue"": false,
        ""hidden"": false,
        ""editable"": false,
        ""displayName"": ""current_size"",
        ""comment"": """"
    }
]
{code}

As you can see in the above fields, the last field's fieldname is ""test"" and displayName is ""current_size"".
Check out this screenshot: !datamodel.png|thumbnail!","16/May/17 12:33 PM;6036b79bf032740068734426;[~accountid:557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d] Sounds good. This doesn't necessarily sound buggy but I can't tell if thats what we want to happen or not. Should this be something documented (if there is nothing to fix)?
","17/May/17 10:30 AM;557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d;[~accountid:6036b79bf032740068734426], yeah. looks like this is an expected behavior and i think we should document this. 

[~accountid:557058:e5764eeb-1543-470e-8844-5997eb32f42f], can you confirm? ","17/May/17 11:06 AM;557058:e5764eeb-1543-470e-8844-5997eb32f42f;[~accountid:557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d], agreed, documenting this as a known issue sounds right to me.","17/May/17 11:16 AM;557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d;cc: [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80], [~accountid:557058:89ace195-adc2-4f25-9924-ded7c8f10bb6],

We should document this as a known issue.","17/May/17 11:48 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d] it's already documented as a known issue for the one Affects Version on the JIRA. See http://docs.splunk.com/Documentation/Splunk/6.5.0/ReleaseNotes/Knownissues#Uncategorized_issues. 

Please add other relevant Affects Versions to make this appear in additional versions of the known issues. ",28/Sep/20 1:17 PM;557058:2d2a7f1d-c9e1-4f3c-95c5-24ed73d0b69d;SevLevelCleanupJI,25/Jan/21 11:00 PM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Closing as part of 6+ months old resolved bugs bulk clean up by QA Metrics Team. CC [~accountid:5ebddcc35862510b798f5abd] [~accountid:5d3547e5c2db730c59b25bdc] [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408].,11/Mar/21 12:04 AM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Rolling back Assignee & Developer from Jan 25th bulk closure operation. CC [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408]. Assignee: mpawar. Developer: None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,17/May/17 11:20 AM
[PUBLIC] Clicking Install multiple times in Install dialog causes error,SPL-141274,806431,Bug,New,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,,,,greg walton,557058:1c264e5f-b507-471f-9d47-375ef4af2d0c,greg walton,557058:1c264e5f-b507-471f-9d47-375ef4af2d0c,19/Apr/17 3:21 PM,30/Jul/25 4:25 PM,,,10.0.x,7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DMC,,,,,0,oper_bug_bash,,,,,,,Clicking the primary button on a dialog that causes a deployment multiple times will cause an error to display.  The button should be disabled after the first click.,"Splunk Version
6.5.0
Splunk Build
2c91bd663bcf",Andrea Hong,Blaine Wastell,greg walton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6053a969695c3900707a9628,5cbf5cf63d24270e75a8f74a,557058:1c264e5f-b507-471f-9d47-375ef4af2d0c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,19/Apr/17 3:19 PM;gwalton;multiple_clicks.mov;https://splunk.atlassian.net/rest/api/3/attachment/content/286109,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@1f8e72b9,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,160790400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Thu Aug 20 04:55:43 UTC 2020,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Operability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ahong(ahong),bwastell(bwastell),ccampbell(ccampbell),echeung(echeung),gwalton(gwalton),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,greg walton,557058:1c264e5f-b507-471f-9d47-375ef4af2d0c,,,,,,,,,,,,,,,,,,0|i3990f:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SG-2,,,,,,,,Restricted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2017-08-14 18:43:23.751,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017-08-14 18:43:23.751,2017-08-14 18:43:23.751,,,,,,,,,,,,,14/Aug/17 11:43 AM;5cbf5cf63d24270e75a8f74a;[~accountid:5d3756fd276fe40c53371f66] Let's look at this for Minty. If it is too big we can decide to wait.,"14/Aug/17 12:47 PM;5d3756fd276fe40c53371f66;[~accountid:5cbf5cf63d24270e75a8f74a], since Code Complete is tomorrow I think we are too late for Minty... unless you've decided this is a blocker.",14/Aug/17 2:25 PM;5cbf5cf63d24270e75a8f74a;We can live with this. I'll remove the Minty tag.,06/May/19 9:59 AM;6053a74994d7b90069f8435f;Bulk updating Mission Team to Operability,"19/Aug/20 9:55 PM;6053a969695c3900707a9628;Removed fixVersion Rarity since the Rarity release branch has been cut as of Aug 17th. If you would like to include a fix for this bug for the release, please request a [Release Exception|https://splunk.atlassian.net/wiki/display/PROD/Release+Exception+Request]. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,19/Apr/17 3:21 PM
[PUBLIC] Splunk having problems extracting json file consisting of 68k plus key-value pairs,SPL-140765,801302,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,As Designed,Victor Ebken,557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792,Luan Tran,557058:92f1e917-9794-47e9-bd18-8566eacb3bb2,Luan Tran,557058:92f1e917-9794-47e9-bd18-8566eacb3bb2,04/Apr/17 10:21 AM,30/Jul/25 4:25 PM,,09/May/18 6:50 PM,10.0.x,6.5.3,6.5.4,6.6.10,6.6.11,6.6.12,6.6.2,6.6.3,6.6.4,6.6.5,6.6.6,6.6.7,6.6.8,6.6.9,7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,,,,6.5.9,,,,,,,,Search - Core,,,,,0,AAPL,field_extraction,json,scalability,SUSTAINING_BACKEND,,,"*Customer:* Apple ISO
*Issue:* Customer is trying to ingest a json event that consists of 68,000 plus KV pairs:
{noformat}
{ ""timestamp"" : ""2017-03-15 12:28:19 PST"",
""SYS/ID"" : ""1"",
""SYS/FIELD2"" : ""VAL2"",
""SYS/FIELD3"" : ""VAL3"",
""SYS/FIELD4"" : ""VAL4"",
""SYS/FIELD5"" : ""VAL5"",
""SYS/FIELD6"" : ""VAL6"",
""SYS/FIELD7"" : ""VAL7"",
""SYS/FIELD8"" : ""VAL8"",
""SYS/FIELD9"" : ""VAL9"",
""SYS/FIELD10"" : ""VAL10"",
""SYS/FIELD11"" : ""VAL11"",
""SYS/FIELD12"" : ""VAL12"",
""SYS/FIELD13"" : ""VAL13"",
""SYS/FIELD14"" : ""VAL14"",
""SYS/FIELD15"" : ""VAL15"",
""SYS/FIELD16"" : ""VAL16"",
""SYS/FIELD17"" : ""VAL17"",
""SYS/FIELD18"" : ""VAL18"",
""SYS/FIELD19"" : ""VAL19"",
""SYS/FIELD20"" : ""VAL20"",
""SYS/FIELD21"" : ""VAL21"",
""SYS/FIELD21"" : ""VAL22"",
""SYS/FIELD23"" : ""VAL23"",
""SYS/FIELD24"" : ""VAL24"",
""SYS/FIELD25"" : ""VAL25"",
""AAA/00001"" : ""1"",
""AAA/00002"" : ""2"",
""AAA/00003"" : ""3"",
""AAA/00004"" : ""4"",
""AAA/00005"" : ""5"",
""AAA/00006"" : ""6"",
""AAA/00007"" : ""7"",
""AAA/00008"" : ""8"",
""AAA/00009"" : ""9"",
""AAA/00010"" : ""10"",
""AAA/00011"" : ""11"",
""AAA/00012"" : ""12"",
""AAA/00013"" : ""13"",
""AAA/00014"" : ""14"",
""AAA/00015"" : ""15"",
""AAA/00016"" : ""16"",
""AAA/00017"" : ""17"",
""AAA/00018"" : ""18"",
""AAA/00019"" : ""19"",
""AAA/00020"" : ""20"",
""AAA/00021"" : ""21"",
""AAA/00022"" : ""22"",
""AAA/00023"" : ""23"",
""AAA/00024"" : ""24"",
""AAA/00025"" : ""25"",
""AAA/00026"" : ""26"",
""AAA/00027"" : ""27"",
""AAA/00028"" : ""28"",
""AAA/00029"" : ""29"",
""AAA/00030"" : ""30"",
""AAA/00031"" : ""31"",
""AAA/00032"" : ""32"",
""AAA/00033"" : ""33"",
""AAA/00034"" : ""34"",
""AAA/00035"" : ""35"",
""AAA/00036"" : ""36"",
""AAA/00037"" : ""37"",
""AAA/00038"" : ""38"",
""AAA/00039"" : ""39"",
""AAA/00040"" : ""40"",
""AAA/00041"" : ""41"",
""AAA/00042"" : ""42"",
""AAA/00043"" : ""43"",
""AAA/00044"" : ""44"",
""AAA/00045"" : ""45"",
""AAA/00046"" : ""46"",
""AAA/00047"" : ""47"",
""AAA/00048"" : ""48"",
""AAA/00049"" : ""49"",
""AAA/00050"" : ""50"",
""AAA/00051"" : ""51"",
""AAA/00052"" : ""52"",
""AAA/00053"" : ""53"",
""AAA/00054"" : ""54"",
""AAA/00055"" : ""55"",
""AAA/00056"" : ""56"",
""AAA/00057"" : ""57"",
""AAA/00058"" : ""58"",
""AAA/00059"" : ""59"",
""AAA/00060"" : ""60"",
""AAA/00061"" : ""61"",
""AAA/00062"" : ""62"",
""AAA/00063"" : ""63"",
""AAA/00064"" : ""64"",
""AAA/00065"" : ""65"",
""AAA/00066"" : ""66"",
""AAA/00067"" : ""67"",
""AAA/00068"" : ""68"",
""AAA/00069"" : ""69"",
""AAA/00070"" : ""70"",
""AAA/00071"" : ""71"",
""AAA/00072"" : ""72"",
""AAA/00073"" : ""73"",
""AAA/00074"" : ""74"",
""AAA/00075"" : ""75"",
""AAA/00076"" : ""76"",
""AAA/00077"" : ""77"",
""AAA/00078"" : ""78"",
""AAA/00079"" : ""79"",
""AAA/00080"" : ""80"",
""AAA/00081"" : ""81"",
""AAA/00082"" : ""82"",
""AAA/00083"" : ""83"",
""AAA/00084"" : ""84"",
""AAA/00085"" : ""85"",
""AAA/00086"" : ""86"",
""AAA/00087"" : ""87"",
""AAA/00088"" : ""88"",
""AAA/00089"" : ""89"",
""AAA/00090"" : ""90"",
""AAA/00091"" : ""91"",
""AAA/00092"" : ""92"",
""AAA/00093"" : ""93"",
""AAA/00094"" : ""94"",
""AAA/00095"" : ""95"",
""AAA/00096"" : ""96"",
""AAA/00097"" : ""97"",
""AAA/00098"" : ""98"",
""AAA/00099"" : ""99"",
""AAA/00100"" : ""100"",
""AAA/00101"" : ""101"",
""AAA/00102"" : ""102"",
""AAA/00103"" : ""103"",
""AAA/00104"" : ""104"",
""AAA/00105"" : ""105"",
""AAA/00106"" : ""106"",
""AAA/00107"" : ""107"",
""AAA/00108"" : ""108"",
""AAA/00109"" : ""109"",
""AAA/00110"" : ""110"",
""AAA/00111"" : ""111"",
""AAA/00112"" : ""112"",
""AAA/00113"" : ""113"",
""AAA/00114"" : ""114"",
""AAA/00115"" : ""115"",
""AAA/00116"" : ""116"",
""AAA/00117"" : ""117"",
""AAA/00118"" : ""118"",
""AAA/00119"" : ""119"",
""AAA/00120"" : ""120"",
""AAA/00121"" : ""121"",
""AAA/00122"" : ""122"",
""AAA/00123"" : ""123"",
""AAA/00124"" : ""124"",
""AAA/00125"" : ""125"",
""AAA/00126"" : ""126"",
""AAA/00127"" : ""127"",
""AAA/00128"" : ""128"",
""AAA/00129"" : ""129"",
""AAA/00130"" : ""130"",
""AAA/00131"" : ""131"",
""AAA/00132"" : ""132"",
""AAA/00133"" : ""133"",
""AAA/00134"" : ""134"",
""AAA/00135"" : ""135"",
""AAA/00136"" : ""136"",
""AAA/00137"" : ""137"",
""AAA/00138"" : ""138"",
""AAA/00139"" : ""139"",
""AAA/00140"" : ""140"",
""AAA/00141"" : ""141"",
""AAA/00142"" : ""142"",
""AAA/00143"" : ""143"",
""AAA/00144"" : ""144"",
""AAA/00145"" : ""145"",
""AAA/00146"" : ""146"",
""AAA/00147"" : ""147"",
""AAA/00148"" : ""148"",
""AAA/00149"" : ""149"",
""AAA/00150"" : ""150"",
""AAA/00151"" : ""151"",
""AAA/00152"" : ""152"",
""AAA/00153"" : ""153"",
""AAA/00154"" : ""154"",
""AAA/00155"" : ""155"",
""AAA/00156"" : ""156"",
""AAA/00157"" : ""157"",
""AAA/00158"" : ""158"",
""AAA/00159"" : ""159"",
""AAA/00160"" : ""160"",
""AAA/00161"" : ""161"",
""AAA/00162"" : ""162"",
""AAA/00163"" : ""163"",
""AAA/00164"" : ""164"",
""AAA/00165"" : ""165"",
""AAA/00166"" : ""166"",
""AAA/00167"" : ""167"",
""AAA/00168"" : ""168"",
""AAA/00169"" : ""169"",
""AAA/00170"" : ""170"",
""AAA/00171"" : ""171"",
""AAA/00172"" : ""172"",
""AAA/00173"" : ""173"",
""AAA/00174"" : ""174"",
""AAA/00175"" : ""175"",
""AAA/00176"" : ""176"",
""AAA/00177"" : ""177"",
""AAA/00178"" : ""178"",
""AAA/00179"" : ""179"",
""AAA/00180"" : ""180"",
""AAA/00181"" : ""181"",
""AAA/00182"" : ""182"",
""AAA/00183"" : ""183"",
""AAA/00184"" : ""184"",
""AAA/00185"" : ""185"",
""AAA/00186"" : ""186"",
""AAA/00187"" : ""187"",
""AAA/00188"" : ""188"",
""AAA/00189"" : ""189"",
""AAA/00190"" : ""190"",
""AAA/00191"" : ""191"",
""AAA/00192"" : ""192"",
""AAA/00193"" : ""193"",
""AAA/00194"" : ""194"",
""AAA/00195"" : ""195"",
""AAA/00196"" : ""196"",
""AAA/00197"" : ""197"",
""AAA/00198"" : ""198"",
""AAA/00199"" : ""199"",
""AAA/00200"" : ""200"",
""AAA/00201"" : ""201"",
""AAA/00202"" : ""202"",
""AAA/00203"" : ""203"",
""AAA/00204"" : ""204"",
""AAA/00205"" : ""205"",
""AAA/00206"" : ""206"",
""AAA/00207"" : ""207"",
""AAA/00208"" : ""208"",
""AAA/00209"" : ""209"",
""AAA/00210"" : ""210"",
""AAA/00211"" : ""211"",
""AAA/00212"" : ""212"",
""AAA/00213"" : ""213"",
""AAA/00214"" : ""214"",
""AAA/00215"" : ""215"",
""AAA/00216"" : ""216"",
""AAA/00217"" : ""217"",
""AAA/00218"" : ""218"",
""AAA/00219"" : ""219"",
""AAA/00220"" : ""220"",
""AAA/00221"" : ""221"",
""AAA/00222"" : ""222"",
""AAA/00223"" : ""223"",
""AAA/00224"" : ""224"",
""AAA/00225"" : ""225"",
""AAA/00226"" : ""226"",
""AAA/00227"" : ""227"",
""AAA/00228"" : ""228"",
""AAA/00229"" : ""229"",
""AAA/00230"" : ""230"",
""AAA/00231"" : ""231"",
""AAA/00232"" : ""232"",
""AAA/00233"" : ""233"",
""AAA/00234"" : ""234"",
""AAA/00235"" : ""235"",
""AAA/00236"" : ""236"",
""AAA/00237"" : ""237"",
""AAA/00238"" : ""238"",
""AAA/00239"" : ""239"",
""AAA/00240"" : ""240"",
""AAA/00241"" : ""241"",
""AAA/00242"" : ""242"",
""AAA/00243"" : ""243"",
""AAA/00244"" : ""244"",
""AAA/00245"" : ""245"",
""AAA/00246"" : ""246"",
""AAA/00247"" : ""247"",
""AAA/00248"" : ""248"",
""AAA/00249"" : ""249"",
""AAA/00250"" : ""250"",
""AAA/00251"" : ""251"",
""AAA/00252"" : ""252"",
""AAA/00253"" : ""253"",
""AAA/00254"" : ""254"",
""AAA/00255"" : ""255"",
""AAA/00256"" : ""256"",
""AAA/00257"" : ""257"",
""AAA/00258"" : ""258"",
""AAA/00259"" : ""259"",
""AAA/00260"" : ""260"",
""AAA/00261"" : ""261"",
""AAA/00262"" : ""262"",
""AAA/00263"" : ""263"",
""AAA/00264"" : ""264"",
""AAA/00265"" : ""265"",
""AAA/00266"" : ""266"",
""AAA/00267"" : ""267"",
""AAA/00268"" : ""268"",
""AAA/00269"" : ""269"",
""AAA/00270"" : ""270"",
""AAA/00271"" : ""271"",
""AAA/00272"" : ""272"",
""AAA/00273"" : ""273"",
""AAA/00274"" : ""274"",
""AAA/00275"" : ""275"",
""AAA/00276"" : ""276"",
""AAA/00277"" : ""277"",
""AAA/00278"" : ""278"",
""AAA/00279"" : ""279"",
""AAA/00280"" : ""280"",
""AAA/00281"" : ""281"",
""AAA/00282"" : ""282"",
""AAA/00283"" : ""283"",
""AAA/00284"" : ""284"",
""AAA/00285"" : ""285"",
""AAA/00286"" : ""286"",
""AAA/00287"" : ""287"",
""AAA/00288"" : ""288"",
""AAA/00289"" : ""289"",
""AAA/00290"" : ""290"",
""AAA/00291"" : ""291"",
""AAA/00292"" : ""292"",
""AAA/00293"" : ""293"",
""AAA/00294"" : ""294"",
""AAA/00295"" : ""295"",
""AAA/00296"" : ""296"",
""AAA/00297"" : ""297"",
""AAA/00298"" : ""298"",
""AAA/00299"" : ""299"",
""AAA/00300"" : ""300"",
""AAA/00301"" : ""301"",
""AAA/00302"" : ""302"",
""AAA/00303"" : ""303"",
""AAA/00304"" : ""304"",
""AAA/00305"" : ""305"",
""AAA/00306"" : ""306"",
""AAA/00307"" : ""307"",
""AAA/00308"" : ""308"",
""AAA/00309"" : ""309"",
""AAA/00310"" : ""310"",
""AAA/00311"" : ""311"",
""AAA/00312"" : ""312"",
""AAA/00313"" : ""313"",
""AAA/00314"" : ""314"",
""AAA/00315"" : ""315"",
""AAA/00316"" : ""316"",
""AAA/00317"" : ""317"",
""AAA/00318"" : ""318"",
""AAA/00319"" : ""319"",
""AAA/00320"" : ""320"",
""AAA/00321"" : ""321"",
""AAA/00322"" : ""322"",
""AAA/00323"" : ""323"",
""AAA/00324"" : ""324"",
""AAA/00325"" : ""325"",
""AAA/00326"" : ""326"",
""AAA/00327"" : ""327"",
""AAA/00328"" : ""328"",
""AAA/00329"" : ""329"",
""AAA/00330"" : ""330"",
""AAA/00331"" : ""331"",
""AAA/00332"" : ""332"",
""AAA/00333"" : ""333"",
""AAA/00334"" : ""334"",
""AAA/00335"" : ""335"",
""AAA/00336"" : ""336"",
""AAA/00337"" : ""337"",
""AAA/00338"" : ""338"",
""AAA/00339"" : ""339"",
""AAA/00340"" : ""340"",
""AAA/00341"" : ""341"",
""AAA/00342"" : ""342"",
""AAA/00343"" : ""343"",
""AAA/00344"" : ""344"",
""AAA/00345"" : ""345"",
""AAA/00346"" : ""346"",
""AAA/00347"" : ""347"",
""AAA/00348"" : ""348"",
""AAA/00349"" : ""349"",
""AAA/00350"" : ""350"",
""AAA/00351"" : ""351"",
""AAA/00352"" : ""352"",
""AAA/00353"" : ""353"",
""AAA/00354"" : ""354"",
""AAA/00355"" : ""355"",
""AAA/00356"" : ""356"",
""AAA/00357"" : ""357"",
""AAA/00358"" : ""358"",
""AAA/00359"" : ""359"",
""AAA/00360"" : ""360"",
""AAA/00361"" : ""361"",
""AAA/00362"" : ""362"",
""AAA/00363"" : ""363"",
""AAA/00364"" : ""364"",
""AAA/00365"" : ""365"",
""AAA/00366"" : ""366"",
""AAA/00367"" : ""367"",
""AAA/00368"" : ""368"",
""AAA/00369"" : ""369"",
""AAA/00370"" : ""370"",
""AAA/00371"" : ""371"",
""AAA/00372"" : ""372"",
""AAA/00373"" : ""373"",
""AAA/00374"" : ""374"",
""AAA/00375"" : ""375"",
""AAA/00376"" : ""376"",
""AAA/00377"" : ""377"",
""AAA/00378"" : ""378"",
""AAA/00379"" : ""379"",
""AAA/00380"" : ""380"",
""AAA/00381"" : ""381"",
""AAA/00382"" : ""382"",
""AAA/00383"" : ""383"",
""AAA/00384"" : ""384"",
""AAA/00385"" : ""385"",
""AAA/00386"" : ""386"",
""AAA/00387"" : ""387"",
""AAA/00388"" : ""388"",
""AAA/00389"" : ""389"",
""AAA/00390"" : ""390"",
""AAA/00391"" : ""391"",
""AAA/00392"" : ""392"",
""AAA/00393"" : ""393"",
...
}
{noformat}

It seems like splunk is currently not able to handle these many KV pairs, or at least the UI doesn't seem to be able to handle them all.",,Alex James,Anil Nagavaram,Antony Wang,Ariel Velasco,Arindam Bhattacharjee,Burch -,Chris Pride,Emanuel House,gagarwal,Harendra Rawat,Jag Kerai,Jo Hornsby,Luan Tran,Maciek Wojcik,Michael Porath,Mitch Blank,Nick Filippi,Octavio Di Sciullo,Sean Delaney,Sourav Pal,Subba Gontla,Uvl Uppuluri,Victor Ebken,,,,,,,,,,,,,6053b78e06cbba006a0e7c22,5bbbd7d2d517b631d894a23d,557058:9b250150-4703-4e0c-a02b-ad536bb0d408,557058:0758dc66-be98-4ab8-9fc8-27897d1eb151,6053b36f311e270068e3f4dc,5af30e9c12b7982a5756e425,557058:761616db-db41-4d02-bf30-e81f409685ed,6053a8dc06cbba006a0dd832,611b53327b538a00696f8341,6053a6d694d7b90069f83e4f,6053aad106cbba006a0def0f,557058:f87e2651-5132-4aed-8b96-f608ee969435,557058:92f1e917-9794-47e9-bd18-8566eacb3bb2,557058:f85a444f-006d-4995-9693-32c9a544c633,557058:79277858-c49f-47fe-be0b-d81b27100ea0,6053b370e394c30069cb5cca,557058:7297dded-b84a-45a6-a1c9-ee6f9d7fa579,557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e,5ba40fce0cfa462eba272cd0,557058:93898f41-a5b5-4693-b32b-33e5eae84901,6053a57390f288007008d5f2,5a7238ee4cf5e3293b23692c,557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792,,,,,,,,,,,,,,0,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,VOC-147,,,,,,,,,,,,,,04/Apr/17 10:38 AM;uuppuluri;MXD_Questions_to_Splunk_v2_2017Mar24.pdf;https://splunk.atlassian.net/rest/api/3/attachment/content/281456,07/May/18 6:05 PM;vebken;end_of_fields.png;https://splunk.atlassian.net/rest/api/3/attachment/content/455413,19/Apr/17 4:13 PM;mpawar;events request.png;https://splunk.atlassian.net/rest/api/3/attachment/content/286323,19/Apr/17 4:13 PM;mpawar;expand row event request.png;https://splunk.atlassian.net/rest/api/3/attachment/content/286322,07/May/18 4:12 PM;vebken;indexed_extractions_json_artifact.zip;https://splunk.atlassian.net/rest/api/3/attachment/content/455382,07/May/18 4:12 PM;vebken;kv_mode_json_artifact.zip;https://splunk.atlassian.net/rest/api/3/attachment/content/455381,04/Apr/17 4:27 PM;luan;search_json.png;https://splunk.atlassian.net/rest/api/3/attachment/content/281653,04/Apr/17 10:21 AM;luan;test.json;https://splunk.atlassian.net/rest/api/3/attachment/content/281389,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7df3b822,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,22.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,143164800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Thu Mar 11 17:08:50 UTC 2021,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,22.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dawang(dawang),ftan(ftan),hrawat(hrawat),jisenberg(jisenberg),luan(luan),mpawar(mpawar),mblank(mblank),odisciullo(odisciullo),uuppuluri(uuppuluri),vebken(vebken),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sustaining,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Victor Ebken,557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792,,,,,,,,,,,,,,,,,,0|i38g7b:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Apple Inc.,467373,012400000005WzMAAU,P4,No,500330000174ho6AAA,,Closed,Waiting on Splunk,Premium,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,467373,,,,,,,,,,,,Sustaining,,,,,,,,Restricted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2017-04-04 17:31:11.635,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3_*:*_1_*:*_1232012753_*|*_5_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_33287479074_*|*_10039_*:*_1_*:*_71046920,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017-04-04 17:31:11.635,2017-04-04 17:31:11.635,,,,,,Victor Ebken,557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792,,,,,,"04/Apr/17 10:31 AM;5a7238ee4cf5e3293b23692c;[~accountid:611b53327b538a00696f8341][~accountid:557058:761616db-db41-4d02-bf30-e81f409685ed][~accountid:6053b78e06cbba006a0e7c22] [~accountid:6053b370e394c30069cb5cca] the context for this is that the customer is trying to onboard a new group that has data of these characteristics:
1) huge event size (may be up to 20mb and 0.5 million kv pairs but average 10K kv pairs)
2) high cardinality across events based on keys (not values) 

As [~accountid:557058:92f1e917-9794-47e9-bd18-8566eacb3bb2]mentioned the UI is not able to handle the event import but looks like we are making progress with oneshot. So believe we can search. The question is:
Do we support this and any known issues (memory, UI, autokv but search works?)?

Thanks for any quick feedback so we can let the customer teams know on the path to take.",04/Apr/17 10:41 AM;5a7238ee4cf5e3293b23692c;Some more context also in the attached PDF. Will also follow up with arch-sme team as well.,"04/Apr/17 3:04 PM;557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e;{quote}
1) huge event size (may be up to 20mb and 0.5 million kv pairs but average 10K kv pairs)
{quote}
Just curious but does this amount of data actually reflect _a single event_? This seems like the equivalent of ingesting one small data base for each event.","04/Apr/17 3:10 PM;6053a6d694d7b90069f83e4f;[~accountid:5a7238ee4cf5e3293b23692c] as discussed, since oneshot worked, so will monitor will work. However if UI is not working then it's more likely UI side of limitation.","04/Apr/17 4:28 PM;557058:92f1e917-9794-47e9-bd18-8566eacb3bb2;Here is my repro, and looks like splunk cannot handle that many fields.  Also not sure why it starts to break that one event into 5 events:
 !search_json.png|thumbnail! 

props.conf stanza used:
{noformat}
[apple_json]
DATETIME_CONFIG =
INDEXED_EXTRACTIONS = json
KV_MODE = none
NO_BINARY_CHECK = true
category = Structured
disabled = false
pulldown_type = true
TRUNCATE = 0
{noformat}","05/Apr/17 9:11 AM;6053b370e394c30069cb5cca;BTW, I agree w/ [~accountid:557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e] here: a 20MB event is nuts.

It's certainly fine to keep investigating this since it might surface interesting information about what happens when you push the core and UI stuff to the breaking point.  There's a good chance we can make some changes that will ""fix"" things at the margins. 
 However, we probably also need to manage customer expections here: this is far outside our normal definition of ""event"" and it's not likely that splunk will ever deal with data like this particularly efficiently even if it can be forced into working!  We're an event-store, not a document-store.  We're optimized for events that are of a more normal size (say, 50 - 20000 bytes, 0-200 fields)

Basically everything about this screams ""it's never going to work well""","05/Apr/17 10:02 AM;557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e;{quote}
It's certainly fine to keep investigating this since it might surface interesting information about what happens when you push the core and UI stuff to the breaking point.
{quote}
Along these lines, one of the most reasonable expectations we should set here is for Splunk to behave _graciously_ when it is pushed in this way.

Both search and the UI, for example, should self-limit themselves in the fetching and display of events so outside the norms, and clearly communicate what is going on to the user.

This is IMHO another scenario where it would be extremely helpful if the product was able to detect that its boundaries are pushed and take pro-active action to avoid breaking, even if that means not doing what the user is asking as long as that is communicated transparently.

Starting from a situation where the product just shits itself on 20MB events is, while reasonably expected, not a good starting point for us to explain to a customer what is reasonable and what isn't. For the customer the product allowed itself to take in these events as they are and tries to display them and therefore a failure in that endeavor with all of its complement of errors, warnings and performance problems is a _defect_.

We should get ahead of this kind of issue by implementing defensive behavior when boundaries are pushed this hard.","12/Apr/17 4:39 PM;557058:92f1e917-9794-47e9-bd18-8566eacb3bb2;[~accountid:557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e], [~accountid:557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d],
{quote}
We should get ahead of this kind of issue by implementing defensive behavior when boundaries are pushed this hard
{quote}
Should I convert this SPL into a CIR, for product supportability improvement?","19/Apr/17 4:21 PM;557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d;UI is just showing what it gets from the events endpoint. Check out these screenshots:

After running a search: !events request.png|thumbnail!
After expanding the first event row: !expand row event request.png|thumbnail!

This needs to be further investigated from the backend side. May be in such case, backend should send an error or warning message to UI explaining what is going on.",27/Apr/17 11:32 AM;557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d;This needs to be assigned to a backend engineer. cc: [~accountid:5bbbd7d2d517b631d894a23d],"01/May/17 9:54 AM;557058:92f1e917-9794-47e9-bd18-8566eacb3bb2;Hi [~accountid:5bbbd7d2d517b631d894a23d], should this one be a product enhancement request, or a defect?","01/Aug/17 2:32 PM;557058:92f1e917-9794-47e9-bd18-8566eacb3bb2;Hi [~accountid:5bbbd7d2d517b631d894a23d], what is the status of this one?  Thanks.","17/Aug/17 12:06 PM;557058:92f1e917-9794-47e9-bd18-8566eacb3bb2;Adding case 529227 -- another Apple use case that is hitting this bug/limitation:
{quote}
Could you please let us know if there is a way to get a CSV with around 400000 columns indexed?
Currently the file does get indexed but all the fields are extracted as EXTRA_FIELD_1, EXTRA_FIELD_2, …. EXTRA_FIELD_999 etc

Thanks and regards,
Mridu

On Aug 15, 2017, at 12:00 PM, Mridusmita Talukdar <mridusmita_talukdar@apple.com> wrote:
Hi Luan,
We have a unique file which needs to be indexed and we need your advice on how to make the format of the file Splunk friendly.
The file is in CSV format. There are ~350000 columns in the file and 3963556 characters in the header column.
What is best way to reformat the file?
If the file is indexed as is, what are repercussions?
Thanks and regards,
Mridu
{quote}","17/Aug/17 4:20 PM;557058:92f1e917-9794-47e9-bd18-8566eacb3bb2;*Customer update*
For case 529227 --
{quote}
We have broken down the file to files with chunks of 500 columns.

The first 4 fields of the parent file are common for the rest of the columns, so we have those 4 fields in all the chunked files. So there are 504 columns from the 2nd file onwards.

What I see now is that for the 1st 12 files the fields are extracted fine, but from the 13th file onwards we see the EXTRA_FIELD keys issue. Each and every field from the 13 file onwards is shown as EXTRA_FIELD*

I think we have hit some limit/bug on the number of keys that can be extracted, but then, is the limit only 12*500 = 6000 keys?
{quote}","23/Mar/18 4:26 PM;557058:39dcb955-2019-41ee-b05b-4b9fc0d93779;Bulk edit fix version to 6.5.9 due to CC of 6.5.8
Please ping [~accountid:557058:39dcb955-2019-41ee-b05b-4b9fc0d93779]/[~accountid:557058:30e41e2b-e9d1-427a-ae34-cf6659ef443a]/[~accountid:5bbbd7d2d517b631d894a23d] if there's any concern","16/Apr/18 12:31 PM;557058:92f1e917-9794-47e9-bd18-8566eacb3bb2;Hi Team,
Do we have any ETA on a fix to this issue?  Also, the customer would like to confirm if the fix to this jira will be cloned into 6.6.x, 7.0.x and NightLight (7.1).  Thanks.","03/May/18 8:57 AM;557058:92f1e917-9794-47e9-bd18-8566eacb3bb2;Hi [~accountid:557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792], should we make this one into an enhancement request?  Thanks.","07/May/18 6:06 PM;557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792;*Status Update*
Poking around the code a bit I found the limit that we were running into for indexed extractions and it's actually a default search limit that we are hitting preventing the fields from being returned. 
{noformat}
[kv]
limit = <integer>
* The maximum number of fields that an automatic key-value field extraction
  (auto kv) can generate at search time.
* If search-time field extractions are disabled (KV_MODE=none in props.conf)
  then this setting determines the number of index-time fields that will be
  returned.
* The summary fields 'host', 'index', 'source', 'sourcetype', 'eventtype',
  'linecount', 'splunk_server', and 'splunk_server_group' do not count against
  this limit and will always be returned.
* Increase this setting if, for example, you have indexed data with a large
  number of columns and want to ensure that searches display all fields from
  the data.
* Default: 100
{noformat}
Bumping this value up to accommodate the expected number of fields in the data will result in all fields being properly returned.

Another alternative (which may be preferred so we don't have to index tens of thousands of extra fields) is using the following props parameter instead of indexed_extractions. Using this parameter changes the extraction to a search time field extraction. 
{noformat}
KV_MODE = json
{noformat}

A word of caution about using these is that with events this big the UI may become unusably slow as documented in some other cases SPL-152340 SPL-148734. 

If desired perhaps we could open an enhancement request for the UI to better handle large events, but from the backend perspective this is as designed.",16/Aug/18 10:22 AM;557058:92f1e917-9794-47e9-bd18-8566eacb3bb2;enhancement request SAR-149 has been filed.,28/Sep/20 1:19 PM;557058:2d2a7f1d-c9e1-4f3c-95c5-24ed73d0b69d;SevLevelCleanupJI,25/Jan/21 10:19 PM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Closing as part of 6+ months old resolved bugs bulk clean up by QA Metrics Team. CC [~accountid:5ebddcc35862510b798f5abd] [~accountid:5d3547e5c2db730c59b25bdc] [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408].,11/Mar/21 9:08 AM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Rolling back Assignee & Developer from Jan 25th bulk closure operation. CC [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408]. Assignee: vebken. Developer: vebken,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,09/May/18 6:50 PM
[PUBLIC] Splunk searches fail when filepath gets too long on Windows,SPL-138654,793614,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P1-Immediate,Delivered,,,Allan Wang,557058:057b83ca-0a20-4e0f-931b-49001957673f,Allan Wang,557058:057b83ca-0a20-4e0f-931b-49001957673f,13/Mar/17 5:36 PM,30/Jul/25 4:25 PM,,31/Oct/19 1:02 PM,10.0.x,6.6.0 GA (Kimono),6.6.1,6.6.10,6.6.11,6.6.12,6.6.2,6.6.3,6.6.4,6.6.5,6.6.6,6.6.7,6.6.8,6.6.9,7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,JackHammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search - Distributed,,,,,0,ES_P1,Windows_Team_2.0,,,,,,"Splunk fails silently when it cannot write files due to exceeding the max filename length on Windows. It causes searches to fail due to

{code}
03-12-2017 17:47:00.455 ERROR dispatchRunner - Unable to find info file at C:\Users\eserv\jbeyers\splunk\var\run\splunk\dispatch\remote_ec2-54-146-136-177.compute-1.amazonaws.com_search_head_0_0_scheduler__admin__SplunkEnterpriseSecuritySuite__RMD5329553901945a562_at_1489366020_4603_B69AAE16-499C-4CEE-93B7-8F6AC89980FF\info.csv
03-12-2017 17:47:00.455 ERROR dispatchRunner - Required argument --search is missing
03-12-2017 17:47:00.455 ERROR dispatchRunner - usage: splunkd search --search=search to dispatch [--id=dispatch id] [--maxbuckets=<buckets>] [--ttl=<ttl>] [--maxout=<maxCount>] [--maxtime=<maxTime>] [--lookups=<enableLookups>] [--peer=name,uri] [--pro --user=<user> --roles=<colon_sep_roles>] [--internalOnly] [--app=<application>] [--bs=<bundle-setup-path>] [--initapps=<applications-to-initialize>] [--runReduce=<boolean>] [--reduceSearch=<reduce-search-string-override-info.csv>] [--startPipelines=<boolean>] [--annotateMessages=<boolean>]\n
{code}

The behavior is also erratic because not all files fail to be written.
For example:
{code}
eserv@ip-0A43BDB3 /cygdrive/c/Users/eserv/jbeyers/splunk/var/run/splunk/dispatch/remote_ec2-54-146-136-177.compute-1.amazonaws.com_search_head_0_0_scheduler__admin__SplunkEnterpriseSecuritySuite__RMD5329553901945a562_at_1489366020_4603_B69AAE16-499C-4CEE-93B7-8F6AC89980FF
$ ls
args.txt  pipeline_sets  search.log
{code}",,Alex James,Allan Wang,Antony Wang,Bei Li,Chris Pride,David Marquardt,Jackson Sie,James Ervin,Jason Beyers,Jo Hornsby,Karthik Sabhanatarajan,Mitch Blank,Sarah Moir,Steve Zhang,Sung Lim,Wim Colgate,,,,,,,,,,,,,,,,,,,,6053b78e06cbba006a0e7c22,557058:057b83ca-0a20-4e0f-931b-49001957673f,557058:9b250150-4703-4e0c-a02b-ad536bb0d408,5cd341243e6fd40fe82c5536,557058:761616db-db41-4d02-bf30-e81f409685ed,6053a2e3686bf50070459604,557058:3dbd6b96-3870-4379-ade1-f8825ebbfe18,557058:ab939132-f05e-4f5f-8bec-6939a8133dcc,6053a29d686bf500704592b6,557058:f87e2651-5132-4aed-8b96-f608ee969435,611be44c883cef0077c90366,6053b370e394c30069cb5cca,557058:ead33141-9902-4d93-9add-b226d2beed55,5af8dc2ef2f8f0082b77630a,6036b832f8c057007083c0e3,557058:ef43a796-f89f-4463-801b-30ded73fb41e,,,,,,,,,,,,,,,,,,,,,0,,,,0,,Users,,,,,,,,,,,,SOLNESS-11587,,,,,,,,,,,,,,,,,,,,,SOLNESS-11510,SPL-35290,SPL-113031,SPL-129334,SPL-99421,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@198c7a95,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,143251200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,Thu Mar 11 15:02:22 UTC 2021,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,allanw(allanw),dawang(dawang),dmarquardt(dmarquardt),ksabhanatarajan(ksabhanatarajan),smoir(smoir),wcolgate(wcolgate),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i37e1b:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dnata,3432119,012400000005WzMAAU,P3,Yes,5005a00002pn1slAAA,,Closed,Resolved - Work Around,Standard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Language,,,,,,,,Restricted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2017-03-14 00:45:55.586,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_83029131225_*|*_10039_*:*_1_*:*_71180391,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017-03-14 00:45:55.586,2017-03-14 00:45:55.586,,,,,,,,,,,,,"13/Mar/17 5:45 PM;611be44c883cef0077c90366;[~accountid:6053a2e3686bf50070459604] - Please triage and assign this accordingly! This has come up quite a few times on Field as well. Would help to have a user facing debugging message.

 ","13/Mar/17 9:00 PM;6053a2e3686bf50070459604;[~accountid:611be44c883cef0077c90366] we've hit this long path issue in Windows countless times and as I recall there's really not a good comprehensive solution. So historically we've just mitigated the problem by shortening path segments or filenames that are overly verbose.

I do, however, think we need to find better ways to make it clear when this problem has been hit instead of silently failing. The difficult cases are searches like summarization that are executed in the background, where it's not as easy to give users direct feedback about the failure.","15/Mar/17 10:15 AM;557058:ead33141-9902-4d93-9add-b226d2beed55;Added to release notes so it's at least a documented known issue. If there is a workaround or an error message that people can identify, please add it to the workaround or summary field.","11/Jun/18 1:50 PM;557058:ef43a796-f89f-4463-801b-30ded73fb41e;[~accountid:6053a2e3686bf50070459604] 100% concur. Perhaps we can add logging in the low level modules when, on windows, we generate something larger than MAX_PATH? There are some sporadic checks for MAX_PATH, and even some with warning messages; but it certainly isn't comprehensive.","31/Oct/19 1:02 PM;557058:ef43a796-f89f-4463-801b-30ded73fb41e;Instead of trying to fix this issue (which we can't really do), instead, there is a new warning emitted during the construction of a long pathname (> MAX_PATH). On Windows, we don't ship debug info, so a series of call sites (addresses) is generated for post processing in house. the script is src/scripts/max-win-pathname.py -- and requires a .pdb, .exe and logfile that contains the warnings",24/Jan/21 11:08 PM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Closing as part of 6+ months old resolved bugs bulk clean up by QA Metrics Team. CC [~accountid:5ebddcc35862510b798f5abd] [~accountid:5d3547e5c2db730c59b25bdc] [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408].,11/Mar/21 7:02 AM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Rolling back Assignee & Developer from Jan 25th bulk closure operation. CC [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408]. Assignee: None. Developer: None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,31/Oct/19 1:02 PM
[PUBLIC] Splunk restart does not create missing server.pem certificate on Windows,SPL-134707,772444,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Workaround,Priya Pandeti,5d3547e5c2db730c59b25bdc,Bill Paul,6053ab37311e270068e398e1,Bill Paul,6053ab37311e270068e398e1,06/Jan/17 10:19 AM,30/Jul/25 4:26 PM,,11/Sep/18 3:49 AM,10.0.x,6.5.0 GA (Ivory),6.5.1,6.5.10,6.5.2,6.5.3,6.5.4,6.5.5,6.5.6,6.5.7,6.5.8,6.5.9,6.6.0 GA (Kimono),6.6.1,6.6.10,6.6.11,6.6.12,6.6.2,6.6.3,6.6.4,6.6.5,6.6.6,6.6.7,6.6.8,6.6.9,7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,,,,,,,,,,,,,,,,,,,,,SSL,,,,,0,nmc,SUSTAINING_BACKEND,,,,,,"The customer is Macnica Networks Corporation and their server certificate expired.  I provided the command to create a new one, but they also wanted to know if deleting server.pem and restarting Splunk would also replace it.  I discovered there is a script which will generate server.pem on startup.  This works on my Linux test system, but not on my Windows test system.  The following error messages are generated.

{noformat}
01-06-2017 10:11:51.034 -0800 ERROR loader - The certificate generation script did not generate the expected certificate file:C:\Program Files\Splunk\etc/auth/server.pem. Splunkd port communication will not work.
01-06-2017 10:11:51.034 -0800 ERROR loader - SSL certificate generation failed.
01-06-2017 10:11:51.034 -0800 ERROR loader - <<<<< EOF (pre-flight-checks)
{noformat}

This should work on all platforms.",,Antony Wang,Bill Paul,Jo Hornsby,Matthew Wirth,Sung Lim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:9b250150-4703-4e0c-a02b-ad536bb0d408,6053ab37311e270068e398e1,557058:f87e2651-5132-4aed-8b96-f608ee969435,5cddd285254e450fd8d227ca,6036b832f8c057007083c0e3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,Users,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Macnica Networks Corporation - End User,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@2186c188,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,146966400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Tue Jan 26 19:38:49 UTC 2021,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dawang(dawang),bpaul(bpaul),jisenberg(jisenberg),ppandeti(ppandeti),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sustaining,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i342lr:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Cummins Inc.,3577995,012400000005WzMAAU,P2,No,5005a0000314kHVAAY,,Closed,Resolved,Standard,Tier1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,431559,,,,,,,,,,,,Sustaining,,,,,,,,Restricted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2020-09-28 21:45:22.807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_52936176637,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Use <code>bin/splunk createssl server-cert -d etc/auth/ -n server</code> to generate a new certificate.,,,,,,,,,,2020-09-28 21:45:22.807,2020-09-28 21:45:22.807,,,,,,Priya Pandeti,5d3547e5c2db730c59b25bdc,,,,,,28/Sep/20 2:45 PM;557058:2d2a7f1d-c9e1-4f3c-95c5-24ed73d0b69d;SevLevelCleanupJI,26/Jan/21 11:38 AM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Closing as part of 6+ months old resolved bugs bulk clean up by QA Metrics Team. CC [~accountid:5ebddcc35862510b798f5abd] [~accountid:5d3547e5c2db730c59b25bdc] [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408].,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,11/Sep/18 3:49 AM
"[PUBLIC] When two datasets have identical names but one is local (private) while the other is global, attempts to view or extend the global dataset use results from the local dataset instead.",SPL-133182,762437,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,As Designed,Asha Andrade,6053b9c7009fee00694cc8d3,sbhadouria,5cdb189883de300fe5d502dd,sbhadouria,5cdb189883de300fe5d502dd,29/Nov/16 6:38 PM,30/Jul/25 4:25 PM,,08/Mar/19 1:04 PM,10.0.x,6.5.0 GA (Ivory),6.5.1,6.5.10,6.5.1612.x,6.5.2,6.5.3,6.5.4,6.5.5,6.5.6,6.5.7,6.5.8,6.5.9,6.6.0 GA (Kimono),6.6.1,6.6.10,6.6.11,6.6.12,6.6.2,6.6.3,6.6.4,6.6.5,6.6.6,6.6.7,6.6.8,6.6.9,7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,JackHammer,,,,,,,,,,,,,,,,,,,Datasets - Table UI,Search - Language,,,,0,,,,,,,,"When there are two datasets with Duplicate name in different context of which one of them is local then when we view or extend the dataset it shows the result of the local one.
To avoid this situation, we should set a flag on the global dataset so the user can be told they will not be able to see the search results of that dataset.",,Alex James,arobbins,Asha Andrade,Cory Burke,David Marquardt,Hema Mohan,Jeff Lloyd [X],Jian Zhang,Karthik Sabhanatarajan,Matthew Ness,Megumi Hora,nromito,sbhadouria,User known,,,,,,,,,,,,,,,,,,,,,,6053b78e06cbba006a0e7c22,611be30c4016870069188d85,6053b9c7009fee00694cc8d3,557058:4e8b44b4-90b9-4f38-bbf7-9ce32d19bab7,6053a2e3686bf50070459604,557058:66a45082-cf26-40ec-9d8c-2592688d5292,61324e85036d9e006ac8bc91,5af8dc2f82ddd25cccb8bfbe,611be44c883cef0077c90366,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,557058:7f1422a9-d293-4ac9-b81d-116bcf8100e4,611bdefb4016870069185d34,5cdb189883de300fe5d502dd,unknown,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,Users,,,,,,,,,,,,SPL-133570,,,,,,,,,,,,,,,,,,,,SPL-140246,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4f610d11,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,266457600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,Fri Apr 14 19:28:52 UTC 2017,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SearchGeneration,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,aandrade(aandrade),cburke(cburke),mness(mness),sbhadouria(sbhadouria),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data Platform,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Karthik Sabhanatarajan,611be44c883cef0077c90366,,,,,,,,,,,,,,,,,,0|i1ajf3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Search Language,,,,,,,,Restricted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SearchLang 12.05.16 - 12.23.16,SearchLang 01.03.17 - 01.13.17,SearchLang 01.16.17 - 02.03.17,SearchLang 02.27.17 - 03.03.17,SearchLang 03.06.17 - 03.17.17,SearchLang 03.20.17 - 04.07.17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2017-03-18 00:16:42.078,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_1_*:*_5510667927_*|*_3_*:*_2_*:*_3375258991_*|*_5_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_62286786437_*|*_10039_*:*_1_*:*_432843865,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017-03-18 00:16:42.078,2017-03-18 00:16:42.078,,,,,,Asha Andrade,6053b9c7009fee00694cc8d3,,,,,,17/Mar/17 5:16 PM;557058:89ace195-adc2-4f25-9924-ded7c8f10bb6;Seems like this is related to the knowledge object name collision issues that are documented here: http://docs.splunk.com/Documentation/Splunk/6.5.0/Knowledge/Knowledgeobjectnamecollisions,31/Mar/17 2:06 PM;6053b9c7009fee00694cc8d3;Tied to https://splunk.atlassian.net/browse/SPL-133181,06/Apr/17 2:23 PM;557058:4e8b44b4-90b9-4f38-bbf7-9ce32d19bab7;cc [~accountid:611bdefb4016870069185d34],"14/Apr/17 12:28 PM;557058:89ace195-adc2-4f25-9924-ded7c8f10bb6;Updated this bug so it will be included in the Kimono release notes, per https://splunk.atlassian.net/wiki/display/PROD/How+to+create+a+Changelog+or+Known+Issue+using+Jira",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,08/Mar/19 1:04 PM
[PUBLIC] Table data rows generated with the addcoltotals command do not show up in PDF,SPL-132925,760928,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,Won't Do,Michael Luo,557058:9208d4e1-3a50-491a-90da-3296c47a9e4e,Tim Turner,6053aecc695c3900707ad400,Tim Turner,6053aecc695c3900707ad400,23/Nov/16 4:37 PM,30/Jul/25 4:24 PM,,28/Jan/20 12:02 PM,10.0.x,6.5.0 GA (Ivory),6.5.1,6.5.10,6.5.2,6.5.3,6.5.4,6.5.5,6.5.6,6.5.7,6.5.8,6.5.9,6.6.0 GA (Kimono),6.6.1,6.6.10,6.6.11,6.6.12,6.6.2,6.6.3,6.6.4,6.6.5,6.6.6,6.6.7,6.6.8,6.6.9,7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,JackHammer,,,,,,,,,,,,,,,,,,,,PDF,,,,,0,enterprise_ui_ticket_scrub_2019,,,,,,,"From the customer:

"" I’m using addcoltotals to sum a column of numbers. This works fine in search, however when I use the search + sendemail, the PDF that Splunk sends is blank. If I remove addcoltotals from the search and then use sendemail it works just fine.
My questions are:
#1 is that expected behavior?
#2 if so, is that behavior documented somewhere?
#3 what other functions are affected in a similar way?""",,Alex Fishkin,Andrew Brown,Boris Kovacevic,Chriest Yu,Emily Cheung,frobinson,Ilia Kislukhin,Irina Korobova,Nick Filippi,Scott Lu,Siegfried Puchbauer,Simon Fishel,Strong Yuan,Tim Turner,,,,,,,,,,,,,,,,,,,,,,557058:76033043-756d-440a-92c2-4fd128afd847,557058:37b4518c-e757-440f-87b7-181f5f425e80,5ac6aaab0658540c0debafcd,5d8c26f6090bd70dc49ac8c8,6053a74994d7b90069f8435f,611be6e8c2f3a50069e26941,557058:306c1e3c-25de-4bf7-acb1-2c6e7ecb9ec7,5abaacc24384cf2a7d40048a,557058:7297dded-b84a-45a6-a1c9-ee6f9d7fa579,5aa2e5f5d0c9c42eefad8009,557058:ffe53cbd-86cd-4cff-8ba7-369fbc58d342,557058:e5764eeb-1543-470e-8844-5997eb32f42f,557058:9d111f3b-bf09-4268-b8ce-28afdeae9d7b,6053aecc695c3900707ad400,,,,,,,,,,,,,,,,,,,,,,,0,,,,0,,Users,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,01/Dec/16 10:48 AM;afishkin;Screen Shot 2016-12-01 at 10.48.24 AM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/247257,01/Dec/16 6:09 PM;slu;Screen Shot 2016-12-02 at 10.05.02 AM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/247335,01/Dec/16 6:16 PM;slu;error in the PDF.png;https://splunk.atlassian.net/rest/api/3/attachment/content/247338,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,iControl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3b961392,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,21.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,178416000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Tue Jan 28 20:02:31 UTC 2020,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,User Experience YVR - Analytics Workspace,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,21.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,afishkin(afishkin),andrewb(andrewb),bkovacevic(bkovacevic),echeung(echeung),frobinson(frobinson),mluo(lmichael),nfilippi(nfilippi),slu(slu),sfishel(sfishel),tturner(tturner),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i32fh3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Closed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,402455,,,,,,,,,,,,Dashboards,,,,,,,,Restricted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2016-11-29 18:49:28.002,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4_*:*_1_*:*_16054058449_*|*_6_*:*_2_*:*_90411942_*|*_10001_*:*_1_*:*_83651912792_*|*_10039_*:*_1_*:*_497545645,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"If you are using <code>addcoltotals</code> to generate a totals data row, renaming the <code>_time</code> field can cause PDF generation issues.
Remove the label and <code>labelfield</code> or change the label to a number to generate the PDF as expected.",,,,,,,,,,2016-11-29 18:49:28.002,2016-11-29 18:49:28.002,,,,,,Michael Luo,557058:9208d4e1-3a50-491a-90da-3296c47a9e4e,,,,,,29/Nov/16 10:49 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:611be6e8c2f3a50069e26941] can you research this or route it to the person who can provide answers? Thanks.,"29/Nov/16 11:23 AM;611be6e8c2f3a50069e26941;[~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80] and [~accountid:6053aecc695c3900707ad400]: As far as I know, this is not expected behavior. Addcoltotals generates a data row, which should be included in PDFs. See http://docs.splunk.com/Documentation/Splunk/6.5.0/Viz/TableFormatsFormatting#Summary_and_data_row_differences

I would call this a bug. I can flag it for the Dashboards scrum, as they handle PDF.","30/Nov/16 8:40 AM;557058:76033043-756d-440a-92c2-4fd128afd847;[~accountid:557058:79277858-c49f-47fe-be0b-d81b27100ea0], as I know we do not support this feature currently","30/Nov/16 9:03 AM;557058:e5764eeb-1543-470e-8844-5997eb32f42f;I agree with [~accountid:611be6e8c2f3a50069e26941], it sounds like the customer is using the {{addcoltotals}} search command, not the client-side table formatting feature.  Sending back to [~accountid:557058:9208d4e1-3a50-491a-90da-3296c47a9e4e].

Possibly related to SPL-132529 which also involves {{addcoltotals}} and PDF.","30/Nov/16 9:35 AM;557058:76033043-756d-440a-92c2-4fd128afd847;[~accountid:557058:e5764eeb-1543-470e-8844-5997eb32f42f], option ""totalsRow"" is not supported in PDF","30/Nov/16 9:39 AM;557058:e5764eeb-1543-470e-8844-5997eb32f42f;[~accountid:557058:76033043-756d-440a-92c2-4fd128afd847], the description doesn't mention the {{totalsRow}} option, only the {{addcoltotals}} search command.","30/Nov/16 9:50 AM;611be6e8c2f3a50069e26941;Yes, this is specifically addcoltotals, which I was told generates a data row, not a summary row. Summary rows are not expected to be in PDF but data rows are.",30/Nov/16 9:54 AM;611be6e8c2f3a50069e26941;[~accountid:557058:e5764eeb-1543-470e-8844-5997eb32f42f] and [~accountid:557058:76033043-756d-440a-92c2-4fd128afd847]: We might want to consider adding this to our known issues list for 6.5--let me know if I can help!,"01/Dec/16 10:24 AM;557058:76033043-756d-440a-92c2-4fd128afd847;tested it in Ivory and JackHammer and result is not empty, [~accountid:6053aecc695c3900707ad400], could you provide mode details
I used ""index=_internal  | stats count by sourcetype| addcoltotals | sendemail to=""afishkin@splunk.com"" sendresults=true sendpdf=true""","01/Dec/16 10:30 AM;6053aecc695c3900707ad400;[~accountid:557058:76033043-756d-440a-92c2-4fd128afd847]Certainly, from the customer :

""When I use this query I get the correct PDF (see attached) 

index=main host=betamax-cat2 ""Unable to check health check""| rex ""(?i) for (?P<adapter>.+)""|timechart count by adapter|sendemail to=""dbcase@icontrol.com"" sendresults=true sendpdf=true 


When I use this query I get a ""blank"" PDF (see attached) 

index=main host=betamax-cat2 ""Unable to check health check""| rex ""(?i) for (?P<adapter>.+)""|timechart count by adapter|addcoltotals label=""Column Totals"" labelfield=_time|addtotals|sendemail to=""dbcase@icontrol.com"" sendresults=true sendpdf=true 
""","01/Dec/16 12:42 PM;557058:76033043-756d-440a-92c2-4fd128afd847;I am getting 
""Splunk search results
year=1 is before 1900; the datetime strftime() methods require year >= 1900"" using following command
index=_internal |timechart count by adapter|addcoltotals label=""Column Totals"" labelfield=_time|addtotals|sendemail to=""afishkin@splunk.com"" sendresults=true sendpdf=true 
I am getting all details using 
index=_internal |timechart count by adapter|addcoltotals label=""Column Totals"" |addtotals|sendemail to=""afishkin@splunk.com"" sendresults=true sendpdf=true 

removing ""labelfield=_time""","01/Dec/16 6:09 PM;5aa2e5f5d0c9c42eefad8009;It looks like the pdfgen does not handle string label for the ""_time"" field very well. In the customer case, the total column row for ""_time"" field is renamed to label=""Column Totals"" which is not a valid year. 
 !Screen Shot 2016-12-02 at 10.05.02 AM.png|thumbnail! 
That is why we are seeing the message in the PDF: 
""year=1 is before 1900; the datetime strftime() methods require year >= 1900""

By removing the label and labelfield or changing the label to a number, the PDF can be generated correctly.",02/Dec/16 7:11 AM;6053aecc695c3900707ad400;Will notify customer. Thanks for the good work guys,"02/Dec/16 10:35 AM;611be6e8c2f3a50069e26941;[~accountid:5aa2e5f5d0c9c42eefad8009]: Should we document this as a known issue with a workaround in the release notes, or do you think this is a limitation that should go into our main docs? cc: [~accountid:557058:e5764eeb-1543-470e-8844-5997eb32f42f] and/or [~accountid:557058:7297dded-b84a-45a6-a1c9-ee6f9d7fa579]",02/Dec/16 6:00 PM;5aa2e5f5d0c9c42eefad8009;[~accountid:557058:7297dded-b84a-45a6-a1c9-ee6f9d7fa579] [~accountid:557058:ffe53cbd-86cd-4cff-8ba7-369fbc58d342] Your call. ,"05/Dec/16 11:48 AM;557058:7297dded-b84a-45a6-a1c9-ee6f9d7fa579;[~accountid:611be6e8c2f3a50069e26941] Sure, let's document this as a known issue for now, until such time as we have a fix for this.

","05/Dec/16 12:01 PM;611be6e8c2f3a50069e26941;Edited this JIRA to prompt the summary + workaround to be added to release notes for 6.5, 6.5.1. Please continue to add ""affects versions"" as needed to put the known issue into subsequent releases until we fix it.","25/Jul/19 4:28 PM;5ac6aaab0658540c0debafcd;Closing due to the issue being older than 2 and half years with no progress, and having a lower priority (not in Critical, Blocker, or Major).","26/Jul/19 5:34 PM;5ac6aaab0658540c0debafcd;Re-opened to allow for extra triaging to happen. Will solicit feedback from larger audience, and re-triage again.","15/Jan/20 9:32 PM;6053a74994d7b90069f8435f;Bulk updating ""Mission Team"" = ""User Experience SF"" based on component ownership",28/Jan/20 12:02 PM;5ac6aaab0658540c0debafcd;Closing due to no progress in over three years. Will re-open if needed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,28/Jan/20 12:02 PM
[PUBLIC] XML error when trying to download uninstalled app,SPL-132151,757234,Bug,Untriaged,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P3-Medium,,,,Cecelia Redding,5d3756fd276fe40c53371f66,Cecelia Redding,5d3756fd276fe40c53371f66,14/Nov/16 1:56 PM,30/Jul/25 4:26 PM,,,10.0.x,6.6.0 GA (Kimono),6.6.1,6.6.10,6.6.11,6.6.12,6.6.2,6.6.3,6.6.4,6.6.5,6.6.6,6.6.7,6.6.8,6.6.9,7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,JackHammer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DMC,,,,,0,FY21Q4,oper_bug_bash,,,,,,"Assume we have 2 Cloud users...

CloudA uninstalls Splunk_TA_windows
CloudB still has the apps page open and sees Splunk_TA_windows is there
CloudB clicks download on Splunk_TA_windows
XML error message returned....",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Users,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14/Nov/16 1:56 PM;ccampbell;Screen Shot 2016-11-14 at 1.55.18 PM.png;https://splunk.atlassian.net/rest/api/3/attachment/content/242333,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5a15300d,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,157334400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,Mon Sep 28 20:47:00 UTC 2020,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Operability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,abrown(abrown),ccampbell(ccampbell),echeung(echeung),jisenberg(jisenberg),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Enterprise,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i31wk7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SG-2,,,,,,,,Restricted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sev3-Moderate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2017-01-24 17:55:39.164,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017-01-24 17:55:39.164,2017-01-24 17:55:39.164,,,,,,,,,,,,,24/Jan/17 9:55 AM;557058:4240d4ce-c6e5-4077-a73c-949f1ad0796b;bulk triaging,09/Mar/17 9:28 AM;557058:4240d4ce-c6e5-4077-a73c-949f1ad0796b;No time before FC milestone - removing from release.,06/May/19 10:02 AM;6053a74994d7b90069f8435f;Bulk updating Mission Team to Operability,28/Sep/20 1:47 PM;557058:2d2a7f1d-c9e1-4f3c-95c5-24ed73d0b69d;SevLevelCleanupJI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,14/Nov/16 1:56 PM
[PUBLIC] Reports/Alerts owned by the deleted user cannot be found in the Orphaned filter for the Reassign Knowledge Objects page,SPL-131880,755768,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P1-Immediate,Won't Fix,Nachiket Mistry,557058:8aa8f82e-3e52-4d3f-8ffd-53641ac8f507,Jian Zhang,5af8dc2f82ddd25cccb8bfbe,Jian Zhang,5af8dc2f82ddd25cccb8bfbe,09/Nov/16 11:44 PM,30/Jul/25 4:26 PM,,11/Nov/16 2:00 PM,10.0.x,6.6.0 GA (Kimono),6.6.1,6.6.10,6.6.11,6.6.12,6.6.2,6.6.3,6.6.4,6.6.5,6.6.6,6.6.7,6.6.8,6.6.9,7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,JackHammer,,,,,,,,,,,,,,,,,,,,,,,6.5.1612.x,,,,,,,,Manager Pages - Misc,,,,,0,ShanghaiCoreUIQA,,,,,,,"Steps to reproduce:
1. Create another admin user admin111
2. Login with admin111 and create report and alert for testing
3. Login with admin and delete user admin111
4. Open reassign page e.g. http://localhost:8000/en-US/manager/search/bulkreassign?owner=-&app=search&search=&sortKey=&count=25&sortDir=&offset=0&appOnly=False

Actual:
Reports/Alerts owned by the deleted user can not be found in orphaned tab 

Expected:
Reports/Alerts owned by the deleted user are found in orphaned tab ","Branch: current
Build: 6.5.1612 7ea5c7fff4e3
os: Mac OS X 10.11.4
Browser: Chrome",Chiranjeevi Balawat,Gregory Runyon,Jian Zhang,Matthew Ness,Nachiket Mistry,Sunny Choi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:1bda4ca0-2f75-4038-b46b-f3d96bf6cf95,6036b881185376007024178f,5af8dc2f82ddd25cccb8bfbe,557058:89ace195-adc2-4f25-9924-ded7c8f10bb6,557058:8aa8f82e-3e52-4d3f-8ffd-53641ac8f507,6053a40806cbba006a0da0e9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,,,0,0,,Global,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-129801,VOC-259,,,,,,09/Nov/16 11:44 PM;jzhang;Capture 2016-11-04 at 15.37.46.png;https://splunk.atlassian.net/rest/api/3/attachment/content/240960,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@6274d890,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,179107200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,Tue Jan 21 04:33:58 UTC 2020,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,jzhang(jzhang),mness(mness),nmistry(nmistry),schoi(schoi),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Nachiket Mistry,557058:8aa8f82e-3e52-4d3f-8ffd-53641ac8f507,,,,,,,,,,,,,,,,,,0|i31q9r:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Fulton Financial Corp,2860208,012400000005WzCAAU,P3,No,5005a00001ydAHiAAM,,Open,Customer Update,Standard,Tier4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SG-1,,,,,,,,Restricted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2016-11-10 18:31:51.78,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5_*:*_1_*:*_277423026_*|*_6_*:*_1_*:*_0_*|*_10039_*:*_1_*:*_137752113,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2016-11-10 18:31:51.78,2016-11-10 18:31:51.78,,,,,,Nachiket Mistry,557058:8aa8f82e-3e52-4d3f-8ffd-53641ac8f507,,,,,,10/Nov/16 10:31 AM;6053a40806cbba006a0da0e9;not sure if this should be on search ui or [~accountid:557058:ecb40f3b-d292-497a-91b7-56a1bea7ce5e]/sg-1? ,10/Nov/16 11:52 AM;557058:8aa8f82e-3e52-4d3f-8ffd-53641ac8f507;[~accountid:5af8dc2f82ddd25cccb8bfbe] were these reports/alerts private to admin111? ,10/Nov/16 5:43 PM;5af8dc2f82ddd25cccb8bfbe;[~accountid:557058:8aa8f82e-3e52-4d3f-8ffd-53641ac8f507] yes.,11/Nov/16 2:00 PM;557058:8aa8f82e-3e52-4d3f-8ffd-53641ac8f507;This is a known limitation for Jackhammer. We are working toward a fix for kimono as part of SPL-129801,20/Jan/20 8:33 PM;557058:89ace195-adc2-4f25-9924-ded7c8f10bb6;Updated title of this ticket in response to DOCGUILD-1922.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,11/Nov/16 2:00 PM
[PUBLIC] JSON indexed_extractions doesn&#39;t work for TCP inputs,SPL-123174,707388,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,As Designed,Liang Han,6053ae19695c3900707acb91,John Berwick,557058:1aecc809-62f1-4b1e-8db2-4c1cf3f6a483,John Berwick,557058:1aecc809-62f1-4b1e-8db2-4c1cf3f6a483,21/Jun/16 4:43 AM,30/Jul/25 4:25 PM,,22/Jun/16 10:55 AM,10.0.x,6.4.1,6.4.10,6.4.11,6.4.2,6.4.3,6.4.4,6.4.5,6.4.6,6.4.7,6.4.8,6.4.9,6.5.0 GA (Ivory),6.5.1,6.5.10,6.5.2,6.5.3,6.5.4,6.5.5,6.5.6,6.5.7,6.5.8,6.5.9,6.6.0 GA (Kimono),6.6.1,6.6.10,6.6.11,6.6.12,6.6.2,6.6.3,6.6.4,6.6.5,6.6.6,6.6.7,6.6.8,6.6.9,7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,JackHammer,,,,,,,,,Indexing,,,,,0,FEATURE_INDEXING,SUSTAINING_BACKEND,,,,,,"The customer is trying to use index-time extraction from JSON data coming from an external app via TCP. The data comes in and parsed correctly, but INDEXED_EXTRACTION = json seems to be ignored - the customer is unable to perform any ' | tstats ' operations on the fields that should have been extracted. 

However, when the customer indexes the same data from a file everything works fine and all fields are available to ' | tstats' as expected. 

I have tested this and get the same issue.
",,Abhinav Nekkanti,Amrit Bath,Emanuel House,Hasan Alayli,Hongxun Liu,John Berwick,Jo Hornsby,Liang Han,Maciek Wojcik,,,,,,,,,,,,,,,,,,,,,,,,,,,557058:a708a97a-085e-4967-b389-eabb650be145,5d0995dcc184b50c22ff1c46,6053a8dc06cbba006a0dd832,557058:86bafc93-843d-48fb-b731-7e670cc5f4d4,6053a3fb81b82500685ced67,557058:1aecc809-62f1-4b1e-8db2-4c1cf3f6a483,557058:f87e2651-5132-4aed-8b96-f608ee969435,6053ae19695c3900707acb91,557058:f85a444f-006d-4995-9693-32c9a544c633,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,,,0,0,,Users,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPL-119641,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Micron Technology, Inc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@79257967,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,292032000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,Wed Jun 22 17:55:19 UTC 2016,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,jberwick(jberwick),lhan(lhan),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i2vbs7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Closed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,360968,,,,,,,,,,,,Sustaining,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sustaining Sprint,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2016-06-22 17:55:19.658,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1. Create a sample JSON file /tmp/1.json containing a single line 
{""key1"":""val1""} 

2. Add to props.conf: 
[json] 
INDEXED_EXTRACTIONS = JSON
DATETIME_CONFIG = NONE
KV_MODE = none 

[source::json_tcp] 
SHOULD_LINEMERGE = false 

3. Set up indexes test1 and test2 

4. add to inputs.conf: 
[monitor:///tmp/1.json] 
disabled = false 
sourcetype = json 
index=test1 

[tcp://9995] 
connection_host = ip 
source = json_tcp 
sourcetype = json 
index = test2 

4. Restart Splunk 

5. use nc to send contents of /tmp/1.json to splunk: 
nc localhost 9995 < /tmp/1.json 

6. search indexes test1 and test2, both should contain a single event from the file in #1 above 

7. run the following search over all time: 
| tstats values(key) where index=test1 

Search results: table with ""val1"" as a value in ""values(key)"" column 

8. run the following search over all time: 
| tstats values(key) where index=test2

Search results: no results found. Expected results: same as in #7 above",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6_*:*_1_*:*_0_*|*_10001_*:*_1_*:*_105246863_*|*_10039_*:*_1_*:*_3489799,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2016-06-22 17:55:19.658,2016-06-22 17:55:19.658,,,,,,Liang Han,6053ae19695c3900707acb91,,,,,,"22/Jun/16 9:00 AM;557058:1aecc809-62f1-4b1e-8db2-4c1cf3f6a483;Client has spoken to customer support and found out that INDEXED_EXTRACTION isn't supported.

Can we get this updated on the documentation for the inputs.conf to clarify that tcp/udp doesn't support those features.","22/Jun/16 10:55 AM;6053ae19695c3900707acb91;Please document that
INDEXED_EXTRACTION is not support for tcp input type.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Done,22/Jun/16 10:55 AM
"[PUBLIC] In SHC, referenced saved real-time searches in a dashboard do not stream results.",SPL-118911,688746,Bug,Closed,SPL,Splunk Server,software,Murali Kulathumani,6053a47966c8790068386b56,,,P2-High,Workaround,Manjunath Karikatti,6053abab45a3bb006816ea7c,Michael Wegener,557058:772ff077-c75f-4dbd-9798-298838c06821,Michael Wegener,557058:772ff077-c75f-4dbd-9798-298838c06821,27/Apr/16 9:45 AM,30/Jul/25 4:24 PM,,16/Aug/16 8:57 AM,10.0.x,6.3.1,6.3.2,6.4.0 GA (Galaxy),6.5.0 GA (Ivory),6.5.1,6.5.10,6.5.2,6.5.3,6.5.4,6.5.5,6.5.6,6.5.7,6.5.8,6.5.9,6.6.0 GA (Kimono),6.6.1,6.6.10,6.6.11,6.6.12,6.6.2,6.6.3,6.6.4,6.6.5,6.6.6,6.6.7,6.6.8,6.6.9,7.0.0 GA (Minty),7.0.1,7.0.10,7.0.11,7.0.12,7.0.13,7.0.2,7.0.3,7.0.4,7.0.5,7.0.6,7.0.7,7.0.8,7.0.9,7.1.0 GA (NightLight),7.1.1,7.1.10,7.1.2,7.1.3,7.1.4,7.1.5,7.1.6,7.1.7,7.1.8,7.1.9,7.2.0 GA (OrangeSwirl),7.2.1,7.2.10,7.2.2,7.2.3,7.2.4,7.2.5,7.2.6,7.2.7,7.2.8,7.2.9,7.3.0 GA (PinkiePie),7.3.1,7.3.2,7.3.3,7.3.4,7.3.5,7.3.6,7.3.7,7.3.8,7.3.9,8.0.0 GA (Quake),8.0.1,8.0.10,8.0.2,8.0.3,8.0.4,8.0.5,8.0.6,8.0.7,8.0.8,8.0.9,8.1.0 GA (Rarity),8.1.1,8.1.10,8.1.11,8.1.12,8.1.13,8.1.14,8.1.2,8.1.3,8.1.4,8.1.5,8.1.6,8.1.7,8.1.8,8.1.9,8.2.0 GA (Scootaloo),8.2.1,8.2.10,8.2.11,8.2.12,8.2.2,8.2.3,8.2.4,8.2.5,8.2.6,8.2.7,8.2.8,8.2.9,9.0.0(Aurum),9.0.1,9.0.10,9.0.2,9.0.3,9.0.4,9.0.5,9.0.6,9.0.7,9.0.8,9.0.9,9.1.0(Beryllium),9.1.1,9.1.10,9.1.2,9.1.3,9.1.4,9.1.5,9.1.6,9.1.7,9.1.8,9.1.9,9.2.0(Cobalt),9.2.1,9.2.2,9.2.3,9.2.4,9.2.5,9.2.6,9.2.7,9.2.8,9.3.0(Duranium),9.3.1,9.3.2,9.3.3,9.3.4,9.3.5,9.3.6,9.4.0(Europium),9.4.1,9.4.2,9.4.3,9.4.4,JackHammer,,,,,,,,,,,,,,,,,Dashboard Search,,,,,0,FEATURE_dashboard,ROLE_search_head,SUSTAINING_BACKEND,,,,,"When using a reference to a realtime saved search in a dashboard panel on a search head that is part of a search head cluster, the realtime search brings back initial results, but the search does not continue to stream any data.  An inline realtime search in a panel works fine, and results continue to stream in.

Here is the complete test dashboard that shows a working in-line real time search and a referenced saved real time search that does not work in a SHC:

{code}
<dashboard>
  <row>
    <panel>
      <table>
        <search>
          <query>index=_internal | stats count by component, log_level</query>
          <earliest>rt-5m</earliest>
          <latest>rtnow</latest>
        </search>
      </table>
    </panel>
  </row>
  <row>
    <panel>
      <table>
        <search ref=""335352_test_2""></search>
      </table>
    </panel>
  </row>
</dashboard>
{code}

Here is the context of the savedsearches.conf for the referenced search above:

{code}
[335352_test_2]
action.email.useNSSubject = 1
alert.suppress = 0
alert.track = 0
auto_summarize.dispatch.earliest_time = -1d@h
cron_schedule = * * * * *
dispatch.earliest_time = rt-5m
dispatch.latest_time = rtnow
search = index=_internal | stats count
{code}

When you open the dashboard, you will see the top panel with the inline search run fine and stream back results.  The bottom panel brings back some initial results but then does not continue to stream them in.  Another noticeable thing is that the blue progress line shows up on the bottom panel but does not go away.

Attaching screen shot showing how bottom panel shows initial results, but does not update and shows stagnant blue progress line.  Will also attach screen recording as well.

I reproduced this on 6.4.0 search head cluster, and customer is experiencing this on 6.3.1 search head cluster.  Does not happen on stand-alone search heads on both 6.3.1 or 6.4.0.

Setting as sev 2 as this affects dashboards for live monitoring in customers environment.

Moving original workaround notes here, so that a link to the updated docs workaround can be put in the ""workaround"" field:
""To work around this problem using one of the below workarounds:
1) Use an inline realtime search 
e.g.:
<dashboard>
  <label>DashboardTest</label>
  <row>
    <panel>
      <table>
        <search>
          <query>index=_internal | stats count </query>
          <earliest>rt-5m</earliest>
          <latest>rtnow</latest>
        </search>
      </table>
    </panel>
  </row>
</dashboard>
2) Create scheduled saved search. Use loadjob inline query in the dashboard to update the dashboard with the results of the saved search.
e.g.:
<dashboard>
  <label>DashboardTest</label>
  <row>
    <panel>
      <table>
        <search>
          <query> | loadjob savedsearch=""admin:search:SavedSearch""</query>
        </search>
       <option name=""refresh.auto.interval"">60</option> 
      </table>
    </panel>
  </row>
</dashboard>
Using inline realtime search workaround would be efficient as realtime search will be running only when the dashboard is being viewed. However  if different users access the dashboard either from the same search head member or different member, there will be a new realtime search spawned for each user.

In case of  saved search, only one instance of the saved search will be run at the scheduled time irrespective of whether multiple users or no user accessing the dashboard.""","6.4.0 search head cluster
6.3.1 search head cluster
6.3.2 search head cluster
Both tested on linux shclusters",Andrew Brown,Antony Wang,dstreit,Emanuel House,frobinson,Jamie Spears,Jeffrey Chang,Kishore Vasudeva,Manjunath Karikatti,Manoj Pawar,Manu Jose,Matteo Garaventa,Matteo Zorzi,Matthew Wong,Michael Wegener,Mike Nelis,Parvesh Jain,Praveen Burgu [X],Siegfried Puchbauer,Sudha lakshmisha,Tianyi Gou,Valentine Prisiajni,Venugopal Balasubramanian,Victor Ebken,,,,,,,,,,,,557058:37b4518c-e757-440f-87b7-181f5f425e80,557058:9b250150-4703-4e0c-a02b-ad536bb0d408,5e629b152c1aee0cddcf09bd,6053a8dc06cbba006a0dd832,611be6e8c2f3a50069e26941,5ecd9216dacd410c1f8ccf61,557058:52c801cc-dc38-4687-bd76-6aeaab916f3c,557058:86ec36cd-ed38-4da0-9ca8-d452b6d025d8,6053abab45a3bb006816ea7c,557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d,5f7671cd837bb80068281d0b,5a848585a08cc5310a6cae0a,5f7670d0287870006a058291,5f191b23e1618b001b1fae8c,557058:772ff077-c75f-4dbd-9798-298838c06821,6053bbaa66c8790068396e26,557058:a6f000d5-3fc9-45a4-9a73-f164c6d46a6c,613250e42aa2800068a44996,557058:ffe53cbd-86cd-4cff-8ba7-369fbc58d342,557058:5fa06468-e0e4-4d85-b25f-e01185474f22,6053a5aa66c8790068387905,6036b79bf032740068734426,61325c6e6e5e1e0071f0b240,557058:af7c0e06-1034-4a1e-9f5a-b20a0ec7b792,,,,,,,,,,,,144000,0,,0%,144000,0,,Users,,,,,,,,,,,,,,,,,,,,,,SPL-131809,,,,,,,,,,,SCSM-31,SPL-125653,SPL-126876,,,,,27/Apr/16 9:38 AM;mwegener;case_335352.jpeg;https://splunk.atlassian.net/rest/api/3/attachment/content/186777,27/Apr/16 9:51 AM;mwegener;case_335352.mov;https://splunk.atlassian.net/rest/api/3/attachment/content/186778,09/May/16 4:13 PM;mpawar;non-working-1st-req.png;https://splunk.atlassian.net/rest/api/3/attachment/content/189152,09/May/16 4:13 PM;mpawar;non-working-2nd-req.png;https://splunk.atlassian.net/rest/api/3/attachment/content/189153,05/May/16 9:08 AM;nromito;norestcall.png;https://splunk.atlassian.net/rest/api/3/attachment/content/188553,11/Jul/16 4:48 AM;mkarikatti;search-job-SID.png;https://splunk.atlassian.net/rest/api/3/attachment/content/204087,11/Jul/16 4:48 AM;mkarikatti;search-job-id=SID.png;https://splunk.atlassian.net/rest/api/3/attachment/content/204086,08/Jul/16 9:28 AM;mkarikatti;search-jobs-sid-REST Calls.png;https://splunk.atlassian.net/rest/api/3/attachment/content/203602,09/May/16 4:13 PM;mpawar;working-1st-req.png;https://splunk.atlassian.net/rest/api/3/attachment/content/189151,09/May/16 4:13 PM;mpawar;working-2nd-req.png;https://splunk.atlassian.net/rest/api/3/attachment/content/189154,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@648c4bc6,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,66.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"{panel:title=Confirmation|borderStyle=dashed|borderColor=#ccc|titleBGColor=#f44336|bgColor=#FFFFCE}
Are you sure you would like it to withdraw the ticket ? Press *Withdraw* to continue or *Cancel* to remain in current status.
{panel}",,,,,,,,,,,143251200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,include in release notes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,Thu Mar 11 03:18:16 UTC 2021,true,adallas(adallas),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0,66.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,andrewb(andrewb),dawang(dawang),frobinson(frobinson),jspears(jspears),mkarikatti(mkarikatti),mpawar(mpawar),mjose(mjose),mwong(mwong),mwegener(mwegener),nromito(nromito),pjain(pjain),spuchbauer(spuchbauer),vprisiajni(vprisiajni),vijayb(vbalasubramanian),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sustaining,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Manjunath Karikatti,6053abab45a3bb006816ea7c,,,,,,,,,,,,,,,,,,0|i2sxnz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Banca 5 S.p.A.,864396,012400000005WzMAAU,P2,No,5000b00001FteiBAAR,,Closed,Resolved - Work Around,Standard,Tier4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"335352, 359203, 358781, 389458",,,,,,,,,,,,Sustaining,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sev2-Critical,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Sustaining Sprint,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,2016-05-05 15:23:55.345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1) Create the saved search outlined in description in a SHC
2) Create the dashboard outlined in description in the SHC
3) Open the dashboard and observe the bottom panel does not continue to stream as a real time panel should",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10008_*:*_2_*:*_4278118597_*|*_3_*:*_4_*:*_225066869_*|*_4_*:*_1_*:*_25787_*|*_5_*:*_2_*:*_949035155_*|*_10001_*:*_3_*:*_4132065440_*|*_10039_*:*_1_*:*_3223060,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,33.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"See [http://docs.splunk.com/Documentation/Splunk/6.5.0/Viz/Savedsearches#Troubleshoot_referenced_real-time_searches_in_search_head_clusters Troubleshoot referenced real-time searches] for workaround details.

",,,,,,,,,,2016-05-05 15:23:55.345,2016-05-05 15:23:55.345,,,,,,Manjunath Karikatti,6053abab45a3bb006816ea7c,,,,,,27/Apr/16 9:51 AM;557058:772ff077-c75f-4dbd-9798-298838c06821;Attaching screen recording that shows the issue outlined above with the dashboard and saved search mentioned [^case_335352.mov] ,"05/May/16 8:23 AM;611bdefb4016870069185d34;*Status Update*
I have reproduced the behavior locally.  An interesting observation that I've made from the repro, the issue is not seen on the shc member where the saved search was created, but it is seen on a different member that had the search replicated over.

*Action(s)*
*Engineering*
Dive into the code/repro to determine a root cause.

*Next Update*
Tomorrow or upon new findings.","05/May/16 9:10 AM;611bdefb4016870069185d34;*Status Update*
Looking at the reproduction REST calls being made, I see the following:
 !norestcall.png|thumbnail! 
After the initial couple of calls to get the results for the savedsearch version of the search, we no longer see these calls being made.  The only calls I see coming from the UI is calls to the search that is defined inline in the dashboard source.  This makes me think that this issue might be UI related.

*Action(s)*
*Engineering*
Drop this into the traffic cop queue for UI triage.

*Next Update*
Upon UI triage.","06/May/16 5:04 PM;557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d;[~accountid:611bdefb4016870069185d34], do you still have this environment setup? If yes then can you please give me an access to your instance?","06/May/16 5:08 PM;611bdefb4016870069185d34;[~accountid:557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d], here is the environment:
{noformat}
nromito-mbp:main nromito$ tsplk status realtime-search-shc
Project: realtime-search-shc
╒═══════════════════════════════════════════╤════════════════════════════════════════════════════════════════════════════════════╤═══════════════╤═══════════════╤═════════════════╕
│ minion_id                                 │ roles                                                                              │ public_ip     │ private_ip    │ instance_type   │
╞═══════════════════════════════════════════╪════════════════════════════════════════════════════════════════════════════════════╪═══════════════╪═══════════════╪═════════════════╡
│                                           │ salt-master                                                                        │ 52.39.210.120 │               │                 │
├───────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────────────┼───────────────┼───────────────┼─────────────────┤
│ nromito-realtime-search-shc-ubuntu-1404-2 │ ['search-head', 'search-head-cluster-member']                                      │ 52.32.35.122  │ 172.31.23.101 │ m3.large        │
├───────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────────────┼───────────────┼───────────────┼─────────────────┤
│ nromito-realtime-search-shc-ubuntu-1404-3 │ ['search-head', 'search-head-cluster-member']                                      │ 52.39.218.198 │ 172.31.24.49  │ m3.large        │
├───────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────────────┼───────────────┼───────────────┼─────────────────┤
│ nromito-realtime-search-shc-ubuntu-1404-0 │ ['indexer']                                                                        │ 52.38.31.19   │ 172.31.16.183 │ m3.large        │
├───────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────────────┼───────────────┼───────────────┼─────────────────┤
│ nromito-realtime-search-shc-ubuntu-1404-1 │ ['search-head', 'search-head-cluster-member', 'search-head-cluster-first-captain'] │ 52.26.141.117 │ 172.31.29.205 │ m3.large        │
├───────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────────────┼───────────────┼───────────────┼─────────────────┤
│ nromito-realtime-search-shc-ubuntu-1404-4 │ ['search-head-cluster-deployer']                                                   │ 52.34.241.188 │ 172.31.25.165 │ m3.large        │
╘═══════════════════════════════════════════╧════════════════════════════════════════════════════════════════════════════════════╧═══════════════╧═══════════════╧═════════════════╛
{noformat}","06/May/16 5:24 PM;557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d;[~accountid:611bdefb4016870069185d34], awesome. thanks.","09/May/16 4:13 PM;557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d;*+Status Update:+*
- So, i tested this problem on following two instances:
-- Works (/): http://52.39.218.198:8000/
-- Does not work (x): http://52.32.35.122:8000/
- The table result is updated based on the response that we get from  results_preview request
- This request is made only if we meet some condition and that is based on the response that we get from the jobs endpoint request.
- Basically, we monitor following properties as part of the job tracker in the response of jobs request
{noformat}
dispatchState
scanCount
resultCount
eventCount
isPaused
eventAvailableCount
isDone
isFailed
isFinalized
doneProgress
numPreviews
{noformat}
- So, if any of the above properties contain a new value, then we schedule the next results_preview request. If we dont see a change, we don't make that request.
- Here is what i have noticed in the above two instances:
-- For http://52.39.218.198:8000/ which works (/):
--- we keep getting different content in the response of jobs request and after that i can see the results_preview request
--- This screenshot shows the response for the previous jobs request: !working-1st-req.png|thumbnail!
--- This screenshot shows the response for the next jobs request: !working-2nd-req.png|thumbnail!
-- For http://52.32.35.122:8000/ which does not work (x):
--- we keep getting the same content in the response of jobs request and due to that there is no results_preview request
--- This screenshot shows the response for the previous jobs request: !non-working-1st-req.png|thumbnail!
--- This screenshot shows the response for the next jobs request: !non-working-2nd-req.png|thumbnail!
- The issue seems to be in the jobs endpoint which is might not be returning the correct/changed content.
- [~accountid:611bdefb4016870069185d34], can you please look at what is going on at this endpoint? Note: In the above testing, i have commented out the first panel which was the inline search and just kept the saved search panel.","10/May/16 1:35 PM;611bdefb4016870069185d34;*Status Update*
I have root caused why we are not getting any changes from the jobs endpoint.  Here is the code at the root cause of the issue:
{code}
if (isget) {

            if (!isrsa) {
                Str rsa_sid = ""rsa_"" + sid;
                const Pathname& rsa_ddir = DispatchCommand::dispatchDir(rsa_sid);
                if (rsa_ddir.isDir())
                {
                    sid = rsa_sid;
                    return;
                }
{code}
If we determine that we already have a replicated artifact for the provided sid to the jobs endpoint, we won't bother replicating the artifact again.  This causes our data to become stale when we're running a realtime search and using the jobs endpoint to determine if there is any new data for us to get.

*Action(s)*
*Engineering*
Determine the best way to fix this issue.

*Next Update*
Upon writing a fix.","10/May/16 3:46 PM;611bdefb4016870069185d34;*Status Update*
One more thing that I have found while debugging this issue, this is what the search is called on the originating search head {noformat}rt_admin__admin__search__RMD5405124ddad0832b1_at_1462918364_0_512FC717-9631-4CF6-B591-C1C004C81723{noformat}, and this is what the replicated artifact is called on the proxying search head {noformat}rsa_scheduler__nobody_c3BsdW5rX2FyY2hpdmVy__RMD5473cbac83d6c9db7_at_1462918620_0_FCE2D699-6F96-4E25-8F1F-2962FA65230C{noformat}

*Action(s)*
*Engineering*
Research a fix.

*Next Update*
Upon completion of fix.","11/May/16 4:44 PM;611bdefb4016870069185d34;*Status Update*
It turns out that we're not actually replicating anything over for the realtime search. We just proxy over the last known status of the search; however, because of:
{code}
bool SHPSlave::updateRunningSids(const std::vector<Str>& alive_sids, Str& err)
{
    ScopedMutex lock(_sidsMux);

    (void)err;
    size_t cnt = 0;
    std::vector<Str>::const_iterator citr;
    for (citr = alive_sids.begin(); citr != alive_sids.end(); citr++)
    {
        const Str& sid = *citr;
        if (_reportedAlive.contains(sid)) {
            cnt++;
            continue;
        }

        Pathname aliveFile  = DispatchCommand::aliveTokenFile(sid);
        bool is_alive = ScopedAliveProcessToken::isAlive(aliveFile);

        // in this step, we only deal with alive sids.
        // if it is not alive but done, it will be reported as removeDispatchSids
        // if it is cancelled, it will be reaped
        if (!is_alive)
            continue;

        addDispatchManagerSID_locked(sid);
    }

    gLogger.debug() << ""# of sids alive = "" << alive_sids.size() << "" num_skipped = "" << cnt;
    return true;
}
{code}
we will not continuously update the status of a running job.  This is why the search that is the proxy will not be continuously updated.
[~accountid:5f7671cd837bb80068281d0b], is it supposed to be a known limitation that we will not continuously update the status of a realtime search (so we can't proxy realtime searches accurately to a dashboard)?  I noticed in Anirban's commit that he says he did this on purpose to limit network usage.  Is this still a concern? Should we be changing this code to update when hitting the jobs endpoint or at least update running searches every heartbeat?

*Action(s)*
*Engineering*
Determine appropriate fix.

*Next Update*
Upon new findings.","13/May/16 3:56 PM;611bdefb4016870069185d34;*Status Update*
I have a fix up for review for this change.  Due to how large the change is, I retargeted this for Ivory.  Since Honeybuzz is coming to a close soon, I thought it would be smarter to let this bake longer in develop (instead of rushing into Honeybuzz) to make sure no regressions come from the change.

*Action(s)*
*Engineering*
Get fix reviewed.

*Next Update*
Upon completion of review.
 ","02/Jun/16 9:52 AM;611bdefb4016870069185d34;*Status Update*
Upon further discussion with Manu, it has been determined that the UI is performing two different REST calls in two different situations.  When trying to proxy from a dashboard (broken case), UI makes this call:
{noformat}
http://52.40.242.222:8000/en-US/splunkd/__raw/servicesNS/-/-/search/jobs?id=rt_admin__admin__search__test_at_1464881344_1_E3CB9186-A0B6-4402-BF22-BF907AD6D8A1&count=1&output_mode=json&_=1464881479806
{noformat}
when trying to proxy from the reports page (working case), UI makes this call:
{noformat}
http://52.40.242.222:8000/en-US/splunkd/__raw/servicesNS/nobody/search/search/jobs/rt_1464882364.4_2D25E9A4-7595-45DA-84D4-A227008DFB86?output_mode=json&_=1464882363979
{noformat}

*Action(s)*
*Engineering*
UI to pick this up and determine if they can fix this on their side.

*Next Update*
Upon UI determining potential fix.","10/Jun/16 2:34 PM;557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d;[~accountid:611bdefb4016870069185d34],

As per my previous findings in [comment|https://splunk.atlassian.net/browse/SPL-118911?focusedCommentId=1206975&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-1206975], the issue is in jobs endpoint because i had compared the dashboards of two different instances. In your findings you have compared dashboard with report. Dashboard and report both works in different way so we cant really compare those for this issue.

Since, we have a repro where we can see how both dashboards work in different way, we should look at what is going on at the jobs endpoint.

[~accountid:6053a5aa66c8790068387905], since you are a TC, please include this as backend issue in the email.","10/Jun/16 5:38 PM;611be458aee32f006f920d46;Hi [~accountid:611bdefb4016870069185d34], since you have a full backend context, can you take this to conclusion?","13/Jun/16 8:01 AM;611bdefb4016870069185d34;[~accountid:611be458aee32f006f920d46], unassigning myself.  I am not part of the SHC team.  This issue is easily reproducible, and sustaining backend and frontend should be able to work together to find a proper solution.",17/Jun/16 2:19 PM;6036b79bf032740068734426;Chewey.com is also reporting an issue with this in a two member search head on 6.3.2. Any updates on a fix/workaround?,"20/Jun/16 5:36 PM;6053abab45a3bb006816ea7c;*Status Update:* [~accountid:6036b79bf032740068734426] I am investigating further on this. From my investigation so far (and also pointed out by Nick), this problem with referenced saved RT search always occurs when the search is proxied to another SHC member. I am stilling checking the logs and the source code to understand under what circumstance the search request gets proxied to another member instead of processing it on the originating member. 

If the search request is handled by member from which dashboard is being viewed, then referenced saved RT search dashboard correctly gets refreshed.

Until a conclusion on whether this is issue is because of the fact that RT search artifact are not replicated or it is a bug requiring a code change, there is a workaround for this issue.
Workaround: 
1) Find the SID of referenced saved RT search, on the originating SHC member.
2) Using the SID find the SHC member which is actually processing the search. 
    You either do a i) ps -ef | grep search or ii) find <dispatch_dir> -name SID  on each SHC member.
3) If the SHC member handling the search is different than the originating (which is the problem scenario), connect to that SHC member and view the dashboard.

I have tried this on my recreate and I could see the dashboard was getting refreshed.

*note to self:*
Found below message on SHC member processing the search. Could be related the fact that we don't support RT search artifact replication.
{code:java}
splunkd.log:06-20-2016 15:33:40.335 -0700 DEBUG SHPMaster - event=SHPMaster::asyncReplicateArtifact sid=rt_admin__admin__search__RMD51edc408b78834c55_at_1466462005_5_7E0903BD-77E7-4D9D-8299-13C7200D97F6
 status=failed msg=""sid is not artifact but a remote search job ""
{code}


*Actions:*
*Engineering:* Confirm whether this is a bug, if yes work on a solution.
*Support:* Suggest the work around to the customer and check if it avoids the problem.
*Next Update:* On finding anything new.","28/Jun/16 12:22 PM;6036b79bf032740068734426;Customer confirmed that connecting directly to the search origin member, that the refresh works as expected. Has there been any progress here? Customer is looking for fix as workaround is not a viable solution for a production environment. ","30/Jun/16 7:04 AM;6053abab45a3bb006816ea7c;[~accountid:6036b79bf032740068734426] I am working on the fix. While my intension is to provide the fix in the next maintenance release, but if we encounter any issues during testing either with regard to functionality or performance, I will have to reassess the target release for the fix. I will update the jira with the progress.","30/Jun/16 7:50 AM;6053abab45a3bb006816ea7c;[~accountid:557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d] With regard to your last comment, I understand the job status REST call made from dashboard query and a report query differ.
Given we are getting the updates with search/jobs/sid REST call, wouldn't it be feasible use the same REST call  (search/jobs/sid  instead of search/jobs?id=sid) for dashboard queries as well ? I understand it might not be that straight forward, just checking with you before I start changing the backend code.",06/Jul/16 6:52 AM;5ecd9216dacd410c1f8ccf61;Any progress on permanent fix??,"06/Jul/16 10:13 AM;6053abab45a3bb006816ea7c;[~accountid:5f7671cd837bb80068281d0b] I am planning to implement the fix on the lines of your suggestion, providing a config parameter ""job_refresh_rate"". What I am proposing is, at ""job_refresh_rate"" interval the dispatch reaper will call another function similar to doReapWork(), but the only thing this new function will do is calling SHPCallback::slave_updateRunningSids() so that job status is sent to captain. This can be done only for realtime jobs (i.e. if sid has rt_  prefix). Let me know your thoughts.","06/Jul/16 11:08 AM;557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d;[~accountid:6053abab45a3bb006816ea7c], not sure if we can do that but may be [~accountid:557058:12f01952-c4c1-40df-b15e-467c00df223d] or [~accountid:557058:ffe53cbd-86cd-4cff-8ba7-369fbc58d342], can share their thoughts on your [comment|https://splunk.atlassian.net/browse/SPL-118911?focusedCommentId=1262483&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-1262483].","08/Jul/16 2:16 AM;6053abab45a3bb006816ea7c;*Status Update:*
I have implemented the fix on sever side. The fix adds a new class 'RtSearchStatusUpdater' which will send the real time search job status to captain at configured interval 'job_refresh_rate'. doReaperWork function of this class is called by the dispatch reaper at 'job_refresh_rate' interval. The PR is in review.
*Next Update:*
On review completion.","08/Jul/16 9:05 AM;5f7671cd837bb80068281d0b;[~accountid:6053abab45a3bb006816ea7c] Did you guys decide offline to move away from this approach ? 

{noformat}
Given we are getting the updates with search/jobs/sid REST call, wouldn't it be feasible use the same REST call (search/jobs/sid instead of search/jobs?id=sid) for dashboard queries as well ? I understand it might not be that straight forward, just checking with you before I start changing the back end code
{noformat}

","08/Jul/16 9:25 AM;6053abab45a3bb006816ea7c;[~accountid:5f7671cd837bb80068281d0b]  What I observed was, the dashboard in question is continuously making 
__raw/servicesNS/-/-/search/jobs?id=SID  REST calls. But once in 30 secs it is also calling the end point search/jobs/SID
So I was expecting the dashboard should have got refreshed at least once in 30 sec, if seach/jobs/SID  was fetching updated status. But the dashboard is not getting refreshed. The screen capture shows search/jobs/SID call made by the UI.
 !search-jobs-sid-REST Calls.png|thumbnail! 

Hence I am not sure whether changing the dashboard to use search/jobs/SID end point, would work ? 

However, if I open another browser and run https://<host name>:<mgmt port>/services/search/jobs/SID,  this brings me the updated data.
I am trying to understand the code flow when we request https://<host name>:<mgmt port>/services/search/jobs/SID.","11/Jul/16 4:49 AM;6053abab45a3bb006816ea7c;[~accountid:557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d] Further investigating this, I think this is an UI issue and can be addressed by a change in UI. Below are the details.

I am using below dashboard which calls the referenced saved RT search.
{code:java}
<dashboard>
  <row>
    <panel>
      <table>
        <search ref=""savedrtsearch""></search>
      </table>
    </panel>
  </row>
</dashboard>

{code}

When this dashboard is launched UI queries the job status using two REST calls
1) GET /en-US/splunkd/__raw/servicesNS/-/-/search/jobs?id=<SID>   This end point is called every one second.
2) GET /en-US/splunkd/__raw/servicesNS/admin/search/search/jobs/<SID>  This end point is called once every 30 seconds.

For the first end point, the request to routed to captain to fetch the job status. Since RT artifacts are not replicated (this is as per current design), the captain sends back the stale job status.

For the second end point, search/jobs/<SID> the request is proxied to the SHC member where RT search is executing, to fetch the latest job status. By inspecting the HTTP response of this query, I can see that server is correctly returning latest job status (refer to event count in the attachement). So I expect once every 30 seconds, the dashboard should have got updated with latest data. But for some reason UI is not updating the dashboard. 
 !search-job-id=SID.png|thumbnail!  !search-job-SID.png|thumbnail! 

We need to investigate why UI is not updating the dashboard on getting a response for search/jobs/<SID> request, in spite of  updates to the properties (event count) the job tracker is monitoring. Once this is addressed, we should also make this 30sec interval configurable.
",11/Jul/16 7:19 AM;5f7671cd837bb80068281d0b;[~accountid:6053abab45a3bb006816ea7c] Thanks for the investigation. I was also wondering why the 30 sec interval calls are not fetching right results. I too agree that this should be fixed at the UI rather than increasing network/load on the cluster. ,"13/Jul/16 3:47 PM;557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d;*+Update:+*

- This update is similar to what i had commented in [comment 1|https://splunk.atlassian.net/browse/SPL-118911?focusedCommentId=1206975&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-1206975] and [comment 2|https://splunk.atlassian.net/browse/SPL-118911?focusedCommentId=1239637&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-1239637] but with some more information.
- The most important thing is that the issue happens only in SHC setup and not on a standalone instance. If you run a search in SHC env for which the search provider is the same instance or if you test the same behavior on other member for which the search provider is other search head, you won't see the issue. The dashboard doesn't have any special behavior for the SHC setup. The issue is clearly on the backend side.
- There are 3 requests which are made from dashboards page:
-# {{/en-US/splunkd/__raw/servicesNS/-/-/search/jobs?id=<SID>}}
--- Based on the response that we get for the request, we decide whether to make the results_preview request.
--- There are certain properties which we monitor to decide whether we will make results_preview request or no. Here is the list of properties:
{noformat}
dispatchState
scanCount
resultCount
eventCount
isPaused
eventAvailableCount
isDone
isFailed
isFinalized
doneProgress
numPreviews
{noformat}
--- If none of the above properties changed in the response of the latest request as compared to the previous one then we don't make results_preview request otherwise we will make a results_preview request.
-# {{/en-US/splunkd/__raw/servicesNS/admin/search/search/jobs/<SID>}}
--- This request is made just to ping the job so that it does not die. We don't rely on this request to make a decision about the results_preview request.
-# {{/en-US/splunkd/__raw/servicesNS/admin/search/search/jobs/<SID>/results_preview}}
--- This request is made to get the actual search results.
- Here is the sequence of requests:
-# {{/en-US/splunkd/__raw/servicesNS/-/-/search/jobs?id=<SID>}}
{noformat}
dispatchState: ""RUNNING""
scanCount: 93
resultCount: 0
eventCount: 93
isPaused: false
eventAvailableCount: 0
isDone: false
isFailed: false
isFinalized: false
doneProgress: 1
numPreviews: 10
{noformat}
-# {{/en-US/splunkd/__raw/servicesNS/admin/search/search/jobs/<SID>/results_preview}}
-# {{/en-US/splunkd/__raw/servicesNS/-/-/search/jobs?id=<SID>}}
{noformat}
dispatchState: ""RUNNING""
scanCount: 177
resultCount: 0
eventCount: 177
isPaused: false
eventAvailableCount: 0
isDone: false
isFailed: false
isFinalized: false
doneProgress: 1
numPreviews: 15
{noformat}
-# {{/en-US/splunkd/__raw/servicesNS/admin/search/search/jobs/<SID>/results_preview}}
-# {{/en-US/splunkd/__raw/servicesNS/admin/search/search/jobs/<SID>}}
-# {{/en-US/splunkd/__raw/servicesNS/-/-/search/jobs?id=<SID>}}
{noformat}
dispatchState: ""RUNNING""
scanCount: 177
resultCount: 0
eventCount: 177
isPaused: false
eventAvailableCount: 0
isDone: false
isFailed: false
isFinalized: false
doneProgress: 1
numPreviews: 15
{noformat}
-# {{/en-US/splunkd/__raw/servicesNS/-/-/search/jobs?id=<SID>}}
{noformat}
dispatchState: ""RUNNING""
scanCount: 177
resultCount: 0
eventCount: 177
isPaused: false
eventAvailableCount: 0
isDone: false
isFailed: false
isFinalized: false
doneProgress: 1
numPreviews: 15
{noformat}
- As you can see in the 1st request, response was changed so we made results_preview request. Then response was again changed in 3rd request so we made another results_preview request. But after that even after 6th and 7th request, the response didn't change at all, so we didnt make the results_preview request. So, we never got the new results.
- What i've noticed is even if results were not getting updated on dashboard, results were actually updated on REST endpoint of the results_preview {{https://localhost:8089/servicesNS/admin/search/search/jobs/<SID>/results_preview}}. But since, we never saw any update in the monitored properties for the jobs request, we never made the results_preview request.
- I have attached a HAR dump to this JIRA for this session which was taken on captain {{http://wimpy.splunk.com:2222/}} from the [~accountid:6053abab45a3bb006816ea7c]'s SHC env setup on wimpy. *wimpy.splunk.com.2222.har*
- When i tested the behavior on member {{http://wimpy.splunk.com:3333/}}, the issue was not reproducible and dashboard  results update was working with correct sequence of requests. Here is the HAR dump for that *wimpy.splunk.com.3333.har*
- The issue is clearly in the {{https://localhost:8089/servicesNS/-/-/search/jobs?id=<SID>}} REST endpoint.
","14/Jul/16 5:58 AM;6053abab45a3bb006816ea7c;[~accountid:557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d] Thanks for the detailed analysis on what is happening at UI end. I do understand search/jobs?id=<SID> REST endpoint is not fetching the updated eventCount. That is because Search Head currently does not support RT search artifacts replication. 
search/jobs?id=<SID>  correctly fetches the updated data when the RT search runs on the same member, this is because the artifacts are available locally. 

{quote}
/en-US/splunkd/__raw/servicesNS/admin/search/search/jobs/<SID>
This request is made just to ping the job so that it does not die. We don't rely on this request to make a decision about the results_preview request.{quote}

I believe it won't be much of overhead on UI (at least as compared to the overhead of fixing this on server side) to make results_preview call if search/jobs/SID had returned updated eventCount. Is that feasible to implement ? Do you see any major impact on UI w.r.t performance if we were to fix this in UI ?
","14/Jul/16 10:56 PM;557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d;[~accountid:6053abab45a3bb006816ea7c],

That will require huge changes and that too at architectural level. This can not be fixed in UI for a maintenance release.","15/Jul/16 7:06 AM;6053abab45a3bb006816ea7c;[~accountid:5f7671cd837bb80068281d0b], [~accountid:557058:a6f000d5-3fc9-45a4-9a73-f164c6d46a6c] UI team (Manoj) has explored the change for this in UI and it seems  a larger change. Should we stick to the change where we push the job status updates from dispatch reaper thread every 30 sec (not using the new function I proposed initially) ?
dispatchReaper -- > doReaperWork --- > SHPCallback::slave_updateRunningSids()    slave_updateRunningSids will add the RT search SID to _dispatchSids which eventually gets sent to the captain. Only downside will be that RT search refreshes will happen at 30 sec interval. 

End user will be able to notice this, as for the RT search running on the same member refreshes are immediate as against 30sec refreshes in case of RT search running on a different SH member.","15/Jul/16 8:34 AM;557058:a6f000d5-3fc9-45a4-9a73-f164c6d46a6c;[~accountid:557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d]
{quote}
That will require huge changes and that too at architectural level. This can not be fixed in UI for a maintenance release.
{quote} 

We understand the change might be significant here . Could we add corresponding UI core people to make a decision on this ? 

Also fixing it from a backend is a problem, since it is adding huge overhead on SHC nodes - Either  member and captain sync on updated status of the real-time job every sec or member could choose to update the status of *every* real-time job every 30secs or so - which is also not efficient. 

Only consumers of *search/jobs/<SID>* endpoint can ask backend for updated status on a job , thus it makes more sense to fix this at consumer end.","15/Jul/16 10:16 AM;557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d;[~accountid:557058:12f01952-c4c1-40df-b15e-467c00df223d], [~accountid:557058:ffe53cbd-86cd-4cff-8ba7-369fbc58d342], [~accountid:5e629b152c1aee0cddcf09bd], can you please provide your thoughts on this?","18/Jul/16 6:17 PM;5f191b23e1618b001b1fae8c;Hi,

Can I know which version will be fixing this bug? My customer is having issue to view their real time dashboard searches from SHC. Thanks. 

Regards,
Matthew Wong","19/Jul/16 3:12 PM;6053abab45a3bb006816ea7c;[~accountid:557058:12f01952-c4c1-40df-b15e-467c00df223d], [~accountid:557058:ffe53cbd-86cd-4cff-8ba7-369fbc58d342] appreciate your comments on addressing this issue through a change in UI. 

{quote}/en-US/splunkd/__raw/servicesNS/admin/search/search/jobs/<SID>
This request is made just to ping the job so that it does not die. We don't rely on this request to make a decision about the results_preview request.{quote}

Is it feasible to make a change in UI to call results_preview after the reply to search/jobs/<SID> indicates a change in eventCount ?","20/Jul/16 1:53 PM;557058:ffe53cbd-86cd-4cff-8ba7-369fbc58d342;{quote}
Is it feasible to make a change in UI to call results_preview after the reply to search/jobs/<SID> indicates a change in eventCount ?
{quote}

This is typically being done, but we don't ask the search/jobs/<SID> endpoint, instead we use a single request to fetch the info for all jobs {{search/jobs?id=<sid1>&id=<sid2>...&count=<n>}} periodically. We only ping search/jobs/<SID> to keep the job alive (reset TTL) - this happens on a different schedule (a lot less frequent).

I guess a long-term fix would be to disable this optimization on SHC. This is non-trivial effort, though.


Another suggestion: how about we somehow force the jobs list endpoint to force refresh the job info for this particular case. If we call

{code}
search/jobs?id=<sid1>&id=<sid2>...&count=<n>&forceRefresh=1
{code}

then the jobs endpoint would refresh the info for each requested job before responding. Would this be feasible on the backend? The frontend change would be fairly trivial and could likely go into a maintenance release.","21/Jul/16 5:59 AM;6053abab45a3bb006816ea7c;[~accountid:557058:ffe53cbd-86cd-4cff-8ba7-369fbc58d342] Thanks for you feedback. 
In your 1st approach are you suggesting that instead of calling {{search/jobs?id=<sid1>&id=<sid2>}}, call {{search/jobs/<SID>}} endpoint and then based on the reply, call {{results_preview}} endpoint to refresh the dashboard? Doing so every 1 sec would generate lot of traffic between search head members and the captain. 

I think if we keep the current mechanism of UI making single request with multiple jobids (  {{search/jobs?id=<sid1>&id=<sid2>}} ) as is and just change reset TTL calls ( seach/jobs/<SID> ) to also call result_preview endpoint (30ses or 15sec intervals), wouldn't that be something do-able in a maintenance release? I might be wrong here, as I do not know the feasibility of implementing this approach.

[~accountid:5f7671cd837bb80068281d0b] Any thoughts ?

For the second approach (forceRefresh argument), I have not assessed the required code changes at backend but I believe it will a considerable change for a maintenance release.
","21/Jul/16 6:06 AM;557058:ffe53cbd-86cd-4cff-8ba7-369fbc58d342;{quote}
I think if we keep the current mechanism of UI making single request with multiple jobids ( search/jobs?id=<sid1>&id=<sid2> ) as is and just change reset TTL calls ( seach/jobs/<SID> ) to also call result_preview endpoint (30ses or 15sec intervals), wouldn't that be something do-able in a maintenance release? I might be wrong here, as I do not know the feasibility of implementing this approach.
{quote}

This is not a viable approach at all, unfortunately. Like I mentioned before, the call to search/jobs/<SID> is functionally complete separate from updating the job information in the client-side implementation here. It's only used to reset the TTL and the response is ignored.","29/Jul/16 10:25 AM;6053abab45a3bb006816ea7c;[~accountid:6036b79bf032740068734426] After several discussions with UI and backend teams (both core and sustaining), we think that fixing this issue (either in UI or in backend) will involve architecture level changes which cannot be done in a maintenance release.

I will later communicate this to product management so that we can open a story to track this and address in a future major release.

Customers currently using referenced RT searches in dashboards, can use one of the workarounds suggested below until the issue is fixed.

Use inline RT searches instead of referenced saved RT searches in the dashboard.
Or 
Use a saved search (non RT search) with auto refresh interval. Downside of using saved search will be that there will be as many search processes created during refresh as the number of users currently viewing the dashboard.
e.g:
{code:java}
   <dashboard>
  <label>TEST</label>
  <row>
    <panel>
      <table>
        <search>
          <query>index=_internal | stats count </query>
        </search>
       <option name=""refresh.auto.interval"">60</option> 
      </table>
    </panel>
  </row>
</dashboard>
{code}
","02/Aug/16 2:54 PM;6053abab45a3bb006816ea7c;CC: [~accountid:557058:b1e398c5-b004-4a33-8152-18daa36cda4b] [~accountid:5f7671cd837bb80068281d0b] [~accountid:557058:ffe53cbd-86cd-4cff-8ba7-369fbc58d342] [~accountid:557058:72c1f891-c7a1-45ba-9a3f-e80a5740160d]
We have explored various solutions for this issue and concluded that the backend solutions explored so far cause an overhead in terms of network traffic between the captain and the members of SHC. Changes either in frontend or backend are non-trivial and are architectural level, having performance implications, requiring considerable test effort as well. Hence we want to target the fix to the next major release (JackHammer), that will allow us to identify the most appropriate solution for the problem either in fronend and/or in backend.
 
Summary of the issue:
 
In a SHC environment, a dashboard powered by a real time search query stops getting refreshed after getting an initial update. This issue occurs only under the scenario where the RT search gets dispatched on a different SHC member than the one where dashboard is being viewed.
 
The issue boils down to the fact that SHC does not replicate real time search artifacts.  
http://docs.splunk.com/Documentation/Splunk/6.3.0/DistSearch/SHCarchitecture#How_the_cluster_handles_search_artifacts.
 
 
UI in this case makes a call to the endpoint  search/jobs?id=<SID>  which doesn’t get proxied to the member on which the RT search is running. This call gets forwarded to captain and captain returns initial RT job status for the requested SID (as the RT job status are not replicated, the captain will always return the initial job status hence the dashboard never gets the latest job status).
 
During investigation, it was found that a call to the endpoint search/jobs/<SID>  gets proxied to the member where RT search is running and the results get streamed back to the dashboard correctly. But UI doesn’t use this endpoint to update the dashboard. The necessary infrastructure to proxy the REST call search/jobs/<SID> to the member where RT search is running, already exists in backend.  Hence we think a change in UI to use search/jobs/<SID> instead of search/jobs?id=<SID> would address this issue. However from our initial discussion with UI team, UI team thinks this is not a viable approach.

Backend solutions explored:
1) On each request to endpoint search/jobs?id=<SID>  from the UI, modify captain to request job status of the SID.
   *Pros:* Addresses the issue. Dashboard gets refreshed.
   *Cons:* UI calls the endpoint search/jobs?id=<SID>  once every second. The proposed solution of requesting the job status on every call
    to search/jobs?id=<SID>  will generate high network traffic between captain and members. This extra load on captain has performance
    implications and might affect heartbeat and other tasks of the captain.

2) Drive the real time job status update from the dispatch reaper thread. In the current implementation, once every 30sec the dispatch   
    reaper thread calls SHPCallback::slave_updateRunningSids to send saved search job status. Modify this code to send job status for real
    time jobs as well.
    *Pros:* Dashboard gets refreshed at 30sec interval.
    *Cons:* Visible difference in bahvior. When the real time job gets dispatched on the same member as the one on where dashboard is
    being viewed, the refreshes are immediate. But if the search gets dispatched on a different members, dashboard refreshes once every 
    30 sec.

3) Impement a new fuction similar to doReapWork(), but the only purpose of this function is to send real time job status
    (SHPCallback::slave_updateRunningSids()) to the captain at a specified interval (configurable)
    *Pros:* Dashboard refreshes at user specified interval.
    *Cons:* Given thousands of SID directories under dispatch directory, this will have performance implications. Part of work done in this
     new function duplicates with doReapWork() in dispatch thread.
 
Frontend Solutions explored:
1) Modify frontend of use search/jobs/<SID> endpoint instead of search/jobs?id=<SID>. But this has been considered not a viable approach by UI team.


We would like create a story to track this issue, discuss with all the parties involved to come up with an appropriate efficient solution either in UI or in backend (or both) targeted for next major release (JackHammer).",03/Aug/16 7:41 AM;6053abab45a3bb006816ea7c;SPL-125653 has been created to track and address this issue in a future release of Splunk.,"09/Aug/16 6:51 AM;6053abab45a3bb006816ea7c;[~accountid:557058:b1e398c5-b004-4a33-8152-18daa36cda4b] [~accountid:5f7671cd837bb80068281d0b] [~accountid:6053b95a66c8790068395481] [~accountid:557058:a6f000d5-3fc9-45a4-9a73-f164c6d46a6c] [~accountid:5d0d153711233f0c4ca0aa17] [~accountid:557058:236db286-0673-4667-9c0b-a9114f695d13] Until this issue is addressed, we need to document this as a known issue in release notes, with details on how to work around this issue.

The draft version mentioning this is as below, kindly review and provide your comments (I can open a separate jira for this if required).

""There is a known issue with a dashboard using realtime referenced saved search in Search Head Cluster environment. A dashboard powered by a realtime referenced saved search will not get updated if the realtime search gets dispatched (by the captain) to a different Search head member than the member on which dashboard is being viewed.

To work around this problem either use an inline realtime search or use a saved search with refresh.auto.interval option, instead of realtime referenced saved search. This issue is specific to Search Head Cluster environments only.""

","09/Aug/16 7:00 AM;557058:a6f000d5-3fc9-45a4-9a73-f164c6d46a6c;[~accountid:6053abab45a3bb006816ea7c] Nice work !!
{quote}
either use an inline realtime search or use a saved search with refresh.auto.interval option, instead of realtime referenced saved search
{quote}
Could we also mention cost associated with both workarounds as well ?","09/Aug/16 7:29 AM;6053abab45a3bb006816ea7c;[~accountid:557058:a6f000d5-3fc9-45a4-9a73-f164c6d46a6c] Thanks, incorporated your comments.

""There is a known issue with a dashboard using realtime referenced saved search in Search Head Cluster environment. A dashboard powered by a realtime referenced saved search will not get updated if the realtime search gets dispatched (by the captain) to a different Search head member than the member on which dashboard is being viewed.

To work around this problem using one of the below workarounds:
1) Use an inline realtime search 
e.g.:

{code:java}
<dashboard>
  <label>DashboardTest</label>
  <row>
    <panel>
      <table>
        <search>
          <query>index=_internal | stats count </query>
          <earliest>rt-5m</earliest>
          <latest>rtnow</latest>
        </search>
      </table>
    </panel>
  </row>
</dashboard>
{code}

2) Create scheduled saved search. Use loadjob inline query in the dashboard to update the dashboard with the results of the saved search.
e.g.:

{code:java}
<dashboard>
  <label>DashboardTest</label>
  <row>
    <panel>
      <table>
        <search>
          <query> | loadjob savedsearch=""admin:search:SavedSearch""</query>
        </search>
       <option name=""refresh.auto.interval"">60</option> 
      </table>
    </panel>
  </row>
</dashboard>
{code}

Using inline realtime search workaround would be efficient as realtime search will be running only when the dashboard is being viewed. However  if different users access the dashboard either from the same search head member or different member, there will be a new realtime search spawned for each user.

In case of  saved search, only one instance of the saved search will be run at the scheduled time irrespective of whether multiple users or no user accessing the dashboard.""","11/Aug/16 8:44 AM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:611be6e8c2f3a50069e26941], is there a logical place in the documentation to explain this behavior to customers? We don't plan to change/fix the behavior, so it's better if we can add it to the core documentation rather than a list of known issues. ","11/Aug/16 11:03 AM;611be6e8c2f3a50069e26941;Hi [~accountid:557058:37b4518c-e757-440f-87b7-181f5f425e80]: My suggestion would be this topic, which discusses searches in dashboards. We could add an H2 or H3 to mention the limitation and workaround for SHC.
http://docs.splunk.com/Documentation/Splunk/6.5.0/Viz/Savedsearches
",16/Aug/16 8:56 AM;6053abab45a3bb006816ea7c;reopening to correct the resolution code to Workaround.,"17/Aug/16 1:25 PM;557058:37b4518c-e757-440f-87b7-181f5f425e80;[~accountid:611be6e8c2f3a50069e26941], that sounds good to me. Let's put it on your to-do list. Thanks.","17/Aug/16 1:39 PM;611be6e8c2f3a50069e26941;Ok, added this task to track the docs update:
https://splunk.atlassian.net/browse/SPL-126876",14/Sep/16 7:02 PM;5f191b23e1618b001b1fae8c;Do we have file documentation or known issue to public? Thanks. ,"12/Oct/16 5:04 PM;611be6e8c2f3a50069e26941;[~accountid:5f191b23e1618b001b1fae8c]: I've added docs here. 
http://docs.splunk.com/Documentation/Splunk/6.5.0/Viz/Savedsearches#Troubleshoot_referenced_real-time_searches_in_search_head_clusters

Could you and/or [~accountid:557058:a6f000d5-3fc9-45a4-9a73-f164c6d46a6c] make sure this looks correct? I've followed the JIRA description and workaround notes.

Thanks!","13/Oct/16 1:25 PM;611be6e8c2f3a50069e26941;Added [~accountid:5f191b23e1618b001b1fae8c]'s recommendations, am confirming some details about the loadjob + refresh option for scheduled saved searches with [~accountid:557058:ffe53cbd-86cd-4cff-8ba7-369fbc58d342]. Might need to update this advice depending on this. ",13/Oct/16 5:34 PM;5f191b23e1618b001b1fae8c;Thanks for the update. ,"27/Oct/16 1:46 PM;611be6e8c2f3a50069e26941;Got [~accountid:557058:ffe53cbd-86cd-4cff-8ba7-369fbc58d342]'s advice on using a refresh interval with the loadjob workaround and it doesn't seem particularly beneficial. It could also be resource intensive, so I've left it out of this workaround. I think that takes care of documentation for this. Thanks, all!

http://docs.splunk.com/Documentation/Splunk/6.5.0/Viz/Savedsearches#Troubleshoot_referenced_real-time_searches_in_search_head_clusters","30/Nov/16 2:31 PM;6053abab45a3bb006816ea7c;[~accountid:611be6e8c2f3a50069e26941] I don't see a reference to this jira in the docs, though workaround is mentioned. I was wondering how customers can track this ?","30/Nov/16 2:34 PM;611be6e8c2f3a50069e26941;This JIRA is flagged as a new known issue. It should show up in the release notes, though the workaround should probably just be a link to the docs information I've created (linked here).",30/Nov/16 2:40 PM;6053abab45a3bb006816ea7c;I cannot find it in 6.5.0 /6.5.1 release notes.,"30/Nov/16 2:45 PM;611be6e8c2f3a50069e26941;That's because the ""affects versions"" in the JIRA don't include 6.5.0 and 6.5.1. They should, though, right? If so, I can add them.","30/Nov/16 2:47 PM;6053abab45a3bb006816ea7c;Thanks [~accountid:611be6e8c2f3a50069e26941], I added them.","30/Nov/16 2:48 PM;611be6e8c2f3a50069e26941;Looks like, per https://splunk.atlassian.net/browse/SPL-125653, this should be noted as affecting versions through JackHammer",30/Nov/16 2:50 PM;6053abab45a3bb006816ea7c;That's correct.,30/Nov/16 2:52 PM;611be6e8c2f3a50069e26941;Great--thanks. That should fix it. ,30/Nov/16 3:08 PM;611be6e8c2f3a50069e26941;Trying something in the workaround field to adjust the link format appearing in known issues. ,26/Jan/21 11:48 AM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Closing as part of 6+ months old resolved bugs bulk clean up by QA Metrics Team. CC [~accountid:5ebddcc35862510b798f5abd] [~accountid:5d3547e5c2db730c59b25bdc] [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408].,10/Mar/21 7:18 PM;557058:9b250150-4703-4e0c-a02b-ad536bb0d408;Rolling back Assignee & Developer from Jan 25th bulk closure operation. CC [~accountid:557058:9b250150-4703-4e0c-a02b-ad536bb0d408]. Assignee: mkarikatti. Developer: mkarikatti,,,,,,,,,,,,,,,,,,Done,16/Aug/16 8:57 AM
