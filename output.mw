= Fixed issues =
This page lists Jira tickets that were marked as fixed and are ready to be published in the release notes.

== 9.1.10 ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-269946 SPL-269946]
| Cascading bundle replication stuck: indexer peer going down during replication can block future replications if it comes back up
| Bundle Replication Enhancement, Search Infra - Bundle Management
| 9.1.10, 9.2.7, 9.3.5, 9.4.3
| P2-High
| Bug
| Closed
| Fixed
| Backport_Approved, emea_support:approved, sinfra:cos:infra_issue, sinfra:sx:bundle_replication_failure, SP:Analytics, support_reviewed, udhanabalan_review
| *_Summary:_* <br/>Intermittently Cascading replication is being stuck due to one indexer<br/>*_Customer Impact:_* <br/>_Knowledge bundle replication is halted and lookups are not being updated on the IDX layer_<br/>*_Description:_* <br/>Customer with a huge IDX enviornemt ~300 indexers, is periodically seeing that the knowledge bundle replication seems to stop. in th elogs on the Captain they see that it is due to the fact that the previous replication is still in effect:<br/>{{01-30-2025 09:09:40.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.}}<br/>This will continue endlessly until Captain is restarted.<br/>This seems to be a continuation of [https://splunk.atlassian.net/browse/SPL-239645|https://splunk.atlassian.net/browse/SPL-239645|smart-link]  but on the version 9.2.4<br/>*_Problem Analysis_*:<br/>Replication seems to be stuck in the endless loop of restarting replication but stopping it, because one of the indexers is still ongoing replication:<br/>{noformat}01-30-2025 09:09:40.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Trigger replication with reason="async replication allowed"{noformat}<br/>{noformat}01-30-2025 09:09:40.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Replication still in progress for peer with uri=https://10.186.25.75:8089 and status=1.{noformat}<br/>{noformat}01-30-2025 09:09:40.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.{noformat}<br/>{noformat}01-30-2025 09:09:40.380 +0000 DEBUG BundleReplicatorThread [1458992 BundleReplicatorThread] - Waiting to try again after cooldown{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Trigger replication with reason="async replication allowed"{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Replication still in progress for peer with uri=https://10.186.25.75:8089 and status=1.{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 DEBUG BundleReplicatorThread [1458992 BundleReplicatorThread] - Waiting to try again after cooldown{noformat}<br/>When looking at the diag of the Indexer in question, no logs are seen regarding the attempt.<br/>Creation of new knowledge bundles is also paused:<br/>[https://drive.google.com/file/d/1cRY-Yr0snOWMXskrlB3-RkbOi9GXVpjA/view?usp=sharing|https://drive.google.com/file/d/1cRY-Yr0snOWMXskrlB3-RkbOi9GXVpjA/view?usp=sharing|smart-link]<br/> <br/>Because of the version "cascade_plan_replication_retry_fast" in distsearch.conf is already set to true  by default. But it seems like replication is having an issue because it does not “Fail“ but stays “in progress“, however, when checking bundle replication status, all nodes are marked as successful(with affected one not being on the list due to cascading):<br/>[https://drive.google.com/file/d/1bfuuR_Idj6JTM3lLgWw9efcK6J9D3b5i/view?usp=sharing|https://drive.google.com/file/d/1bfuuR_Idj6JTM3lLgWw9efcK6J9D3b5i/view?usp=sharing|smart-link] <br/>The issue is seen very intermittently. Customer saw a big drop in occurrences after the 9.2.4 upgrade and even stopped observing issue during ney year freeze, but it started re-occurring since.<br/>Fresh diags are available on google drive:<br/>SHC:<br/>[https://drive.google.com/file/d/1Bd19lA-woq-MzkM0SXTQc-7mQDAOGNr0/view?usp=sharing|https://drive.google.com/file/d/1Bd19lA-woq-MzkM0SXTQc-7mQDAOGNr0/view?usp=sharing|smart-link] <br/>IDX:<br/>[https://drive.google.com/file/d/1-MjpbYnWbsBiWGkt9YwywRxT3hHQktd5/view?usp=sharing|https://drive.google.com/file/d/1-MjpbYnWbsBiWGkt9YwywRxT3hHQktd5/view?usp=sharing|smart-link] <br/>And also on SplunkBOT:<br/>SHC:<br/>[https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=fd9e1cea-4ebd-4bf4-b208-1f45e34b4c62&case_numbertok=3637549&hosttok=gz-zu2appplv201&case_filetok=gz-zu2appplv201-sh_-20250130-121456-C96MUFV4&found_anontok=0&job_typetok=classic|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=fd9e1cea-4ebd-4bf4-b208-1f45e34b4c62&case_numbertok=3637549&hosttok=gz-zu2appplv201&case_filetok=gz-zu2appplv201-sh_-20250130-121456-C96MUFV4&found_anontok=0&job_typetok=classic]<br/>IDX:<br/>[https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=cc144e51-6e78-4d83-85ce-c4f169af50cb&case_numbertok=3637549&hosttok=gx-zu2pssplv093&case_filetok=gx-zu2pssplv093-idx_-20250130-121243-vS9FwYiT&found_anontok=0&job_typetok=classic|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=cc144e51-6e78-4d83-85ce-c4f169af50cb&case_numbertok=3637549&hosttok=gx-zu2pssplv093&case_filetok=gx-zu2pssplv093-idx_-20250130-121243-vS9FwYiT&found_anontok=0&job_typetok=classic]<br/>*_Workaround_*<br/>_Restart SHC Captain or trigger Captain re-election_
|}

== 9.2.3 ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-228117 SPL-228117]
| "file" is incorrectly listed as a supported scheme for ingest actions in outputs.conf.spec
| Ingest Actions
| 9.2.3
| P2-High
| Bug
| Resolved
| Fixed
| Backport_Approved
| {{    * Currently "s3" and "file" are the only supported schemes.}} <br/>is currently in [outputs.conf.spec.in|http://outputs.conf.spec.in/] , see [here|https://cd.splunkdev.com/splcore/main/-/merge_requests/55032/diffs#c6357a323e2b79127bb9bb12fc22750613cd1e74]<br/>We should remove any mention of official support for “file” or “NFS” until we launch file system / NFS support in 9.0.X
|}

== 9.2.7 ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-269946 SPL-269946]
| Cascading bundle replication stuck: indexer peer going down during replication can block future replications if it comes back up
| Bundle Replication Enhancement, Search Infra - Bundle Management
| 9.1.10, 9.2.7, 9.3.5, 9.4.3
| P2-High
| Bug
| Closed
| Fixed
| Backport_Approved, emea_support:approved, sinfra:cos:infra_issue, sinfra:sx:bundle_replication_failure, SP:Analytics, support_reviewed, udhanabalan_review
| *_Summary:_* <br/>Intermittently Cascading replication is being stuck due to one indexer<br/>*_Customer Impact:_* <br/>_Knowledge bundle replication is halted and lookups are not being updated on the IDX layer_<br/>*_Description:_* <br/>Customer with a huge IDX enviornemt ~300 indexers, is periodically seeing that the knowledge bundle replication seems to stop. in th elogs on the Captain they see that it is due to the fact that the previous replication is still in effect:<br/>{{01-30-2025 09:09:40.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.}}<br/>This will continue endlessly until Captain is restarted.<br/>This seems to be a continuation of [https://splunk.atlassian.net/browse/SPL-239645|https://splunk.atlassian.net/browse/SPL-239645|smart-link]  but on the version 9.2.4<br/>*_Problem Analysis_*:<br/>Replication seems to be stuck in the endless loop of restarting replication but stopping it, because one of the indexers is still ongoing replication:<br/>{noformat}01-30-2025 09:09:40.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Trigger replication with reason="async replication allowed"{noformat}<br/>{noformat}01-30-2025 09:09:40.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Replication still in progress for peer with uri=https://10.186.25.75:8089 and status=1.{noformat}<br/>{noformat}01-30-2025 09:09:40.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.{noformat}<br/>{noformat}01-30-2025 09:09:40.380 +0000 DEBUG BundleReplicatorThread [1458992 BundleReplicatorThread] - Waiting to try again after cooldown{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Trigger replication with reason="async replication allowed"{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Replication still in progress for peer with uri=https://10.186.25.75:8089 and status=1.{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 DEBUG BundleReplicatorThread [1458992 BundleReplicatorThread] - Waiting to try again after cooldown{noformat}<br/>When looking at the diag of the Indexer in question, no logs are seen regarding the attempt.<br/>Creation of new knowledge bundles is also paused:<br/>[https://drive.google.com/file/d/1cRY-Yr0snOWMXskrlB3-RkbOi9GXVpjA/view?usp=sharing|https://drive.google.com/file/d/1cRY-Yr0snOWMXskrlB3-RkbOi9GXVpjA/view?usp=sharing|smart-link]<br/> <br/>Because of the version "cascade_plan_replication_retry_fast" in distsearch.conf is already set to true  by default. But it seems like replication is having an issue because it does not “Fail“ but stays “in progress“, however, when checking bundle replication status, all nodes are marked as successful(with affected one not being on the list due to cascading):<br/>[https://drive.google.com/file/d/1bfuuR_Idj6JTM3lLgWw9efcK6J9D3b5i/view?usp=sharing|https://drive.google.com/file/d/1bfuuR_Idj6JTM3lLgWw9efcK6J9D3b5i/view?usp=sharing|smart-link] <br/>The issue is seen very intermittently. Customer saw a big drop in occurrences after the 9.2.4 upgrade and even stopped observing issue during ney year freeze, but it started re-occurring since.<br/>Fresh diags are available on google drive:<br/>SHC:<br/>[https://drive.google.com/file/d/1Bd19lA-woq-MzkM0SXTQc-7mQDAOGNr0/view?usp=sharing|https://drive.google.com/file/d/1Bd19lA-woq-MzkM0SXTQc-7mQDAOGNr0/view?usp=sharing|smart-link] <br/>IDX:<br/>[https://drive.google.com/file/d/1-MjpbYnWbsBiWGkt9YwywRxT3hHQktd5/view?usp=sharing|https://drive.google.com/file/d/1-MjpbYnWbsBiWGkt9YwywRxT3hHQktd5/view?usp=sharing|smart-link] <br/>And also on SplunkBOT:<br/>SHC:<br/>[https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=fd9e1cea-4ebd-4bf4-b208-1f45e34b4c62&case_numbertok=3637549&hosttok=gz-zu2appplv201&case_filetok=gz-zu2appplv201-sh_-20250130-121456-C96MUFV4&found_anontok=0&job_typetok=classic|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=fd9e1cea-4ebd-4bf4-b208-1f45e34b4c62&case_numbertok=3637549&hosttok=gz-zu2appplv201&case_filetok=gz-zu2appplv201-sh_-20250130-121456-C96MUFV4&found_anontok=0&job_typetok=classic]<br/>IDX:<br/>[https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=cc144e51-6e78-4d83-85ce-c4f169af50cb&case_numbertok=3637549&hosttok=gx-zu2pssplv093&case_filetok=gx-zu2pssplv093-idx_-20250130-121243-vS9FwYiT&found_anontok=0&job_typetok=classic|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=cc144e51-6e78-4d83-85ce-c4f169af50cb&case_numbertok=3637549&hosttok=gx-zu2pssplv093&case_filetok=gx-zu2pssplv093-idx_-20250130-121243-vS9FwYiT&found_anontok=0&job_typetok=classic]<br/>*_Workaround_*<br/>_Restart SHC Captain or trigger Captain re-election_
|}

== 9.2.8 ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-280184 SPL-280184]
| [CLONE] [sustain/cobalt] MachineTypeFilter doesn't work when DS works in clustered mode - applications are getting wrongfully updated into clients when performing OS filtering in serverclass
| Deployment Server
| 9.2.8
| P2-High
| Bug
| Resolved
| Fixed
| Backport_Approved, dwest_reviewed, emea_support:approved, SP:Not-DP, support_reviewed
| This ticket is a backport of [https://splunk.atlassian.net/browse/SPL-270345|https://splunk.atlassian.net/browse/SPL-270345|smart-link]
|-
| [https://splunk.atlassian.net/browse/SPL-270345 SPL-270345]
| MachineTypeFilter doesn't work when DS works in clustered mode - applications are getting wrongfully updated into clients when performing OS filtering in serverclass
| Deployment Server
| 10.0.x, 9.2.8, 9.3.6, 9.4.4
| P2-High
| Bug
| Closed
| Fixed
| Backport_Approved, dwest_reviewed, emea_support:approved, SP:Not-DP, support_reviewed
| *_Summary:_* _The customer is deploying apps from the Deployment server to deployment clients using the MachineTypeFiltering in serverclass but the apps are deploying to both Windows and Linux servers._<br/>*_Customer Impact:_* _Due to this the apps are getting wrongfully installed on the servers where they would not persist._<br/>*_Description:_* _Describe what the customer was trying to achieve when they first noticed the problem._<br/>* _How long has the customer been tracking this issue and what is the frequency? Has the customer made any changes to their environment just before the issue started?_<br/>After upgrading the Splunk to 9.4.0 version they were facing this issue<br/>* _What errors/warnings/behaviors are being reported? Be specific and provide evidence._<br/>N/A<br/>* _What is the expectation when this problem is resolved?_<br/>The OS filtration must be done correctly and deployed to the correct OS<br/>* _If the customer and/or Support engineer tried any mitigation, provide ALL of them and their outcomes._<br/>We tried checking over the call by creating a “raitec_app1_case_test“ serverclass and observed the same behavior <br/>*_Problem Analysis_*<br/>* _In your own words, describe support's analysis of the problem._<br/>When applying MachineTypeFilter  in server class the app will be deployed to the clients based on the OS level. Suppose we mention the server class to deploy the apps in linux but the apps are deployed to windows machine.<br/> For Example [serverClass:linux_server:app:grz_linux_server_base-infos] <br/>machineTypesFilter = linux-* ..>>>= is frequently deployed to multiple Windows hosts.<br/> [serverClass:win-server_domino_prod_common:app:grz_win-server_domino_prod_common] <br/>is also found on several hosts where it is not supposed to be. <br/>At the same time, the following applications are absent but should be deployed: [serverClass:windows_server:app:grz_windows_server_base-infos] [serverClass:windows_server:app:grz_server_events] [serverClass:windows_server:app:Splunk_TA_microsoft-iis] <br/>The following hosts have confirmed issues: <br/>AZURECONN9911 - W2016 <br/>AZUREDSC7901 - W2022 <br/>AZUREDSC9901 - W2022 <br/>AZURECONN9921 - W2016 <br/>AZURECONN1101 - W2016 <br/>AZURECONN9901 - W2016 <br/>AZURETASK9901 - W2022.<br/>* _Provide logs and accompanying data with reference to the above confluence page and all the relevant documentation based on your analysis to support your observation and theories._<br/>This was a known issue and it was present on 9.2.x, 9.3.0 , but as per previous JIRA it was fixed on 9.2.2 and 9.3.1. But my customer is on 9.4.0.<br/>[https://docs.splunk.com/Documentation/Splunk/9.2.2/ReleaseNotes/Fixedissues#Distributed_deployment.2C_forwarder.2C_deployment_server_issues|https://docs.splunk.com/Documentation/Splunk/9.2.2/ReleaseNotes/Fixedissues#Distributed_deployment.2C_forwarder.2C_deployment_server_issues]<br/>* _Provide links to diags either in Splunkbot OR attached via Google Drive._<br/>The Diag files where the MachineType Filter Configured in serverclass:<br/>2 Deployment server:<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-51case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-01-08_12-50-12.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-51case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-01-08_12-50-12.tar.gz]<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-81case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-01-08_12-50-00.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-81case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-01-08_12-50-00.tar.gz]<br/>Windows server diag file:<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-93case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC9901-2025-01-08.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-93case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC9901-2025-01-08.tar.gz]<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-33case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC7901-2025-01-08.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-33case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC7901-2025-01-08.tar.gz]<br/>The diag files with *no “MachineTypeFilter”* configured in serverclass:<br/>2 Deployment servers diag files:<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-90case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-02-04_15-31-49.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-90case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-02-04_15-31-49.tar.gz]<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-83case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-02-04_15-36-22.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-83case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-02-04_15-36-22.tar.gz]<br/>Linux diag file:<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-18case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-tcapp2152-2025-02-04.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-18case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-tcapp2152-2025-02-04.tar.gz]<br/>Windows diag file:<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-15case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-LNO9905-2025-02-04.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-15case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-LNO9905-2025-02-04.tar.gz]<br/>* _If you include screenshots, please label them accordingly and link them as thumbnails here or in the comments section with explanations._<br/>We got on a meeting and asked the customer to reproduce the issue, and then we created a test server class and the apps deployed in to both linux and windows machine.<br/>!image-20250205-150919.png|width=338,height=116,alt="image-20250205-150919.png"!<br/>Windows machines where apps are updated for 60 minutes window<br/>!image-20250205-150927.png|width=1501,height=226,alt="image-20250205-150927.png"!<br/>* _What is the ask from Engineering from a code-fix or product engineering standpoint?_<br/>Code fix <br/>*_Workaround_*<br/>We have requested the customer to not use MachineTypeFilter in server class and now they are not observing any issue.
|-
| [https://splunk.atlassian.net/browse/SPL-279657 SPL-279657]
| Distributed search may lose information about bundles from cluster peers on new search generations, removes peers when best-effort mode is enabled.
| Distributed Search
| 10.0.x, 9.2.8, 9.3.6, 9.4.4, develop, Nutella(9.2.2406.107), Oreo, Pocky
| P1-Immediate
| Bug
| Closed
| Fixed
| Backport_Approved
| When we add cluster peers after a search generation change, we try to prime their information (including available bundles) before exposing the information to search. [https://splunk.atlassian.net/browse/SPL-216146|https://splunk.atlassian.net/browse/SPL-216146|smart-link] introduced a regression through which, by mistake, that information is being exposed prematurely, with no bundles available. This will cause a short pause in search requests when best-effort mode is disabled, as the search head blocks search while it refreshes its bundle information, but with best-effort enabled we’ll see incomplete results and warnings straight away.<br/>In the codebase, what happens is that {{DistributedPeerManager::prepClusterPeersForPotentialAdd()}}relies on its local {{HeartbeatTransactionSynchronizer}} collecting peer information before it publishes it all, but [commit d6cf0fbdf2bd changed the auth transaction such that it will always update|https://cd.splunkdev.com/splcore/main/-/commit/d6cf0fbdf2bd#d700c0d60e4cdc6ba2d58fdd1287c87e00c09884_279_287] the {{DistributedPeerManager}} directly.<br/>Found through code inspection while investigating [https://splunk.atlassian.net/browse/SPL-268481|https://splunk.atlassian.net/browse/SPL-268481|smart-link].
|}

== 9.3.5 ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-269946 SPL-269946]
| Cascading bundle replication stuck: indexer peer going down during replication can block future replications if it comes back up
| Bundle Replication Enhancement, Search Infra - Bundle Management
| 9.1.10, 9.2.7, 9.3.5, 9.4.3
| P2-High
| Bug
| Closed
| Fixed
| Backport_Approved, emea_support:approved, sinfra:cos:infra_issue, sinfra:sx:bundle_replication_failure, SP:Analytics, support_reviewed, udhanabalan_review
| *_Summary:_* <br/>Intermittently Cascading replication is being stuck due to one indexer<br/>*_Customer Impact:_* <br/>_Knowledge bundle replication is halted and lookups are not being updated on the IDX layer_<br/>*_Description:_* <br/>Customer with a huge IDX enviornemt ~300 indexers, is periodically seeing that the knowledge bundle replication seems to stop. in th elogs on the Captain they see that it is due to the fact that the previous replication is still in effect:<br/>{{01-30-2025 09:09:40.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.}}<br/>This will continue endlessly until Captain is restarted.<br/>This seems to be a continuation of [https://splunk.atlassian.net/browse/SPL-239645|https://splunk.atlassian.net/browse/SPL-239645|smart-link]  but on the version 9.2.4<br/>*_Problem Analysis_*:<br/>Replication seems to be stuck in the endless loop of restarting replication but stopping it, because one of the indexers is still ongoing replication:<br/>{noformat}01-30-2025 09:09:40.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Trigger replication with reason="async replication allowed"{noformat}<br/>{noformat}01-30-2025 09:09:40.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Replication still in progress for peer with uri=https://10.186.25.75:8089 and status=1.{noformat}<br/>{noformat}01-30-2025 09:09:40.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.{noformat}<br/>{noformat}01-30-2025 09:09:40.380 +0000 DEBUG BundleReplicatorThread [1458992 BundleReplicatorThread] - Waiting to try again after cooldown{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Trigger replication with reason="async replication allowed"{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Replication still in progress for peer with uri=https://10.186.25.75:8089 and status=1.{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 DEBUG BundleReplicatorThread [1458992 BundleReplicatorThread] - Waiting to try again after cooldown{noformat}<br/>When looking at the diag of the Indexer in question, no logs are seen regarding the attempt.<br/>Creation of new knowledge bundles is also paused:<br/>[https://drive.google.com/file/d/1cRY-Yr0snOWMXskrlB3-RkbOi9GXVpjA/view?usp=sharing|https://drive.google.com/file/d/1cRY-Yr0snOWMXskrlB3-RkbOi9GXVpjA/view?usp=sharing|smart-link]<br/> <br/>Because of the version "cascade_plan_replication_retry_fast" in distsearch.conf is already set to true  by default. But it seems like replication is having an issue because it does not “Fail“ but stays “in progress“, however, when checking bundle replication status, all nodes are marked as successful(with affected one not being on the list due to cascading):<br/>[https://drive.google.com/file/d/1bfuuR_Idj6JTM3lLgWw9efcK6J9D3b5i/view?usp=sharing|https://drive.google.com/file/d/1bfuuR_Idj6JTM3lLgWw9efcK6J9D3b5i/view?usp=sharing|smart-link] <br/>The issue is seen very intermittently. Customer saw a big drop in occurrences after the 9.2.4 upgrade and even stopped observing issue during ney year freeze, but it started re-occurring since.<br/>Fresh diags are available on google drive:<br/>SHC:<br/>[https://drive.google.com/file/d/1Bd19lA-woq-MzkM0SXTQc-7mQDAOGNr0/view?usp=sharing|https://drive.google.com/file/d/1Bd19lA-woq-MzkM0SXTQc-7mQDAOGNr0/view?usp=sharing|smart-link] <br/>IDX:<br/>[https://drive.google.com/file/d/1-MjpbYnWbsBiWGkt9YwywRxT3hHQktd5/view?usp=sharing|https://drive.google.com/file/d/1-MjpbYnWbsBiWGkt9YwywRxT3hHQktd5/view?usp=sharing|smart-link] <br/>And also on SplunkBOT:<br/>SHC:<br/>[https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=fd9e1cea-4ebd-4bf4-b208-1f45e34b4c62&case_numbertok=3637549&hosttok=gz-zu2appplv201&case_filetok=gz-zu2appplv201-sh_-20250130-121456-C96MUFV4&found_anontok=0&job_typetok=classic|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=fd9e1cea-4ebd-4bf4-b208-1f45e34b4c62&case_numbertok=3637549&hosttok=gz-zu2appplv201&case_filetok=gz-zu2appplv201-sh_-20250130-121456-C96MUFV4&found_anontok=0&job_typetok=classic]<br/>IDX:<br/>[https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=cc144e51-6e78-4d83-85ce-c4f169af50cb&case_numbertok=3637549&hosttok=gx-zu2pssplv093&case_filetok=gx-zu2pssplv093-idx_-20250130-121243-vS9FwYiT&found_anontok=0&job_typetok=classic|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=cc144e51-6e78-4d83-85ce-c4f169af50cb&case_numbertok=3637549&hosttok=gx-zu2pssplv093&case_filetok=gx-zu2pssplv093-idx_-20250130-121243-vS9FwYiT&found_anontok=0&job_typetok=classic]<br/>*_Workaround_*<br/>_Restart SHC Captain or trigger Captain re-election_
|}

== 9.3.6 ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-280185 SPL-280185]
| [CLONE] [sustain/duranium] MachineTypeFilter doesn't work when DS works in clustered mode - applications are getting wrongfully updated into clients when performing OS filtering in serverclass
| Deployment Server
| 9.3.6
| P2-High
| Bug
| Resolved
| Fixed
| Backport_Approved, dwest_reviewed, emea_support:approved, SP:Not-DP, support_reviewed
| This ticket is a backport of [https://splunk.atlassian.net/browse/SPL-270345|https://splunk.atlassian.net/browse/SPL-270345|smart-link]
|-
| [https://splunk.atlassian.net/browse/SPL-270345 SPL-270345]
| MachineTypeFilter doesn't work when DS works in clustered mode - applications are getting wrongfully updated into clients when performing OS filtering in serverclass
| Deployment Server
| 10.0.x, 9.2.8, 9.3.6, 9.4.4
| P2-High
| Bug
| Closed
| Fixed
| Backport_Approved, dwest_reviewed, emea_support:approved, SP:Not-DP, support_reviewed
| *_Summary:_* _The customer is deploying apps from the Deployment server to deployment clients using the MachineTypeFiltering in serverclass but the apps are deploying to both Windows and Linux servers._<br/>*_Customer Impact:_* _Due to this the apps are getting wrongfully installed on the servers where they would not persist._<br/>*_Description:_* _Describe what the customer was trying to achieve when they first noticed the problem._<br/>* _How long has the customer been tracking this issue and what is the frequency? Has the customer made any changes to their environment just before the issue started?_<br/>After upgrading the Splunk to 9.4.0 version they were facing this issue<br/>* _What errors/warnings/behaviors are being reported? Be specific and provide evidence._<br/>N/A<br/>* _What is the expectation when this problem is resolved?_<br/>The OS filtration must be done correctly and deployed to the correct OS<br/>* _If the customer and/or Support engineer tried any mitigation, provide ALL of them and their outcomes._<br/>We tried checking over the call by creating a “raitec_app1_case_test“ serverclass and observed the same behavior <br/>*_Problem Analysis_*<br/>* _In your own words, describe support's analysis of the problem._<br/>When applying MachineTypeFilter  in server class the app will be deployed to the clients based on the OS level. Suppose we mention the server class to deploy the apps in linux but the apps are deployed to windows machine.<br/> For Example [serverClass:linux_server:app:grz_linux_server_base-infos] <br/>machineTypesFilter = linux-* ..>>>= is frequently deployed to multiple Windows hosts.<br/> [serverClass:win-server_domino_prod_common:app:grz_win-server_domino_prod_common] <br/>is also found on several hosts where it is not supposed to be. <br/>At the same time, the following applications are absent but should be deployed: [serverClass:windows_server:app:grz_windows_server_base-infos] [serverClass:windows_server:app:grz_server_events] [serverClass:windows_server:app:Splunk_TA_microsoft-iis] <br/>The following hosts have confirmed issues: <br/>AZURECONN9911 - W2016 <br/>AZUREDSC7901 - W2022 <br/>AZUREDSC9901 - W2022 <br/>AZURECONN9921 - W2016 <br/>AZURECONN1101 - W2016 <br/>AZURECONN9901 - W2016 <br/>AZURETASK9901 - W2022.<br/>* _Provide logs and accompanying data with reference to the above confluence page and all the relevant documentation based on your analysis to support your observation and theories._<br/>This was a known issue and it was present on 9.2.x, 9.3.0 , but as per previous JIRA it was fixed on 9.2.2 and 9.3.1. But my customer is on 9.4.0.<br/>[https://docs.splunk.com/Documentation/Splunk/9.2.2/ReleaseNotes/Fixedissues#Distributed_deployment.2C_forwarder.2C_deployment_server_issues|https://docs.splunk.com/Documentation/Splunk/9.2.2/ReleaseNotes/Fixedissues#Distributed_deployment.2C_forwarder.2C_deployment_server_issues]<br/>* _Provide links to diags either in Splunkbot OR attached via Google Drive._<br/>The Diag files where the MachineType Filter Configured in serverclass:<br/>2 Deployment server:<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-51case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-01-08_12-50-12.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-51case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-01-08_12-50-12.tar.gz]<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-81case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-01-08_12-50-00.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-81case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-01-08_12-50-00.tar.gz]<br/>Windows server diag file:<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-93case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC9901-2025-01-08.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-93case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC9901-2025-01-08.tar.gz]<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-33case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC7901-2025-01-08.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-33case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC7901-2025-01-08.tar.gz]<br/>The diag files with *no “MachineTypeFilter”* configured in serverclass:<br/>2 Deployment servers diag files:<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-90case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-02-04_15-31-49.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-90case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-02-04_15-31-49.tar.gz]<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-83case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-02-04_15-36-22.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-83case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-02-04_15-36-22.tar.gz]<br/>Linux diag file:<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-18case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-tcapp2152-2025-02-04.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-18case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-tcapp2152-2025-02-04.tar.gz]<br/>Windows diag file:<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-15case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-LNO9905-2025-02-04.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-15case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-LNO9905-2025-02-04.tar.gz]<br/>* _If you include screenshots, please label them accordingly and link them as thumbnails here or in the comments section with explanations._<br/>We got on a meeting and asked the customer to reproduce the issue, and then we created a test server class and the apps deployed in to both linux and windows machine.<br/>!image-20250205-150919.png|width=338,height=116,alt="image-20250205-150919.png"!<br/>Windows machines where apps are updated for 60 minutes window<br/>!image-20250205-150927.png|width=1501,height=226,alt="image-20250205-150927.png"!<br/>* _What is the ask from Engineering from a code-fix or product engineering standpoint?_<br/>Code fix <br/>*_Workaround_*<br/>We have requested the customer to not use MachineTypeFilter in server class and now they are not observing any issue.
|-
| [https://splunk.atlassian.net/browse/SPL-279657 SPL-279657]
| Distributed search may lose information about bundles from cluster peers on new search generations, removes peers when best-effort mode is enabled.
| Distributed Search
| 10.0.x, 9.2.8, 9.3.6, 9.4.4, develop, Nutella(9.2.2406.107), Oreo, Pocky
| P1-Immediate
| Bug
| Closed
| Fixed
| Backport_Approved
| When we add cluster peers after a search generation change, we try to prime their information (including available bundles) before exposing the information to search. [https://splunk.atlassian.net/browse/SPL-216146|https://splunk.atlassian.net/browse/SPL-216146|smart-link] introduced a regression through which, by mistake, that information is being exposed prematurely, with no bundles available. This will cause a short pause in search requests when best-effort mode is disabled, as the search head blocks search while it refreshes its bundle information, but with best-effort enabled we’ll see incomplete results and warnings straight away.<br/>In the codebase, what happens is that {{DistributedPeerManager::prepClusterPeersForPotentialAdd()}}relies on its local {{HeartbeatTransactionSynchronizer}} collecting peer information before it publishes it all, but [commit d6cf0fbdf2bd changed the auth transaction such that it will always update|https://cd.splunkdev.com/splcore/main/-/commit/d6cf0fbdf2bd#d700c0d60e4cdc6ba2d58fdd1287c87e00c09884_279_287] the {{DistributedPeerManager}} directly.<br/>Found through code inspection while investigating [https://splunk.atlassian.net/browse/SPL-268481|https://splunk.atlassian.net/browse/SPL-268481|smart-link].
|}

== 9.4.3 ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-269946 SPL-269946]
| Cascading bundle replication stuck: indexer peer going down during replication can block future replications if it comes back up
| Bundle Replication Enhancement, Search Infra - Bundle Management
| 9.1.10, 9.2.7, 9.3.5, 9.4.3
| P2-High
| Bug
| Closed
| Fixed
| Backport_Approved, emea_support:approved, sinfra:cos:infra_issue, sinfra:sx:bundle_replication_failure, SP:Analytics, support_reviewed, udhanabalan_review
| *_Summary:_* <br/>Intermittently Cascading replication is being stuck due to one indexer<br/>*_Customer Impact:_* <br/>_Knowledge bundle replication is halted and lookups are not being updated on the IDX layer_<br/>*_Description:_* <br/>Customer with a huge IDX enviornemt ~300 indexers, is periodically seeing that the knowledge bundle replication seems to stop. in th elogs on the Captain they see that it is due to the fact that the previous replication is still in effect:<br/>{{01-30-2025 09:09:40.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.}}<br/>This will continue endlessly until Captain is restarted.<br/>This seems to be a continuation of [https://splunk.atlassian.net/browse/SPL-239645|https://splunk.atlassian.net/browse/SPL-239645|smart-link]  but on the version 9.2.4<br/>*_Problem Analysis_*:<br/>Replication seems to be stuck in the endless loop of restarting replication but stopping it, because one of the indexers is still ongoing replication:<br/>{noformat}01-30-2025 09:09:40.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Trigger replication with reason="async replication allowed"{noformat}<br/>{noformat}01-30-2025 09:09:40.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Replication still in progress for peer with uri=https://10.186.25.75:8089 and status=1.{noformat}<br/>{noformat}01-30-2025 09:09:40.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.{noformat}<br/>{noformat}01-30-2025 09:09:40.380 +0000 DEBUG BundleReplicatorThread [1458992 BundleReplicatorThread] - Waiting to try again after cooldown{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Trigger replication with reason="async replication allowed"{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 DEBUG DistributedPeerManager [1458992 BundleReplicatorThread] - Replication still in progress for peer with uri=https://10.186.25.75:8089 and status=1.{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 INFO DistributedPeerManager [1458992 BundleReplicatorThread] - Bundle replication blocked, since older replication is in progress or bundles are not fully indexed.{noformat}<br/>{noformat}01-30-2025 09:09:41.380 +0000 DEBUG BundleReplicatorThread [1458992 BundleReplicatorThread] - Waiting to try again after cooldown{noformat}<br/>When looking at the diag of the Indexer in question, no logs are seen regarding the attempt.<br/>Creation of new knowledge bundles is also paused:<br/>[https://drive.google.com/file/d/1cRY-Yr0snOWMXskrlB3-RkbOi9GXVpjA/view?usp=sharing|https://drive.google.com/file/d/1cRY-Yr0snOWMXskrlB3-RkbOi9GXVpjA/view?usp=sharing|smart-link]<br/> <br/>Because of the version "cascade_plan_replication_retry_fast" in distsearch.conf is already set to true  by default. But it seems like replication is having an issue because it does not “Fail“ but stays “in progress“, however, when checking bundle replication status, all nodes are marked as successful(with affected one not being on the list due to cascading):<br/>[https://drive.google.com/file/d/1bfuuR_Idj6JTM3lLgWw9efcK6J9D3b5i/view?usp=sharing|https://drive.google.com/file/d/1bfuuR_Idj6JTM3lLgWw9efcK6J9D3b5i/view?usp=sharing|smart-link] <br/>The issue is seen very intermittently. Customer saw a big drop in occurrences after the 9.2.4 upgrade and even stopped observing issue during ney year freeze, but it started re-occurring since.<br/>Fresh diags are available on google drive:<br/>SHC:<br/>[https://drive.google.com/file/d/1Bd19lA-woq-MzkM0SXTQc-7mQDAOGNr0/view?usp=sharing|https://drive.google.com/file/d/1Bd19lA-woq-MzkM0SXTQc-7mQDAOGNr0/view?usp=sharing|smart-link] <br/>IDX:<br/>[https://drive.google.com/file/d/1-MjpbYnWbsBiWGkt9YwywRxT3hHQktd5/view?usp=sharing|https://drive.google.com/file/d/1-MjpbYnWbsBiWGkt9YwywRxT3hHQktd5/view?usp=sharing|smart-link] <br/>And also on SplunkBOT:<br/>SHC:<br/>[https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=fd9e1cea-4ebd-4bf4-b208-1f45e34b4c62&case_numbertok=3637549&hosttok=gz-zu2appplv201&case_filetok=gz-zu2appplv201-sh_-20250130-121456-C96MUFV4&found_anontok=0&job_typetok=classic|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=fd9e1cea-4ebd-4bf4-b208-1f45e34b4c62&case_numbertok=3637549&hosttok=gz-zu2appplv201&case_filetok=gz-zu2appplv201-sh_-20250130-121456-C96MUFV4&found_anontok=0&job_typetok=classic]<br/>IDX:<br/>[https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=cc144e51-6e78-4d83-85ce-c4f169af50cb&case_numbertok=3637549&hosttok=gx-zu2pssplv093&case_filetok=gx-zu2pssplv093-idx_-20250130-121243-vS9FwYiT&found_anontok=0&job_typetok=classic|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000gufcyaaa&job_idtok=cc144e51-6e78-4d83-85ce-c4f169af50cb&case_numbertok=3637549&hosttok=gx-zu2pssplv093&case_filetok=gx-zu2pssplv093-idx_-20250130-121243-vS9FwYiT&found_anontok=0&job_typetok=classic]<br/>*_Workaround_*<br/>_Restart SHC Captain or trigger Captain re-election_
|}

== 9.4.4 ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-280186 SPL-280186]
| [CLONE] [sustain/europium] MachineTypeFilter doesn't work when DS works in clustered mode - applications are getting wrongfully updated into clients when performing OS filtering in serverclass
| Deployment Server
| 9.4.4
| P2-High
| Bug
| Resolved
| Fixed
| Backport_Approved, dwest_reviewed, emea_support:approved, SP:Not-DP, support_reviewed
| This ticket is a backport of [https://splunk.atlassian.net/browse/SPL-270345|https://splunk.atlassian.net/browse/SPL-270345|smart-link]
|-
| [https://splunk.atlassian.net/browse/SPL-270345 SPL-270345]
| MachineTypeFilter doesn't work when DS works in clustered mode - applications are getting wrongfully updated into clients when performing OS filtering in serverclass
| Deployment Server
| 10.0.x, 9.2.8, 9.3.6, 9.4.4
| P2-High
| Bug
| Closed
| Fixed
| Backport_Approved, dwest_reviewed, emea_support:approved, SP:Not-DP, support_reviewed
| *_Summary:_* _The customer is deploying apps from the Deployment server to deployment clients using the MachineTypeFiltering in serverclass but the apps are deploying to both Windows and Linux servers._<br/>*_Customer Impact:_* _Due to this the apps are getting wrongfully installed on the servers where they would not persist._<br/>*_Description:_* _Describe what the customer was trying to achieve when they first noticed the problem._<br/>* _How long has the customer been tracking this issue and what is the frequency? Has the customer made any changes to their environment just before the issue started?_<br/>After upgrading the Splunk to 9.4.0 version they were facing this issue<br/>* _What errors/warnings/behaviors are being reported? Be specific and provide evidence._<br/>N/A<br/>* _What is the expectation when this problem is resolved?_<br/>The OS filtration must be done correctly and deployed to the correct OS<br/>* _If the customer and/or Support engineer tried any mitigation, provide ALL of them and their outcomes._<br/>We tried checking over the call by creating a “raitec_app1_case_test“ serverclass and observed the same behavior <br/>*_Problem Analysis_*<br/>* _In your own words, describe support's analysis of the problem._<br/>When applying MachineTypeFilter  in server class the app will be deployed to the clients based on the OS level. Suppose we mention the server class to deploy the apps in linux but the apps are deployed to windows machine.<br/> For Example [serverClass:linux_server:app:grz_linux_server_base-infos] <br/>machineTypesFilter = linux-* ..>>>= is frequently deployed to multiple Windows hosts.<br/> [serverClass:win-server_domino_prod_common:app:grz_win-server_domino_prod_common] <br/>is also found on several hosts where it is not supposed to be. <br/>At the same time, the following applications are absent but should be deployed: [serverClass:windows_server:app:grz_windows_server_base-infos] [serverClass:windows_server:app:grz_server_events] [serverClass:windows_server:app:Splunk_TA_microsoft-iis] <br/>The following hosts have confirmed issues: <br/>AZURECONN9911 - W2016 <br/>AZUREDSC7901 - W2022 <br/>AZUREDSC9901 - W2022 <br/>AZURECONN9921 - W2016 <br/>AZURECONN1101 - W2016 <br/>AZURECONN9901 - W2016 <br/>AZURETASK9901 - W2022.<br/>* _Provide logs and accompanying data with reference to the above confluence page and all the relevant documentation based on your analysis to support your observation and theories._<br/>This was a known issue and it was present on 9.2.x, 9.3.0 , but as per previous JIRA it was fixed on 9.2.2 and 9.3.1. But my customer is on 9.4.0.<br/>[https://docs.splunk.com/Documentation/Splunk/9.2.2/ReleaseNotes/Fixedissues#Distributed_deployment.2C_forwarder.2C_deployment_server_issues|https://docs.splunk.com/Documentation/Splunk/9.2.2/ReleaseNotes/Fixedissues#Distributed_deployment.2C_forwarder.2C_deployment_server_issues]<br/>* _Provide links to diags either in Splunkbot OR attached via Google Drive._<br/>The Diag files where the MachineType Filter Configured in serverclass:<br/>2 Deployment server:<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-51case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-01-08_12-50-12.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-51case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-01-08_12-50-12.tar.gz]<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-81case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-01-08_12-50-00.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-81case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-01-08_12-50-00.tar.gz]<br/>Windows server diag file:<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-93case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC9901-2025-01-08.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-93case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC9901-2025-01-08.tar.gz]<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-33case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC7901-2025-01-08.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-33case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC7901-2025-01-08.tar.gz]<br/>The diag files with *no “MachineTypeFilter”* configured in serverclass:<br/>2 Deployment servers diag files:<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-90case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-02-04_15-31-49.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-90case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-02-04_15-31-49.tar.gz]<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-83case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-02-04_15-36-22.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-83case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-02-04_15-36-22.tar.gz]<br/>Linux diag file:<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-18case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-tcapp2152-2025-02-04.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-18case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-tcapp2152-2025-02-04.tar.gz]<br/>Windows diag file:<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-15case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-LNO9905-2025-02-04.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-15case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-LNO9905-2025-02-04.tar.gz]<br/>* _If you include screenshots, please label them accordingly and link them as thumbnails here or in the comments section with explanations._<br/>We got on a meeting and asked the customer to reproduce the issue, and then we created a test server class and the apps deployed in to both linux and windows machine.<br/>!image-20250205-150919.png|width=338,height=116,alt="image-20250205-150919.png"!<br/>Windows machines where apps are updated for 60 minutes window<br/>!image-20250205-150927.png|width=1501,height=226,alt="image-20250205-150927.png"!<br/>* _What is the ask from Engineering from a code-fix or product engineering standpoint?_<br/>Code fix <br/>*_Workaround_*<br/>We have requested the customer to not use MachineTypeFilter in server class and now they are not observing any issue.
|-
| [https://splunk.atlassian.net/browse/SPL-279657 SPL-279657]
| Distributed search may lose information about bundles from cluster peers on new search generations, removes peers when best-effort mode is enabled.
| Distributed Search
| 10.0.x, 9.2.8, 9.3.6, 9.4.4, develop, Nutella(9.2.2406.107), Oreo, Pocky
| P1-Immediate
| Bug
| Closed
| Fixed
| Backport_Approved
| When we add cluster peers after a search generation change, we try to prime their information (including available bundles) before exposing the information to search. [https://splunk.atlassian.net/browse/SPL-216146|https://splunk.atlassian.net/browse/SPL-216146|smart-link] introduced a regression through which, by mistake, that information is being exposed prematurely, with no bundles available. This will cause a short pause in search requests when best-effort mode is disabled, as the search head blocks search while it refreshes its bundle information, but with best-effort enabled we’ll see incomplete results and warnings straight away.<br/>In the codebase, what happens is that {{DistributedPeerManager::prepClusterPeersForPotentialAdd()}}relies on its local {{HeartbeatTransactionSynchronizer}} collecting peer information before it publishes it all, but [commit d6cf0fbdf2bd changed the auth transaction such that it will always update|https://cd.splunkdev.com/splcore/main/-/commit/d6cf0fbdf2bd#d700c0d60e4cdc6ba2d58fdd1287c87e00c09884_279_287] the {{DistributedPeerManager}} directly.<br/>Found through code inspection while investigating [https://splunk.atlassian.net/browse/SPL-268481|https://splunk.atlassian.net/browse/SPL-268481|smart-link].
|-
| [https://splunk.atlassian.net/browse/SPL-278716 SPL-278716]
| KV store upgrade to server version 7.0 fails when SSL compression is not set to its default value.
| KV Store
| 9.4.4
| P2-High
| Bug
| Closed
| Fixed
| kvstore:cos:ssl_compression, kvstore:sx:mongo_7_upgrade_failed, kvstore:type:local, kvstore-oncall, reviewer-BrandonFernandez, support_reviewed
| Notes: Customer is usually free 7am-3pm Central US time if Zoom session require, they are happy to jump on a call move around schedule if needed. <br/>Customer recently upgrade Splunk to 9.4.2 version. They have total of 3 members in this clustered environment. <br/>All three members have auto upgrade disabled<br/>{{etc/system/local/server.conf kvstoreUpgradeOnStartupEnabled = false}}<br/>Auto upgrade has been turn off which is good.<br/>Checked migration.log<br/>{{KVStore upgrade version file created: "/opt/splunk/var/run/splunk/kvstore_upgrade/versionFile42".}}<br/>{{-> Currently used KVSTore version=4.2.17-linux-splunk-v4. Expected version=4.2 or version=7.0}}<br/>{{-> Currently configured KVSTore database path="/opt/splunk/var/lib/splunk/kvstore"}}<br/>{{Active KVStore version upgrade precheck PASSED}}<br/>Checked all certs: <br/>the active combination that is in use is:<br/>{{sslRootCAPath = /etc/ssl/localhost.splunk._ca_chain.pem}}<br/>{{sslPassword = DIAG_REDACTED_ENCRYPTED_PASSWORD}}<br/>{{serverCert = /etc/ssl/localhost.splunk.crt+chain+key}}<br/>Mongod.log showing <br/>2025-06-05T17:16:46.104Z I ELECTION [replexec-0] VoteRequester(term 76) failed to receive response from [splunkazure-shc-e5f79cb.prod-cus-01-az1.github.net|https://splunkazure-shc-e5f79cb.prod-cus-01-az1.github.net/]:8191: HostUnreachable: Error connecting to [splunkazure-shc-e5f79cb.prod-cus-01-az1.github.net|https://splunkazure-shc-e5f79cb.prod-cus-01-az1.github.net/]:8191 (10.132.0.233:8191) :: caused by :: compression disabled<br/>Recommended customer to set <br/>{{[sslConfig] }}<br/>{{useClientSSLCompression = true }}<br/>{{SplunkdClientSSLCompression = true}}<br/> And run the manual upgrade again see if successful. <br/>Rolling restart done. 2 of the 3 nodes are fine, but one is saying<br/>replicationStatus:Down<br/>(All Screenshots attached on the JIRA)<br/>All three latest diags after rolling restart and sslconfig set as true attach on the Jira. <br/>Cus-01-az1<br/>[https://downloadsvc.splunk.com/download/support/06-06-2025/uploadsvc-16case3771316-06-06-2025-USER-0035a00003UUJwnAAH-diag-splunkazure-shc-e5f79cb.prod-cus-01-az1.github.net-2025-06-06_10-00-24.tar.gz|https://downloadsvc.splunk.com/download/support/06-06-2025/uploadsvc-16case3771316-06-06-2025-USER-0035a00003UUJwnAAH-diag-splunkazure-shc-e5f79cb.prod-cus-01-az1.github.net-2025-06-06_10-00-24.tar.gz] <br/>Cus-01-az2<br/>[https://downloadsvc.splunk.com/download/support/06-06-2025/uploadsvc-67case3771316-06-06-2025-USER-0035a00003UUJwnAAH-diag-splunkazure-shc-068d198.prod-cus-01-az2.github.net-2025-06-06_10-00-26.tar.gz|https://downloadsvc.splunk.com/download/support/06-06-2025/uploadsvc-67case3771316-06-06-2025-USER-0035a00003UUJwnAAH-diag-splunkazure-shc-068d198.prod-cus-01-az2.github.net-2025-06-06_10-00-26.tar.gz]<br/>Cus-01-az3<br/>[https://downloadsvc.splunk.com/download/support/06-06-2025/uploadsvc-72case3771316-06-06-2025-USER-0035a00003UUJwnAAH-diag-splunkazure-shc-037f411.prod-cus-01-az3.github.net-2025-06-06_10-00-28.tar.gz|https://downloadsvc.splunk.com/download/support/06-06-2025/uploadsvc-72case3771316-06-06-2025-USER-0035a00003UUJwnAAH-diag-splunkazure-shc-037f411.prod-cus-01-az3.github.net-2025-06-06_10-00-28.tar.gz]<br/>We would like the mongod team to help us identify why the upgrade is failing.
|}

== 10.0.2503.x ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-253690 SPL-253690]
| CLONE - [release note] Issue with Splunk Enterprise versions 9.1.x through 9.4.x when http proxy is used
| Search, Upgrade
| 10.0.2503.x, 10.0.x
| P2-High
| Bug
| Closed
| Fixed
| AxpTriaged, cfit_reviewed
| Splunk was installed on the 9.1.1 version on a basic environment with one search head and indexer and a proxy configured in between them. From the search head Under Distributed search>>search peers we can the indexer was connected and healthy. But when we run a simple search it returns the error "Received error from proxy server". <br/>Now, Splunk version 9.0.4 was installed on the same instance using Docker. After the successful installation of the 9.0.4 version the health status of the indexer remained healthy and when we ran a basic it returned the results with no errors.<br/>Please note that there were no changes in the configuration while changing the versions.<br/>List of proxies tested by the customer:<br/>* Squid 4.15 on RHEL<br/>* Squid 3.5.7 in LAB-2<br/>* Squid 6.4 in LAB-2<br/>* Envoyproxy 1.27.1<br/>* Cntlm 0.92.3<br/>* Bluecoat Proxy 6.7.5 SWG<br/>Please find the search logs and DIAGs below<br/>----<br/>UPDATE 3/21/24:<br/>We were able to reproduce a very similar behavior to the customer in our internal labs.<br/>The issue was not observed on 9.0.4 but was seen on both 9.1.1 and 9.2.<br/>*a.) WORKING: On 9.0.4 setup with SH and Indexer:* <br/>In the search head search.log we can see that we make a call to the “indexer” ipaddress {{10.202.9.10:8089}} to the {{/services/streams/search}} endpoint. <br/>*_Here in this version, we do get the results from the search query._**From search.log without error:*<br/>{noformat}03-21-2024 08:05:01.921 DEBUG SearchResultParser [1513664 DistributedSearchResultCollectorThread] - https://10.202.9.10:8089/services/streams/search?sh_sid=1711008300.322 The protocol header is [splunk 9.0.4,124991,7]<br/>03-21-2024 08:05:01.921 DEBUG SearchResultParser [1513664 DistributedSearchResultCollectorThread] - extraction successful header bytes left 124991 body_bytes 7<br/>03-21-2024 08:05:01.921 DEBUG SearchResultTransaction [1513664 DistributedSearchResultCollectorThread] - Received data for peer=https://10.202.9.10:8089/services/streams/search?sh_sid=1711008300.322 of size=32768{noformat}<br/>In the access.log from the squid proxy server we observe "200" for these requests and also we are getting results from the search query:<br/>/var/log/squid/access.log:<br/>{noformat}1711008143.104  57763 10.202.9.191 TCP_TUNNEL/200 2319609 CONNECT 10.202.9.10:8089 - HIER_DIRECT/10.202.9.10 -{noformat}<br/>*Under server.conf for SH and Indexer*<br/>{noformat}[proxyConfig]<br/>http_proxy = http://10.202.20.39:8000{noformat}<br/>*NOTE:* 10.202.9.191 (SH for 9.0.4), 10.202.9.10 (IDX for 9.0.4), 10.202.20.39 (Proxy server)*Test Instance Proxy server:*<br/>{noformat}For CLI:<br/>ssh://splunker@10.202.20.39:22<br/>Username: splunker<br/>Password: splk{noformat}<br/>*Test Instance for SH 9.0.4:*<br/>{noformat}For UI:<br/>http://10.202.9.191:8000/<br/>User : admin<br/>Password: 5up3rn0va<br/>For CLI: <br/>ssh://splunker@10.202.9.191<br/>User: splunker<br/>Password: splk{noformat}<br/>*Test Instance for IDX 9.0.4:*<br/>{noformat}On CLI:<br/>ssh://splunker@10.202.9.10:22<br/>User: splunker<br/>Password: splk<br/>For UI:<br/>http://10.202.9.10:8000/<br/>User : admin<br/>Password: 5up3rn0va{noformat}<br/>---------------------------------------------------------------------------------------------------------------------------------------------<br/>*b.) NON-WORKING: On 9.1.1 setup with SH and Indexer(Non-working):* Here we don't get any results for our specific search query, and do get the following error on the UI:<br/>{noformat}The following error(s) and caution(s) occurred while the search ran. Therefore, search results might be incomplete. Hide errors.<br/>Received error from proxy server: Socket error: Resource temporarily unavailable<br/>Unknown error for indexer: [idx1] . Search Results might be incomplete. If this occurs frequently, check on the peer.{noformat}<br/>!image (10).png|width=1351,height=280!<br/>In the search head search.log we can see that we make a call to the proxy server {{10.202.20.39}} to the the endpoint {{/services/streams/search}} where in version 9.04 we made a call to the indexer IP on this endpoint.<br/>*On the search.log:*<br/>{noformat}03-21-2024 08:10:16.473 ERROR SearchResultTransaction [124962 DistributedSearchResultCollectorThread] - Got status 502 from https://10.202.20.39:8089/services/streams/search?sh_sid=1711008616.267<br/>......<br/>03-21-2024 08:10:16.473 ERROR SearchResultParser [124962 DistributedSearchResultCollectorThread] - HTTP error status message from https://10.202.20.39:8089/services/streams/search?sh_sid=1711008616.267: Received error from proxy server:{noformat}<br/>In the access.log from the squid proxy server we observe "503" for these requests:<br/>*/var/log/squid/access.log:*<br/>{noformat}1711008891.640  27979 10.202.20.217 TCP_TUNNEL/200 3366 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -<br/>1711008891.640  28009 10.202.20.217 TCP_TUNNEL/200 3736 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -<br/>1711008891.640  28003 10.202.20.217 TCP_TUNNEL/200 4476 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -<br/>1711008891.640  27996 10.202.20.217 TCP_TUNNEL/200 3736 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -<br/>1711008891.640  27991 10.202.20.217 TCP_TUNNEL/200 3819 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -<br/>1711008891.640  27984 10.202.20.217 TCP_TUNNEL/200 2831 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -<br/>1711008894.943    133 10.202.9.191 TCP_TUNNEL/200 6198 CONNECT beam.scs.splunk.com:443 - HIER_DIRECT/13.225.142.55 -<br/>1711008898.765      0 10.202.20.217 NONE/503 0 CONNECT 10.202.20.39:8089 - HIER_NONE/- -<br/>1711008898.779      0 10.202.20.217 NONE/503 0 CONNECT 10.202.20.39:8089 - HIER_NONE/- -<br/>1711008898.784      0 10.202.20.217 NONE/503 0 CONNECT 10.202.20.39:8089 - HIER_NONE/- -{noformat}<br/>*_We observe the same behavior as 9.1.1 and in 9.2.0.1 as well._* Under server.conf for SH and Indexer we point to the same proxy server as for 9.0.4 test above:<br/>{noformat}[proxyConfig]<br/>http_proxy = http://10.202.20.39:8000{noformat}<br/>*Note:* 10.202.20.217 (IP of SH 9.1.1), 10.202.21.187 (IP of IDX 9.1.1), 10.202.20.39 (proxy server)*Test Instance Proxy server:*<br/>{noformat}For CLI:<br/>URL: ssh://splunker@10.202.20.39:22<br/>Username: splunker<br/>Password: splk{noformat}<br/>*Test Instance SH 9.1.1:*<br/>{noformat}http://10.202.20.217:8000/<br/>User : admin<br/>Password: 5up3rn0va<br/>For CLI:<br/>User: splunker<br/>Password: splk{noformat}<br/>*Test Instance IDX 9.1.1:*<br/>{noformat}http://10.202.21.187:8000/<br/>User : admin<br/>Password: 5up3rn0va<br/>For CLI:<br/>User: splunker<br/>Password: splk{noformat}<br/>We have attached SH and IDX diags from our test to the jira. The environment is also available to access.<br/>PLEASE NOTE, the customer saw the same behavior but their proxy server returned 502 errors and not 503 but I suspect this is related to how their network is setup differently to ours internal tests.
|-
| [https://splunk.atlassian.net/browse/SPL-239436 SPL-239436]
| [PUBLIC]  In standard mode federated search, outputlookup existence check on RSH causes search to terminate early although it is not run on RSH
| Search Federation
| 10.0.2503.x, Q release
| P2-High
| Bug
| Closed
| Fixed
| &mdash;
| When a command like {{search index=index_df_1 | outputlookup iplocation.csv}} executed, the {{outputlookup }}command gets sent to the remote search head although it is executed in phase 1 locally.  If the remote search head does not have this {{lookup}} defined, the RSH will error out first before executing the search, returning 0 result to FSH.
|-
| [https://splunk.atlassian.net/browse/SPL-262259 SPL-262259]
| Splunk to Splunk federated searches do not utilize the dispatch.index_earliest and dispatch.index_latest  parameters in the saved search configuration when the search is dispatched to the remote search head, leading to incorrect results
| Search Federation
| 10.0.2503.x
| P3-Medium
| Bug
| Closed
| Fixed
| &mdash;
| see [https://splunk.slack.com/archives/C01VA6P7TD4/p1724961650680989|https://splunk.slack.com/archives/C01VA6P7TD4/p1724961650680989|smart-link] <br/>Federated Searches doesn't utilize the dispatch.index_earliest and dispatch.index_latest  params coming from the saved searches configuration when dispatching the search on the RSH.<br/>{noformat}[Test - CloudDispatchBug]<br/>cron_schedule = */15 * * * *<br/>dispatch.earliest_time = -1d@d<br/>dispatch.index_earliest = -20m@m<br/>dispatch.index_latest = -5m@m<br/>dispatch.latest_time = +1d@d<br/>search = index=test source="Test - Generate Test Data for Bug" \<br/>| eval event=_raw\<br/>| rex field=_raw "id=\"(?<id>[^\"]+)\""\<br/>| table _time, source, event, id\<br/>| collect addtime=true index=mr-stuff-test{noformat}<br/>When configured this way where {{index_earliest}} and {{index_latest}} are configs in the saved search. It is not dispatched to the RSH. See code:<br/>{noformat}DistributedSearchResultCollectionManager::setupFederatedTransaction{noformat}<br/>{noformat}   // TODO: In the case of an all time search et and lt = 0.0<br/>    // Adding this check since default endpoint behavior without et and lt specified is an ALL TIME search<br/>    if(info._search_lt > info._search_et) {<br/>        postData << "earliest_time";<br/>        postData << '=';<br/>        postData << info._search_et;<br/>        postData << '&';<br/>        postData << "latest_time";<br/>        postData << '=';<br/>        postData << info._search_lt;<br/>        postData << '&';<br/>    }<br/>    else if(info._search_lt.isZero() && !info._search_et.isZero())<br/>    {<br/>        // Case where user has provided only earliest_time and no latest_time.<br/>        // Not passing latest_time so that the RSH can determine the default<br/>        // value to keep in line with regular search behavior (SPL-241502)<br/>        postData << "earliest_time";<br/>        postData << '=';<br/>        postData << info._search_et;<br/>        postData << '&';<br/>    }{noformat}<br/>This needs to be fixed.<br/>Workaround: <br/>These configs can be added as a part of SPL query. In this way these parameters are sent to the RSH. See <br/>[https://docs.splunk.com/Documentation/Splunk/9.3.0/SearchReference/SearchTimeModifiers#List_of_time_modifiers|https://docs.splunk.com/Documentation/Splunk/9.3.0/SearchReference/SearchTimeModifiers#List_of_time_modifiers] <br/>{noformat}Test - CloudDispatchBug]<br/>cron_schedule = */15 * * * *<br/>dispatch.earliest_time = -1d@d<br/>dispatch.latest_time = +1d@d<br/>search = index=test _index_earliest=-20m@m _index_latest=-5m@m source="Test - Generate Test Data for Bug" \<br/>| eval event=_raw\<br/>| rex field=_raw "id=\"(?<id>[^\"]+)\""\<br/>| table _time, source, event, id\<br/>| collect addtime=true index=mr-stuff-test{noformat}
|}

== 10.0.x ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-280187 SPL-280187]
| [CLONE] [sustain/splunk-10.0] MachineTypeFilter doesn't work when DS works in clustered mode - applications are getting wrongfully updated into clients when performing OS filtering in serverclass
| Deployment Server
| 10.0.x
| P2-High
| Bug
| Resolved
| Fixed
| Backport_Approved, bis_release_notes_10.0.2503.2, dwest_reviewed, emea_support:approved, SP:Not-DP, support_reviewed
| This ticket is a backport of [https://splunk.atlassian.net/browse/SPL-270345|https://splunk.atlassian.net/browse/SPL-270345|smart-link]
|-
| [https://splunk.atlassian.net/browse/SPL-270345 SPL-270345]
| MachineTypeFilter doesn't work when DS works in clustered mode - applications are getting wrongfully updated into clients when performing OS filtering in serverclass
| Deployment Server
| 10.0.x, 9.2.8, 9.3.6, 9.4.4
| P2-High
| Bug
| Closed
| Fixed
| Backport_Approved, dwest_reviewed, emea_support:approved, SP:Not-DP, support_reviewed
| *_Summary:_* _The customer is deploying apps from the Deployment server to deployment clients using the MachineTypeFiltering in serverclass but the apps are deploying to both Windows and Linux servers._<br/>*_Customer Impact:_* _Due to this the apps are getting wrongfully installed on the servers where they would not persist._<br/>*_Description:_* _Describe what the customer was trying to achieve when they first noticed the problem._<br/>* _How long has the customer been tracking this issue and what is the frequency? Has the customer made any changes to their environment just before the issue started?_<br/>After upgrading the Splunk to 9.4.0 version they were facing this issue<br/>* _What errors/warnings/behaviors are being reported? Be specific and provide evidence._<br/>N/A<br/>* _What is the expectation when this problem is resolved?_<br/>The OS filtration must be done correctly and deployed to the correct OS<br/>* _If the customer and/or Support engineer tried any mitigation, provide ALL of them and their outcomes._<br/>We tried checking over the call by creating a “raitec_app1_case_test“ serverclass and observed the same behavior <br/>*_Problem Analysis_*<br/>* _In your own words, describe support's analysis of the problem._<br/>When applying MachineTypeFilter  in server class the app will be deployed to the clients based on the OS level. Suppose we mention the server class to deploy the apps in linux but the apps are deployed to windows machine.<br/> For Example [serverClass:linux_server:app:grz_linux_server_base-infos] <br/>machineTypesFilter = linux-* ..>>>= is frequently deployed to multiple Windows hosts.<br/> [serverClass:win-server_domino_prod_common:app:grz_win-server_domino_prod_common] <br/>is also found on several hosts where it is not supposed to be. <br/>At the same time, the following applications are absent but should be deployed: [serverClass:windows_server:app:grz_windows_server_base-infos] [serverClass:windows_server:app:grz_server_events] [serverClass:windows_server:app:Splunk_TA_microsoft-iis] <br/>The following hosts have confirmed issues: <br/>AZURECONN9911 - W2016 <br/>AZUREDSC7901 - W2022 <br/>AZUREDSC9901 - W2022 <br/>AZURECONN9921 - W2016 <br/>AZURECONN1101 - W2016 <br/>AZURECONN9901 - W2016 <br/>AZURETASK9901 - W2022.<br/>* _Provide logs and accompanying data with reference to the above confluence page and all the relevant documentation based on your analysis to support your observation and theories._<br/>This was a known issue and it was present on 9.2.x, 9.3.0 , but as per previous JIRA it was fixed on 9.2.2 and 9.3.1. But my customer is on 9.4.0.<br/>[https://docs.splunk.com/Documentation/Splunk/9.2.2/ReleaseNotes/Fixedissues#Distributed_deployment.2C_forwarder.2C_deployment_server_issues|https://docs.splunk.com/Documentation/Splunk/9.2.2/ReleaseNotes/Fixedissues#Distributed_deployment.2C_forwarder.2C_deployment_server_issues]<br/>* _Provide links to diags either in Splunkbot OR attached via Google Drive._<br/>The Diag files where the MachineType Filter Configured in serverclass:<br/>2 Deployment server:<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-51case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-01-08_12-50-12.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-51case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-01-08_12-50-12.tar.gz]<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-81case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-01-08_12-50-00.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-81case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-01-08_12-50-00.tar.gz]<br/>Windows server diag file:<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-93case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC9901-2025-01-08.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-93case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC9901-2025-01-08.tar.gz]<br/>[https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-33case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC7901-2025-01-08.tar.gz|https://downloadsvc.splunk.com/download/splunk/01-08-2025/uploadsvc-33case3641593-01-08-2025-USER-0030b000028J1oXAAS-diag-AZUREDSC7901-2025-01-08.tar.gz]<br/>The diag files with *no “MachineTypeFilter”* configured in serverclass:<br/>2 Deployment servers diag files:<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-90case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-02-04_15-31-49.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-90case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9902-2025-02-04_15-31-49.tar.gz]<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-83case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-02-04_15-36-22.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-83case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-spldep9903-2025-02-04_15-36-22.tar.gz]<br/>Linux diag file:<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-18case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-tcapp2152-2025-02-04.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-18case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-tcapp2152-2025-02-04.tar.gz]<br/>Windows diag file:<br/>[https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-15case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-LNO9905-2025-02-04.tar.gz|https://downloadsvc.splunk.com/download/splunk/02-04-2025/uploadsvc-15case3641593-02-04-2025-USER-0030b000028J1oXAAS-diag-LNO9905-2025-02-04.tar.gz]<br/>* _If you include screenshots, please label them accordingly and link them as thumbnails here or in the comments section with explanations._<br/>We got on a meeting and asked the customer to reproduce the issue, and then we created a test server class and the apps deployed in to both linux and windows machine.<br/>!image-20250205-150919.png|width=338,height=116,alt="image-20250205-150919.png"!<br/>Windows machines where apps are updated for 60 minutes window<br/>!image-20250205-150927.png|width=1501,height=226,alt="image-20250205-150927.png"!<br/>* _What is the ask from Engineering from a code-fix or product engineering standpoint?_<br/>Code fix <br/>*_Workaround_*<br/>We have requested the customer to not use MachineTypeFilter in server class and now they are not observing any issue.
|-
| [https://splunk.atlassian.net/browse/SPL-279657 SPL-279657]
| Distributed search may lose information about bundles from cluster peers on new search generations, removes peers when best-effort mode is enabled.
| Distributed Search
| 10.0.x, 9.2.8, 9.3.6, 9.4.4, develop, Nutella(9.2.2406.107), Oreo, Pocky
| P1-Immediate
| Bug
| Closed
| Fixed
| Backport_Approved
| When we add cluster peers after a search generation change, we try to prime their information (including available bundles) before exposing the information to search. [https://splunk.atlassian.net/browse/SPL-216146|https://splunk.atlassian.net/browse/SPL-216146|smart-link] introduced a regression through which, by mistake, that information is being exposed prematurely, with no bundles available. This will cause a short pause in search requests when best-effort mode is disabled, as the search head blocks search while it refreshes its bundle information, but with best-effort enabled we’ll see incomplete results and warnings straight away.<br/>In the codebase, what happens is that {{DistributedPeerManager::prepClusterPeersForPotentialAdd()}}relies on its local {{HeartbeatTransactionSynchronizer}} collecting peer information before it publishes it all, but [commit d6cf0fbdf2bd changed the auth transaction such that it will always update|https://cd.splunkdev.com/splcore/main/-/commit/d6cf0fbdf2bd#d700c0d60e4cdc6ba2d58fdd1287c87e00c09884_279_287] the {{DistributedPeerManager}} directly.<br/>Found through code inspection while investigating [https://splunk.atlassian.net/browse/SPL-268481|https://splunk.atlassian.net/browse/SPL-268481|smart-link].
|-
| [https://splunk.atlassian.net/browse/SPL-263518 SPL-263518]
| [9.3.0] Upgrade removes thruput processor and group=per_(source|sourcetype|index|host)_thruput in metrics.log for universalforwarders.
| Pipeline Enhancements, Universal Forwarder
| 10.0.x
| P2-High
| Bug
| Closed
| Fixed
| enterprise-ui-reviewed, Reviewer-BrianOsburn
| h3. Summary: <br/>Universal Forwarders post upgrading to version 9.3.0 loses visibility in {{metrics.logs}} for the group - {{group=per_sourcetype_thruput}}<br/>h3. Customer impact: <br/>Universal Forwarders no longer report a vital part of logs, limiting the abilities of their monitoring dashboards.<br/>Description/Problem Analysis:<br/>* Upgrade to Universal Forwarder 9.3.0 causes the {{group=per_(source|sourcetype|index|host)_thruput}} under the {{metrics.log}} to be not logged.<br/>* This is visible for universal forwarders that were/are being upgraded from 9.2.2 either via custom script or manual upgrade following the doc - [https://docs.splunk.com/Documentation/Splunk/9.3.1/Installation/AboutupgradingREADTHISFIRST|https://docs.splunk.com/Documentation/Splunk/9.3.1/Installation/AboutupgradingREADTHISFIRST|smart-link]  <br/>* Before the upgrade, the group logs relevant metrics logs and pushes them to the universal forwarders, but this stops after the 9.3.0 upgrade as evident in the below screenshot -<br/>(Date range - before the upgrade)<br/>!image-20240930-132341.png|width=97.9820796879928%,alt="image-20240930-132341.png"!<br/>(Date range - after the upgrade performed at 27 Sept -01:00 AM GMT+05:30)<br/>!image-20240930-132709.png|width=97.9820796879928%,alt="image-20240930-132709.png"!<br/>* We have tested this to be positive for any OS installation where the Splunk-Forwarder services are upgraded to 9.3.0.<br/>*- Windows UF -* <br/>*Hostname:* *windernaut-UF-02*<br/>*IP: 10.202.21.189* <br/>*Username: Splunker* <br/>*Passwd: W\!n_5up3rn0va\!*<br/>*Splunk_username: admin*<br/>*Splunk_userpass: admin@123*<br/>*- Linux UF -* <br/>*Hostname: sleekylinux-UF-02*<br/>*IP: 10.202.35.115*<br/>*Username: Splunker*<br/>*Passwd: splk*<br/>*Splunk_username: admin*<br/>*Splunk_userpass: admin@123*<br/>* Diags for Splunk Universal Forwarders during our testing are dropped in the drive share here - <br/>Internal testing diags - [https://drive.google.com/drive/folders/1pzunRB6X3BOod_ezHpKKTVBK99PO8nKP?usp=drive_link|https://drive.google.com/drive/folders/1pzunRB6X3BOod_ezHpKKTVBK99PO8nKP?usp=drive_link|smart-link]  <br/>Customer-provided diags -   [https://drive.google.com/drive/folders/1jVXCtGF_8i4XviZrVruCnuidFXZGanbG?usp=sharing|https://drive.google.com/drive/folders/1jVXCtGF_8i4XviZrVruCnuidFXZGanbG?usp=sharing|smart-link]
|-
| [https://splunk.atlassian.net/browse/SPL-253690 SPL-253690]
| CLONE - [release note] Issue with Splunk Enterprise versions 9.1.x through 9.4.x when http proxy is used
| Search, Upgrade
| 10.0.2503.x, 10.0.x
| P2-High
| Bug
| Closed
| Fixed
| AxpTriaged, cfit_reviewed
| Splunk was installed on the 9.1.1 version on a basic environment with one search head and indexer and a proxy configured in between them. From the search head Under Distributed search>>search peers we can the indexer was connected and healthy. But when we run a simple search it returns the error "Received error from proxy server". <br/>Now, Splunk version 9.0.4 was installed on the same instance using Docker. After the successful installation of the 9.0.4 version the health status of the indexer remained healthy and when we ran a basic it returned the results with no errors.<br/>Please note that there were no changes in the configuration while changing the versions.<br/>List of proxies tested by the customer:<br/>* Squid 4.15 on RHEL<br/>* Squid 3.5.7 in LAB-2<br/>* Squid 6.4 in LAB-2<br/>* Envoyproxy 1.27.1<br/>* Cntlm 0.92.3<br/>* Bluecoat Proxy 6.7.5 SWG<br/>Please find the search logs and DIAGs below<br/>----<br/>UPDATE 3/21/24:<br/>We were able to reproduce a very similar behavior to the customer in our internal labs.<br/>The issue was not observed on 9.0.4 but was seen on both 9.1.1 and 9.2.<br/>*a.) WORKING: On 9.0.4 setup with SH and Indexer:* <br/>In the search head search.log we can see that we make a call to the “indexer” ipaddress {{10.202.9.10:8089}} to the {{/services/streams/search}} endpoint. <br/>*_Here in this version, we do get the results from the search query._**From search.log without error:*<br/>{noformat}03-21-2024 08:05:01.921 DEBUG SearchResultParser [1513664 DistributedSearchResultCollectorThread] - https://10.202.9.10:8089/services/streams/search?sh_sid=1711008300.322 The protocol header is [splunk 9.0.4,124991,7]<br/>03-21-2024 08:05:01.921 DEBUG SearchResultParser [1513664 DistributedSearchResultCollectorThread] - extraction successful header bytes left 124991 body_bytes 7<br/>03-21-2024 08:05:01.921 DEBUG SearchResultTransaction [1513664 DistributedSearchResultCollectorThread] - Received data for peer=https://10.202.9.10:8089/services/streams/search?sh_sid=1711008300.322 of size=32768{noformat}<br/>In the access.log from the squid proxy server we observe "200" for these requests and also we are getting results from the search query:<br/>/var/log/squid/access.log:<br/>{noformat}1711008143.104  57763 10.202.9.191 TCP_TUNNEL/200 2319609 CONNECT 10.202.9.10:8089 - HIER_DIRECT/10.202.9.10 -{noformat}<br/>*Under server.conf for SH and Indexer*<br/>{noformat}[proxyConfig]<br/>http_proxy = http://10.202.20.39:8000{noformat}<br/>*NOTE:* 10.202.9.191 (SH for 9.0.4), 10.202.9.10 (IDX for 9.0.4), 10.202.20.39 (Proxy server)*Test Instance Proxy server:*<br/>{noformat}For CLI:<br/>ssh://splunker@10.202.20.39:22<br/>Username: splunker<br/>Password: splk{noformat}<br/>*Test Instance for SH 9.0.4:*<br/>{noformat}For UI:<br/>http://10.202.9.191:8000/<br/>User : admin<br/>Password: 5up3rn0va<br/>For CLI: <br/>ssh://splunker@10.202.9.191<br/>User: splunker<br/>Password: splk{noformat}<br/>*Test Instance for IDX 9.0.4:*<br/>{noformat}On CLI:<br/>ssh://splunker@10.202.9.10:22<br/>User: splunker<br/>Password: splk<br/>For UI:<br/>http://10.202.9.10:8000/<br/>User : admin<br/>Password: 5up3rn0va{noformat}<br/>---------------------------------------------------------------------------------------------------------------------------------------------<br/>*b.) NON-WORKING: On 9.1.1 setup with SH and Indexer(Non-working):* Here we don't get any results for our specific search query, and do get the following error on the UI:<br/>{noformat}The following error(s) and caution(s) occurred while the search ran. Therefore, search results might be incomplete. Hide errors.<br/>Received error from proxy server: Socket error: Resource temporarily unavailable<br/>Unknown error for indexer: [idx1] . Search Results might be incomplete. If this occurs frequently, check on the peer.{noformat}<br/>!image (10).png|width=1351,height=280!<br/>In the search head search.log we can see that we make a call to the proxy server {{10.202.20.39}} to the the endpoint {{/services/streams/search}} where in version 9.04 we made a call to the indexer IP on this endpoint.<br/>*On the search.log:*<br/>{noformat}03-21-2024 08:10:16.473 ERROR SearchResultTransaction [124962 DistributedSearchResultCollectorThread] - Got status 502 from https://10.202.20.39:8089/services/streams/search?sh_sid=1711008616.267<br/>......<br/>03-21-2024 08:10:16.473 ERROR SearchResultParser [124962 DistributedSearchResultCollectorThread] - HTTP error status message from https://10.202.20.39:8089/services/streams/search?sh_sid=1711008616.267: Received error from proxy server:{noformat}<br/>In the access.log from the squid proxy server we observe "503" for these requests:<br/>*/var/log/squid/access.log:*<br/>{noformat}1711008891.640  27979 10.202.20.217 TCP_TUNNEL/200 3366 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -<br/>1711008891.640  28009 10.202.20.217 TCP_TUNNEL/200 3736 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -<br/>1711008891.640  28003 10.202.20.217 TCP_TUNNEL/200 4476 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -<br/>1711008891.640  27996 10.202.20.217 TCP_TUNNEL/200 3736 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -<br/>1711008891.640  27991 10.202.20.217 TCP_TUNNEL/200 3819 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -<br/>1711008891.640  27984 10.202.20.217 TCP_TUNNEL/200 2831 CONNECT 10.202.21.187:8089 - HIER_DIRECT/10.202.21.187 -<br/>1711008894.943    133 10.202.9.191 TCP_TUNNEL/200 6198 CONNECT beam.scs.splunk.com:443 - HIER_DIRECT/13.225.142.55 -<br/>1711008898.765      0 10.202.20.217 NONE/503 0 CONNECT 10.202.20.39:8089 - HIER_NONE/- -<br/>1711008898.779      0 10.202.20.217 NONE/503 0 CONNECT 10.202.20.39:8089 - HIER_NONE/- -<br/>1711008898.784      0 10.202.20.217 NONE/503 0 CONNECT 10.202.20.39:8089 - HIER_NONE/- -{noformat}<br/>*_We observe the same behavior as 9.1.1 and in 9.2.0.1 as well._* Under server.conf for SH and Indexer we point to the same proxy server as for 9.0.4 test above:<br/>{noformat}[proxyConfig]<br/>http_proxy = http://10.202.20.39:8000{noformat}<br/>*Note:* 10.202.20.217 (IP of SH 9.1.1), 10.202.21.187 (IP of IDX 9.1.1), 10.202.20.39 (proxy server)*Test Instance Proxy server:*<br/>{noformat}For CLI:<br/>URL: ssh://splunker@10.202.20.39:22<br/>Username: splunker<br/>Password: splk{noformat}<br/>*Test Instance SH 9.1.1:*<br/>{noformat}http://10.202.20.217:8000/<br/>User : admin<br/>Password: 5up3rn0va<br/>For CLI:<br/>User: splunker<br/>Password: splk{noformat}<br/>*Test Instance IDX 9.1.1:*<br/>{noformat}http://10.202.21.187:8000/<br/>User : admin<br/>Password: 5up3rn0va<br/>For CLI:<br/>User: splunker<br/>Password: splk{noformat}<br/>We have attached SH and IDX diags from our test to the jira. The environment is also available to access.<br/>PLEASE NOTE, the customer saw the same behavior but their proxy server returned 502 errors and not 503 but I suspect this is related to how their network is setup differently to ours internal tests.
|-
| [https://splunk.atlassian.net/browse/SPL-262543 SPL-262543]
| CLONE - telemetry-metric local endpoint getting 401 errors
| Search - Telemetry
| 10.0.x, Pocky
| P3-Medium
| Bug
| Closed
| Fixed
| enterprise-ui
| Customer is reporting 401s from the /telemetry-metric endpoint with {{/servicesNS/None/None/telemetry-metric}} appearing in the splunkd logsInvestigation:<br/>* expanded the time window for the search of the warning logs and the partial results show that this issue is happening before lindt (confirmed up to kitkat thus far)<br/>* upon further investigation of the logs, the routes with "issues" are later passing with a new sessionkey, meaning that this is not a route specific issue but a sessionkey issue<br/>* when there is an invalid session key, the parameters app and owner have the appropriate values meaning the UI is properly passing the parameters to the endpoint<br/>* not sure why {{None}} is being logged on the splunkd side but I would assume the backend is overriding it off of the session key?<br/>Ask: We need assistance from SANDI Team to investigate the 401 errors.<br/>h1. Summary<br/>Increased 401 responses from the /telemetry-metric endpoint due to missing app and namespace ({{/servicesNS/None/None/telemetry-metric}}).<br/>* The telemetry is non-critical and does not affect any product/feature functionality<br/>No workarounds
|}

== 10.1.2507.x ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-273098 SPL-273098]
| [PUBLIC] "Show Source" UI returns a javascript error when it should return a 404 error
| Search, Search Federation
| 10.1.2507.x
| P2-High
| Bug
| Resolved
| Fixed
| enterprise-ui-platform-search, EUI-Unprioritized, EUI-Untriaged
| Background info:<br/>The “Show Source” *Field Action* runs a search that show the indexed events that precede and follow an event in your search results. The “Show Source” *Field Action* is documented [here|https://docs.splunk.com/Documentation/Splunk/9.0.4/Knowledge/Controlworkflowactionappearanceinfieldandeventmenus]. *Field Actions* - which were added to the Splunk platform around 2010 - are  actually a demonstration of a feature called [workflow actions|https://docs.splunk.com/Documentation/Splunk/9.0.4/Knowledge/CreateworkflowactionsinSplunkWeb]. [This Answers post from 2013|https://community.splunk.com/t5/Splunk-Search/no-of-events-in-show-source-view/td-p/94355] provides a bit more context about what the “Show Source” *Field Action* actually does & what endpoint it uses.<br/>Problem:<br/>Show Source UI makes a REST API call to the /jobs/<SID>/events endpoint to get the events. However, it doesn’t handle errors from the API call properly. For example, a common error is 404 error and it fails with a Javascript error. Because of the Javascript error, the Show Source UI is stuck with “Loading…” message. <br/>In the screenshot below, the UI shows “Loading…” message while the Console shows the API response message with the Javascript error stack trace.<br/>!image-20250320-203937.png|width=1392,height=1527,alt="image-20250320-203937.png"!<br/>Steps to reproduce:<br/>* Do a search. Click on “Show Source” for an event. This is the normal flow and Show Source UI should work as expected.<br/>* The Show Source UI URL is something like this [http://myhost:1234/en-US/app/search/show_source?sid=1742502726.30&offset=1&latest_time=|http://mycoder:21000/en-US/app/search/show_source?sid=1742502726.30&offset=1&latest_time=] . Change the SID is to a new SID (e.g., “123”). This should result in 404 for an unknown SID.<br/>* Expected: UI should handle 404 error and display an error message<br/>* Actual: it fails with a Javascript error. Because of the Javascript error, the Show Source UI is stuck with “Loading…” message (see the screenshot above). <br/>Note: in the steps to reproduce above, we manually edit SID to quickly reproduce the issue. However, this is not the only way this could happen. This 404 error could also happen when the SID has expired after 10 minutes and the user then tries to refresh the Show Source UI.<br/>Other possible errors from the API beside the 404 error above are 400 errors for validation errors and 500 errors for server-side exceptions.
|-
| [https://splunk.atlassian.net/browse/SPL-238738 SPL-238738]
| [PUBLIC] Federated Search for Splunk does not support the "Show Source" Field Action
| Search, Search Federation
| 10.1.2507.x, 10.2.x
| Unknown
| Bug
| Closed
| Fixed
| beryllium-defer-accept
| The “Show Source” *Field Action*, which is designed to run a search that show the indexed events that precede and follow an event in your search results, does not work in standard or transparent mode federated search. <br/>This issue was reported by a customer and subsequently discussed in [this Slack thread|https://splunk.slack.com/archives/C01VA6P7TD4/p1681232435292239]. [~accountid:6053a293009fee00694bc4e6] did some follow-up testing and verified that there does appear to be a problem with Show Source and federated searches. <br/>The other *Field Action* options - “Build Event Type” and “Extract Fields” appear to work as expected. <br/>I am filing this ticket so we can get it added as a known issue to the release notes. <br/>The “Show Source” *Field Action* is documented [here|https://docs.splunk.com/Documentation/Splunk/9.0.4/Knowledge/Controlworkflowactionappearanceinfieldandeventmenus]. *Field Actions* - which were added to the Splunk platform around 2010 - are  actually a demonstration of a feature called [workflow actions|https://docs.splunk.com/Documentation/Splunk/9.0.4/Knowledge/CreateworkflowactionsinSplunkWeb]. [This Answers post from 2013|https://community.splunk.com/t5/Splunk-Search/no-of-events-in-show-source-view/td-p/94355] provides a bit more context about what the “Show Source” *Field Action* actually does & what endpoint it uses.
|-
| [https://splunk.atlassian.net/browse/SPL-253757 SPL-253757]
| standard-mode federated search should alert ( and block the search ) when it is run in real time
| Search Federation
| 10.1.2507.x
| P2-High
| Bug
| Resolved
| Fixed
| &mdash;
| As of now we don’t alert the user if they  are using {{standard}} mode while using realtime search. <br/>We should alert the user in the UI AND fail the whole search so that the user gets only full result of his search
|-
| [https://splunk.atlassian.net/browse/SPL-253755 SPL-253755]
| transparent-mode federated search should alert ( and block the search ) when it is run in real time
| Search Federation
| 10.1.2507.x
| P2-High
| Bug
| Resolved
| Fixed
| psr-release-flubber
| As of now we don’t alert the user if they are using {{transparent}} mode while using realtime search.<br/>In transparent mode we should show an alert on the UI and not send the search to the RSH only ( but execute it locally )<br/>Some more information : <br/>[https://help.splunk.com/en/splunk-enterprise/search/search-manual/9.2/search-and-report-in-real-time/about-real-time-searches-and-reports|https://help.splunk.com/en/splunk-enterprise/search/search-manual/9.2/search-and-report-in-real-time/about-real-time-searches-and-reports|smart-link] <br/>* search-realtime is based on getting data directly from the indexing pipeline…. and the plan is to not support it in the future….. hence we don’t want to add support for it for federated search<br/>* indexed-realtime gives you basically the same thing but with slightly more latency<br/>I believe you can use {{bool shouldExecuteRTWindow() const { return _realtime && (!_rt_earliest.empty() || !_rt_latest.empty()); }}}  in {{src/framework/SearchResultsInfo.h}} to know if the search is realtime search.<br/>cc [~accountid:712020:73ddc4da-9d6d-4655-ac18-678b0963cda1] .
|}

== 10.2.x ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-238738 SPL-238738]
| [PUBLIC] Federated Search for Splunk does not support the "Show Source" Field Action
| Search, Search Federation
| 10.1.2507.x, 10.2.x
| Unknown
| Bug
| Closed
| Fixed
| beryllium-defer-accept
| The “Show Source” *Field Action*, which is designed to run a search that show the indexed events that precede and follow an event in your search results, does not work in standard or transparent mode federated search. <br/>This issue was reported by a customer and subsequently discussed in [this Slack thread|https://splunk.slack.com/archives/C01VA6P7TD4/p1681232435292239]. [~accountid:6053a293009fee00694bc4e6] did some follow-up testing and verified that there does appear to be a problem with Show Source and federated searches. <br/>The other *Field Action* options - “Build Event Type” and “Extract Fields” appear to work as expected. <br/>I am filing this ticket so we can get it added as a known issue to the release notes. <br/>The “Show Source” *Field Action* is documented [here|https://docs.splunk.com/Documentation/Splunk/9.0.4/Knowledge/Controlworkflowactionappearanceinfieldandeventmenus]. *Field Actions* - which were added to the Splunk platform around 2010 - are  actually a demonstration of a feature called [workflow actions|https://docs.splunk.com/Documentation/Splunk/9.0.4/Knowledge/CreateworkflowactionsinSplunkWeb]. [This Answers post from 2013|https://community.splunk.com/t5/Splunk-Search/no-of-events-in-show-source-view/td-p/94355] provides a bit more context about what the “Show Source” *Field Action* actually does & what endpoint it uses.
|-
| [https://splunk.atlassian.net/browse/SPL-277301 SPL-277301]
| The default sourcetype has 'NO_BINARY_CHECK' as 'true' on Web UI while it's not set in props.conf
| UI - Misc
| 10.2.x
| P3-Medium
| Bug
| Closed
| Fixed
| Reviewer-sylim, support-reviewed
| h3. Summary :<br/>My customer found that ‘NO_BINARY_CHECK’ is displayed as ‘true’ by default on Web UI for the default sourcetypes when they cloned the default sourcetype.<br/>They found, however, there is no ‘NO_BINARY_CHECK’ setting in ‘props.conf’ for the default sourcetype whilst cloned one has the setting in ‘props.conf’.<br/>h3. Customer Impact : <br/>Since the customer can set ‘NO_BINARY_CHECK' as 'true' for 'csv' if they’d like by either one of the below.<br/>* Modify ‘props.conf’<br/>* Open the sourcetype, ‘csv’, and click ‘Save’<br/>h3. Description :<br/>My customer cloned ‘csv' sourcetype as ‘csv_gear’ on Web UI by the following steps and confirmed both sourcetypes have ‘NO_BINARY_CHECK’ setting as ‘true’ on Web UI.<br/>[Steps]<br/>(1) 'Settings' -> 'Source types'<br/>(2) Click 'Clone' for 'csv', and name it as 'csv_gear'<br/>However, they found that the original sourcetype, ‘csv’ doesn’t have corresponding setting in ‘props.conf’.<br/>||*Web UI*||*props.conf*||<br/>|!csv_the_original_sourcetype.png|width=50%!|{noformat}#splunk btool props list --debug csv<br/>/opt/splunk/etc/system/default/props.conf [csv]<br/>/opt/splunk/etc/system/default/props.conf ADD_EXTRA_TIME_FIELDS = True<br/>/opt/splunk/etc/system/default/props.conf ANNOTATE_PUNCT = True<br/>/opt/splunk/etc/system/default/props.conf AUTO_KV_JSON = true<br/>/opt/splunk/etc/system/default/props.conf BREAK_ONLY_BEFORE =<br/>/opt/splunk/etc/system/default/props.conf BREAK_ONLY_BEFORE_DATE = True<br/>/opt/splunk/etc/system/default/props.conf CHARSET = UTF-8<br/>/opt/splunk/etc/system/default/props.conf DATETIME_CONFIG = /etc/datetime.xml<br/>/opt/splunk/etc/system/default/props.conf DEPTH_LIMIT = 1000<br/>/opt/splunk/etc/system/default/props.conf DETERMINE_TIMESTAMP_DATE_WITH_SYSTEM_TIME = false<br/>/opt/splunk/etc/system/default/props.conf HEADER_MODE =<br/>/opt/splunk/etc/system/default/props.conf INDEXED_EXTRACTIONS = csv<br/>/opt/splunk/etc/system/default/props.conf KV_MODE = none<br/>/opt/splunk/etc/system/default/props.conf LB_CHUNK_BREAKER_TRUNCATE = 2000000<br/>/opt/splunk/etc/system/default/props.conf LEARN_MODEL = true<br/>/opt/splunk/etc/system/default/props.conf LEARN_SOURCETYPE = true<br/>/opt/splunk/etc/system/default/props.conf LINE_BREAKER_LOOKBEHIND = 100<br/>/opt/splunk/etc/system/default/props.conf MATCH_LIMIT = 100000<br/>/opt/splunk/etc/system/default/props.conf MAX_DAYS_AGO = 2000<br/>/opt/splunk/etc/system/default/props.conf MAX_DAYS_HENCE = 2<br/>/opt/splunk/etc/system/default/props.conf MAX_DIFF_SECS_AGO = 3600<br/>/opt/splunk/etc/system/default/props.conf MAX_DIFF_SECS_HENCE = 604800<br/>/opt/splunk/etc/system/default/props.conf MAX_EVENTS = 256<br/>/opt/splunk/etc/system/default/props.conf MAX_EXPECTED_EVENT_LINES = 7<br/>/opt/splunk/etc/system/default/props.conf MAX_TIMESTAMP_LOOKAHEAD = 128<br/>/opt/splunk/etc/system/default/props.conf MUST_BREAK_AFTER =<br/>/opt/splunk/etc/system/default/props.conf MUST_NOT_BREAK_AFTER =<br/>/opt/splunk/etc/system/default/props.conf MUST_NOT_BREAK_BEFORE =<br/>/opt/splunk/etc/system/default/props.conf SEGMENTATION = indexing<br/>/opt/splunk/etc/system/default/props.conf SEGMENTATION-all = full<br/>/opt/splunk/etc/system/default/props.conf SEGMENTATION-inner = inner<br/>/opt/splunk/etc/system/default/props.conf SEGMENTATION-outer = outer<br/>/opt/splunk/etc/system/default/props.conf SEGMENTATION-raw = none<br/>/opt/splunk/etc/system/default/props.conf SEGMENTATION-standard = standard<br/>/opt/splunk/etc/system/default/props.conf SHOULD_LINEMERGE = False<br/>/opt/splunk/etc/system/default/props.conf TRANSFORMS =<br/>/opt/splunk/etc/system/default/props.conf TRUNCATE = 10000<br/>/opt/splunk/etc/system/default/props.conf category = Structured<br/>/opt/splunk/etc/system/default/props.conf description = Comma-separated value format. Set header and other settings in "Delimited Settings"<br/>/opt/splunk/etc/system/default/props.conf detect_trailing_nulls = false<br/>/opt/splunk/etc/system/default/props.conf maxDist = 100<br/>/opt/splunk/etc/system/default/props.conf priority =<br/>/opt/splunk/etc/system/default/props.conf pulldown_type = true<br/>/opt/splunk/etc/system/default/props.conf sourcetype =<br/>/opt/splunk/etc/system/default/props.conf termFrequencyWeightedDist = false<br/>/opt/splunk/etc/system/default/props.conf unarchive_cmd_start_mode = shell{noformat}|<br/>|!csv_gear_the_cloned_sourcetype-20250520-075326.png|width=587,height=580,alt="csv_gear_the_cloned_sourcetype-20250520-075326.png"!|{noformat}#splunk btool props list --debug csv_gear<br/>/opt/splunk/etc/system/local/props.conf   [csv_gear]<br/>/opt/splunk/etc/system/default/props.conf ADD_EXTRA_TIME_FIELDS = True<br/>/opt/splunk/etc/system/default/props.conf ANNOTATE_PUNCT = True<br/>/opt/splunk/etc/system/default/props.conf AUTO_KV_JSON = true<br/>/opt/splunk/etc/system/default/props.conf BREAK_ONLY_BEFORE =<br/>/opt/splunk/etc/system/default/props.conf BREAK_ONLY_BEFORE_DATE = True<br/>/opt/splunk/etc/system/default/props.conf CHARSET = UTF-8<br/>/opt/splunk/etc/system/local/props.conf   DATETIME_CONFIG =<br/>/opt/splunk/etc/system/default/props.conf DEPTH_LIMIT = 1000<br/>/opt/splunk/etc/system/default/props.conf DETERMINE_TIMESTAMP_DATE_WITH_SYSTEM_TIME = false<br/>/opt/splunk/etc/system/default/props.conf HEADER_MODE =<br/>/opt/splunk/etc/system/local/props.conf   INDEXED_EXTRACTIONS = csv<br/>/opt/splunk/etc/system/local/props.conf   KV_MODE = none<br/>/opt/splunk/etc/system/default/props.conf LB_CHUNK_BREAKER_TRUNCATE = 2000000<br/>/opt/splunk/etc/system/default/props.conf LEARN_MODEL = true<br/>/opt/splunk/etc/system/default/props.conf LEARN_SOURCETYPE = true<br/>/opt/splunk/etc/system/local/props.conf   LINE_BREAKER = ([\r\n]+)<br/>/opt/splunk/etc/system/default/props.conf LINE_BREAKER_LOOKBEHIND = 100<br/>/opt/splunk/etc/system/default/props.conf MATCH_LIMIT = 100000<br/>/opt/splunk/etc/system/default/props.conf MAX_DAYS_AGO = 2000<br/>/opt/splunk/etc/system/default/props.conf MAX_DAYS_HENCE = 2<br/>/opt/splunk/etc/system/default/props.conf MAX_DIFF_SECS_AGO = 3600<br/>/opt/splunk/etc/system/default/props.conf MAX_DIFF_SECS_HENCE = 604800<br/>/opt/splunk/etc/system/default/props.conf MAX_EVENTS = 256<br/>/opt/splunk/etc/system/default/props.conf MAX_EXPECTED_EVENT_LINES = 7<br/>/opt/splunk/etc/system/default/props.conf MAX_TIMESTAMP_LOOKAHEAD = 128<br/>/opt/splunk/etc/system/default/props.conf MUST_BREAK_AFTER =<br/>/opt/splunk/etc/system/default/props.conf MUST_NOT_BREAK_AFTER =<br/>/opt/splunk/etc/system/default/props.conf MUST_NOT_BREAK_BEFORE =<br/>/opt/splunk/etc/system/local/props.conf   NO_BINARY_CHECK = true   <---<br/>/opt/splunk/etc/system/default/props.conf SEGMENTATION = indexing<br/>/opt/splunk/etc/system/default/props.conf SEGMENTATION-all = full<br/>/opt/splunk/etc/system/default/props.conf SEGMENTATION-inner = inner<br/>/opt/splunk/etc/system/default/props.conf SEGMENTATION-outer = outer<br/>/opt/splunk/etc/system/default/props.conf SEGMENTATION-raw = none<br/>/opt/splunk/etc/system/default/props.conf SEGMENTATION-standard = standard<br/>/opt/splunk/etc/system/local/props.conf   SHOULD_LINEMERGE = false<br/>/opt/splunk/etc/system/default/props.conf TRANSFORMS =<br/>/opt/splunk/etc/system/default/props.conf TRUNCATE = 10000<br/>/opt/splunk/etc/system/local/props.conf   category = Structured<br/>/opt/splunk/etc/system/local/props.conf   description = Comma-separated value format. Set header and other settings in "Delimited Settings"<br/>/opt/splunk/etc/system/default/props.conf detect_trailing_nulls = false<br/>/opt/splunk/etc/system/default/props.conf maxDist = 100<br/>/opt/splunk/etc/system/default/props.conf priority =<br/>/opt/splunk/etc/system/local/props.conf   pulldown_type = 1<br/>/opt/splunk/etc/system/default/props.conf sourcetype =<br/>/opt/splunk/etc/system/default/props.conf termFrequencyWeightedDist = false<br/>/opt/splunk/etc/system/default/props.conf unarchive_cmd_start_mode = shell{noformat}|<br/>h3. Quetions : <br/>Based on the behaviour, I think this is a cosmetic issue.<br/>However, since this is confusing the customer, would you investigate and fix this if possible, please?<br/>h3. Problem Analysis :<br/>(1) Confirm this behaviour on the latest version, 9.4.2 and old version, 8.0.6 as well as the version the customer uses, 9.2.1.<br/>(2) Confirmed that the default value for ‘NO_BINARY_CHECK’ is ‘false’ in the doc.<br/>[https://docs.splunk.com/Documentation/Splunk/latest/Admin/Propsconf|https://docs.splunk.com/Documentation/Splunk/latest/Admin/Propsconf|smart-link] <br/>{noformat}NO_BINARY_CHECK = <boolean><br/>* When set to true, Splunk software processes binary files.<br/>* Can only be used on the basis of [<sourcetype>], or [source::<source>],<br/>  not [host::<host>].<br/>* Default: false (binary files are ignored).<br/>* This setting applies at input time, when data is first read by Splunk<br/>  software, such as on a forwarder that has configured inputs acquiring the<br/>  data.{noformat}<br/>(3) Confirmed that the default setting, ‘false’, is applied to the original sourcetype, ‘csv’ as a binary file isn’t processed.<br/><inputs.conf><br/>{noformat}[monitor:///data/test*]<br/>_rcvbuf = 1572864<br/>disabled = false<br/>index = main<br/>sourcetype = csv{noformat}<br/><splunkd.log><br/>{noformat}05-20-2025 08:25:33.654 +0000 INFO  TailReader [18524 tailreader0] - Ignoring file '/data/testing' due to: binary{noformat}<br/>(4) Checked ‘Network’ on ‘Developer tools’ when I clicked the sourcetype, ‘csv’, on ‘Source Types’, however I don’t see any.<br/>!Developer_Tools.png|width=100%,alt="Developer_Tools.png"!<br/>(5) Also, I don’t see any related logs in ‘splunkd_access.log’ and 'web_access.log' when I clicked the sourcetype, ‘csv’ on ‘Source Types’.<br/>(6) Found ‘NO_BINARY_CHECK’ in ‘common.js’ on Sources on ‘Source Types’ page.<br/>    I don’t have any understanding about Java Script, but it looks like something is done around ‘transposeFromPropsToUI’ and 'transposeFromUIToProps'.<br/>{noformat}                transposeFromPropsToUI: function(e) {<br/>                    var n = {};<br/>                    return !e || t.isEmpty(e) || ("CURRENT" === e.DATETIME_CONFIG ? n["ui.timestamp.mode"] = "current" : -1 !== String(e.DATETIME_CONFIG).indexOf(".xml") && "/etc/datetime.xml" !== e.DATETIME_CONFIG ? (n["ui.timestamp.mode"] = "filename",<br/>                    n["ui.timestamp.filename"] = e.DATETIME_CONFIG) : e.TIME_FORMAT || e.TIME_PREFIX || e.TZ || e.TIMESTAMP_FIELDS || e.MAX_TIMESTAMP_LOOKAHEAD && e.MAX_TIMESTAMP_LOOKAHEAD !== this.defaults["ui.timestamp.lookahead"] ? (n["ui.timestamp.mode"] = "advanced",<br/>                    (e.TIME_FORMAT || "" === e.TIME_FORMAT) && (n["ui.timestamp.format"] = e.TIME_FORMAT),<br/>                    (e.TIMESTAMP_FIELDS || "" === e.TIMESTAMP_FIELDS) && (n["ui.timestamp.fields"] = e.TIMESTAMP_FIELDS),<br/>                    (e.TIME_PREFIX || "" === e.TIME_PREFIX) && (n["ui.timestamp.prefix"] = e.TIME_PREFIX),<br/>                    (e.TZ || "" === e.TZ) && (n["ui.timestamp.timezone"] = e.TZ),<br/>                    e.MAX_TIMESTAMP_LOOKAHEAD && e.MAX_TIMESTAMP_LOOKAHEAD !== this.defaults["ui.timestamp.lookahead"] && (n["ui.timestamp.lookahead"] = e.MAX_TIMESTAMP_LOOKAHEAD)) : n["ui.timestamp.mode"] = "auto",<br/>                    !1 === e.SHOULD_LINEMERGE || "false" === (e.SHOULD_LINEMERGE + "" || "").toLowerCase() ? e.LINE_BREAKER ? (n["ui.eventbreak.mode"] = "regex",<br/>                    n["ui.eventbreak.regexmode"] = "linebreaker",<br/>                    n["ui.eventbreak.regex"] = e.LINE_BREAKER) : n["ui.eventbreak.mode"] = "everyline" : e.BREAK_ONLY_BEFORE ? (n["ui.eventbreak.mode"] = "regex",<br/>                    n["ui.eventbreak.regexmode"] = "before",<br/>                    n["ui.eventbreak.regex"] = e.BREAK_ONLY_BEFORE) : e.MUST_BREAK_AFTER ? (n["ui.eventbreak.mode"] = "regex",<br/>                    n["ui.eventbreak.regexmode"] = "after",<br/>                    n["ui.eventbreak.regex"] = e.MUST_BREAK_AFTER) : n["ui.eventbreak.mode"] = "auto",<br/>                    "PREAMBLE_REGEX"in e && (n["ui.structured.preamble_pattern"] = e.PREAMBLE_REGEX),<br/>                    "FIELD_HEADER_REGEX"in e && (n["ui.structured.header_line_prefix"] = e.FIELD_HEADER_REGEX,<br/>                    t.isEmpty(e.FIELD_HEADER_REGEX) || (n["ui.structured.header_mode"] = "regex")),<br/>                    "HEADER_FIELD_LINE_NUMBER"in e && (n["ui.structured.header_line_number"] = e.HEADER_FIELD_LINE_NUMBER,<br/>                    t.isEmpty(e.HEADER_FIELD_LINE_NUMBER) || (n["ui.structured.header_mode"] = "line")),<br/>                    "HEADER_FIELD_DELIMITER"in e && (n["ui.structured.header_field_delimiter"] = e.HEADER_FIELD_DELIMITER),<br/>                    "HEADER_FIELD_QUOTE"in e && (n["ui.structured.header_field_quote"] = e.HEADER_FIELD_QUOTE),<br/>                    "FIELD_NAMES"in e && (n["ui.structured.header_fields"] = e.FIELD_NAMES,<br/>                    t.isEmpty(e.FIELD_NAMES) || (n["ui.structured.header_mode"] = "custom")),<br/>                    "FIELD_DELIMITER"in e && (n["ui.structured.event_field_delimiter"] = e.FIELD_DELIMITER),<br/>                    "FIELD_QUOTE"in e && (n["ui.structured.event_field_quote"] = e.FIELD_QUOTE),<br/>                    "METRIC-SCHEMA-TRANSFORMS"in e && (n["ui.metric_transforms.schema_name"] = e["METRIC-SCHEMA-TRANSFORMS"]),<br/>                    "NO_BINARY_CHECK"in e && (n["ui.misc.process_binary_files"] = e.NO_BINARY_CHECK),<br/>                    e.INDEXED_EXTRACTIONS ? n["ui.misc.previous_indexed_extractions"] = !0 : e["TRANSFORMS-EXTRACT"] && (this.entry.content.set("INDEXED_EXTRACTIONS", e["TRANSFORMS-EXTRACT"]),<br/>                    n["ui.misc.previous_indexed_extractions"] = !0)),<br/>                    n<br/>                },<br/>                transposeFromUIToProps: function(e) {<br/>                    var n = this<br/>                      , i = {};<br/>                    if (!e || t.isEmpty(e))<br/>                        return i;<br/>                    function r(e) {<br/>                        var t = n.entry.content.get(e);<br/>                        null != t && (i[e] = "")<br/>                    }<br/>                    function o() {<br/>                        r("TZ"),<br/>                        r("TIME_FORMAT"),<br/>                        r("TIME_PREFIX"),<br/>                        r("TIMESTAMP_FIELDS"),<br/>                        n.defaults["ui.timestamp.lookahead"] !== n.entry.content.get("MAX_TIMESTAMP_LOOKAHEAD") && r("MAX_TIMESTAMP_LOOKAHEAD")<br/>                    }<br/>                    if ("ui.timestamp.mode"in e && ("current" === e["ui.timestamp.mode"] ? (i.DATETIME_CONFIG = "CURRENT",<br/>                    o()) : "advanced" === e["ui.timestamp.mode"] ? (i.DATETIME_CONFIG = "",<br/>                    i.TIME_FORMAT = e["ui.timestamp.format"],<br/>                    i.TIME_PREFIX = e["ui.timestamp.prefix"],<br/>                    i.TZ = e["ui.timestamp.timezone"],<br/>                    i.MAX_TIMESTAMP_LOOKAHEAD = e["ui.timestamp.lookahead"],<br/>                    i.TIMESTAMP_FIELDS = e["ui.timestamp.fields"]) : "auto" === e["ui.timestamp.mode"] ? (i.DATETIME_CONFIG = "",<br/>                    o()) : "filename" === e["ui.timestamp.mode"] && (i.DATETIME_CONFIG = e["ui.timestamp.filename"])),<br/>                    "ui.eventbreak.mode"in e)<br/>                        if ("everyline" === e["ui.eventbreak.mode"])<br/>                            i.SHOULD_LINEMERGE = !1,<br/>                            i.LINE_BREAKER = "([\\r\\n]+)";<br/>                        else if ("regex" === e["ui.eventbreak.mode"])<br/>                            i.SHOULD_LINEMERGE = "linebreaker" !== e["ui.eventbreak.regexmode"],<br/>                            i.LINE_BREAKER = e["ui.eventbreak.regex"],<br/>                            r("BREAK_ONLY_BEFORE_DATE");<br/>                        else {<br/>                            i.SHOULD_LINEMERGE = !0;<br/>                            const e = this.entry.content.get("BREAK_ONLY_BEFORE")<br/>                              , t = this.entry.content.get("MUST_BREAK_AFTER")<br/>                              , n = this.entry.content.get("LINE_BREAKER");<br/>                            i.LINE_BREAKER = e || t || n || "([\\r\\n]+)",<br/>                            i.BREAK_ONLY_BEFORE_DATE = !0<br/>                        }             <br/>                    return "ui.structured.preamble_pattern"in e && !t.isUndefined(e["ui.structured.preamble_pattern"]) && (i.PREAMBLE_REGEX = e["ui.structured.preamble_pattern"]),<br/>                    "ui.structured.header_line_prefix"in e && !t.isUndefined(e["ui.structured.header_line_prefix"]) && (i.FIELD_HEADER_REGEX = e["ui.structured.header_line_prefix"]),<br/>                    "ui.structured.header_line_number"in e && !t.isUndefined(e["ui.structured.header_line_number"]) && (i.HEADER_FIELD_LINE_NUMBER = e["ui.structured.header_line_number"]),<br/>                    "ui.structured.header_fields"in e && !t.isUndefined(e["ui.structured.header_fields"]) && (i.FIELD_NAMES = e["ui.structured.header_fields"]),<br/>                    "ui.structured.header_field_delimiter"in e && !t.isUndefined(e["ui.structured.header_field_delimiter"]) && (i.HEADER_FIELD_DELIMITER = e["ui.structured.header_field_delimiter"]),<br/>                    "ui.structured.header_field_quote"in e && !t.isUndefined(e["ui.structured.header_field_quote"]) && (i.HEADER_FIELD_QUOTE = e["ui.structured.header_field_quote"]),<br/>                    "ui.structured.event_field_delimiter"in e && !t.isUndefined(e["ui.structured.event_field_delimiter"]) && (i.FIELD_DELIMITER = e["ui.structured.event_field_delimiter"]),<br/>                    "ui.structured.event_field_quote"in e && !t.isUndefined(e["ui.structured.event_field_quote"]) && (i.FIELD_QUOTE = e["ui.structured.event_field_quote"]),<br/>                    "ui.metric_transforms.schema_name"in e && !t.isUndefined(e["ui.metric_transforms.schema_name"]) && (i["METRIC-SCHEMA-TRANSFORMS"] = e["ui.metric_transforms.schema_name"]),<br/>                    "ui.misc.process_binary_files"in e && !t.isUndefined(e["ui.misc.process_binary_files"]) && (i.NO_BINARY_CHECK = e["ui.misc.process_binary_files"]),<br/>                    i<br/>                },{noformat}<br/>(7) It seems like ‘transposeFromPropsToUI’ and ‘transposeFromUIToProps' are defined in 'sourcetype.js’.<br/>[https://cd.splunkdev.com/eui/splunkcore-web-ui/-/blob/develop/web/search_mrsparkle/exposed/js/models/knowledgeobjects/Sourcetype.js|https://cd.splunkdev.com/eui/splunkcore-web-ui/-/blob/develop/web/search_mrsparkle/exposed/js/models/knowledgeobjects/Sourcetype.js]<br/>[https://cd.splunkdev.com/eui/splunkcore-web-ui/-/blob/develop/web_v2/search_mrsparkle/exposed/js/models/knowledgeobjects/Sourcetype.js|https://cd.splunkdev.com/eui/splunkcore-web-ui/-/blob/develop/web_v2/search_mrsparkle/exposed/js/models/knowledgeobjects/Sourcetype.js]<br/>{noformat}            /*<br/>             * Transposition helper methods to convert from/to ui namespace<br/>             ****************************************************************<br/>             */<br/>            transposeFromPropsToUI: function (props) {<br/>                var attr = {};<br/>                if (!props || _.isEmpty(props)) {<br/>                    return attr;<br/>                }<br/>...<br/>                // 6) misc<br/>                if ('NO_BINARY_CHECK' in props) {<br/>                    attr['ui.misc.process_binary_files'] = props.NO_BINARY_CHECK;<br/>                }<br/>...<br/>                return attr;<br/>            },<br/>            transposeFromUIToProps: function (uiAttrs) {<br/>                var self = this;<br/>                var props = {};<br/>                if (!uiAttrs || _.isEmpty(uiAttrs)) {<br/>                    return props;<br/>                }<br/>...<br/>                // 5) misc<br/>                if (<br/>                    'ui.misc.process_binary_files' in uiAttrs &&<br/>                    !_.isUndefined(uiAttrs['ui.misc.process_binary_files'])<br/>                ) {<br/>                    props.NO_BINARY_CHECK = uiAttrs['ui.misc.process_binary_files'];<br/>                }<br/>                return props;<br/>            },{noformat}<br/>(8) In “sourcetype.js', I see the following, which might be the reason why ‘NO_BINARY_CHECK’ is displayed as ‘true’.<br/>{noformat}            defaults: {<br/>                // witness if INDEXED_EXTRACTIONS has been set previously<br/>                // must be an attribute so it will be present in any model clones<br/>                'ui.misc.previous_indexed_extractions': false,<br/>                'ui.misc.process_binary_files': true, // NO_BINARY_CHECK = true <---<br/>                'ui.timestamp.mode': 'auto', // possible values: auto/current/advanced<br/>                'ui.timestamp.format': undefined,<br/>                'ui.timestamp.prefix': undefined,<br/>                'ui.timestamp.timezone': undefined,<br/>                'ui.timestamp.lookahead': 128,<br/>                'ui.eventbreak.mode': 'auto', // possible values: auto/everyline/regex<br/>                'ui.eventbreak.regexmode': 'linebreaker', // possible values: before/after/linebreaker<br/>                'ui.eventbreak.regex': undefined,<br/>                'ui.structured.preamble_pattern': undefined,<br/>                'ui.structured.header_mode': 'auto',<br/>                'ui.metric_transforms.schema_name': undefined,<br/>                'ui.structured.header_line_prefix': undefined,<br/>                'ui.structured.header_line_number': undefined,<br/>                'ui.structured.header_field_delimiter': undefined,<br/>                'ui.structured.header_field_quote': undefined,<br/>                'ui.structured.header_fields': undefined,<br/>                'ui.structured.event_field_delimiter': undefined,<br/>                'ui.structured.event_field_quote': undefined,<br/>                'ui.structured.timestamp_fields': undefined,<br/>                name: '',<br/>            },<br/>{noformat}<br/>(9) Went through our database, Jira, SFDC, Slack, KB, strangely, nobody raised this before.<br/>h3. Workaround :<br/>The customer can set ‘NO_BINARY_CHECK' as 'true' for 'csv' if they’d like by either one of the below.<br/>* Modify ‘props.conf’<br/>* Open the sourcetype, ‘csv’, and click ‘Save’<br/>h3. Information :<br/>diag-so1-2025-05-21_07-16-10.tar.gz … diag file from our in-house environment.
|}

== develop ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-279657 SPL-279657]
| Distributed search may lose information about bundles from cluster peers on new search generations, removes peers when best-effort mode is enabled.
| Distributed Search
| 10.0.x, 9.2.8, 9.3.6, 9.4.4, develop, Nutella(9.2.2406.107), Oreo, Pocky
| P1-Immediate
| Bug
| Closed
| Fixed
| Backport_Approved
| When we add cluster peers after a search generation change, we try to prime their information (including available bundles) before exposing the information to search. [https://splunk.atlassian.net/browse/SPL-216146|https://splunk.atlassian.net/browse/SPL-216146|smart-link] introduced a regression through which, by mistake, that information is being exposed prematurely, with no bundles available. This will cause a short pause in search requests when best-effort mode is disabled, as the search head blocks search while it refreshes its bundle information, but with best-effort enabled we’ll see incomplete results and warnings straight away.<br/>In the codebase, what happens is that {{DistributedPeerManager::prepClusterPeersForPotentialAdd()}}relies on its local {{HeartbeatTransactionSynchronizer}} collecting peer information before it publishes it all, but [commit d6cf0fbdf2bd changed the auth transaction such that it will always update|https://cd.splunkdev.com/splcore/main/-/commit/d6cf0fbdf2bd#d700c0d60e4cdc6ba2d58fdd1287c87e00c09884_279_287] the {{DistributedPeerManager}} directly.<br/>Found through code inspection while investigating [https://splunk.atlassian.net/browse/SPL-268481|https://splunk.atlassian.net/browse/SPL-268481|smart-link].
|}

== Lindt_Tock(9.1.2312.200) ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-242301 SPL-242301]
| [PUBLIC] [DMA] The UI trigger for summary rebuild doesn't work for some accelerated data models that have no root-event dataset and have a reporting command in first root search dataset
| Data Model - UI, Search - Datamodel Acceleration
| Lindt_Tock(9.1.2312.200)
| P3-Medium
| Bug
| Untriaged
| &mdash;
| cobalt-defer-accept, juicyfruit-defer-accept, kitkat-defer-accept
| The Data Models that do not have any root dataset of {{event}} type and have more than one root datasets of {{search}} type with the first root search dataset having a reporting command in the {{baseSearch}} , would lead to this issue.<br/>Steps to reproduce:<br/># Create a Data Model that has multiple root search datasets and the first root search dataset having a reporting command. You can create one by uploading the provided JSON ({{test_internal_audit_logs.json)}}<br/>!Screenshot 2023-07-21 at 11.56.28 AM.png|width=1904,height=743!<br/># Accelerate the Data Model. Wait for it to reach 100% completion<br/># Do a "rebuild" trigger on that Data Model through web-ui<br/># The “rebuild” trigger fails with the error below<br/>!Screenshot 2023-07-21 at 11.57.53 AM.png|width=699,height=508!<br/>The reason why above happens is explained below:<br/>For instance, above Data Model {{test_internal_audit_logs}} has two root search datasets:<br/># fully_completed_searches<br/># failed_searches<br/>The Data Model ID(s) in order for the above two datasets using nomenclature {{DM_<app>_<datamodel>.<object_id>}}<br/>will be in the same order:<br/># DM_search_test_internal_audit_logs<br/># DM_search_test_internal_audit_logs.failed_searches<br/>Notice above that the first root search dataset doesn’t have the object-id ({{fully_completed_searches}})<br/>Now, the Web-UI Rebuild trigger issues a {{DELETE}} on the very first root-event or root-search dataset. The first root dataset cannot be accelerated since it has a reporting command in baseSearch constraint:<br/>{noformat}"baseSearch": "index=\"_audit\" action=\"search\" info=\"completed\" fully_completed_search=\"true\" | stats count BY action, event_count, scan_count, result_count, search_et, search_lt, exec_time, host, search_type"{noformat}<br/>It can be seen by issuing a {{DELETE}} using curl like below:<br/>{noformat}curl -ku "admin" -X DELETE https://127.0.0.1:8089/services/admin/summarization/tstats:DM_search_test_internal_audit_logs?count=0<br/>Enter host password for user 'admin':<br/><?xml version="1.0" encoding="UTF-8"?><br/><response><br/>  <messages><br/>    <msg type="ERROR">Cannot find saved search for summary id=DM_search_test_internal_audit_logs.</msg><br/>  </messages><br/></response>{noformat}<br/>Now, the second dataset which is actually accelerated can’t be rebuild from Web UI and needs to be done through curl request below:<br/>{noformat}curl -ku "admin" -X DELETE https://127.0.0.1:8089/services/admin/summarization/tstats:DM_search_test_internal_audit_logs.failed_searches?count=0<br/>{noformat}
|}

== Nutella ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-238421 SPL-238421]
| [Schwab/CapOne/Itau Bank]scheduler performance issues with high skipped search (user-prefs.conf contention for timezone)
| Search - Scheduler
| Nutella
| P2-High
| Bug
| Resolved
| Fixed
| COMP_SEARCH_SCHEDULER, sinfra:cos:slow_conf_cache, sinfra:sx:skipped_searches, SPLUNK_CONF
| Schwab has been struggling with scheduler performance issues for some time now. We have taken several steps to try and improve the skip/deferred searches however we have reached a wall.<br/>*Summary:* <br/>* SHC members resource usage looks normal (mem,cpu,disk latency)<br/>* IDXC members resource usage looks normal (mem,cpu,disk latency)<br/>* 10,112 savedsearches enabled<br/>* scheduled search distribution high every 15m peaking at ~7200 at top of hour<br/>* captain computed concurrency peaks for ~10m every 15m where we see high amount of skipped/deferred search<br/>* clusterwide scheduler concurrency limit:1785<br/>* member delegatejob endpoint mostly below 10s<br/>{noformat}source=*/splunkd_access.log /shcluster/member/delegatejob/   | timechart max(spent) as spent_ms by host{noformat}<br/>* also see high skip ratio of 50% during RR of the SHC<br/>*Data Collected:*<br/>started captain pstacks on a 15m "hot minute" start time:<br/>[https://downloadsvc.splunk.com/download/splunk/04-05-2023/uploadsvc-46case3156796-04-05-2023-USER-0034000000p9F9OAAU-stacks-108269-s3147cdc-2023-04-05T12h30m22s951182096ns-0400.tar.xz|https://downloadsvc.splunk.com/download/splunk/04-05-2023/uploadsvc-46case3156796-04-05-2023-USER-0034000000p9F9OAAU-stacks-108269-s3147cdc-2023-04-05T12h30m22s951182096ns-0400.tar.xz]<br/>!schwab_flamegraph.svg|width=1200,height=550!<br/>diag of captain: <br/>[https://downloadsvc.splunk.com/download/splunk/04-05-2023/uploadsvc-44case3156796-04-05-2023-USER-0034000000p9F9OAAU-diag-s3147cdc-2023-04-05_13-08-46.tar.gz|https://downloadsvc.splunk.com/download/splunk/04-05-2023/uploadsvc-44case3156796-04-05-2023-USER-0034000000p9F9OAAU-diag-s3147cdc-2023-04-05_13-08-46.tar.gz]<br/>We see a perf bottle neck of scheduler going into user-prefs.conf conf system to get the timezone to be user for a search<br/>!image-20230406-235443.png|width=1191,height=373!<br/>this same code path is hit by auth, and many other places, <br/>!image-20230406-235515.png|width=1189,height=537!<br/>and thus creates major amounts skips , and greatly slows down scheduler.<br/>they have over 12,000 users, and nearly all of them have timezone preferences <br/>{noformat}96/s3147cdc-sh_-20230406-181638-2QlpSfR3/etc/users$ find . -name user-prefs.conf | grep local | wc -l<br/>12023{noformat}
|}

== Nutella(9.2.2406.107) ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-279657 SPL-279657]
| Distributed search may lose information about bundles from cluster peers on new search generations, removes peers when best-effort mode is enabled.
| Distributed Search
| 10.0.x, 9.2.8, 9.3.6, 9.4.4, develop, Nutella(9.2.2406.107), Oreo, Pocky
| P1-Immediate
| Bug
| Closed
| Fixed
| Backport_Approved
| When we add cluster peers after a search generation change, we try to prime their information (including available bundles) before exposing the information to search. [https://splunk.atlassian.net/browse/SPL-216146|https://splunk.atlassian.net/browse/SPL-216146|smart-link] introduced a regression through which, by mistake, that information is being exposed prematurely, with no bundles available. This will cause a short pause in search requests when best-effort mode is disabled, as the search head blocks search while it refreshes its bundle information, but with best-effort enabled we’ll see incomplete results and warnings straight away.<br/>In the codebase, what happens is that {{DistributedPeerManager::prepClusterPeersForPotentialAdd()}}relies on its local {{HeartbeatTransactionSynchronizer}} collecting peer information before it publishes it all, but [commit d6cf0fbdf2bd changed the auth transaction such that it will always update|https://cd.splunkdev.com/splcore/main/-/commit/d6cf0fbdf2bd#d700c0d60e4cdc6ba2d58fdd1287c87e00c09884_279_287] the {{DistributedPeerManager}} directly.<br/>Found through code inspection while investigating [https://splunk.atlassian.net/browse/SPL-268481|https://splunk.atlassian.net/browse/SPL-268481|smart-link].
|}

== Oreo ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-279657 SPL-279657]
| Distributed search may lose information about bundles from cluster peers on new search generations, removes peers when best-effort mode is enabled.
| Distributed Search
| 10.0.x, 9.2.8, 9.3.6, 9.4.4, develop, Nutella(9.2.2406.107), Oreo, Pocky
| P1-Immediate
| Bug
| Closed
| Fixed
| Backport_Approved
| When we add cluster peers after a search generation change, we try to prime their information (including available bundles) before exposing the information to search. [https://splunk.atlassian.net/browse/SPL-216146|https://splunk.atlassian.net/browse/SPL-216146|smart-link] introduced a regression through which, by mistake, that information is being exposed prematurely, with no bundles available. This will cause a short pause in search requests when best-effort mode is disabled, as the search head blocks search while it refreshes its bundle information, but with best-effort enabled we’ll see incomplete results and warnings straight away.<br/>In the codebase, what happens is that {{DistributedPeerManager::prepClusterPeersForPotentialAdd()}}relies on its local {{HeartbeatTransactionSynchronizer}} collecting peer information before it publishes it all, but [commit d6cf0fbdf2bd changed the auth transaction such that it will always update|https://cd.splunkdev.com/splcore/main/-/commit/d6cf0fbdf2bd#d700c0d60e4cdc6ba2d58fdd1287c87e00c09884_279_287] the {{DistributedPeerManager}} directly.<br/>Found through code inspection while investigating [https://splunk.atlassian.net/browse/SPL-268481|https://splunk.atlassian.net/browse/SPL-268481|smart-link].
|}

== Pocky ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-279657 SPL-279657]
| Distributed search may lose information about bundles from cluster peers on new search generations, removes peers when best-effort mode is enabled.
| Distributed Search
| 10.0.x, 9.2.8, 9.3.6, 9.4.4, develop, Nutella(9.2.2406.107), Oreo, Pocky
| P1-Immediate
| Bug
| Closed
| Fixed
| Backport_Approved
| When we add cluster peers after a search generation change, we try to prime their information (including available bundles) before exposing the information to search. [https://splunk.atlassian.net/browse/SPL-216146|https://splunk.atlassian.net/browse/SPL-216146|smart-link] introduced a regression through which, by mistake, that information is being exposed prematurely, with no bundles available. This will cause a short pause in search requests when best-effort mode is disabled, as the search head blocks search while it refreshes its bundle information, but with best-effort enabled we’ll see incomplete results and warnings straight away.<br/>In the codebase, what happens is that {{DistributedPeerManager::prepClusterPeersForPotentialAdd()}}relies on its local {{HeartbeatTransactionSynchronizer}} collecting peer information before it publishes it all, but [commit d6cf0fbdf2bd changed the auth transaction such that it will always update|https://cd.splunkdev.com/splcore/main/-/commit/d6cf0fbdf2bd#d700c0d60e4cdc6ba2d58fdd1287c87e00c09884_279_287] the {{DistributedPeerManager}} directly.<br/>Found through code inspection while investigating [https://splunk.atlassian.net/browse/SPL-268481|https://splunk.atlassian.net/browse/SPL-268481|smart-link].
|-
| [https://splunk.atlassian.net/browse/SPL-262543 SPL-262543]
| CLONE - telemetry-metric local endpoint getting 401 errors
| Search - Telemetry
| 10.0.x, Pocky
| P3-Medium
| Bug
| Closed
| Fixed
| enterprise-ui
| Customer is reporting 401s from the /telemetry-metric endpoint with {{/servicesNS/None/None/telemetry-metric}} appearing in the splunkd logsInvestigation:<br/>* expanded the time window for the search of the warning logs and the partial results show that this issue is happening before lindt (confirmed up to kitkat thus far)<br/>* upon further investigation of the logs, the routes with "issues" are later passing with a new sessionkey, meaning that this is not a route specific issue but a sessionkey issue<br/>* when there is an invalid session key, the parameters app and owner have the appropriate values meaning the UI is properly passing the parameters to the endpoint<br/>* not sure why {{None}} is being logged on the splunkd side but I would assume the backend is overriding it off of the session key?<br/>Ask: We need assistance from SANDI Team to investigate the 401 errors.<br/>h1. Summary<br/>Increased 401 responses from the /telemetry-metric endpoint due to missing app and namespace ({{/servicesNS/None/None/telemetry-metric}}).<br/>* The telemetry is non-critical and does not affect any product/feature functionality<br/>No workarounds
|}

== Q release ==
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-239436 SPL-239436]
| [PUBLIC]  In standard mode federated search, outputlookup existence check on RSH causes search to terminate early although it is not run on RSH
| Search Federation
| 10.0.2503.x, Q release
| P2-High
| Bug
| Closed
| Fixed
| &mdash;
| When a command like {{search index=index_df_1 | outputlookup iplocation.csv}} executed, the {{outputlookup }}command gets sent to the remote search head although it is executed in phase 1 locally.  If the remote search head does not have this {{lookup}} defined, the RSH will error out first before executing the search, returning 0 result to FSH.
|}

== Unscheduled ==
Tickets below do not yet have a scheduled fix version.
{| class="wikitable sortable"
! Ticket !! Summary !! Components !! Fix version(s) !! Priority !! Issue type !! Status !! Resolution !! Labels !! Notes
|-
| [https://splunk.atlassian.net/browse/SPL-174406 SPL-174406]
| [PUBLIC] [systemd] Root unable to run splunk cli if SPLUNK_OS_USER is set
| Admin - CLI
| Unscheduled
| P2-High
| Bug
| Untriaged
| &mdash;
| cftt_reviewed, oper_bug_bash
| Similar issue as SPL-109254 <br/>To reproduce:<br/>- enable boot-start as the user 'nginx'  splunkforwarder/bin/splunk help enable boot-start -user nginx -systemd-managed 1<br/>- systemctl start SplunkForwarder<br/>- log in to root<br/>- splunkforwarder/bin/splunk list monitor<br/>Splunk will throw an error stating permission denied.<br/>I would like to run splunkforwarder as low privileged user nginx and monitor /var/log/nginx logs, it is is very inconvenient of unable to run splunk under root, for example<br/>- switch to nginx user to run splunk command<br/>- switch to root to restart splunk<br/>- config does not work, has to switch to nginx user run splunk command agin<br/>- switch to root to restrt splunk<br/>Note: nginx does not have sudo privileges
|-
| [https://splunk.atlassian.net/browse/SPL-146802 SPL-146802]
| [PUBLIC] Distributed environment requires index defined on search head for log event alerts
| Alerting, Search - Distributed
| Unscheduled
| P2-High
| Bug
| Reopened
| &mdash;
| alerting, distributedsearch, SUST_TC_SKIP, workgroupC
| It seems that when a customer with a distributed search environment tries to set up a log event alert action, they must specify the path of the index that lives on the search head. This is the case even when they have set up their environment correctly (i.e. they have turned off indexing on the search head - I've confirmed this with the customer). Here's a summary from Answers:<br/>{color:#d04437}The summarized data forwarding to indexers works fine. Other internal logs forwarding to indexers work fine. But, the Alert Action Log event alone fails.{color}<br/>{color:#d04437}For this to work -- We have to define index in Search Head as well.{color}<br/>{color:#d04437}Note: the data wont be stored on Search Head, eventually the Alert Action's Log event will be forwarded to indexer. But the definition on SH is must.{color}<br/>{color:#d04437}Not sure if it is a bug in Splunk or it is working as expected.{color}<br/>{color:#d04437}I have added a comment in Alert documentation and a discussion is on with splunk folks :){color}
|-
| [https://splunk.atlassian.net/browse/SPL-177447 SPL-177447]
| [PUBLIC]  [Cascading Replication] Bundle replication takes longer than expected time for indexers that have bundleEnforcerBlacklist configured
| Bundle Replication Enhancement
| Unscheduled
| P2-High
| Bug
| New
| &mdash;
| caused_flakiness, quake_punt_ok, quake_systest_preqac, requires-triage, systest_quake
| Steps to reproduce:<br/> # set a distribute search(3 SHC, 7 Indexers IDC, 1 standalone indexer) with cascading replication<br/> # create a kv-store based lookup<br/> # use a script to make a kv store collection data keep updating<br/> # create a scheduled savesearch to run a search with the lookup every 1 minutes<br/> # check the bundle status on all the peers, you can find the bundle replication every 2 mins<br/> # change the standalone indexer's bundleEnforcerBlacklist to block the bundle replication on this peer.<br/> # check the result.<br/> <br/>Result:<br/> # the bundle no longer replicate to the standalone indexer, since the bundle is blacklisted<br/> # check the splunk log on the standalone indexer, it tells *WARN*.<br/>{code:java}<br/>10-03-2019 18:37:48.080 +0800 WARN  CascadingUploadHandler - Failed to apply delta for search-head=A513C830-58BC-45B4-A236-88A64F695C6D with filePath=/root/splunk/var/run/searchpeers/A513C830-58BC-45B4-A236-88A64F695C6D-1570098912-1570099031.delta and currCheckSum=12928352177236211333 and currTimestamp=1570099031 and prevChecksum=6689478038752077008 and prevTimestamp=1570098912 and exception=File apps/search_app/local/collections.conf in knowledge bundle is either not in white list or else excluded by black list. Bundle /root/splunk/var/run/searchpeers/A513C830-58BC-45B4-A236-88A64F695C6D-1570099031.1d94e88d20c6ce0d.tmp will be removed.<br/>{code}<br/> # check the splunkd log on the SH, it tells *Error* like bellow<br/>{code:java}<br/>10-03-2019 04:55:34.405 -0700 ERROR CascadePlan - planId=6EE094C0-1666-4075-8401-212AA32DE0DF state updated to payload_send_failed.{code}<br/> # Check the replication status on SH, it tells "apply_failed" on that peer<br/> # Check the bundle status on the other peers, the replication period extend from 2 mins to 18 mins, because the SH will retry to replication the bundle to the failed standalone indexer.<br/>Actually, the bundleEnforcerBlacklist is a feature, it should not be treated directly as a failure on Search head, so there should not retry and block the next replication during that period. If there are more than 1 blacklisted indexers, the period might be even longer.<br/> <br/>And anther point is, when fixing this issue, in cascading mode,  please consider that the blacklisted indexer also have possibility to be chosen as an "receiver" and then behave as  a "sender"
|-
| [https://splunk.atlassian.net/browse/SPL-192792 SPL-192792]
| [PUBLIC] tsidxWritingLevel and other fields are set empty after updating index in UI
| Conf System, REST API
| Unscheduled
| P2-High
| Bug
| Untriaged
| &mdash;
| &mdash;
| build with commit: 561bedfc8524<br/>1; create a metric index with Timestamp Resolution = S<br/>in index.conf<br/>{noformat}<br/>[second_metric]<br/>coldPath = $SPLUNK_DB/second_metric/colddb<br/>datatype = metric<br/>enableDataIntegrityControl = 0<br/>enableTsidxReduction = 0<br/>homePath = $SPLUNK_DB/second_metric/db<br/>maxTotalDataSizeMB = 512000<br/>thawedPath = $SPLUNK_DB/second_metric/thaweddb{noformat}<br/>run query: | rest /services/configs/conf-indexes | fields title, tsidxWritingLevel <br/>the tsidxWritingLevel  of second_metric is 1<br/> <br/>2, edit index, update Timestamp Resolution to the millisecond <br/>in index.conf<br/>{noformat}<br/>[second_metric]<br/>coldPath = $SPLUNK_DB/second_metric/colddb<br/>datatype = metric<br/>enableDataIntegrityControl = 0<br/>enableTsidxReduction = 0<br/>homePath = $SPLUNK_DB/second_metric/db<br/>maxTotalDataSizeMB = 512000<br/>thawedPath = $SPLUNK_DB/second_metric/thaweddb<br/>archiver.enableDataArchive = 0<br/>bucketRebuildMemoryHint = 0<br/>compressRawdata = 1<br/>enableOnlineBucketRepair = 1<br/>hotBucketStreaming.deleteHotsAfterRestart = 0<br/>hotBucketStreaming.removeRemoteSlicesOnRoll = 0<br/>hotBucketStreaming.reportStatus = 0<br/>hotBucketStreaming.sendSlices = 0<br/>metric.enableFloatingPointCompression = 1<br/>metric.stubOutRawdataJournal = 1<br/>metric.timestampResolution = ms<br/>minHotIdleSecsBeforeForceRoll = 0<br/>rtRouterQueueSize = <br/>rtRouterThreads = <br/>selfStorageThreads = <br/>suspendHotRollByDeleteQuery = 0<br/>syncMeta = 1<br/>tsidxWritingLevel = <br/>{noformat}<br/>run query: | rest /services/configs/conf-indexes | fields title, tsidxWritingLevel <br/> <br/>the tsidxWritingLevel  of second_metric is empty <br/> <br/>please verify if its as design.
|-
| [https://splunk.atlassian.net/browse/SPL-160286 SPL-160286]
| [PUBLIC] The data preview for the Add Data workflow does not display for Log to Metrics source types
| Data Preview, Metric Store
| Unscheduled
| P2-High
| Bug
| New
| &mdash;
| &mdash;
| This bug was created so that this issue appears in the release notes for OrangeSwirl and later. Close it when task SPL-157693 is closed.
|-
| [https://splunk.atlassian.net/browse/SPL-279750 SPL-279750]
| After upgrade to 9.3.0+, UFs are reporting 'Failed to download app', while DS reports 'app does not exist on the server’. This issue does not appear if you configure the environment in such a way that each app is assigned to a maximum of one server class
| Deployment Server
| Unscheduled
| P3-Medium
| Bug
| Closed
| Fixed
| &mdash;
| After Splunk upgrade (both Deployment Servers and UFs) from 9.2.2 to 9.3.1, the deployment of apps using their Deployment Server is not working properly - Universal Forwarders are reporting ‘'Failed to download app'’, while Deployment Server is reporting that app ‘does not exist on the server’.<br/>The problem happens with a couple of apps and a couple of forwarders - not all the apps and all the forwarders are affected, but different apps are affected with different forwarders. The issue is present with untouched apps from Splunk Base, modified apps from Splunk Base and also custom written apps just containing inputs.conf.<br/>Customer has two different infrastructures with separate clients and splunk environment.<br/>The other infrastructure is having the exact same behavior,<br/>Deployment Server is showing following messages:<br/>{noformat}03-28-2025 10:28:33.953 +0100 WARN  FileDownloadRestHandler [861717 TcpChannelThread] - File: default:spl_hf_01:rosen_hf_routing does not exist on the server. FQ name= /opt/splunk/var/run/tmp/_global_bundles/rosen_hf_routing-1743074739.bundle{noformat}<br/>While forwarders are showing:<br/>{noformat}03-28-2025 10:25:49.133 +0100 ERROR DeployedServerclass [926172 HttpClientPollingThread_41B7AE41-1551-4B65-BB65-874B2D06DB51] - name=spl_hf_01 Failed to download app=Splunk_TA_windows_901{noformat}<br/>The Deployment Server log indicates that the app "does not exist on the server", and specify the .bundle files in '/opt/splunk/var/run/tmp/_global_bundles/' - while it was confirmed that the .bundle file is present and the permissions are set correctly.<br/>Following changes were already tested:<br/>* changed restartSplunkd to 1, help for some time, but issue reappeared after a couple of days<br/>* upgraded to 9.3.3, which also helped for a couple of days, but then the issue reappeared again<br/>* shutdown DS, clear '/opt/splunk/var/run/tmp/_global_bundles/', start DS (this fixes the issue for a short time to publish required apps on short notice.)<br/>Additionally, the serverclass.conf on DS was checked for correct naming, folder permissions on DS were verified, new apps and server classed were tested.<br/>Diags with debug logging enabled:<br/>[Heavy Forwarder|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0010b00001si66aaas&job_idtok=210de0aa-3e39-446e-baf7-77fa973060de&case_numbertok=3674731&hosttok=linuxsplhfprd03.roseninspection.net&case_filetok=linuxsplhfprd03.roseninspection.net-hf_-20250423-205248-HHL1D27M&found_anontok=0&job_typetok=classic&form.mount.tok=raw&form.mount.filter=&form.network.tok=raw&form.network.filter=&form.interfaces.filter=&form.cpuInfo.tok=raw&form.cpuInfo.filter=&form.processes.tok=raw&form.processes.filter=&form.ulimit.tok=raw&form.ulimit.filter=&form.memory.tok=raw&form.memory.filter=&form.kvstore.filter=&form.searchPeerBundlesDir.filter=&form.sinkholeDir.filter=&form.authDir.filter=]<br/>[Deployment Server|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0010b00001si66aaas&job_idtok=2812d714-e0bf-4312-861e-a55b7f22e518&case_numbertok=3674731&hosttok=linuxspldsprd01.roseninspection.net&case_filetok=linuxspldsprd01.roseninspection.net-sh_ds_-20250423-205319-KD6sQfBo&found_anontok=0&job_typetok=classic&form.mount.tok=raw&form.mount.filter=&form.network.tok=raw&form.network.filter=&form.interfaces.filter=&form.cpuInfo.tok=raw&form.cpuInfo.filter=&form.processes.tok=raw&form.processes.filter=&form.ulimit.tok=raw&form.ulimit.filter=&form.memory.tok=raw&form.memory.filter=&form.kvstore.filter=&form.searchPeerBundlesDir.filter=&form.sinkholeDir.filter=&form.authDir.filter=]<br/>Test scenario present in the logs:<br/># Change stateOnClient = noop on serverclass level at 10:31 28/03/2025<br/># Reload serverclass at 10:31:45 28/03/2025<br/># Confirmed that the issue is not present then.<br/># Disabled stateOnClient = noop at 10:45:00 28/03/2025
|-
| [https://splunk.atlassian.net/browse/SPL-141274 SPL-141274]
| [PUBLIC] Clicking Install multiple times in Install dialog causes error
| DMC
| Unscheduled
| P2-High
| Bug
| New
| &mdash;
| oper_bug_bash
| Clicking the primary button on a dialog that causes a deployment multiple times will cause an error to display.  The button should be disabled after the first click.
|-
| [https://splunk.atlassian.net/browse/SPL-143981 SPL-143981]
| [PUBLIC] Uninstall app dialog does not show the app name correctly when the app doesn't have the label
| DMC
| Unscheduled
| P2-High
| Bug
| New
| &mdash;
| oper_bug_bash
| Repro steps:<br/> # Install an private app that doesn't have app label<br/> # Go to app listing page and uninstall the app<br/>Results:<br/> # Uninstall in process dialog doesn't show app name<br/> # Uninstall confirm dialog show 'null' as app name<br/>Expected:<br/>If app label doesn't exist, fall back to app id.
|-
| [https://splunk.atlassian.net/browse/SPL-141982 SPL-141982]
| [PUBLIC] Upload modal should use size=large File element
| DMC
| Unscheduled
| P2-High
| Bug
| New
| &mdash;
| oper_bug_bash
| In the upload UI, we use a size=<default> File element. The user has to make sure to drag file the to the element exactly.<br/>We should use the size=large File instead.
|-
| [https://splunk.atlassian.net/browse/SPL-132151 SPL-132151]
| [PUBLIC] XML error when trying to download uninstalled app
| DMC
| Unscheduled
| P3-Medium
| Bug
| Untriaged
| &mdash;
| FY21Q4, oper_bug_bash
| Assume we have 2 Cloud users...<br/>CloudA uninstalls Splunk_TA_windows<br/>CloudB still has the apps page open and sees Splunk_TA_windows is there<br/>CloudB clicks download on Splunk_TA_windows<br/>XML error message returned....
|-
| [https://splunk.atlassian.net/browse/SPL-260620 SPL-260620]
| [non MVP] Revert does not revert versions associated to permission changes
| Enterprise Dashboards
| Unscheduled
| P3-Medium
| Bug
| New
| &mdash;
| &mdash;
| A new version is created when a dashboard’s permissions changes. Currently, you cannot revert the permissions changes. Workaround is to manually change permissions. <br/>*AC*<br/>* Allow users to revert permissions changes
|-
| [https://splunk.atlassian.net/browse/SPL-270271 SPL-270271]
| Scheduled email exports of large dashboards compress images to approximately 1440 x 960 pixels, leading to blurry PDFs.
| Enterprise Dashboards
| Unscheduled
| P2-High
| Bug
| New
| &mdash;
| &mdash;
| *Observed Behavior:*<br/>For Studio dashboards with large dimensions the quality of scheduled exports are severely reduced and can become unusable in terms of the visibility of data and text. See `large-dashboard-dark.png` inside of our integration snapshots in the enterprise dashboards repo for an example of this quality reduction.<br/>*Expected Behavior:* <br/>The exported dashboard via scheduled export should have consistent quality and usability as a result across all dimensions. Users should be able to produce a single usable PDF export via scheduled export for dashboards with large dimensions.<br/>*Steps To Reproduce:*<br/>1. Create a studio dashboard and modify the canvas dimensions to something like 1440x6000 and enable the `Actual size` display mode<br/>2. Fill the canvas with various visualizations<br/>3. Send a test email to yourself via the scheduled export dialog inside the dashboard<br/>4. Observe that the PDF content is blurred and due to resolution compression
|-
| [https://splunk.atlassian.net/browse/SPL-258394 SPL-258394]
| Health Report for destination output issues show Last 50 detailed logs in Indexer Cluster nodes but not in Search Head or Cluster Manager
| Health Report
| Unscheduled
| P2-High
| Bug
| Untriaged
| &mdash;
| &mdash;
| Health Report Issues as posted in [https://docs.google.com/presentation/d/1OZF2cfbBXIXVbegRqmr908GGJVB7HFQe/edit?disco=AAABQeZaxKs|https://docs.google.com/presentation/d/1OZF2cfbBXIXVbegRqmr908GGJVB7HFQe/edit?disco=AAABQeZaxKs|smart-link] <br/>The health report details page seems to be behaving differently in an index cluster. On the search head and the CM, in distributed mode, we don't see the last 50 messages. The logs are only displayed if I log into the UI on an indexer, which a customer would rarely do.<br/>This is specifically for logs related to File System or S3 output issues. Not sure if it also occurs for other types of logs. See images posted in [https://splunk.slack.com/archives/C0797JHL7SM/p1719482578774679|https://splunk.slack.com/archives/C0797JHL7SM/p1719482578774679|smart-link]
|-
| [https://splunk.atlassian.net/browse/SPL-231587 SPL-231587]
| [PUBLIC] [SPIN-OFF] - Crash in search process in PrecacheUsersThread when max_searches_per_process is set lower than default
| Identity - Authentication (AuthN), Search - Core
| Unscheduled
| P2-High
| Crash
| Untriaged
| &mdash;
| cmpkanban, mr_regression
| I found a crash during I was testing 08187535c166<br/>{code}<br/>[build 08187535c166] 2020-12-03 19:52:36<br/>Received fatal signal 6 (Aborted).<br/> Cause:<br/>   Signal sent by PID 23620 running under UID 2023.<br/> Crashing thread: PrecacheUsersThread<br/> Registers:<br/>    RIP:  [0x00007F2F9AFF2387] gsignal + 55 (libc.so.6 + 0x36387)<br/>    RDI:  [0x0000000000005C44]<br/>    RSI:  [0x0000000000005C4D]<br/>    RBP:  [0x00007F2F957FE570]<br/>    RSP:  [0x00007F2F957FE428]<br/>    RAX:  [0x0000000000000000]<br/>    RBX:  [0x000055FE946A1750]<br/>    RCX:  [0xFFFFFFFFFFFFFFFF]<br/>    RDX:  [0x0000000000000006]<br/>    R8:  [0x0000000000000100]<br/>    R9:  [0x00007F2F9AC00080]<br/>    R10:  [0x0000000000000008]<br/>    R11:  [0x0000000000000206]<br/>    R12:  [0x000055FE9398EA10]<br/>    R13:  [0x00007F2F9A433908]<br/>    R14:  [0x00007F2F957FE6F8]<br/>    R15:  [0x00007F2F957FE718]<br/>    EFL:  [0x0000000000000206]<br/>    TRAPNO:  [0x0000000000000000]<br/>    ERR:  [0x0000000000000000]<br/>    CSGSFS:  [0x0000000000000033]<br/>    OLDMASK:  [0x0000000000000000]<br/> OS: Linux<br/> Arch: x86-64<br/> Backtrace (PIC build):<br/>{code}
|-
| [https://splunk.atlassian.net/browse/SPL-176514 SPL-176514]
| [PUBLIC] Offline rebuild of unsearchable bucket may lead to stale information in dbinspect searches
| Indexing, S2
| Unscheduled
| P2-High
| Bug
| New
| &mdash;
| &mdash;
| It appears offline rebuild of a unsearchable bucket does not update the .bucketManifest. At startup, the DDM uses these values and if S2 is enabled for migration, we'll upload the bucket with invalid manifest. -This is problematic (currently) in S2 as it fails bootstrapping validation.- Misspoke about failing bootstrapping. Bootstrapping works. Download works. Its just that dbinspect is incorrect.<br/>https://splunk.atlassian.net/browse/CO-132896<br/>Repro:<br/> 1) Index data to splunk<br/> 2) roll bucket<br/> 3) shutdown splunk<br/> 4) delete search files for the bucket (everything except the journal.gz)<br/> 5) delete entry in .bucketManifest<br/> 6) start splunk. This should rescan & rebuild the .bucketManifest based on files in db<br/> 7) Confirm .bucketManifest shows 0 values for raw size, event count, host count, source count, source type count, but valid journal size.<br/> ie<br/>{noformat}<br/>"_internal~0~57F238A8-ED60-4A18-A5F9-0FEF09D72EFB","db_1568310518_1568299727_0",0,0,0,0,0,45056,1568663140,0,"",0,36727<br/>{noformat}<br/>8) stop splunk<br/> 9) rebuild bucket<br/> 10) start splunk<br/> 11) Confirm .bucketManifest still shows values for unsearchable bucket (same as before)<br/> ie<br/>{noformat}<br/>"_internal~0~57F238A8-ED60-4A18-A5F9-0FEF09D72EFB","db_1568310518_1568299727_0",0,0,0,0,0,45056,1568663140,0,"",0,36727<br/>{noformat}
|-
| [https://splunk.atlassian.net/browse/SPL-228646 SPL-228646]
| [PUBLIC] Restart is needed when AWS access key pairs rotate (w/o grace period) or other S3 config settings for Ingest Actions become invalid
| Ingest Actions, Network Output
| Unscheduled
| P3-Medium
| Bug
| Untriaged
| &mdash;
| fy23q4-cmp-operability, Hersheys-Reviewed, intern_candidate_core_framework, mr_55580, Q3-2022, qa_da_issue
| If I, as a Splunk Administrator, enter invalid configuration for Ingest Actions, I should be able to recover any data that was ingest/routed when the configuration was invalid.<br/>For example, if I entered an invalid access key/secret key, ingest data that is routed with ingestion, but then realize my mistake, I should be able to enter a fixed access key/secret key, and all data I previously ingested should use the new access key/secret key.
|-
| [https://splunk.atlassian.net/browse/SPL-250849 SPL-250849]
| Compression method for Filesystem destination is immutable once created since changing it leads to a corrupt file.
| Ingest Actions
| Unscheduled
| P3-Medium
| Bug
| New
| &mdash;
| pre_merge_ia_fs
| Setup a FS destination with {{compression=none}}, then setup a HEC input to send events to be routed to FS destinations.  After sending the a few events, in splunk web, edit FS destination and change the compression to gzip.  Let a few more events to be ingested.<br/>After following the sequence above, the resulting file contains some events that are uncompressed (raw) and the later part of the file will contains compressed data. So, effectively we have a mix of uncompressed and compressed event in the same file<br/>{{outputs.conf}}<br/>{noformat}[rfs:testfs]<br/>compression = gzip   <=== initially set it to none, then change it to gzip<br/>description = Routing to /tmp/testfs<br/>dropEventsOnUploadError = false<br/>format = ndjson<br/>format.json.index_time_fields = true<br/>format.ndjson.index_time_fields = true<br/>fs.appendToFileUntilSizeMB = 1<br/>fs.timeBeforeClosingFileSecs = 10<br/>partitionBy = day<br/>path = file:///tmp/testfs{noformat}
|-
| [https://splunk.atlassian.net/browse/SPL-146820 SPL-146820]
| [PUBLIC] [CUSTOMER] Unable to access some settings/manager pages (data model editor) if starting from the setup page of a non-visible app
| Manager Pages - Misc
| Unscheduled
| P2-High
| Bug
| Untriaged
| &mdash;
| &mdash;
| Reproduced with CIM 4.9.1 with 7.0.1. A customer also reported in Splunk Answers with 4.9.1 on 7.0.0: https://answers.splunk.com/answers/593663/common-information-model-cim-data-model-editor-mis.html I encouraged him to file a bug but his support contract is still under renewal. Because I was able to reproduce this internally, I'm filing it. <br/>If you try to view certain manager pages after starting in the context of a non-visible app, the manager pages 404 because they return to the app context. This example uses CIM because it's what I used to reproduce and is a non-visible app.<br/># Install CIM on Enterprise.<br/># Go to Manage Apps  and click "Set Up" for the Splunk_SA_CIM <br/>## Note that the URL is in the app context: splunk-es/en-US/app/Splunk_SA_CIM/cim_setup?action=edit<br/># Click Settings > Data models<br/>## Note that the URL is in the manager context: splunk-es/en-US/manager/Splunk_SA_CIM/data_model_manager<br/># Click a data model to edit it.<br/>## Note that the URL changes back to the app context: splunk-es/en-US/app/Splunk_SA_CIM/data_model_editor<br/># Observe a 404:  !Screen Shot 2017-11-29 at 5.19.08 PM.png|thumbnail! <br/># Even worse, when you click the "open this in search" from the 404, you don't find any results because it defaults to searching a log_level of ERROR even though this error was logged with a log_level of INFO: "[5a1f5c79517f00702f7f90] error:133 - Masking the original 404 message: 'App "Splunk_SA_CIM" does not support direct UI access. ' with 'Page not found!' for security reasons"<br/>If you start in the search and reporting app (or another visible app) it works fine.<br/>Is there a way to redirect someone to a visible app context so this doesn't fail so epically and mysteriously in the future? Or some other workaround?
|-
| [https://splunk.atlassian.net/browse/SPL-186365 SPL-186365]
| [PUBLIC] Users are able to create/clone knowledge objects into apps where they lack permissions
| Manager Pages - Misc, Security
| Unscheduled
| P2-High
| Bug
| Untriaged
| &mdash;
| SUSTAINING_FRONTEND, z0928
| h1. Updated 17 April 2020<br/>Scope of this bug fix: <br/>The following Knowledge Objects list destination apps for which the user does not have permission on create and/or on clone: <br/> * Field Transformation<br/> * Eventtype<br/> * Lookup Definition,<br/> * Macros,<br/> * UI, <br/> * Field Extraction,<br/> * Lookup Table<br/>Expected behaviour: The user should only see the new app in destination apps list. <br/>See below for more details. <br/>---------------------------------<br/> <br/>h1. Description<br/>1) Setting the permissions of the apps is not reflecting in the Cloning page for the knowledge objects. If you limit the permission for the apps, we still can see the Apps when cloning objects for certain knowledge objects.<br/>2) The Cloning page for the Knowledge Objects are not consistency, we see different apps being listed in the Cloning pages, as below.<br/>3) The issue was firstly reported in SPL-159600 and SPL-186365, but the JIRAs did not cover all lack of consistency and permission for all KO.<br/>h1. Search, Report and Alerts<br/>1) and Go to Settings > Create a New Report into the new_app and save it.  In settings > Search, Report and Alerts, in the new_app, new_report click on Clone and the App dropbox will show only the new_app, which is correct<br/>!image-2020-04-14-21-31-31-461.png|width=530,height=276!  <br/>Expected: This is correct, since the user is only able to see the new app in the dropdown. <br/>h1. Data Models<br/>1) Go to Settings > Create a new Datamodel and In Settings > Datamodels, check the App dropdown <br/>!image-2020-04-14-21-35-52-534.png|width=384,height=331!<br/>Expected: This is correct, since the user is only able to see the new app in the dropdown. <br/>h1. EventType<br/>1) Create an EventType and clone the eventype, check the destination app<br/>!image-2020-04-14-21-38-56-397.png|width=584,height=350!<br/>Expected: This is not correct, the user should only be able to see the new app, and not the additional apps for which the user does not have read or write access. <br/>h1. Tag<br/>1) Create a new tag and try to clone, there is no option to Destination app<br/>!image-2020-04-14-21-45-10-991.png!<br/>Expected: user should be able to select a Destination App at the time of tag creation and tag cloning. In both cases, the user should only see the new app in the dropdown. <br/>h1. Fields Aliases, Calculated Fields, Transformation, Sourcetype Renaming, Workflow Actions<br/>1) Create a the knowledge object and clone it<br/>!image-2020-04-14-21-52-44-634.png|width=658,height=460!<br/>Expected: Both when creating and cloning a field transformation, the user should only see the new app in the dropdown list. Instead, the user sees apps for which it does not have read/write access. <br/>h1. Field Extraction<br/>1) Create Field Extraction, there is no option to clone it<br/>!image-2020-04-14-21-56-25-693.png|width=624,height=220!<br/>Expected: The user should only see the new app when creating a field extraction. There is no clone option for field extraction, which is inconsistent with other knowledge objects. <br/>h1. Lookup Table<br/>1) create a lookup table, not able to clone it. No option to clone the lookup table<br/>!image-2020-04-14-21-59-23-739.png|width=549,height=170!<br/>Expected: The user should only see the new app when creating a lookup table. There is no clone option for lookup table, which is inconsistent with other knowledge objects. <br/>h1. Lookup Definition and Automatic Lookup<br/>1) Create the lookup definition, clone the lookup definition and check the Destination app<br/>!image-2020-04-14-22-00-50-016.png|width=538,height=299!<br/>Expected: Both when creating and cloning a lookup definition, the user should only see the new app in the dropdown list. Instead, the user sees apps for which it does not have read/write access. <br/>h1. User Interface and Macros<br/>1) Create a new UI  or a Macro and try to clone it<br/>!image-2020-04-14-22-04-55-099.png|width=462,height=405!<br/> <br/>Expected: Both when creating and cloning macros, the user should only see the new app in the dropdown list. Instead, the user sees apps for which it does not have read/write access.
|-
| [https://splunk.atlassian.net/browse/SPL-192936 SPL-192936]
| [PUBLIC] [millisecond] Subsecond search - When you update metric.timestampResolution via the UI, it is not updated on the search head index.conf file. This does not affect search functionality.
| Metric Store
| Unscheduled
| P3-Medium
| Bug
| New
| &mdash;
| z0928
| log in cloud stack (indexer cluster) as sc_admin user<br/>create metric index with metric.timestampResolution as second <br/>verify in index.conf<br/> <br/>{noformat}<br/>[second-to-milli]<br/>coldPath = $SPLUNK_DB/second-to-milli/colddb<br/>datatype = metric<br/>frozenTimePeriodInSecs = 1<br/>homePath = $SPLUNK_DB/second-to-milli/db<br/>maxGlobalRawDataSizeMB = 10<br/>thawedPath = $SPLUNK_DB/second-to-milli/thaweddb<br/>{noformat}<br/> <br/>go to edit index page, update the Timestamp Resolution to milliseconds and save <br/>indexes.conf has updated on all indexers but not updated on the search head
|-
| [https://splunk.atlassian.net/browse/SPL-203100 SPL-203100]
| [PUBLIC] Summary page on monitoring console doesn't show correct RF/SF when not running on the CM.
| Monitoring Console
| Unscheduled
| P3-Medium
| Bug
| Closed
| Fixed
| AxpTriaged
| Customer set RF=2 and SF=2 on CM but when accessing summary page on MC, the RF is appearing as 3. <br/>Configurations are set correctly on CM and on Indexer Clustering: Status panel, it appears correctly as 2.<br/>We tried it in our lab and MC is also showing RF = 3 and SF =2. It appears to be showing static info and not reflecting the actual configurations.<br/>Attached screenshots showing the issue.<br/>CM diag:<br/>[https://downloadsvc.splunk.com/download/splunk/02-23-2021/uploadsvc-37case2124885-02-23-2021-USER-0030b000026MfFtAAK-diag-splunk-mgmt-clsmaster-2021-01-13_15-59-04.tar.gz]<br/>MC diag:<br/>[https://downloadsvc.splunk.com/download/splunk/03-28-2021/uploadsvc-89case2124885-03-28-2021-USER-0030b00001yM2GVAA0-diag-splunk-mgmt-lic-dmc-2021-03-25_15-37-08.tar.gz]<br/>Please ignore any connectivity errors found in splunkd.log in MC diag as it has been resolved.
|-
| [https://splunk.atlassian.net/browse/SPL-193389 SPL-193389]
| [PUBLIC] [SmartStore_on_GCP] Parallel upload is not supported in gcp-sse-kms encryption mode
| S2
| Unscheduled
| P2-High
| Bug
| Reopened
| &mdash;
| gcp_s2, z0928
| With build 8021deff7d66(stack: gcp-spl182068-test) uploading buckets to GCS with encryption gcp-sse-kms got multiple errors.<br/>Steps to reproduce:<br/> # Configure volume with sse-kms encryption<br/> # Create index under the volume and ingest data<br/> # Roll buckets to warm to trigger bucket upload<br/> # Search for logs about bucket upload, there are many errors<br/>Volume & index configuration used in my test:<br/>{code:java}<br/>[volume:sse-kms-new]<br/>storageType = remote<br/>path = gs://s2testing<br/>remote.gs.project_id = s2-encryption-kms<br/>remote.gs.credential_file = s2-encryption-kms-e3b16995402e.json<br/>remote.gs.encryption = gcp-sse-kms<br/>remote.gs.gcp_kms.locations = us-central1<br/>remote.gs.gcp_kms.key_ring = thu-keyring<br/>remote.gs.gcp_kms.key = thu-key-test<br/>[idx-sse-kms-0810]<br/>remotePath = volume:sse-kms-new/$_index_name<br/>homePath = $SPLUNK_DB/idx-sse-kms-0810/db<br/>coldPath = $SPLUNK_DB/idx-sse-kms-0810/colddb<br/>thawedPath = $SPLUNK_DB/idx-sse-kms-0810/thaweddb<br/>maxGlobalRawDataSizeMB = 100000<br/>maxGlobalDataSizeMB = 0<br/>maxTotalDataSizeMB = 0<br/>frozenTimePeriodInSecs = 764000<br/>{code}<br/>Below query shows errors from the beginning of one bucket uploading:<br/>[index=_internal source=*splunkd.log cache_id="*idx-sse-kms-0810~5~2721E446-7C19-4D66-9BB3-AEF67BD5398B*"|https://gcp-spl182068-test.stg.splunkcloud.com/en-GB/app/search/search?q=search%20index%3D_internal%20source%3D*splunkd.log%20cache_id%3D%22bid%7Cidx-sse-kms-0810~5~2721E446-7C19-4D66-9BB3-AEF67BD5398B%7C%22&display.page.search.mode=smart&dispatch.sample_ratio=1&workload_pool=standard_perf&earliest=&latest=1597113224.328&display.general.type=events&display.page.search.tab=statistics&display.prefs.events.offset=40&sid=1597115425.1164]<br/> <br/>08-11-2020 02:33:11.494 +0000 ERROR CacheManager - action=upload, cache_id="bid|idx-sse-kms-0810~5~2721E446-7C19-4D66-9BB3-AEF67BD5398B|", status=failed, reason="HTTP Error 3: Permanent error in ComposeObject: \{\n  "error": {\n    "code": 400,\n    "message": "Compose operation does not support Cloud KMS keys.",\n    "errors": [\n      {\n        "message": "Compose operation does not support Cloud KMS keys.",\n        "domain": "global",\n        "reason": "invalid"\n      }\n    ]\n  }\n}\n [INVALID_ARGUMENT]", elapsed_ms=1388
|-
| [https://splunk.atlassian.net/browse/SPL-168314 SPL-168314]
| [PUBLIC] SmartStore standalone instance + Monitoring Console: Bootstrapping panel needs to reflect the standalone bootstrapping process
| S2
| Unscheduled
| P3-Medium
| Bug
| New
| &mdash;
| indexing_bug_bash
| S2 standalone + monitoring console currently only reflects the status of bootstrapping in a indexer clustering environment.<br/>S2 standalone hits different codepaths and logs differently while bootstrapping(bootstrap status endpoint are different too)
|-
| [https://splunk.atlassian.net/browse/SPL-255514 SPL-255514]
| "| timechart count" search is causing Splunk to crash with "Crashing thread: searchOrchestrator"
| Search
| Unscheduled
| P2-High
| Bug
| In Progress
| &mdash;
| dwest_reviewed, emea_support:approved, support_reviewed
| Splunk crashes when executing the following search:<br/>{noformat}| timechart count{noformat}<br/>Splunk instance is not stopped, but the following crash log is generated:<br/>{noformat}[build 050c9bca8588] 2024-05-06 11:27:07<br/>Received fatal signal 11 (Segmentation fault) on PID 228293.<br/> Cause:<br/>   No memory mapped at address [0x0000000000000000].<br/> Crashing thread: searchOrchestrator<br/> Registers:<br/>    RIP:  [0x0000000000000000] ?<br/>    RDI:  [0x00007FD620D5A200]<br/>    RSI:  [0x0000563B0345AE34]<br/>    RBP:  [0x00007FD61A7FD6A0]<br/>    RSP:  [0x00007FD61A7FB908]<br/>    RAX:  [0x0000563B059E9418]<br/>    RBX:  [0x00007FD61A7FBA50]<br/>    RCX:  [0x00007FD61A7FB928]<br/>    RDX:  [0x0000000000000001]<br/>    R8:  [0x00007FD61A7FB730]<br/>    R9:  [0x00007FD61A7FB640]<br/>    R10:  [0x0000563B05B51A70]<br/>    R11:  [0x0000000000000000]<br/>    R12:  [0x0000000000000001]<br/>    R13:  [0x00007FD61A7FBA50]<br/>    R14:  [0x00007FD620C0D3C0]<br/>    R15:  [0x00007FD61A7FB9A0]<br/>    EFL:  [0x0000000000010206]<br/>    TRAPNO:  [0x000000000000000E]<br/>    ERR:  [0x0000000000000014]<br/>    CSGSFS:  [0x002B000000000033]<br/>    OLDMASK:  [0x0000000000000000]<br/> OS: Linux<br/> Arch: x86-64<br/> Backtrace (PIC build):<br/> Linux / Q100SPUL4537 / 4.12.14-122.194-default / #1 SMP Mon Feb 12 14:50:38 UTC 2024 (dcbe7be) / x86_64<br/> /etc/SuSE-release: SUSE Linux Enterprise Server 12 (x86_64)<br/> glibc version: 2.22<br/> glibc release: stable<br/> MAP: 563b0000a000-563b05927000 r-xp 00000000 fe:0d 2729522                    /usd/as16544a/soft/splunk/bin/splunkd<br/> MAP: 563b05928000-563b05a69000 r--p 0591d000 fe:0d 2729522                    /usd/as16544a/soft/splunk/bin/splunkd<br/> MAP: 563b05a69000-563b05a71000 rw-p 05a5e000 fe:0d 2729522                    /usd/as16544a/soft/splunk/bin/splunkd<br/> MAP: 563b05a71000-563b05b52000 rw-p 00000000 00:00 0 <br/> MAP: 7fd605df5000-7fd605df6000 ---p 00000000 00:00 0 <br/> MAP: 7fd605df6000-7fd605ff6000 rw-p 00000000 00:00 0 <br/> MAP: 7fd6069fb000-7fd6069fc000 ---p 00000000 00:00 0 <br/> MAP: 7fd6069fc000-7fd606bfc000 rw-p 00000000 00:00 0 <br/> MAP: 7fd606bfc000-7fd606bfd000 ---p 00000000 00:00 0 <br/> MAP: 7fd606bfd000-7fd606dfd000 rw-p 00000000 00:00 0 <br/> MAP: 7fd606dfd000-7fd606dfe000 ---p 00000000 00:00 0 <br/> MAP: 7fd606dfe000-7fd606ffe000 rw-p 00000000 00:00 0 <br/> MAP: 7fd607400000-7fd60a600000 rw-p 00000000 00:00 0 <br/> MAP: 7fd60a800000-7fd60aa00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd60abf5000-7fd60abf6000 ---p 00000000 00:00 0 <br/> MAP: 7fd60abf6000-7fd60adf6000 rw-p 00000000 00:00 0 <br/> MAP: 7fd60b000000-7fd60ba00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd60c200000-7fd60e200000 rw-p 00000000 00:00 0 <br/> MAP: 7fd60e200000-7fd60ea00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd60ea00000-7fd60f200000 rw-p 00000000 00:00 0 <br/> MAP: 7fd60f200000-7fd610000000 rw-p 00000000 00:00 0 <br/> MAP: 7fd610000000-7fd610600000 rw-p 00000000 00:00 0 <br/> MAP: 7fd610600000-7fd610e00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd610ffb000-7fd610ffc000 ---p 00000000 00:00 0 <br/> MAP: 7fd610ffc000-7fd6111fc000 rw-p 00000000 00:00 0 <br/> MAP: 7fd611200000-7fd615800000 rw-p 00000000 00:00 0 <br/> MAP: 7fd6159ff000-7fd615a00000 ---p 00000000 00:00 0 <br/> MAP: 7fd615a00000-7fd615c00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd615c00000-7fd616400000 rw-p 00000000 00:00 0 <br/> MAP: 7fd616400000-7fd617200000 rw-p 00000000 00:00 0 <br/> MAP: 7fd617200000-7fd617a00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd617a00000-7fd617e00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd617ffc000-7fd617ffd000 ---p 00000000 00:00 0 <br/> MAP: 7fd617ffd000-7fd6181fd000 rw-p 00000000 00:00 0 <br/> MAP: 7fd618200000-7fd618400000 rw-p 00000000 00:00 0 <br/> MAP: 7fd618600000-7fd618c00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd618ffc000-7fd618ffd000 ---p 00000000 00:00 0 <br/> MAP: 7fd618ffd000-7fd6191fd000 rw-p 00000000 00:00 0 <br/> MAP: 7fd619200000-7fd61a400000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61a5fe000-7fd61a5ff000 ---p 00000000 00:00 0 <br/> MAP: 7fd61a5ff000-7fd61a7ff000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61a800000-7fd61ae00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61ae00000-7fd61b400000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61b5ff000-7fd61b600000 ---p 00000000 00:00 0 <br/> MAP: 7fd61b600000-7fd61b800000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61b800000-7fd61bc00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61bdff000-7fd61be00000 ---p 00000000 00:00 0 <br/> MAP: 7fd61be00000-7fd61c000000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61c000000-7fd61c200000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61c3fd000-7fd61c3fe000 ---p 00000000 00:00 0 <br/> MAP: 7fd61c3fe000-7fd61c5fe000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61c600000-7fd61cc00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61cdff000-7fd61ce00000 ---p 00000000 00:00 0 <br/> MAP: 7fd61ce00000-7fd61d000000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61d000000-7fd61d200000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61d3ff000-7fd61d400000 ---p 00000000 00:00 0 <br/> MAP: 7fd61d400000-7fd61d600000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61d600000-7fd61d800000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61d9ff000-7fd61da00000 ---p 00000000 00:00 0 <br/> MAP: 7fd61da00000-7fd61dc00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61dc00000-7fd61e400000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61e5ff000-7fd61e600000 ---p 00000000 00:00 0 <br/> MAP: 7fd61e600000-7fd61ea00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61ea00000-7fd61ee00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd61ee00000-7fd620600000 rw-p 00000000 00:00 0 <br/> MAP: 7fd6207ff000-7fd620800000 ---p 00000000 00:00 0 <br/> MAP: 7fd620800000-7fd620a00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd620a00000-7fd620e00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd620ffe000-7fd620fff000 ---p 00000000 00:00 0 <br/> MAP: 7fd620fff000-7fd6211ff000 rw-p 00000000 00:00 0 <br/> MAP: 7fd6211ff000-7fd621200000 ---p 00000000 00:00 0 <br/> MAP: 7fd621200000-7fd621400000 rw-p 00000000 00:00 0 <br/> MAP: 7fd621400000-7fd621600000 rw-p 00000000 00:00 0 <br/> MAP: 7fd6217ff000-7fd621800000 ---p 00000000 00:00 0 <br/> MAP: 7fd621800000-7fd621a00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd621a00000-7fd621e00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd621fff000-7fd622000000 ---p 00000000 00:00 0 <br/> MAP: 7fd622000000-7fd622200000 rw-p 00000000 00:00 0 <br/> MAP: 7fd622200000-7fd624a00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd624bff000-7fd624c00000 ---p 00000000 00:00 0 <br/> MAP: 7fd624c00000-7fd624e00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd624e00000-7fd625000000 rw-p 00000000 00:00 0 <br/> MAP: 7fd6251ff000-7fd625200000 ---p 00000000 00:00 0 <br/> MAP: 7fd625200000-7fd625400000 rw-p 00000000 00:00 0 <br/> MAP: 7fd625400000-7fd625a00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd625a6f000-7fd625a8f000 rwxp 00000000 00:00 0 <br/> MAP: 7fd625a9f000-7fd625aaf000 rwxp 00000000 00:00 0 <br/> MAP: 7fd625abf000-7fd625acf000 rwxp 00000000 00:00 0 <br/> MAP: 7fd625aef000-7fd625bff000 rwxp 00000000 00:00 0 <br/> MAP: 7fd625bff000-7fd625c00000 ---p 00000000 00:00 0 <br/> MAP: 7fd625c00000-7fd625e00000 rw-p 00000000 00:00 0 <br/> MAP: 7fd625e00000-7fd626800000 rw-p 00000000 00:00 0 <br/> MAP: 7fd626806000-7fd6268f6000 rwxp 00000000 00:00 0 <br/> MAP: 7fd6268f6000-7fd626918000 r-xp 00000000 00:2a 6950247                    /lib64/libgcc_s.so.1<br/> MAP: 7fd626918000-7fd626919000 r--p 00021000 00:2a 6950247                    /lib64/libgcc_s.so.1<br/> MAP: 7fd626919000-7fd62691a000 rw-p 00022000 00:2a 6950247                    /lib64/libgcc_s.so.1<br/> MAP: 7fd626920000-7fd626930000 rwxp 00000000 00:00 0 <br/> MAP: 7fd626930000-7fd626c00000 rwxp 00000000 00:00 0 <br/> MAP: 7fd626c00000-7fd627000000 rw-p 00000000 00:00 0 <br/> MAP: 7fd627003000-7fd627004000 ---p 00000000 00:00 0 <br/> MAP: 7fd627004000-7fd62700d000 rw-p 00000000 00:00 0 <br/> MAP: 7fd62700d000-7fd62700e000 ---p 00000000 00:00 0 <br/> MAP: 7fd62700e000-7fd62702e000 rwxp 00000000 00:00 0 <br/> MAP: 7fd62702e000-7fd627033000 rw-p 00000000 00:00 0 <br/> MAP: 7fd627033000-7fd6271ce000 r-xp 00000000 00:2a 5870486                    /lib64/libc-2.22.so<br/> MAP: 7fd6271ce000-7fd6271d2000 r--p 0019b000 00:2a 5870486                    /lib64/libc-2.22.so<br/> MAP: 7fd6271d2000-7fd6271d4000 rw-p 0019f000 00:2a 5870486                    /lib64/libc-2.22.so<br/> MAP: 7fd6271d4000-7fd6271d8000 rw-p 00000000 00:00 0 <br/> MAP: 7fd6271d8000-7fd6272d3000 r-xp 00000000 00:2a 5870494                    /lib64/libm-2.22.so<br/> MAP: 7fd6272d3000-7fd6272d4000 r--p 000fb000 00:2a 5870494                    /lib64/libm-2.22.so<br/> MAP: 7fd6272d4000-7fd6272d5000 rw-p 000fc000 00:2a 5870494                    /lib64/libm-2.22.so<br/> MAP: 7fd6272d5000-7fd627589000 r-xp 00000000 fe:0d 2671333                    /usd/as16544a/soft/splunk/lib/libcrypto.so.1.0.0<br/> MAP: 7fd627589000-7fd62758a000 ---p 002b4000 fe:0d 2671333                    /usd/as16544a/soft/splunk/lib/libcrypto.so.1.0.0<br/> MAP: 7fd62758a000-7fd6275a8000 r--p 002b4000 fe:0d 2671333                    /usd/as16544a/soft/splunk/lib/libcrypto.so.1.0.0<br/> MAP: 7fd6275a8000-7fd6275b6000 rw-p 002d2000 fe:0d 2671333                    /usd/as16544a/soft/splunk/lib/libcrypto.so.1.0.0<br/> MAP: 7fd6275b6000-7fd6275bb000 rw-p 00000000 00:00 0 <br/> MAP: 7fd6275bb000-7fd627627000 r-xp 00000000 fe:0d 2672828                    /usd/as16544a/soft/splunk/lib/libssl.so.1.0.0<br/> MAP: 7fd627627000-7fd627628000 ---p 0006c000 fe:0d 2672828                    /usd/as16544a/soft/splunk/lib/libssl.so.1.0.0<br/> MAP: 7fd627628000-7fd62762c000 r--p 0006c000 fe:0d 2672828                    /usd/as16544a/soft/splunk/lib/libssl.so.1.0.0<br/> MAP: 7fd62762c000-7fd627632000 rw-p 00070000 fe:0d 2672828                    /usd/as16544a/soft/splunk/lib/libssl.so.1.0.0<br/> MAP: 7fd627632000-7fd62764a000 r-xp 00000000 00:2a 5870526                    /lib64/noelision/libpthread-2.22.so<br/> MAP: 7fd62764a000-7fd62764b000 r--p 00017000 00:2a 5870526                    /lib64/noelision/libpthread-2.22.so<br/> MAP: 7fd62764b000-7fd62764c000 rw-p 00018000 00:2a 5870526                    /lib64/noelision/libpthread-2.22.so<br/> MAP: 7fd62764c000-7fd627650000 rw-p 00000000 00:00 0 <br/> MAP: 7fd627650000-7fd6277c8000 r-xp 00000000 fe:0d 2670637                    /usd/as16544a/soft/splunk/lib/libsqlite3.so.0.8.6<br/> MAP: 7fd6277c8000-7fd6277cb000 r--p 00177000 fe:0d 2670637                    /usd/as16544a/soft/splunk/lib/libsqlite3.so.0.8.6<br/> MAP: 7fd6277cb000-7fd6277d1000 rw-p 0017a000 fe:0d 2670637                    /usd/as16544a/soft/splunk/lib/libsqlite3.so.0.8.6<br/> MAP: 7fd6277d1000-7fd6277d3000 rw-p 00000000 00:00 0 <br/> MAP: 7fd6277d3000-7fd6277e4000 r-xp 00000000 fe:0d 2670643                    /usd/as16544a/soft/splunk/lib/libbz2.so.1.0.3<br/> MAP: 7fd6277e4000-7fd6277e5000 ---p 00011000 fe:0d 2670643                    /usd/as16544a/soft/splunk/lib/libbz2.so.1.0.3<br/> MAP: 7fd6277e5000-7fd6277e6000 r--p 00011000 fe:0d 2670643                    /usd/as16544a/soft/splunk/lib/libbz2.so.1.0.3<br/> MAP: 7fd6277e6000-7fd6277e7000 rw-p 00012000 fe:0d 2670643                    /usd/as16544a/soft/splunk/lib/libbz2.so.1.0.3<br/> MAP: 7fd6277e7000-7fd627893000 r-xp 00000000 fe:0d 2672904                    /usd/as16544a/soft/splunk/lib/libarchive.so.13.6.2<br/> MAP: 7fd627893000-7fd627896000 r--p 000ab000 fe:0d 2672904                    /usd/as16544a/soft/splunk/lib/libarchive.so.13.6.2<br/> MAP: 7fd627896000-7fd627897000 rw-p 000ae000 fe:0d 2672904                    /usd/as16544a/soft/splunk/lib/libarchive.so.13.6.2<br/> MAP: 7fd627897000-7fd627898000 rw-p 00000000 00:00 0 <br/> MAP: 7fd627898000-7fd62789a000 r-xp 00000000 00:2a 5870492                    /lib64/libdl-2.22.so<br/> MAP: 7fd62789a000-7fd62789b000 r--p 00002000 00:2a 5870492                    /lib64/libdl-2.22.so<br/> MAP: 7fd62789b000-7fd62789c000 rw-p 00003000 00:2a 5870492                    /lib64/libdl-2.22.so<br/> MAP: 7fd62789c000-7fd62789d000 rw-p 00000000 00:00 0 <br/> MAP: 7fd62789d000-7fd6278f2000 r-xp 00000000 fe:0d 2670642                    /usd/as16544a/soft/splunk/lib/libxmlsec1-openssl.so.1.2.24<br/> MAP: 7fd6278f2000-7fd6278f3000 ---p 00055000 fe:0d 2670642                    /usd/as16544a/soft/splunk/lib/libxmlsec1-openssl.so.1.2.24<br/> MAP: 7fd6278f3000-7fd6278f6000 r--p 00055000 fe:0d 2670642                    /usd/as16544a/soft/splunk/lib/libxmlsec1-openssl.so.1.2.24<br/> MAP: 7fd6278f6000-7fd6278f7000 rw-p 00058000 fe:0d 2670642                    /usd/as16544a/soft/splunk/lib/libxmlsec1-openssl.so.1.2.24<br/> MAP: 7fd6278f7000-7fd6278f8000 rw-p 00000000 00:00 0 <br/> MAP: 7fd6278f8000-7fd627973000 r-xp 00000000 fe:0d 2670640                    /usd/as16544a/soft/splunk/lib/libxmlsec1.so.1.2.24<br/> MAP: 7fd627973000-7fd627974000 ---p 0007b000 fe:0d 2670640                    /usd/as16544a/soft/splunk/lib/libxmlsec1.so.1.2.24<br/> MAP: 7fd627974000-7fd627976000 r--p 0007b000 fe:0d 2670640                    /usd/as16544a/soft/splunk/lib/libxmlsec1.so.1.2.24<br/> MAP: 7fd627976000-7fd627978000 rw-p 0007d000 fe:0d 2670640                    /usd/as16544a/soft/splunk/lib/libxmlsec1.so.1.2.24<br/> MAP: 7fd627978000-7fd6279ca000 r-xp 00000000 fe:0d 2670614                    /usd/as16544a/soft/splunk/lib/libxslt.so.1.1.34<br/> MAP: 7fd6279ca000-7fd6279cb000 ---p 00052000 fe:0d 2670614                    /usd/as16544a/soft/splunk/lib/libxslt.so.1.1.34<br/> MAP: 7fd6279cb000-7fd6279cc000 r--p 00052000 fe:0d 2670614                    /usd/as16544a/soft/splunk/lib/libxslt.so.1.1.34<br/> MAP: 7fd6279cc000-7fd6279cd000 rw-p 00053000 fe:0d 2670614                    /usd/as16544a/soft/splunk/lib/libxslt.so.1.1.34<br/> MAP: 7fd6279cd000-7fd6279cf000 rw-p 00000000 00:00 0 <br/> MAP: 7fd6279cf000-7fd627b84000 r-xp 00000000 fe:0d 2672827                    /usd/as16544a/soft/splunk/lib/libxml2.so.2.9.10<br/> MAP: 7fd627b84000-7fd627b85000 ---p 001b5000 fe:0d 2672827                    /usd/as16544a/soft/splunk/lib/libxml2.so.2.9.10<br/> MAP: 7fd627b85000-7fd627b8d000 r--p 001b5000 fe:0d 2672827                    /usd/as16544a/soft/splunk/lib/libxml2.so.2.9.10<br/> MAP: 7fd627b8d000-7fd627b8f000 rw-p 001bd000 fe:0d 2672827                    /usd/as16544a/soft/splunk/lib/libxml2.so.2.9.10<br/> MAP: 7fd627b8f000-7fd627b90000 rw-p 00000000 00:00 0 <br/> MAP: 7fd627b90000-7fd627c52000 r-xp 00000000 fe:0d 2672906                    /usd/as16544a/soft/splunk/lib/libpcre2-8.so.0.11.0<br/> MAP: 7fd627c52000-7fd627c53000 ---p 000c2000 fe:0d 2672906                    /usd/as16544a/soft/splunk/lib/libpcre2-8.so.0.11.0<br/> MAP: 7fd627c53000-7fd627c54000 r--p 000c2000 fe:0d 2672906                    /usd/as16544a/soft/splunk/lib/libpcre2-8.so.0.11.0<br/> MAP: 7fd627c54000-7fd627c55000 rw-p 000c3000 fe:0d 2672906                    /usd/as16544a/soft/splunk/lib/libpcre2-8.so.0.11.0<br/> MAP: 7fd627c55000-7fd627c5c000 r-xp 00000000 00:2a 5870520                    /lib64/librt-2.22.so<br/> MAP: 7fd627c5c000-7fd627c5d000 r--p 00006000 00:2a 5870520                    /lib64/librt-2.22.so<br/> MAP: 7fd627c5d000-7fd627c5e000 rw-p 00007000 00:2a 5870520                    /lib64/librt-2.22.so<br/> MAP: 7fd627c5e000-7fd627c5f000 rw-p 00000000 00:00 0 <br/> MAP: 7fd627c64000-7fd627c65000 rw-p 00000000 00:00 0 <br/> MAP: 7fd627c65000-7fd627c75000 rwxp 00000000 00:00 0 <br/> MAP: 7fd627c75000-7fd627caa000 r-xp 00000000 fe:0d 2670618                    /usd/as16544a/soft/splunk/lib/libbson-1.0.so.0.0.0<br/> MAP: 7fd627caa000-7fd627cab000 ---p 00035000 fe:0d 2670618                    /usd/as16544a/soft/splunk/lib/libbson-1.0.so.0.0.0<br/> MAP: 7fd627cab000-7fd627cae000 r--p 00035000 fe:0d 2670618                    /usd/as16544a/soft/splunk/lib/libbson-1.0.so.0.0.0<br/> MAP: 7fd627cae000-7fd627caf000 rw-p 00038000 fe:0d 2670618                    /usd/as16544a/soft/splunk/lib/libbson-1.0.so.0.0.0<br/> MAP: 7fd627caf000-7fd627cb5000 rw-p 00000000 00:00 0 <br/> MAP: 7fd627cb5000-7fd627d49000 r-xp 00000000 fe:0d 2671334                    /usd/as16544a/soft/splunk/lib/libmongoc-1.0.so.0.0.0<br/> MAP: 7fd627d49000-7fd627d4a000 r--p 00093000 fe:0d 2671334                    /usd/as16544a/soft/splunk/lib/libmongoc-1.0.so.0.0.0<br/> MAP: 7fd627d4a000-7fd627d4d000 rw-p 00094000 fe:0d 2671334                    /usd/as16544a/soft/splunk/lib/libmongoc-1.0.so.0.0.0<br/> MAP: 7fd627d4d000-7fd627d67000 r-xp 00000000 fe:0d 2670649                    /usd/as16544a/soft/splunk/lib/libz.so.1.2.11<br/> MAP: 7fd627d67000-7fd627d68000 ---p 0001a000 fe:0d 2670649                    /usd/as16544a/soft/splunk/lib/libz.so.1.2.11<br/> MAP: 7fd627d68000-7fd627d69000 r--p 0001a000 fe:0d 2670649                    /usd/as16544a/soft/splunk/lib/libz.so.1.2.11<br/> MAP: 7fd627d69000-7fd627d6a000 rw-p 0001b000 fe:0d 2670649                    /usd/as16544a/soft/splunk/lib/libz.so.1.2.11<br/> MAP: 7fd627d6a000-7fd627d6b000 rw-p 00000000 00:00 0 <br/> MAP: 7fd627d6b000-7fd627d6c000 r-xp 00000000 fe:0d 2672846                    /usd/as16544a/soft/splunk/lib/libdlstub.so.1.0.0<br/> MAP: 7fd627d6c000-7fd627d6d000 r--p 00000000 fe:0d 2672846                    /usd/as16544a/soft/splunk/lib/libdlstub.so.1.0.0<br/> MAP: 7fd627d6d000-7fd627d6e000 rw-p 00001000 fe:0d 2672846                    /usd/as16544a/soft/splunk/lib/libdlstub.so.1.0.0<br/> MAP: 7fd627d6e000-7fd627db7000 r-xp 00000000 fe:0d 2670645                    /usd/as16544a/soft/splunk/lib/libjemalloc.so.2<br/> MAP: 7fd627db7000-7fd627dba000 r--p 00048000 fe:0d 2670645                    /usd/as16544a/soft/splunk/lib/libjemalloc.so.2<br/> MAP: 7fd627dba000-7fd627dbb000 rw-p 0004b000 fe:0d 2670645                    /usd/as16544a/soft/splunk/lib/libjemalloc.so.2<br/> MAP: 7fd627dbb000-7fd627dbc000 rw-p 00000000 00:00 0 <br/> MAP: 7fd627dbc000-7fd627dd7000 r-xp 00000000 fe:0d 2672843                    /usd/as16544a/soft/splunk/lib/libdlwrapper.so.1.0.0<br/> MAP: 7fd627dd7000-7fd627dd9000 r--p 0001a000 fe:0d 2672843                    /usd/as16544a/soft/splunk/lib/libdlwrapper.so.1.0.0<br/> MAP: 7fd627dd9000-7fd627dda000 rw-p 0001c000 fe:0d 2672843                    /usd/as16544a/soft/splunk/lib/libdlwrapper.so.1.0.0<br/> MAP: 7fd627dda000-7fd627ddb000 rw-p 00000000 00:00 0 <br/> MAP: 7fd627ddb000-7fd627dfc000 r-xp 00000000 00:2a 5870478                    /lib64/ld-2.22.so<br/> MAP: 7fd627dfc000-7fd627dfd000 r--p 00021000 00:2a 5870478                    /lib64/ld-2.22.so<br/> MAP: 7fd627dfd000-7fd627dfe000 rw-p 00022000 00:2a 5870478                    /lib64/ld-2.22.so<br/> MAP: 7fd627dfe000-7fd627dff000 rw-p 00000000 00:00 0 <br/> MAP: 7ffe2a434000-7ffe2a455000 rw-p 00000000 00:00 0                          [stack]<br/> MAP: 7ffe2a494000-7ffe2a497000 r--p 00000000 00:00 0                          [vvar]<br/> MAP: 7ffe2a497000-7ffe2a499000 r-xp 00000000 00:00 0                          [vdso]<br/> MAP: ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]<br/>Last errno: 0<br/>Threads running: 12<br/>Runtime: 1095574.175207s<br/>argv: [splunkd -h 17.242.27.240 -p 8089 restart]<br/>Process renamed: [splunkd pid=16318] splunkd -h 17.242.27.240 -p 8089 restart [process-runner]<br/>Process renamed: [splunkd pid=16318] search --id=1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB --maxbuckets=0 --ttl=600 --maxout=500000 --maxtime=8640000 --lookups=1 --reduce_freq=10 --user=j902080 --pro --roles=fi_cap_waf-koord:fi_cloud_sr_fi_cloud_mkp_app-aaa_etaps:fi_cloud_sr_fi_cloud_mkp_app-aaa_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aaa_prod:fi_cloud_sr_fi_cloud_mkp_app-aaa_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-adrui_etaps:fi_cloud_sr_fi_cloud_mkp_app-adrui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-adrui_prod:fi_cloud_sr_fi_cloud_mkp_app-adrui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-apv_etaps:fi_cloud_sr_fi_cloud_mkp_app-apv_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-apv_prod:fi_cloud_sr_fi_cloud_mkp_app-apv_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aspm_etaps:fi_cloud_sr_fi_cloud_mkp_app-aspm_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aspm_prod:fi_cloud_sr_fi_cloud_mkp_app-aspm_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ats_etaps:fi_cloud_sr_fi_cloud_mkp_app-ats_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ats_prod:fi_cloud_sr_fi_cloud_mkp_app-ats_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aue_etaps:fi_cloud_sr_fi_cloud_mkp_app-aue_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-aue_prod:fi_cloud_sr_fi_cloud_mkp_app-aue_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-avgui_etaps:fi_cloud_sr_fi_cloud_mkp_app-avgui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-avgui_prod:fi_cloud_sr_fi_cloud_mkp_app-avgui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cas_etaps:fi_cloud_sr_fi_cloud_mkp_app-casa_etaps:fi_cloud_sr_fi_cloud_mkp_app-casa_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-casa_prod:fi_cloud_sr_fi_cloud_mkp_app-casa_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cbs_etaps:fi_cloud_sr_fi_cloud_mkp_app-cbs_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cbs_prod:fi_cloud_sr_fi_cloud_mkp_app-cbs_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cfgkui_etaps:fi_cloud_sr_fi_cloud_mkp_app-cfgkui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cfgkui_prod:fi_cloud_sr_fi_cloud_mkp_app-cfgkui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cmd_etaps:fi_cloud_sr_fi_cloud_mkp_app-cmd_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-cmd_prod:fi_cloud_sr_fi_cloud_mkp_app-cmd_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-dkkw_prod:fi_cloud_sr_fi_cloud_mkp_app-dkkw_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-epi_etaps:fi_cloud_sr_fi_cloud_mkp_app-epi_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fiss_etaps:fi_cloud_sr_fi_cloud_mkp_app-fiss_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fiss_prod:fi_cloud_sr_fi_cloud_mkp_app-fiss_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_etaps:fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_etaps:fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_prod:fi_cloud_sr_fi_cloud_mkp_app-fkpsep_etaps:fi_cloud_sr_fi_cloud_mkp_app-fkpsep_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-fkpsep_prod:fi_cloud_sr_fi_cloud_mkp_app-fkpsep_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-gbsui_etaps:fi_cloud_sr_fi_cloud_mkp_app-gbsui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-gbsui_prod:fi_cloud_sr_fi_cloud_mkp_app-gbsui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ikdui_etaps:fi_cloud_sr_fi_cloud_mkp_app-ikdui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ikdui_prod:fi_cloud_sr_fi_cloud_mkp_app-ikdui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-imoui_etaps:fi_cloud_sr_fi_cloud_mkp_app-imoui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-imoui_prod:fi_cloud_sr_fi_cloud_mkp_app-imoui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ipk_etaps:fi_cloud_sr_fi_cloud_mkp_app-ipk_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-kwitt_etaps:fi_cloud_sr_fi_cloud_mkp_app-kwitt_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-lap_etaps:fi_cloud_sr_fi_cloud_mkp_app-lap_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-lap_prod:fi_cloud_sr_fi_cloud_mkp_app-lqrui_etaps:fi_cloud_sr_fi_cloud_mkp_app-lqrui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-lst_etaps:fi_cloud_sr_fi_cloud_mkp_app-lst_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-lst_prod:fi_cloud_sr_fi_cloud_mkp_app-lst_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-mkg_etaps:fi_cloud_sr_fi_cloud_mkp_app-mkg_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-mkg_prod:fi_cloud_sr_fi_cloud_mkp_app-mkg_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-mprui_etaps:fi_cloud_sr_fi_cloud_mkp_app-mprui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-mprui_prod:fi_cloud_sr_fi_cloud_mkp_app-mprui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-npl_etaps:fi_cloud_sr_fi_cloud_mkp_app-npl_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-npl_prod:fi_cloud_sr_fi_cloud_mkp_app-npl_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-onb-hub_etaps:fi_cloud_sr_fi_cloud_mkp_app-onb-hub_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-onb-hub_prod:fi_cloud_sr_fi_cloud_mkp_app-onb-hub_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-onbhub_etaps:fi_cloud_sr_fi_cloud_mkp_app-onbhub_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-onbhub_prod:fi_cloud_sr_fi_cloud_mkp_app-ptbo_etaps:fi_cloud_sr_fi_cloud_mkp_app-ptbo_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-ptbo_prod:fi_cloud_sr_fi_cloud_mkp_app-ptbo_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-tbxui_etaps:fi_cloud_sr_fi_cloud_mkp_app-tbxui_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-tbxui_prod:fi_cloud_sr_fi_cloud_mkp_app-tbxui_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-und_etaps:fi_cloud_sr_fi_cloud_mkp_app-und_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-und_prod:fi_cloud_sr_fi_cloud_mkp_app-und_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-zbm_prod:fi_cloud_sr_fi_cloud_mkp_app-zgm_etaps:fi_cloud_sr_fi_cloud_mkp_app-zgm_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-zgm_prod:fi_cloud_sr_fi_cloud_mkp_app-zgm_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_app-zoo_etaps:fi_cloud_sr_fi_cloud_mkp_app-zoo_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_app-zoo_prod:fi_cloud_sr_fi_cloud_mkp_app-zoo_prod-unmasked:fi_cloud_sr_fi_cloud_mkp_bapicontainer_etaps:fi_cloud_sr_fi_cloud_mkp_fgcenter_etaps:fi_cloud_sr_fi_cloud_mkp_idhfrontendcontainer_etaps:fi_cloud_sr_fi_cloud_mkp_innovate_etaps-unmasked:fi_cloud_sr_fi_cloud_mkp_mkpbapi_etaps:fi_cloud_sr_fi_cloud_mkp_mkpcont_etaps:fi_cloud_sr_fi_cloud_mkp_mkpstationaer_etaps:fi_cloud_sr_fi_cloud_mkp_onb_etaps:fi_cloud_sr_fi_cloud_mkp_pfm_etaps:fi_cloud_sr_fi_cloud_mkp_pkp_etaps:fi_cloud_sr_fi_cloud_mkp_plattform_etaps:fi_cloud_sr_fi_cloud_mkp_plattform_etaps_unmasked:fi_cloud_sr_fi_cloud_mkp_plattform_prod:fi_cloud_sr_fi_cloud_mkp_plattform_prod_unmasked:fi_cloud_sr_fi_cloud_mkp_sbcontainer_etaps:fi_cloud_sr_fi_cloud_openshift-audit_etaps:fi_cloud_sr_fi_cloud_openshift-audit_siu:fi_cloud_sr_fi_cloud_openshift-metrics_etaps:fi_cloud_sr_fi_cloud_openshift-metrics_siu:fi_cloud_sr_fi_cloud_openshift-objects_etaps:fi_cloud_sr_fi_cloud_openshift-objects_siu:fi_cloud_sr_fi_cloud_openshift_etaps:fi_cloud_sr_fi_cloud_openshift_siu:fi_siem_app_fi_ui_openshift_ro:fi_siem_app_if:fi_siem_app_mkg:fi_siem_cap_cloud:fi_siem_cap_if:fi_siem_cap_mkg:fi_siem_sr_if_eial_dm:fi_siem_sr_if_eial_km:fi_siem_sr_if_kits_dm:fi_siem_sr_if_kits_km:fi_siem_sr_if_prod_dm:fi_siem_sr_if_prod_km:fi_siem_sr_if_slpp_dm:fi_siem_sr_if_slpp_km:fi_siem_sr_if_stat_eial_km:fi_siem_sr_if_stat_prod_km:fi_siem_sr_if_stat_slpp_km:fi_siem_sr_mkg_eial_dm:fi_siem_sr_mkg_kits_dm:fi_siem_sr_mkg_prod_dm:fi_siem_sr_mkg_slpp_dm<br/>Regex JIT enabled<br/>RE2 regex engine enabled<br/>using CLOCK_MONOTONIC<br/>Preforked process=0/17523: process_runtime_msec=208750, search=0/26886, search_runtime_msec=244, new_user=N, export_search=N, args_size=7299, completed_searches=7, user_changes=2, cache_rotations=7<br/>Thread: "searchOrchestrator", did_join=1, ready_to_run=Y, main_thread=N, token=140557544318720<br/>MutexByte: MutexByte-waiting={none}<br/>====================Search Result information====================<br/>SearchID:1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB<br/>DispatchDir:/usd/as16544a/soft/splunk/var/run/splunk/dispatch/1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB<br/>Internal Error:<br/>Number of phases:0<br/>=================================================================<br/>-------------------- FILE CONTENTS OF "/usd/as16544a/soft/splunk/var/run/splunk/dispatch/1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB/info.csv"<br/>"_sid","_timestamp",now,"_startTime","_endTime","_search_StartTime","_provenance","_rt_earliest","_rt_latest","_rtspan","_scan_count","_drop_count","_maxevents","_countMap","_search_StartUp_Spent","_columnOrder","_keySet","_indexes","_remoteServers","_max_remote_servers","_group_list","is_remote_sorted","rt_backfill","read_raw","sample_ratio","sample_seed","sample_exact","enable_event_stream","remote_log_download_mode","_default_group","_timeline_events_preview","_rtoptions","field_rendering","_query_finished","_request_finalization","_fully_completed_search","_auth_token","_splunkd_port","_splunkd_protocol","_splunkd_uri","prd_preview_mode","prd_preview_reducer_enabled","internal_only","summary_mode","summary_maxtimespan","summary_stopped","is_batch_mode","_retry_count","kv_store_settings","kv_store_additional_settings","_dsi_id","_base_lispy","_root_sid","_shp_id","_search","_remote_search","_reduce_search","_search_type","_datamodel_map","_optional_fields_json","_tstats_reduce","_tstats_search_filter","summary_id","required_tags_to_summarize","generation_id",site,"bucket_map_id",label,"savedsearch_label","is_saved_search","is_flex_search","is_shc_mode","is_cluster_slave","is_adhoc_proxied","search_can_be_event_type",realtime,"indexed_realtime","indexed_realtime_offset","replay_speed","_rt_batch_retry","_read_buckets_since_startup","_ppc.app","_ppc.user","_ppc.bs","_bundle_version","_tz","_is_scheduled","_is_summary_index","_is_remote","_orig_search_head","_search_metrics","workload_pool","workload_info","_indexContainWildcard","workload_search_time_range","contains_delete_command","check_dangerous_command","_bs_thread_count","_bs_pipeline_identifier","_is_export","_exported_results","_is_keepalive","_rate_limit_retry_enabled","_results_format","_compression_algorithm","_force_compat_results","_execution_plan","phase_id","_report_tsidx_search_inspections","_is_federation_enabled","_fsh_streaming_phase_only","_fsh_sid","_is_fsh_remote_search","_fsh_server_name","_fsh_providers","_fsh_roles","_fsh_user","_remote_provider_name","_fsh_search_info","_use_fsh_ko","fsh_version","_fsh_guid"?"1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB","1714987626.890354000","1714987626.000000000","1714899600.000000000","1714987626.000000000","1714987626.879618000","UI:Search","","","",0,0,0,"duration.startup.configuration;119;duration.startup.handoff;75;invocations.startup.configuration;1;invocations.startup.handoff;1;",0,"","","","",0,"",0,0,1,1,0,0,1,disabledSavedSearches,"*",0,"","",1,0,1,"RjCLM_Ka0AIeD3A9^ZsnayQPlUnjixEUg^eKSY90ibD105amTkAsi8fnnB6awdxvTwCxOWPxQF_0RfB_MmQbf^7zjyd^01cos9hlDTtXGt1W2dfxa19hpDb8WA2EJFL5pgx52Jz_^VVUQExHpIxyWIkmThLACBpglwmo1hV4FuKbwo",8089,https,"https://17.242.27.240:8089",0,0,0,all,"",0,0,0,"external_kvstore;0;hosts;d100spul4607v111.d100.intern:8191\;q100spul4037v111.q100.intern:8191\;q100spul4537v111.q100.intern:8191\;d100spul4107v111.d100.intern:8191\;;local;17.242.27.240:8191;read_preference;9B484ECA-FEC0-49FF-82DE-436655BDDFCB;replica_set_name;splunkrs;sh_id;8AE98483-80B5-425E-8D47-AAAB98F3F35D;status;ready;","hosts_guids;28AA9C0F-66D8-4F58-8CE7-1101CC006CA2\;3690C9B3-DA4A-400A-A1B5-87BEBA648061\;9B484ECA-FEC0-49FF-82DE-436655BDDFCB\;F3CFEF34-E2A4-4AF5-9984-307A694C9646\;;",0,"","","8AE98483-80B5-425E-8D47-AAAB98F3F35D","| timechart count","","","","","{}","","","","",0,default,0,"","",0,0,1,0,0,0,0,0,0,0,0,0,"fi_UI_openshift",j902080,"$SPLUNK_ETC",14908619050624449031,"### SERIALIZED TIMEZONE FORMAT 1.0;Y3208 NW 4C 4D 54;Y7200 YW 43 45 53 54;Y3600 NW 43 45 54;Y7200 YS 43 45 53 54;Y3600 NS 43 45 54;Y10800 YW 43 45 4D 54;Y10800 YS 43 45 4D 54;Y7200 YG 43 45 53 54;Y3600 NG 43 45 54;Y7200 YW 43 45 53 54;Y3600 NW 43 45 54;@-639010800 4;@323830800 7;@338950800 8;@354675600 7;@370400400 8;@386125200 7;@401850000 8;@417574800 7;@433299600 8;@449024400 7;@465354000 8;@481078800 7;@496803600 8;@512528400 7;@528253200 8;@543978000 7;@559702800 8;@575427600 7;@591152400 8;@606877200 7;@622602000 8;@638326800 7;@654656400 8;@670381200 7;@686106000 8;@701830800 7;@717555600 8;@733280400 7;@749005200 8;@764730000 7;@780454800 8;@796179600 7;@811904400 8;@828234000 7;@846378000 8;@859683600 7;@877827600 8;@891133200 7;@909277200 8;@922582800 7;@941331600 8;@954032400 7;@972781200 8;@985482000 7;@1004230800 8;@1017536400 7;@1035680400 8;@1048986000 7;@1067130000 8;@1080435600 7;@1099184400 8;@1111885200 7;@1130634000 8;@1143334800 7;@1162083600 8;@1174784400 7;@1193533200 8;@1206838800 7;@1224982800 8;@1238288400 7;@1256432400 8;@1269738000 7;@1288486800 8;@1301187600 7;@1319936400 8;@1332637200 7;@1351386000 8;@1364691600 7;@1382835600 8;@1396141200 7;@1414285200 8;@1427590800 7;@1445734800 8;@1459040400 7;@1477789200 8;@1490490000 7;@1509238800 8;@1521939600 7;@1540688400 8;@1553994000 7;@1572138000 8;@1585443600 7;@1603587600 8;@1616893200 7;@1635642000 8;@1648342800 7;@1667091600 8;@1679792400 7;@1698541200 8;@1711846800 7;@1729990800 8;@1743296400 7;@1761440400 8;@1774746000 7;@1792890000 8;@1806195600 7;@1824944400 8;@1837645200 7;@1856394000 8;@1869094800 7;@1887843600 8;@1901149200 7;@1919293200 8;@1932598800 7;@1950742800 8;@1964048400 7;@1982797200 8;@1995498000 7;@2014246800 8;@2026947600 7;@2045696400 8;@2058397200 7;@2077146000 8;@2090451600 7;@2108595600 8;@2121901200 7;@2140045200 8;@2153350800 9;@2172099600 10;@2184800400 9;@2203549200 10;@2216250000 9;@2234998800 10;@2248304400 9;@2266448400 10;@2279754000 9;@2297898000 10;@2311203600 9;@2329347600 10;@2342653200 9;@2361402000 10;@2374102800 9;@2392851600 10;@2405552400 9;@2424301200 10;@2437606800 9;@2455750800 10;@2469056400 9;@2487200400 10;@2500506000 9;@2519254800 10;@2531955600 9;@2550704400 10;@2563405200 9;@2582154000 10;@2595459600 9;@2613603600 10;@2626909200 9;@2645053200 10;@2658358800 9;F;$",0,0,0,"","{""ConsideredBuckets"":0,""EliminatedBuckets"":0,""ConsideredEvents"":0,""TotalSlicesInBuckets"":0,""DecompressedSlices"":0,""FieldMetadata_Events"":"""",""Partition"":{}}","","",0,"",0,0,1,0,0,0,0,0,srs,none,0,classic,4294967295,0,0,0,"",0,"","","","","","",0,"",""?<br/>--------------------<br/>====================Ending info.csv==============================<br/>=================================================================<br/>-------------------- LAST FEW LINE OF FILE "/usd/as16544a/soft/splunk/var/run/splunk/dispatch/1714987626.51542_9B484ECA-FEC0-49FF-82DE-436655BDDFCB/search.log"<br/> Last few lines of the file):<br/>    05-06-2024 11:27:06.891 INFO  dispatchRunner [16318 MainThread] - Search process mode: preforked (reused process) (build 050c9bca8588).<br/>    05-06-2024 11:27:06.910 INFO  dispatchRunner [16318 MainThread] - registering build time modules, count=1<br/>    05-06-2024 11:27:06.910 INFO  dispatchRunner [16318 MainThread] - registering search time components of build time module name=vix<br/>    05-06-2024 11:27:06.911 INFO  BundlesSetup [16318 MainThread] - Setup stats for /usd/as16544a/soft/splunk/etc: wallclock_elapsed_msec=119, cpu_time_used=0.118814, shared_services_generation=2, shared_services_population=1<br/>    05-06-2024 11:27:06.960 INFO  AuthenticationProviderLDAP [16318 MainThread] - strategy="V998DPV1-SH-IF" has no valid value for 'ldap_negative_cache_timeout'. Defaulting to 86400<br/>    05-06-2024 11:27:06.962 INFO  UserManagerPro [16318 MainThread] - Load authentication: forcing roles="fi_cap_waf-koord, fi_cloud_sr_fi_cloud_mkp_app-aaa_etaps, fi_cloud_sr_fi_cloud_mkp_app-aaa_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aaa_prod, fi_cloud_sr_fi_cloud_mkp_app-aaa_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-adrui_etaps, fi_cloud_sr_fi_cloud_mkp_app-adrui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-adrui_prod, fi_cloud_sr_fi_cloud_mkp_app-adrui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-apv_etaps, fi_cloud_sr_fi_cloud_mkp_app-apv_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-apv_prod, fi_cloud_sr_fi_cloud_mkp_app-apv_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aspm_etaps, fi_cloud_sr_fi_cloud_mkp_app-aspm_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aspm_prod, fi_cloud_sr_fi_cloud_mkp_app-aspm_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ats_etaps, fi_cloud_sr_fi_cloud_mkp_app-ats_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ats_prod, fi_cloud_sr_fi_cloud_mkp_app-ats_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aue_etaps, fi_cloud_sr_fi_cloud_mkp_app-aue_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-aue_prod, fi_cloud_sr_fi_cloud_mkp_app-aue_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-avgui_etaps, fi_cloud_sr_fi_cloud_mkp_app-avgui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-avgui_prod, fi_cloud_sr_fi_cloud_mkp_app-avgui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cas_etaps, fi_cloud_sr_fi_cloud_mkp_app-casa_etaps, fi_cloud_sr_fi_cloud_mkp_app-casa_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-casa_prod, fi_cloud_sr_fi_cloud_mkp_app-casa_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cbs_etaps, fi_cloud_sr_fi_cloud_mkp_app-cbs_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cbs_prod, fi_cloud_sr_fi_cloud_mkp_app-cbs_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cfgkui_etaps, fi_cloud_sr_fi_cloud_mkp_app-cfgkui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cfgkui_prod, fi_cloud_sr_fi_cloud_mkp_app-cfgkui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cmd_etaps, fi_cloud_sr_fi_cloud_mkp_app-cmd_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-cmd_prod, fi_cloud_sr_fi_cloud_mkp_app-cmd_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-dkkw_prod, fi_cloud_sr_fi_cloud_mkp_app-dkkw_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-epi_etaps, fi_cloud_sr_fi_cloud_mkp_app-epi_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fiss_etaps, fi_cloud_sr_fi_cloud_mkp_app-fiss_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fiss_prod, fi_cloud_sr_fi_cloud_mkp_app-fiss_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_etaps, fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsbs_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_etaps, fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsdt_prod, fi_cloud_sr_fi_cloud_mkp_app-fkpsep_etaps, fi_cloud_sr_fi_cloud_mkp_app-fkpsep_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-fkpsep_prod, fi_cloud_sr_fi_cloud_mkp_app-fkpsep_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-gbsui_etaps, fi_cloud_sr_fi_cloud_mkp_app-gbsui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-gbsui_prod, fi_cloud_sr_fi_cloud_mkp_app-gbsui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ikdui_etaps, fi_cloud_sr_fi_cloud_mkp_app-ikdui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ikdui_prod, fi_cloud_sr_fi_cloud_mkp_app-ikdui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-imoui_etaps, fi_cloud_sr_fi_cloud_mkp_app-imoui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-imoui_prod, fi_cloud_sr_fi_cloud_mkp_app-imoui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ipk_etaps, fi_cloud_sr_fi_cloud_mkp_app-ipk_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-kwitt_etaps, fi_cloud_sr_fi_cloud_mkp_app-kwitt_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-lap_etaps, fi_cloud_sr_fi_cloud_mkp_app-lap_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-lap_prod, fi_cloud_sr_fi_cloud_mkp_app-lqrui_etaps, fi_cloud_sr_fi_cloud_mkp_app-lqrui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-lst_etaps, fi_cloud_sr_fi_cloud_mkp_app-lst_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-lst_prod, fi_cloud_sr_fi_cloud_mkp_app-lst_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-mkg_etaps, fi_cloud_sr_fi_cloud_mkp_app-mkg_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-mkg_prod, fi_cloud_sr_fi_cloud_mkp_app-mkg_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-mprui_etaps, fi_cloud_sr_fi_cloud_mkp_app-mprui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-mprui_prod, fi_cloud_sr_fi_cloud_mkp_app-mprui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-npl_etaps, fi_cloud_sr_fi_cloud_mkp_app-npl_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-npl_prod, fi_cloud_sr_fi_cloud_mkp_app-npl_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-onb-hub_etaps, fi_cloud_sr_fi_cloud_mkp_app-onb-hub_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-onb-hub_prod, fi_cloud_sr_fi_cloud_mkp_app-onb-hub_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-onbhub_etaps, fi_cloud_sr_fi_cloud_mkp_app-onbhub_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-onbhub_prod, fi_cloud_sr_fi_cloud_mkp_app-ptbo_etaps, fi_cloud_sr_fi_cloud_mkp_app-ptbo_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-ptbo_prod, fi_cloud_sr_fi_cloud_mkp_app-ptbo_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-tbxui_etaps, fi_cloud_sr_fi_cloud_mkp_app-tbxui_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-tbxui_prod, fi_cloud_sr_fi_cloud_mkp_app-tbxui_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-und_etaps, fi_cloud_sr_fi_cloud_mkp_app-und_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-und_prod, fi_cloud_sr_fi_cloud_mkp_app-und_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-zbm_prod, fi_cloud_sr_fi_cloud_mkp_app-zgm_etaps, fi_cloud_sr_fi_cloud_mkp_app-zgm_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-zgm_prod, fi_cloud_sr_fi_cloud_mkp_app-zgm_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_app-zoo_etaps, fi_cloud_sr_fi_cloud_mkp_app-zoo_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_app-zoo_prod, fi_cloud_sr_fi_cloud_mkp_app-zoo_prod-unmasked, fi_cloud_sr_fi_cloud_mkp_bapicontainer_etaps, fi_cloud_sr_fi_cloud_mkp_fgcenter_etaps, fi_cloud_sr_fi_cloud_mkp_idhfrontendcontainer_etaps, fi_cloud_sr_fi_cloud_mkp_innovate_etaps-unmasked, fi_cloud_sr_fi_cloud_mkp_mkpbapi_etaps, fi_cloud_sr_fi_cloud_mkp_mkpcont_etaps, fi_cloud_sr_fi_cloud_mkp_mkpstationaer_etaps, fi_cloud_sr_fi_cloud_mkp_onb_etaps, fi_cloud_sr_fi_cloud_mkp_pfm_etaps, fi_cloud_sr_fi_cloud_mkp_pkp_etaps, fi_cloud_sr_fi_cloud_mkp_plattform_etaps, fi_cloud_sr_fi_cloud_mkp_plattform_etaps_unmasked, fi_cloud_sr_fi_cloud_mkp_plattform_prod, fi_cloud_sr_fi_cloud_mkp_plattform_prod_unmasked, fi_cloud_sr_fi_cloud_mkp_sbcontainer_etaps, fi_cloud_sr_fi_cloud_openshift-audit_etaps, fi_cloud_sr_fi_cloud_openshift-audit_siu, fi_cloud_sr_fi_cloud_openshift-metrics_etaps, fi_cloud_sr_fi_cloud_openshift-metrics_siu, fi_cloud_sr_fi_cloud_openshift-objects_etaps, fi_cloud_sr_fi_cloud_openshift-objects_siu, fi_cloud_sr_fi_cloud_openshift_etaps, fi_cloud_sr_fi_cloud_openshift_siu, fi_siem_app_fi_ui_openshift_ro, fi_siem_app_if, fi_siem_app_mkg, fi_siem_cap_cloud, fi_siem_cap_if, fi_siem_cap_mkg, fi_siem_sr_if_eial_dm, fi_siem_sr_if_eial_km, fi_siem_sr_if_kits_dm, fi_siem_sr_if_kits_km, fi_siem_sr_if_prod_dm, fi_siem_sr_if_prod_km, fi_siem_sr_if_slpp_dm, fi_siem_sr_if_slpp_km, fi_siem_sr_if_stat_eial_km, fi_siem_sr_if_stat_prod_km, fi_siem_sr_if_stat_slpp_km, fi_siem_sr_mkg_eial_dm, fi_siem_sr_mkg_kits_dm, fi_siem_sr_mkg_prod_dm, fi_siem_sr_mkg_slpp_dm"<br/>    05-06-2024 11:27:06.962 INFO  UserManager [88054 RunDispatch] - Setting user context: splunk-system-user<br/>    05-06-2024 11:27:06.962 INFO  UserManager [88054 RunDispatch] - Done setting user context: NULL -> splunk-system-user<br/>    05-06-2024 11:27:06.964 INFO  UserManager [88054 RunDispatch] - Unwound user context: splunk-system-user -> NULL<br/>    05-06-2024 11:27:06.964 INFO  UserManager [88054 RunDispatch] - Setting user context: j902080<br/>    05-06-2024 11:27:06.964 INFO  UserManager [88054 RunDispatch] - Done setting user context: NULL -> j902080<br/>    05-06-2024 11:27:06.965 INFO  dispatchRunner [88054 RunDispatch] - search context: user="j902080", app="fi_UI_openshift", bs-pathname="/usd/as16544a/soft/splunk/etc"<br/>    05-06-2024 11:27:06.966 INFO  SearchParser [88054 RunDispatch] - PARSING: | timechart count<br/>    05-06-2024 11:27:06.966 INFO  dispatchRunner [88054 RunDispatch] - Search running in non-clustered mode<br/>    05-06-2024 11:27:06.966 INFO  dispatchRunner [88054 RunDispatch] - SearchHeadInitSearchMs=3<br/>    05-06-2024 11:27:06.966 INFO  dispatchRunner [88054 RunDispatch] - Executing the Search orchestrator and iterator model (dfs=false).<br/>    05-06-2024 11:27:06.966 INFO  SearchOrchestrator [88054 RunDispatch] - SearchOrchestrator getting constructed<br/>    05-06-2024 11:27:06.966 INFO  SearchOrchestrator [88054 RunDispatch] -  Initialized the SRI<br/>    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Initializing feature flags from config. feature_seed=2630698466<br/>    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=parallelreduce:enablePreview:true<br/>    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=search:search_retry:false<br/>    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=search:search_retry_realtime:false<br/>    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=parallelreduce:autoAppliedPercentage:false<br/>    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=subsearch:enableConcurrentPipelineProcessing:false<br/>    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=stats:allow_stats_v2:true<br/>    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=search_optimization::set_required_fields:stats:false<br/>    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=searchresults:srs2:false<br/>    05-06-2024 11:27:06.967 INFO  SearchFeatureFlags [88054 RunDispatch] - Setting feature_flag=search:read_final_results_from_timeliner:true<br/>    05-06-2024 11:27:06.967 INFO  SearchOrchestrator [88054 RunDispatch] - Search feature_flags={"v":1,"enabledFeatures":["parallelreduce:enablePreview","stats:allow_stats_v2","search:read_final_results_from_timeliner"],"disabledFeatures":["search:search_retry","search:search_retry_realtime","parallelreduce:autoAppliedPercentage","subsearch:enableConcurrentPipelineProcessing","search_optimization::set_required_fields:stats","searchresults:srs2"]}<br/>    05-06-2024 11:27:06.967 INFO  ISplunkDispatch [88054 RunDispatch] - Not running in splunkd. Bundle replication not triggered.<br/>    05-06-2024 11:27:06.967 INFO  SearchOrchestrator [88057 searchOrchestrator] - Initialzing the run time settings for the orchestrator.<br/>    05-06-2024 11:27:06.967 INFO  UserManager [88057 searchOrchestrator] - Setting user context: j902080<br/>    05-06-2024 11:27:06.967 INFO  UserManager [88057 searchOrchestrator] - Done setting user context: NULL -> j902080<br/>    05-06-2024 11:27:06.967 INFO  AdaptiveSearchEngineSelector [88057 searchOrchestrator] - Search execution_plan=classic<br/>    05-06-2024 11:27:06.967 INFO  SearchOrchestrator [88057 searchOrchestrator] - Creating the search DAG.<br/>    05-06-2024 11:27:06.967 INFO  SearchParser [88057 searchOrchestrator] - PARSING: | timechart count<br/>    05-06-2024 11:27:06.967 INFO  DispatchStorageManagerInfo [88057 searchOrchestrator] - Successfully created new dispatch directory for search job. sid=ea3a13baffe5525d_tmp dispatch_dir=/usd/as16544a/soft/splunk/var/run/splunk/dispatch/ea3a13baffe5525d_tmp<br/>    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100<br/>    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200<br/>    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100<br/>    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200<br/>    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (timechart) args: count, AS, count, by, _time<br/>    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free<br/>    05-06-2024 11:27:06.969 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count<br/>    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false<br/>    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (timechart) args: count, AS, count, by, _time<br/>    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free<br/>    05-06-2024 11:27:06.969 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count<br/>    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false<br/>    05-06-2024 11:27:06.969 INFO  SearchParser [88057 searchOrchestrator] - PARSING: bin _time span=rtspan | prestats count by _time<br/>    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100<br/>    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200<br/>    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (stats) args: count, by, _time<br/>    05-06-2024 11:27:06.969 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free<br/>    05-06-2024 11:27:06.969 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count<br/>    05-06-2024 11:27:06.969 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false<br/>    05-06-2024 11:27:06.969 INFO  DispatchThread [88057 searchOrchestrator] - BatchMode: allowBatchMode: 1, conf(1): 1, timeline/Status buckets(0):0, realtime(0):0, report pipe empty(0):0, reqTimeOrder(0):0, summarize(0):0, statefulStreaming(0):0<br/>    05-06-2024 11:27:06.969 INFO  DispatchThread [88057 searchOrchestrator] - required fields list to add to remote search = _time,prestats_reserved_*,psrsvd_*<br/>    05-06-2024 11:27:06.969 INFO  DispatchCommandProcessor [88057 searchOrchestrator] - summaryHash=2cdebc05dd9685a6 summaryId=8AE98483-80B5-425E-8D47-AAAB98F3F35D_fi_UI_openshift_j902080_2cdebc05dd9685a6 remoteSearch=prebin  _time span=rtspan  | prestats  count by _time<br/>    05-06-2024 11:27:06.969 INFO  DispatchCommandProcessor [88057 searchOrchestrator] - summaryHash=NS7f796cf728faa99f summaryId=8AE98483-80B5-425E-8D47-AAAB98F3F35D_fi_UI_openshift_j902080_NS7f796cf728faa99f remoteSearch=prebin _time span=rtspan | prestats count by _time<br/>    05-06-2024 11:27:06.969 INFO  DispatchThread [88057 searchOrchestrator] - Getting summary ID for summaryHash=NS7f796cf728faa99f<br/>    05-06-2024 11:27:07.016 INFO  DispatchThread [88057 searchOrchestrator] - Did not find a usable summary_id, setting info._summary_mode=none, not modifying input summary_id=8AE98483-80B5-425E-8D47-AAAB98F3F35D_fi_UI_openshift_j902080_NS7f796cf728faa99f<br/>    05-06-2024 11:27:07.016 INFO  SearchParser [88057 searchOrchestrator] - PARSING: | timechart count<br/>    05-06-2024 11:27:07.016 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100<br/>    05-06-2024 11:27:07.016 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200<br/>    05-06-2024 11:27:07.016 INFO  StatsContext [88057 searchOrchestrator] -  Setting chunk size min=65536 max=1048576 double_every=100<br/>    05-06-2024 11:27:07.016 INFO  StatsContext [88057 searchOrchestrator] -  Setting max memory usage to 209715200<br/>    05-06-2024 11:27:07.016 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (timechart) args: count, AS, count, by, _time<br/>    05-06-2024 11:27:07.016 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free<br/>    05-06-2024 11:27:07.017 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count<br/>    05-06-2024 11:27:07.017 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false<br/>    05-06-2024 11:27:07.017 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsV2 (timechart) args: count, AS, count, by, _time<br/>    05-06-2024 11:27:07.017 INFO  StatsProcessorV2 [88057 searchOrchestrator] - StatsProcessorV2::processArguments: Unaligned accesses are free<br/>    05-06-2024 11:27:07.017 INFO  StatsAggregations [88057 searchOrchestrator] - Instantiating Stats function group_count for key=, alias=count<br/>    05-06-2024 11:27:07.017 INFO  StatsContext [88057 searchOrchestrator] - shouldFallbackToOldStats: _use_v2_level=USE_V2_ALL, fallback=false<br/>    05-06-2024 11:27:07.017 INFO  AstOptimizer [88057 searchOrchestrator] - SrchOptMetrics optimize_toJson=0.000280740<br/>    05-06-2024 11:27:07.017 INFO  SearchParser [88057 searchOrchestrator] - PARSING: | timechart count<br/>    05-06-2024 11:27:07.017 INFO  FederatedProviderVisitor [88057 searchOrchestrator] - Federated Whole Search Remote Execution Enabled: false<br/>    05-06-2024 11:27:07.017 INFO  AstOptimizer [88057 searchOrchestrator] - Data federation feature enabled: 0<br/>    05-06-2024 11:27:07.017 INFO  ProjElim [88057 searchOrchestrator] - Black listed processors=[addinfo]<br/>    05-06-2024 11:27:07.033 ERROR AstCommandNode [88057 searchOrchestrator] - AstOptimizerException. Cannot unlink the only command_node=timechart<br/>    05-06-2024 11:27:07.033 INFO  ScopedTimer [88057 searchOrchestrator] - search.optimize 0.016420383<br/>--------------------<br/>====================Ending search.log============================<br/>x86 CPUID registers:<br/>         0: 00000016 756E6547 6C65746E 49656E69<br/>         1: 00050654 07200800 7FFEFBFF BFEBFBFF<br/>         2: 76036301 00F0B5FF 00000000 00C30000<br/>         3: 00000000 00000000 00000000 00000000<br/>         4: 00000000 00000000 00000000 00000000<br/>         5: 00000040 00000040 00000003 00002020<br/>         6: 00000077 00000002 00000009 00000000<br/>         7: 00000000 00000000 00000000 00000000<br/>         8: 00000000 00000000 00000000 00000000<br/>         9: 00000000 00000000 00000000 00000000<br/>         A: 07300404 00000000 00000000 00000603<br/>         B: 00000000 00000000 0000004D 00000007<br/>         C: 00000000 00000000 00000000 00000000<br/>         D: 00000000 00000000 00000000 00000000<br/>         E: 00000000 00000000 00000000 00000000<br/>         F: 00000000 00000000 00000000 00000000<br/>        10: 00000000 00000000 00000000 00000000<br/>        11: 00000000 00000000 00000000 00000000<br/>        12: 00000000 00000000 00000000 00000000<br/>        13: 00000000 00000000 00000000 00000000<br/>        14: 00000000 00000000 00000000 00000000<br/>        15: 00000002 000000A8 00000000 00000000<br/>        16: 00000834 00000E74 00000064 00000000<br/>  80000000: 80000008 00000000 00000000 00000000<br/>  80000001: 00000000 00000000 00000121 2C100800<br/>  80000002: 65746E49 2952286C 6F655820 2952286E<br/>  80000003: 6C6F4720 31362064 43203033 40205550<br/>  80000004: 312E3220 7A484730 00000000 00000000<br/>  80000005: 00000000 00000000 00000000 00000000<br/>  80000006: 00000000 00000000 01006040 00000000<br/>  80000007: 00000000 00000000 00000000 00000100<br/>  80000008: 0000302E 00000000 00000000 00000000<br/>terminating...{noformat}<br/>Diagnostic file: [https://downloadsvc.splunk.com/download/splunk/05-06-2024/uploadsvc-60case3476794-05-06-2024-USER-0035a00002XKxqxAAD-diag-Q100SPUL4537-2024-05-06_12-05-45.tar.gz|https://downloadsvc.splunk.com/download/splunk/05-06-2024/uploadsvc-60case3476794-05-06-2024-USER-0035a00002XKxqxAAD-diag-Q100SPUL4537-2024-05-06_12-05-45.tar.gz]<br/>Diagnostic file in SplunkBOT: [https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000wia6baac&job_idtok=63b74811-a6da-4201-8de6-94b987caf0f0&case_numbertok=3476794&hosttok=Q100SPUL4537&case_filetok=Q100SPUL4537-sh_-20240506-111713-iyoHE9rk&found_anontok=0&job_typetok=classic&form.mount.tok=raw&form.mount.filter=&form.network.tok=raw&form.network.filter=&form.interfaces.filter=&form.cpuInfo.tok=raw&form.cpuInfo.filter=&form.processes.tok=raw&form.processes.filter=&form.ulimit.tok=raw&form.ulimit.filter=&form.memory.tok=raw&form.memory.filter=&form.kvstore.filter=&form.searchPeerBundlesDir.filter=&form.sinkholeDir.filter=&form.authDir.filter=|https://splunkbot.splunk.com/en-GB/app/SplunkBOT/overview?indextok=0014000000wia6baac&job_idtok=63b74811-a6da-4201-8de6-94b987caf0f0&case_numbertok=3476794&hosttok=Q100SPUL4537&case_filetok=Q100SPUL4537-sh_-20240506-111713-iyoHE9rk&found_anontok=0&job_typetok=classic&form.mount.tok=raw&form.mount.filter=&form.network.tok=raw&form.network.filter=&form.interfaces.filter=&form.cpuInfo.tok=raw&form.cpuInfo.filter=&form.processes.tok=raw&form.processes.filter=&form.ulimit.tok=raw&form.ulimit.filter=&form.memory.tok=raw&form.memory.filter=&form.kvstore.filter=&form.searchPeerBundlesDir.filter=&form.sinkholeDir.filter=&form.authDir.filter=]<br/>Crash dump: [https://downloadsvc.splunk.com/download/splunk/05-06-2024/uploadsvc-81case3476794-05-06-2024-USER-0035a00002XKxqxAAD-core-searchOrchestra-228293-1714987627.gz|https://downloadsvc.splunk.com/download/splunk/05-06-2024/uploadsvc-81case3476794-05-06-2024-USER-0035a00002XKxqxAAD-core-searchOrchestra-228293-1714987627.gz]<br/>I understand that this might not be the intended use of timechart command, however it should not cause splunk to crash.<br/>The log above is from customer environment on 9.0.6. In my lab environment I was able to reproduce the issue on 9.0.6, 9.1.4 and 9.2.0.1. <br/>The access to 9.2.0.1 environment and the steps to reproduce can be found below:<br/>Splunk Instance: 10.202.11.171 | Linux credentials: splunker:splk | Splunk Credentials: admin:5up3rn0va<br/># Navigate to Apps > Search & Reporting ([http://10.202.11.171:8000/en-GB/app/search/search|http://10.202.11.171:8000/en-GB/app/search/search])<br/># Execute the following search: <br/>{noformat}| timechart count{noformat}<br/># Crash log is generated. To find it, run a new search: <br/>{noformat}index=_internal "Crashing thread: searchOrchestrator"{noformat}
|-
| [https://splunk.atlassian.net/browse/SPL-212495 SPL-212495]
| [PUBLIC] Excessive logging 'WARN SearchResultsFiles Unable to parse site_label, label=invalid due to err="Invalid site id: invalid"' for SearchResultsFiles
| Search, Search Head Clustering
| Unscheduled
| P2-High
| Bug
| WAITING FOR REPORTER
| &mdash;
| &mdash;
| *Problem Statement:* <br/> Version 8.2.2 seems to have the same issue as Jira SPL-196040<br/>09-11-2021 16:42:41.042 +0200 WARN SearchResultsFiles [36391 TcpChannelThread] - Unable to parse site_label, label=invalid due to err="Invalid site id: invalid<br/> <br/>The workaround described in that JIRA, seems that it isn't working:<br/>======<br/>Increase the minimum logging level for SearchResultsFiles:<br/><pre><br/>#log-local.cfg<br/>[splunkd]<br/>category.SearchResultsFiles=ERROR<br/></pre><br/>[https://docs.splunk.com/Documentation/Splunk/latest/Troubleshooting/Enabledebuglogging#log-local.cfg]
|-
| [https://splunk.atlassian.net/browse/SPL-176812 SPL-176812]
| [PUBLIC] Multiple SH Clustering with single deployer can't use datamodel summary sharing
| Search - Datamodel Acceleration
| Unscheduled
| P3-Medium
| Bug
| New
| &mdash;
| StructuredSearch_Reviewed
| In a system with multiple search head clusters but a single deployer, all clusters will be given the same knowledge object configurations. This means you can't designate a cluster as the summary owner/source, because the acceleration.source_guid setting will also get deployed to itself and currently there is no logic to check/ignore the setting if its referencing its own ID. This is not an issue where there is a separate deployer per cluster, or for single search head instances.
|-
| [https://splunk.atlassian.net/browse/SPL-225037 SPL-225037]
| [PUBLIC] [Federated search UI] Remote dataset dropdown menu resets to "Index" after selecting federated provider
| Search Federation, UI - Misc
| Unscheduled
| P3-Medium
| Bug
| New
| &mdash;
| aurum-bug-bash, Aurum-Reviewed, axp_customization_migrated
| Please see attached video for repro <br/>[http://10.224.127.164:6461/en-US/manager/search/federated_search|http://10.224.127.164:6461/en-US/manager/search/federated_search] (admin/Chang3d\!)<br/># Log into search head on aurum build *d1ea205911f0*<br/># Go to Settings → Federated Search <br/>## Create federated provider mapping to RSH <br/>## Create federated index (Federated Indexes → Add federated index)<br/># Under “remote dataset”, select “saved search”. Then choose your provider in the “federated provider” dropdown<br/># “Remote dataset” has been reset to “index”
|-
| [https://splunk.atlassian.net/browse/SPL-238501 SPL-238501]
| [PUBLIC] Federated search "outputlookup" command cannot add data to local lookup table
| Search Federation
| Unscheduled
| P2-High
| Bug
| Untriaged
| &mdash;
| beryllium-defer-accept, Icebreaker-Reviewed
| When a federated search query contains {{outputlookup}} command, this command is always sent to remote peers in both standard and transparent mode searches.  Currently users cannot add the remote search data to a local lookup table by using {{outputlookup}} .<br/>For example: {{index=federated:index_main | outputlookup test_lookup}}<br/>This entire command is sent to RSH, and if RSH does not have {{test_lookup}} defined, it errors out and returns 0 result.
|-
| [https://splunk.atlassian.net/browse/SPL-226877 SPL-226877]
| [PUBLIC] Federated Search UI Error: Cannot create saved search dataset for federated index if dataset name contains space
| Search Federation
| Unscheduled
| Unknown
| Bug
| Untriaged
| &mdash;
| FederatedSearch_UI
| !image-20220715-164741.png|width=750,height=569!<br/>Steps to repro:<br/># Set up standard mode federated search between FSH and RSH on latest current build<br/># On RSH, create a saved search called “ss with space”<br/># On FSH, go to Federated Search → Create new federated index (see attached screenshot)<br/># Creation fails due to spaces in saved search name
|-
| [https://splunk.atlassian.net/browse/SPL-244248 SPL-244248]
| [PUBLIC] Federated Search, Enterprise --> Cloud configuration: Performance degradation increases when the number of indexers increases in the RSH
| Search Federation
| Unscheduled
| P2-High
| Bug
| Untriaged
| &mdash;
| Account_Escalated, cfit_reviewed, cislo-p2-medium, Escalation
| Summary:<br/>For Distributed Searches, each connection to an indexer has a DistributedSearchResultCollectorThread to receive data, handle it, and add to the job queue. On the other hand, Federated Searches have only one connection to RSH and one thread to add to the job queue. This could be a bottle neck for Federated Search:<br/># If DistributedSearchResultCollectorThread is slow to add to the job queue, we have many idle worker threads (default is 5)<br/># All data are sent through a connection. If 1 connection is slow, multiple connections could benefit from parallelism like Distributed Search. (This depends on the network; in our testing environment, the connection speed is excellent and this is never encountered in testing.)<br/>Potential solutions:<br/>* In this workaround [https://splunk.atlassian.net/browse/SPL-239298?focusedCommentId=13187070|https://splunk.atlassian.net/browse/SPL-239298?focusedCommentId=13187070|smart-link] , we reduce the worker threads to reduce thread contention, but this impacts all searches. For a short-term fix, we could add a new setting {{max_workers_searchparser_federated}} so that Federated Search can use a different setting without impacting other searches.<br/>* For an enhancement to address the problem 1 above, we could look into adding more threads to process the data received from RSH and add to the job queue.<br/>* To address the problem 2 above, it’s a bigger change to re-architect FSH to use multiple connections to query RSH.
|-
| [https://splunk.atlassian.net/browse/SPL-213745 SPL-213745]
| [PUBLIC] Standard mode federated search: Unable to set federated index as default index
| Search Federation
| Unscheduled
| P2-High
| Bug
| Untriaged
| &mdash;
| Aurum-Reviewed, df-bugs, df-qa, release_notes_candidate
| We can set federated indexes as default indexes, but this change does not take effect. Please see repro steps below: <br/> # Set up federated search FSH and RSH using splunk-jaws tool<br/> # On FSH, go to Settings --> Roles --> admin --> indexes --> check "default" next to federated:index_df_1_1 and index_df_1<br/> ## If index_df_1 does not appear in the index list, you may need to add it by entering "index_df_1*" in the Wildcards section<br/> # Restart FSH to refresh authorize.conf. (Sometimes you also need to run *./splunk reload auth* for role changes to fully take effect<br/> # On FSH, run command *| tstats count by index* - this should give you a list of all default indexes and their event counts  <br/> # We get results from local index_df_1, but not federated:index_df_1_1
|-
| [https://splunk.atlassian.net/browse/SPL-249666 SPL-249666]
| FS-StandardMode : Standalone sub-search with HEAD doesn't return any results
| Search Federation
| Unscheduled
| P2-High
| Bug
| Untriaged
| &mdash;
| found_by_federated_search_sli
| *CMD  :  [search index=federated:internal | head 5]*<br/>  Above search command doesn't return any results even though the index has valid data .<br/>!image-20240118-182310.png|width=1321,height=483!
|-
| [https://splunk.atlassian.net/browse/SPL-267899 SPL-267899]
| rsh does not return the error string to the fsh
| Search Federation
| Unscheduled
| P3-Medium
| Bug
| Closed
| Fixed
| &mdash;
| # create a federated dataset that points to the data model {{internal_server}} on the {{rsh}}<br/># DO NOT create the lookup {{host.csv}} on the {{rsh}}  ( nor the {{fsh}} ) .run the following qeury<br/> 3. run the following query :<br/>{noformat}|tstats  prestats=true count from datamodel=federated:remote_internal_server by host | lookup host.csv host |stats count by host_description{noformat}<br/>Since the lookup {{host.csv}} does not exist on the {{rsh}} the query fails on the {{rsh}}… however on the {{fsh}} we see the following message : <br/>{noformat}Failed to find the type of remote federated dataset. (error accessing remoteProvider='https://10.241.180.84:8089/servicesNS/user01/search/search/parser?output_mode=json', statusCode='400', description='Bad Request' while trying to get query type for query=' | tstats prestats=true summariesonly=false allow_old_summaries=false count FROM datamodel=internal_server BY host | lookup host.csv host | stats count by host_description' from deployment providerName='remote_deployment_1').<br/>{noformat}<br/>This is happening since the parser endpoint fails ( most likely in the eval process ).<br/>We need to see if we can pass from the {{rsh}} to the {{fsh}} the right error which is actually <br/>{noformat}Error in 'lookup' command: Could not construct lookup 'host.csv, host'. See search.log for more details.<br/>The lookup table 'host.csv' does not exist or is not available.<br/>The search job has failed due to an error. You may be able view the job in the Job Inspector.{noformat}<br/>!Screenshot 2024-12-11 at 11.44.50 PM.png|width=2047,height=671,alt="Screenshot 2024-12-11 at 11.44.50 PM.png"!<br/>!Screenshot 2024-12-11 at 11.45.19 PM.png|width=1208,height=593,alt="Screenshot 2024-12-11 at 11.45.19 PM.png"!
|-
| [https://splunk.atlassian.net/browse/SPL-264529 SPL-264529]
| Search with mcatalog command returns missing metrics when used with append=t and last index is not valid
| Search Federation
| Unscheduled
| P2-High
| Bug
| Untriaged
| &mdash;
| &mdash;
| Search with {{mcatalog append=t}} and the last index is invalid, search results miss some metrics from the first valid index.  Event count is a lot less too.<br/>{noformat}| mcatalog prestats=t values(_dims) WHERE index=_metrics | mcatalog prestats=t append=t values(_dims) WHERE index=invalid_metrics_index | stats values(_dims) {noformat}<br/>The above search returns ~10 results. But there are some inconsistent results seen every time a search is run. <br/>If only run search on index=_metrics, the search returns 61 results consistently.<br/>{noformat}| mcatalog prestats=t values(_dims) WHERE index=_metrics | stats values(_dims) {noformat}<br/>But if I add a valid index name after the invalid name, such as {{index=_invalid_index OR index=_metrics}}, I see full results again, like below<br/>{noformat}| mcatalog prestats=t values(_dims) WHERE index=_metrics | mcatalog prestats=t append=t values(_dims) WHERE index=invalid_index OR index=_metrics | stats values(_dims){noformat}<br/>!Missing_results_from_append_invalid_index.png|width=1775,height=574,alt="Missing_results_from_append_invalid_index.png"!<br/>!Full_results_from_single_index_search.png|width=1759,height=955,alt="Full_results_from_single_index_search.png"!
|-
| [https://splunk.atlassian.net/browse/SPL-152330 SPL-152330]
| [PUBLIC] After installing Splunk on Windows using msiexec and the "GENRANDOMPASSWORD=1" option (and if generated password ends with backslash) admin is unable to login with msg "No users exist. Please set up a new user."
| Security
| Unscheduled
| P3-Medium
| Bug
| New
| &mdash;
| CF_d247, CF_triaging, d69, NL_triaging
| After installing Splunk on Windows using msiexec and the "GENRANDOMPASSWORD=1" option admin is unable to login with msg "No users exist. Please set up a new user."<br/>{noformat}<br/>C:\Windows\system32\msiexec.exe /i splunk-7.1.0-935b28aa68d8-x64-release.msi /l install_splunk-7.1.0-935b28aa68d8-x64-release.msi.log AGREETOLICENSE=Yes /quiet MINPASSWORDLOWERCASELEN=1 MINPASSWORDUPPERCASELEN=1 MINPASSWORDLEN=1 MINPASSWORDSPECIALCHARLEN=1 GENRANDOMPASSWORD=1 MINPASSWORDDIGITLEN=1<br/>{noformat}<br/>If the generated password ends with backslash, the login will fail with error: "No users exist. Please set up a user.", e.g.<br/>{noformat}<br/>[user_info]<br/>USERNAME=admin<br/>PASSWORD=eX3\<br/>{noformat}
|-
| [https://splunk.atlassian.net/browse/SPL-221239 SPL-221239]
| [PUBLIC] System Introspect App fails when universal forwarder is installed at non-admin user
| Universal Forwarder
| Unscheduled
| Unknown
| Bug
| Reopened
| &mdash;
| Aurum-Reviewed, beryllium-defer-accept, dataedge_triaged, GDI_board
| Splunk universal forwarder is installed with msa account and *SET_ADMIN_USER = 0*, enable the "*System Introspect App*" from default apps by setting "*state = enabled*". Monitor splunkd logs and you will the following error <br/>{code:java}<br/>ERROR IntrospectionGenerator:resource_usage [8896 ExecProcessor] -  RU - Splunk was started with insufficient privileges to collect resource usage metrics. Please modify the service properties to run with Administrator privileges. Exiting.<br/>{code}<br/>However if the installation is done with SET_ADMIN_USER = 1, splund log does not have the error and contains this log instead<br/>{code:java}<br/>ModularInputs [6716 MainThread] - Introspection setup completed for scheme "powershell2".<br/>{code}
|-
| [https://splunk.atlassian.net/browse/SPL-278604 SPL-278604]
| Release notes content unavailable in docs for some versions of universal forwarder
| Universal Forwarder
| Unscheduled
| Unknown
| Bug
| Untriaged
| &mdash;
| &mdash;
| With the migration from docs.splunk.com to help.splunk.com we need to manually recreate information about UF known and fixed issues for every maintenance release. This work is planned but will take some time. The identical information is available in the Splunk Enterprise known and fixed issues lists, so customers can access it there in the interim.
|-
| [https://splunk.atlassian.net/browse/SPL-177008 SPL-177008]
| [PUBLIC] [WLM] Workload management fails to enable for addition of a pool with 1% cpu and 1% memory
| Workload Management
| Unscheduled
| P3-Medium
| Bug
| New
| &mdash;
| quake_punt_ok, systest_quake, WLM
| 1. setup a single instance <br/>2. enable systemd<br/>3. create many pools, enable wlm successfully<br/>{noformat}<br/>[workload_pool:default_search_pool]<br/>category = search<br/>cpu_weight = 100<br/>default_category_pool = 1<br/>mem_weight = 100<br/>[workload_pool:default_ingest_pool]<br/>category = ingest<br/>cpu_weight = 100<br/>default_category_pool = 1<br/>mem_weight = 100<br/>[workload_pool:report_acceleration_pool1]<br/>category = search<br/>cpu_weight = 100<br/>default_category_pool = 0<br/>mem_weight = 100<br/>[workload_pool:report_acceleration_pool2]<br/>category = search<br/>cpu_weight = 100<br/>default_category_pool = 0<br/>mem_weight = 100<br/>[workload_pool:datamodel_acceleration_pool1]<br/>category = search<br/>cpu_weight = 100<br/>default_category_pool = 0<br/>mem_weight = 100<br/>[workload_pool:datamodel_acceleration_pool2]<br/>category = search<br/>cpu_weight = 100<br/>default_category_pool = 0<br/>mem_weight = 100<br/>[workload_pool:summary_index_pool1]<br/>category = search<br/>cpu_weight = 100<br/>default_category_pool = 0<br/>mem_weight = 100<br/>[workload_pool:summary_index_pool2]<br/>category = search<br/>cpu_weight = 100<br/>default_category_pool = 0<br/>mem_weight = 100<br/>[workload_pool:search_time_range_pool1]<br/>category = search<br/>cpu_weight = 100<br/>default_category_pool = 0<br/>mem_weight = 100<br/>[workload_pool:search_time_range_pool2]<br/>category = search<br/>cpu_weight = 100<br/>default_category_pool = 0<br/>mem_weight = 100<br/>[workload_pool:user_pool1]<br/>category = search<br/>cpu_weight = 100<br/>default_category_pool = 0<br/>mem_weight = 100<br/>[workload_pool:user_pool2]<br/>category = search<br/>cpu_weight = 100<br/>default_category_pool = 0<br/>mem_weight = 100<br/>[workload_pool:system_user_pool1]<br/>category = search<br/>cpu_weight = 100<br/>default_category_pool = 0<br/>mem_weight = 100<br/>[workload_pool:system_user_pool2]<br/>category = search<br/>cpu_weight = 100<br/>default_category_pool = 0<br/>mem_weight = 100<br/>[general]<br/>enabled = 1<br/>{noformat}<br/>4. add a pool whose cpu and memory is 1%, the wlm failed to enable.<br/>{noformat}<br/>Failed to setup workload pools in workload configuration. Check the status of workload management preflight checks for additional information.<br/>{noformat}<br/>5. delete the pool, the wlm can be enabled successfully..<br/>Env to reproduce: <br/>https://sv3-orca-0313e0b2.sv.splunk.com:8000/en-US/manager/system/workload_management. (admin/changed)
|-
| [https://splunk.atlassian.net/browse/SPL-204740 SPL-204740]
| [PUBLIC] [WLM][scootaloo] - Deletion of a workload pool is allowed if there is a 'disabled' rule that is related to that workload pool and this can cause errors if the rule is re-enabled later
| Workload Management
| Unscheduled
| P3-Medium
| Bug
| New
| &mdash;
| cf_systest, CMP_systest, known_issue_approved, known_issue_reviewed, psr_review_bugs_sys, Scootaloo_Review, systest_scootaloo
| build: 4e657ada6683 (8.2.0)<br/>platform: Linux<br/>WLM rules and pools are created.<br/> <br/>a pool being able to be deleted, when associated rules is disabled. After that when rule is enabled again, there is error message shows up on page. So should we ever allow deleting the pool if rules is associated?<br/>error messages and pool/rule configured are attached<br/>note that When it is enabled, the pool can not be deleted.
|-
| [https://splunk.atlassian.net/browse/SPL-268110 SPL-268110]
| Workload Management monitoring console dashboard shows memory usage exceeds the configured memory limit and the "CPU used (# of cores)" shows "0"
| Workload Management
| Unscheduled
| P2-High
| Bug
| New
| &mdash;
| Reviewer-sylim, support-reviewed
| As discussed in Slack: [https://splunk.slack.com/archives/C9Q6F6X37/p1730203376037169?thread_ts=1729746970.265219&cid=C9Q6F6X37|https://splunk.slack.com/archives/C9Q6F6X37/p1730203376037169?thread_ts=1729746970.265219&cid=C9Q6F6X37|smart-link] <br/>The customer has attempted to use Splunk's workload management by setting a search pool with a 1%, 2% or 5% memory limit:<br/>{{etc/apps/search/local/workload_pools.conf}}<br/>{noformat}[workload_pool:limit_user_pool]<br/>category = search<br/>cpu_weight = 10<br/>default_category_pool = 0<br/>mem_weight = 1<br/>#mem_weight = 2<br/>#mem_weight = 5{noformat}<br/>However, the monitoring console dashboard {{Resource Usage > Workload Management > Workload Management Activity: Instance}} shows that the limit_user_pool memory usage exceeds the limit of 225MB, and all the "CPU used (# of cores)" shows "0"<br/>!image-20241216-225251.png|width=1836,height=665,alt="image-20241216-225251.png"!<br/>Output of {{| rest splunk_server=QCLMSMNSH01 /services/server/status/resource-usage/splunk-processes}}that was run on the Splunk Monitoring Console instance:<br/>[^QCLMSMNSH01_splunk-processes.csv]<br/>+*Customer Environment*+<br/>8 x Indexer 9.1.0.2 (cluster) <- 8 x Search Head 9.1.0.2<br/>Cluster Manager 9.1.0.2<br/>Distributed Monitoring Console 9.1.0.2<br/>+*Diags from customer:*+<br/>qclmsmndp01: monitoring console ([google drive|https://drive.google.com/file/d/1-5tBnopi2SKjFpSwuMNRhCvXt3VRGpVB/view?usp=drive_link] [splunkbot|https://splunkbot.splunk.com/en-US/app/SplunkBOT/overview?indextok=0014000000pgi7zaat&job_idtok=bf987edd-b93d-4815-894e-b691ef141af5&case_numbertok=3633637&hosttok=qclmsmndp02&case_filetok=qclmsmndp02-sh_-20241216-222357-bE4bpkpr&found_anontok=0&job_typetok=classic&form.mount.tok=raw&form.mount.filter=&form.network.tok=raw&form.network.filter=&form.interfaces.filter=&form.cpuInfo.tok=raw&form.cpuInfo.filter=&form.processes.tok=raw&form.processes.filter=&form.ulimit.tok=raw&form.ulimit.filter=&form.memory.tok=raw&form.memory.filter=&form.kvstore.filter=&form.searchPeerBundlesDir.filter=&form.sinkholeDir.filter=&form.authDir.filter=])<br/>qclmsmnsh02: search head captain ([google drive|https://drive.google.com/file/d/1-2G5JzqdwfXdq7vU3pRg97rlRgIgrqbm/view?usp=drive_link] [splunkbot|https://splunkbot.splunk.com/en-US/app/SplunkBOT/overview?indextok=0014000000pgi7zaat&job_idtok=ff7b687e-9958-49b9-bece-51c12a7a265f&case_numbertok=3633637&hosttok=qclmsmnsh02&case_filetok=qclmsmnsh02-sh_-20241216-222438-Jft6weIa&found_anontok=0&job_typetok=classic])<br/>qclmsmnid02: indexer ([google drive|https://drive.google.com/file/d/1-1r1Z3k1k7QKhGcZAVaOdChgu3D4HN4y/view?usp=drive_link] [splunkbot|https://splunkbot.splunk.com/en-US/app/SplunkBOT/overview?indextok=0014000000pgi7zaat&job_idtok=03994c75-9cc4-463f-9b55-d25da93dc796&case_numbertok=3633637&hosttok=qclmsmnid02&case_filetok=qclmsmnid02-idx_-20241216-222338-oCHA7Pgr&found_anontok=0&job_typetok=classic])<br/>+*Business Impact:*+<br/>Splunk workload management is implemented to prevent runaway searches and Out of Memory conditions on the Splunk instances. According to the Splunk docs [https://docs.splunk.com/Documentation/Splunk/9.3.2/Workloads/Keyconcepts|https://docs.splunk.com/Documentation/Splunk/9.3.2/Workloads/Keyconcepts] "the workload pool can only use memory up to the limit for which it has been configured."<br/>However a review of the "Workload Management Activity: Instance" dashboard shows that the "Memory Used" by the Workload Pools are significantly more than the configured "Memory Limit" and CPU usage is always at "0".<br/>Based on this dashboard workload management is not enforcing memory limits correctly and it is unclear if CPU usage is being managed at all. As a result high-priority searches could be unexpectedly terminated due to OOM conditions, or the Splunk process itself could crash, leading to an outage which should have been avoidable.<br/>+*The Customer would like to know:*+<br/># Why the memory usage exceeds the limit? (According to document [https://docs.splunk.com/Documentation/Splunk/9.3.2/Workloads/Keyconcepts|https://docs.splunk.com/Documentation/Splunk/9.3.2/Workloads/Keyconcepts] “the workload pool can only use memory up to the limit for which it has been configured. ”)<br/># Why no data for the CPU usage?
|-
| [https://splunk.atlassian.net/browse/SPL-143947 SPL-143947]
| [PUBLIC] [cloud] Report acceleration is broken for users with a configured role-based access filter
| ~Deprecated - Cloud, Search - Report Acceleration
| Unscheduled
| P2-High
| Bug
| New
| &mdash;
| coresearch_reviewed_FY22Q1, found_on_cloud, searchpipeline_cleanup_apr2022, StructuredSearch_Reviewed, systest_minty
| With a power user, after enable acceleration for reports on search head, the "Summary Status" always shows the progress is 0%, and there are no acceleration related messages (like "Using summaries for search, summary_id=XXX") in search jobs.<br/>I can reproduce this issue on stackmakr/victor stacks, but cannot reproduce this with on-prem environments.<br/>*Reproduce steps:*<br/>1. Apply a stackmakr/victor stack<br/>2. Create a new power user, and add "_internal" index for power user, so we can use internal data to produce this issue<br/>3. Login as power user, create a report like "index=_internal | stats count by host", enable acceleration for report, use "1 day" for summary range<br/>4. Wait and check the "Report Acceleration Summaries" status, and run the search "index=_internal | stats count by host", check if there are acceleration related messages in search  jobs.
|}
